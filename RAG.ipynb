{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfMLWR3/4DDpXdKLdovHH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeyagsen1/RAG/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeQt1fQ_StPl",
        "outputId": "41809bc5-b6a8-4bc0-cc9f-e38d7e2b0b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['228 Chapter 6. Complexity Theory z0, 0 1 0 1 1 2 0 z0, 1 0 1 1 2 0 1 z0, 0 1 1 2 0 1 0 z0, 1 1 2 0 1 0 1 z0, 1 2 0 1 0 1 1 z0, 2 0 1 0 1 z2, 1 2 0 1 0 z2, 1 0 2 0 1 z2, 0 0 0 2 0 1 z1, 1 0 0 2 Figure 6.6 The computation of the Turing machine on input x  11. The pair state,symbol indicates the position of the tape head.      z0 0 z0, 0      z0 1 z0, 1     z2  2 z0, 2     z2  0 z2, 1       z1, 1 z2, 0 3. The states z0 and z2, and the three symbols of the alphabet yield twelve tile types      z0 z0, 0 0     z0  z0, 0 0      z0 z0, 1 1     z0  z0, 1 1      z0 z0, 2 2     z0  z0, 2 2      z2 z2, 0 0     z2  z2, 0 0      z2 z2, 1 1     z2  z2, 1 1      z2 z2, 2 2     z2  z2, 2 2 The computation of the Turing machine on input x  11 consists of nine computation steps. During this computation, the tape head visits exactly six cells. Therefore, the frame for the domino game has nine rows and six columns. This frame is given in Figure 6.7. In Figure 6.8, you nd the solution of the domino game. Observe that this solution is nothing but an equivalent way of writing the computation of Figure 6.6. Hence, the computation of the Turing machine corresponds to a solution of the domino game in fact, the converse also holds.', '6 Chapter 1. Introduction a R is reexive For every element in a A, we have a, a R. b R is symmetric For all a and b in A, if a, b R, then also b, a R. c R is transitive For all a, b, and c in A, if a, b R and b, c R, then also a, c R. 13. A graph G  V, E is a pair consisting of a set V , whose elements are called vertices, and a set E, where each element of E is a pair of distinct vertices. The elements of E are called edges. The gure below shows some wellknown graphs K5 the complete graph on ve vertices, K3,3 the complete bipartite graph on 2  3  6 vertices, and the Peterson graph. K5 K3,3 Peterson graph The degree of a vertex v, denoted by degv, is dened to be the number of edges that are incident on v. A path in a graph is a sequence of vertices that are connected by edges. A path is a cycle, if it starts and ends at the same vertex. A simple path is a path without any repeated vertices. A graph is connected, if there is a path between every pair of vertices. 14. In the context of strings, an alphabet is a nite set, whose elements are called symbols. Examples of alphabets are   0, 1 and   a, b, c, . . . , z. 15. A string over an alphabet  is a nite sequence of symbols, where each symbol is an element of . The length of a string w, denoted by w, is the number of symbols contained in w. The empty string, denoted by', '138 Chapter 4. Turing Machines and the ChurchTuring Thesis state control . . . 2 2 2 a a b a b b a b a b 2 2 2 . . . ? . . . 2 2 2 b a a b 2 a b 2 2 2 . . . ? Figure 4.1 A Turing machine with k  2 tapes. 2. Each tape has a tape head which can move along the tape, one cell per move. It can also read the cell it currently scans and replace the symbol in this cell by another symbol. 3. There is a state control, which can be in any one of a nite number of states. The nite set of states is denoted by Q. The set Q contains three special states a start state, an accept state, and a reject state. The Turing machine performs a sequence of computation steps. In one such step, it does the following 1. Immediately before the computation step, the Turing machine is in a state r of Q, and each of the k tape heads is on a certain cell. 2. Depending on the current state r and the k symbols that are read by the tape heads, a the Turing machine switches to a state r of Q which may be equal to r, b each tape head writes a symbol of  in the cell it is currently scanning this symbol may be equal to the symbol currently stored in the cell, and']\n",
            "228 chapter 6. complexity theory z0, 0 1 0 1 1 2 0 z0, 1 0 1 1 2 0 1 z0, 0 1 1 2 0 1 0 z0, 1 1 2 0 1 0 1 z0, 1 2 0 1 0 1 1 z0, 2 0 1 0 1 z2, 1 2 0 1 0 z2, 1 0 2 0 1 z2, 0 0 0 2 0 1 z1, 1 0 0 2 figure 6. 6 the computation of the turing machine on input x 11. the pair state, symbol indicates the position of the tape head. z0 0 z0, 0 z0 1 z0, 1 z2 2 z0, 2 z2 0 z2, 1 z1, 1 z2, 0 3. the states z0 and z2, and the three symbols of the alphabet yield twelve tile types z0 z0, 0 0 z0 z0, 0 0 z0 z0, 1 1 z0 z0, 1 1 z0 z0, 2 2 z0 z0, 2 2 z2 z2, 0 0 z2 z2, 0 0 z2 z2, 1 1 z2 z2, 1 1 z2 z2, 2 2 z2 z2, 2 2 the computation of the turing machine on input x 11 consists of nine computation steps. during this computation, the tape head visits exactly six cells. therefore, the frame for the domino game has nine rows and six columns. this frame is given in figure 6. 7. in figure 6. 8, you nd the solution of the domino game. observe that this solution is nothing but an equivalent way of writing the computation of figure 6. 6. hence, the computation of the turing machine corresponds to a solution of the domino game in fact, the converse also holds. 6 chapter 1. introduction a r is reexive for every element in a a, we have a, a r. b r is symmetric for all a and b in a, if a, b r, then also b, a r. c r is transitive for all a, b, and c in a, if a, b\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Load a retrieval model\n",
        "retrieval_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Step 2: Dynamically load and clean text data\n",
        "def load_texts_from_directory(directory_path):\n",
        "    texts = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
        "                raw_text = file.read()\n",
        "                cleaned_text = clean_text(raw_text)\n",
        "                texts.append(cleaned_text)\n",
        "    return texts\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans the input text by removing special characters and redundant whitespace.\"\"\"\n",
        "    # Remove excessive whitespace and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove non-alphanumeric characters (keep essential punctuation)\n",
        "    text = re.sub(r'[^a-zA-Z0-9,.!? ]+', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "text_files_dir = \"/content/texts\"  # Replace with your text directory path\n",
        "texts = load_texts_from_directory(text_files_dir)\n",
        "corpus_embeddings = retrieval_model.encode(texts, convert_to_tensor=True)\n",
        "\n",
        "# Step 3: Load the BLIP model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Step 4: RAG pipeline with cleaned context\n",
        "def rag_pipeline(query, image_path):\n",
        "    # Embed the query\n",
        "    query_embedding = retrieval_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Retrieve the top-k relevant passages\n",
        "    top_k = 3\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n",
        "    relevant_texts = [texts[hit['corpus_id']] for hit in hits[0]]\n",
        "    print(relevant_texts)\n",
        "\n",
        "    # Combine and clean relevant texts\n",
        "    context = \" \".join(relevant_texts)\n",
        "    context_inputs = processor.tokenizer(context, truncation=True, max_length=450, return_tensors=\"pt\")\n",
        "    truncated_context = processor.tokenizer.decode(context_inputs['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, text=truncated_context, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    # Generate a response\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example Usage\n",
        "query = \"What is described in the image?\"\n",
        "image_path = \"/content/images/page_91_full_page_shape_3.png\"  # Replace with your image path\n",
        "response = rag_pipeline(query, image_path)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image\n",
        "!pip install pymupdf\n",
        "!pip install pillow\n",
        "!pip install pytesseract\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UNckCb9iKJ2B",
        "outputId": "acb4cbb3-5885-4ca7-bc60-5d013d80dddd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.1\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tqvRaYRaK6D3",
        "outputId": "b19be106-883b-4afe-fb93-9f2013c57735"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import fitz  # PyMuPDF\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 1. PDF'den Metin ve Görüntü Çıkarma\n",
        "def extract_text_and_images(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    images = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        text += page.get_text()\n",
        "\n",
        "        # Extract images\n",
        "        for img_index, img in enumerate(page.get_images(full=True)):\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            images.append((page_num, base_image[\"image\"]))\n",
        "\n",
        "    return text, images\n",
        "\n",
        "# 2. Görsellerin OCR ile Metin Açıklaması\n",
        "def image_to_text(image_data):\n",
        "    from PIL import Image\n",
        "    import io\n",
        "\n",
        "    image = Image.open(io.BytesIO(image_data))\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "# 3. Metin Vektörleştirme\n",
        "def text_to_vector(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n",
        "    return embeddings.numpy()\n",
        "\n",
        "# 4. FAISS ile Vektör Veritabanı Oluşturma\n",
        "def create_faiss_index(vectors):\n",
        "    dimension = vectors.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
        "    index.add(vectors)\n",
        "    return index\n",
        "\n",
        "# 5. RAG İle Arama Fonksiyonu\n",
        "def search_query(query, index, vectors, texts):\n",
        "    query_vector = text_to_vector(query)\n",
        "    D, I = index.search(query_vector, k=5)  # Top 5 results\n",
        "    return [texts[i] for i in I[0]]\n",
        "\n",
        "# PDF Dosyasını İşleme\n",
        "pdf_path = \"/content/TheoryOfComputation.pdf\"\n",
        "text, images = extract_text_and_images(pdf_path)\n",
        "\n",
        "# Görüntü Metinlerini ve Vektörleri Toplama\n",
        "texts = [text] + [image_to_text(img[1]) for img in images]\n",
        "vectors = np.vstack([text_to_vector(t) for t in texts])\n",
        "\n",
        "# FAISS Veritabanı\n",
        "index = create_faiss_index(vectors)\n",
        "\n",
        "# Örnek Sorgu\n",
        "query = \"DFA and NFA comparison\"\n",
        "results = search_query(query, index, vectors, texts)\n",
        "print(\"Search Results:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8SZ7iQ4JmWa",
        "outputId": "9f6a359f-d870-4f7c-c3fe-e57502cfa213"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Results: ['Introduction to Theory of Computation\\nAnil Maheshwari\\nMichiel Smid\\nSchool of Computer Science\\nCarleton University\\nOttawa\\nCanada\\n{anil,michiel}@scs.carleton.ca\\nAugust 29, 2024\\nii\\nContents\\nContents\\nPreface\\nvi\\n1\\nIntroduction\\n1\\n1.1\\nPurpose and motivation\\n. . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1.1\\nComplexity theory\\n. . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.2\\nComputability theory . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.3\\nAutomata theory . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.1.4\\nThis course\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.2\\nMathematical preliminaries\\n. . . . . . . . . . . . . . . . . . .\\n4\\n1.3\\nProof techniques\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.3.1\\nDirect proofs\\n. . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.3.2\\nConstructive proofs . . . . . . . . . . . . . . . . . . . .\\n9\\n1.3.3\\nNonconstructive proofs . . . . . . . . . . . . . . . . . .\\n10\\n1.3.4\\nProofs by contradiction . . . . . . . . . . . . . . . . . .\\n11\\n1.3.5\\nThe pigeon hole principle . . . . . . . . . . . . . . . . .\\n12\\n1.3.6\\nProofs by induction . . . . . . . . . . . . . . . . . . . .\\n13\\n1.3.7\\nMore examples of proofs . . . . . . . . . . . . . . . . .\\n15\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2\\nFinite Automata and Regular Languages\\n21\\n2.1\\nAn example: Controling a toll gate . . . . . . . . . . . . . . .\\n21\\n2.2\\nDeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . . . .\\n23\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton . . . . . . . . . .\\n26\\n2.2.2\\nA second example of a ﬁnite automaton\\n. . . . . . . .\\n28\\n2.2.3\\nA third example of a ﬁnite automaton\\n. . . . . . . . .\\n29\\n2.3\\nRegular operations . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n2.4\\nNondeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . .\\n35\\n2.4.1\\nA ﬁrst example . . . . . . . . . . . . . . . . . . . . . .\\n35\\niv\\nContents\\n2.4.2\\nA second example . . . . . . . . . . . . . . . . . . . . .\\n37\\n2.4.3\\nA third example . . . . . . . . . . . . . . . . . . . . . .\\n38\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\n. . . .\\n39\\n2.5\\nEquivalence of DFAs and NFAs . . . . . . . . . . . . . . . . .\\n41\\n2.5.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n2.6\\nClosure under the regular operations\\n. . . . . . . . . . . . . .\\n48\\n2.7\\nRegular expressions . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n2.8\\nEquivalence of regular expressions and regular languages . . .\\n57\\n2.8.1\\nEvery regular expression describes a regular language .\\n58\\n2.8.2\\nConverting a DFA to a regular expression\\n. . . . . . .\\n61\\n2.9\\nThe pumping lemma and nonregular languages . . . . . . . . .\\n68\\n2.9.1\\nApplications of the pumping lemma . . . . . . . . . . .\\n70\\n2.10 Higman’s Theorem . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.1 Dickson’s Theorem . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.2 Proof of Higman’s Theorem . . . . . . . . . . . . . . .\\n78\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n3\\nContext-Free Languages\\n91\\n3.1\\nContext-free grammars . . . . . . . . . . . . . . . . . . . . . .\\n91\\n3.2\\nExamples of context-free grammars . . . . . . . . . . . . . . .\\n94\\n3.2.1\\nProperly nested parentheses . . . . . . . . . . . . . . .\\n94\\n3.2.2\\nA context-free grammar for a nonregular language . . .\\n95\\n3.2.3\\nA context-free grammar for the complement of a non-\\nregular language\\n. . . . . . . . . . . . . . . . . . . . .\\n97\\n3.2.4\\nA context-free grammar that veriﬁes addition\\n. . . . .\\n98\\n3.3\\nRegular languages are context-free . . . . . . . . . . . . . . . . 100\\n3.3.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 102\\n3.4\\nChomsky normal form\\n. . . . . . . . . . . . . . . . . . . . . . 104\\n3.4.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 109\\n3.5\\nPushdown automata\\n. . . . . . . . . . . . . . . . . . . . . . . 112\\n3.6\\nExamples of pushdown automata\\n. . . . . . . . . . . . . . . . 116\\n3.6.1\\nProperly nested parentheses . . . . . . . . . . . . . . . 116\\n3.6.2\\nStrings of the form 0n1n\\n. . . . . . . . . . . . . . . . . 117\\n3.6.3\\nStrings with b in the middle . . . . . . . . . . . . . . . 118\\n3.7\\nEquivalence of pushdown automata and context-free grammars 120\\n3.8\\nThe pumping lemma for context-free languages\\n. . . . . . . . 124\\n3.8.1\\nProof of the pumping lemma . . . . . . . . . . . . . . . 125\\n3.8.2\\nApplications of the pumping lemma . . . . . . . . . . . 128\\nContents\\nv\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n4\\nTuring Machines and the Church-Turing Thesis\\n137\\n4.1\\nDeﬁnition of a Turing machine . . . . . . . . . . . . . . . . . . 137\\n4.2\\nExamples of Turing machines\\n. . . . . . . . . . . . . . . . . . 141\\n4.2.1\\nAccepting palindromes using one tape\\n. . . . . . . . . 141\\n4.2.2\\nAccepting palindromes using two tapes . . . . . . . . . 142\\n4.2.3\\nAccepting anbncn using one tape . . . . . . . . . . . . . 143\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2} . . . . 145\\n4.2.5\\nAccepting ambncmn using one tape . . . . . . . . . . . . 147\\n4.3\\nMulti-tape Turing machines . . . . . . . . . . . . . . . . . . . 148\\n4.4\\nThe Church-Turing Thesis . . . . . . . . . . . . . . . . . . . . 151\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n5\\nDecidable and Undecidable Languages\\n157\\n5.1\\nDecidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n5.1.1\\nThe language ADFA . . . . . . . . . . . . . . . . . . . . 158\\n5.1.2\\nThe language ANFA . . . . . . . . . . . . . . . . . . . . 159\\n5.1.3\\nThe language ACFG . . . . . . . . . . . . . . . . . . . . 160\\n5.1.4\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 161\\n5.1.5\\nThe Halting Problem . . . . . . . . . . . . . . . . . . . 163\\n5.2\\nCountable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n5.2.1\\nThe Halting Problem revisited . . . . . . . . . . . . . . 168\\n5.3\\nRice’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n5.3.1\\nProof of Rice’s Theorem . . . . . . . . . . . . . . . . . 171\\n5.4\\nEnumerability . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n5.4.1\\nHilbert’s problem . . . . . . . . . . . . . . . . . . . . . 174\\n5.4.2\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 176\\n5.5\\nWhere does the term “enumerable” come from? . . . . . . . . 177\\n5.6\\nMost languages are not enumerable . . . . . . . . . . . . . . . 180\\n5.6.1\\nThe set of enumerable languages is countable\\n. . . . . 180\\n5.6.2\\nThe set of all languages is not countable . . . . . . . . 181\\n5.6.3\\nThere are languages that are not enumerable . . . . . . 183\\n5.7\\nThe relationship between decidable and enumerable languages 184\\n5.8\\nA language A such that both A and A are not enumerable . . 186\\n5.8.1\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 186\\n5.8.2\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 188\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\nvi\\nContents\\n6\\nComplexity Theory\\n197\\n6.1\\nThe running time of algorithms . . . . . . . . . . . . . . . . . 197\\n6.2\\nThe complexity class P . . . . . . . . . . . . . . . . . . . . . . 199\\n6.2.1\\nSome examples . . . . . . . . . . . . . . . . . . . . . . 199\\n6.3\\nThe complexity class NP . . . . . . . . . . . . . . . . . . . . . 202\\n6.3.1\\nP is contained in NP . . . . . . . . . . . . . . . . . . . 208\\n6.3.2\\nDeciding NP-languages in exponential time\\n. . . . . . 208\\n6.3.3\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . 211\\n6.4\\nNon-deterministic algorithms\\n. . . . . . . . . . . . . . . . . . 211\\n6.5\\nNP-complete languages\\n. . . . . . . . . . . . . . . . . . . . . 213\\n6.5.1\\nTwo examples of reductions . . . . . . . . . . . . . . . 215\\n6.5.2\\nDeﬁnition of NP-completeness . . . . . . . . . . . . . . 220\\n6.5.3\\nAn NP-complete domino game\\n. . . . . . . . . . . . . 222\\n6.5.4\\nExamples of NP-complete languages . . . . . . . . . . 231\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\n7\\nSummary\\n239\\nPreface\\nThis is a free textbook for an undergraduate course on the Theory of Com-\\nputation, which we have been teaching at Carleton University since 2002.\\nUntil the 2011/2012 academic year, this course was oﬀered as a second-year\\ncourse (COMP 2805) and was compulsory for all Computer Science students.\\nStarting with the 2012/2013 academic year, the course has been downgraded\\nto a third-year optional course (COMP 3803).\\nWe have been developing this book since we started teaching this course.\\nCurrently, we cover most of the material from Chapters 2–5 during a 12-week\\nterm with three hours of classes per week.\\nThe material from Chapter 6, on Complexity Theory, is taught in the\\nthird-year course COMP 3804 (Design and Analysis of Algorithms). In the\\nearly years of COMP 2805, we gave a two-lecture overview of Complexity\\nTheory at the end of the term. Even though this overview has disappeared\\nfrom the course, we decided to keep Chapter 6. This chapter has not been\\nrevised/modiﬁed for a long time.\\nThe course as we teach it today has been inﬂuenced by the following two\\ntextbooks:\\n• Introduction to the Theory of Computation (second edition), by Michael\\nSipser, Thomson Course Technnology, Boston, 2006.\\n• Einf¨uhrung in die Theoretische Informatik, by Klaus Wagner, Springer-\\nVerlag, Berlin, 1994.\\nBesides reading this text, we recommend that you also take a look at\\nthese excellent textbooks, as well as one or more of the following ones:\\n• Elements of the Theory of Computation (second edition), by Harry\\nLewis and Christos Papadimitriou, Prentice-Hall, 1998.\\nviii\\n• Introduction to Languages and the Theory of Computation (third edi-\\ntion), by John Martin, McGraw-Hill, 2003.\\n• Introduction to Automata Theory, Languages, and Computation (third\\nedition), by John Hopcroft, Rajeev Motwani, Jeﬀrey Ullman, Addison\\nWesley, 2007.\\nPlease let us know if you ﬁnd errors, typos, simpler proofs, comments,\\nomissions, or if you think that some parts of the book “need improvement”.\\nChapter 1\\nIntroduction\\n1.1\\nPurpose and motivation\\nThis course is on the Theory of Computation, which tries to answer the\\nfollowing questions:\\n• What are the mathematical properties of computer hardware and soft-\\nware?\\n• What is a computation and what is an algorithm? Can we give rigorous\\nmathematical deﬁnitions of these notions?\\n• What are the limitations of computers?\\nCan “everything” be com-\\nputed? (As we will see, the answer to this question is “no”.)\\nPurpose of the Theory of Computation: Develop formal math-\\nematical models of computation that reﬂect real-world computers.\\nThis ﬁeld of research was started by mathematicians and logicians in the\\n1930’s, when they were trying to understand the meaning of a “computation”.\\nA central question asked was whether all mathematical problems can be\\nsolved in a systematic way. The research that started in those days led to\\ncomputers as we know them today.\\nNowadays, the Theory of Computation can be divided into the follow-\\ning three areas: Complexity Theory, Computability Theory, and Automata\\nTheory.\\n2\\nChapter 1.\\nIntroduction\\n1.1.1\\nComplexity theory\\nThe main question asked in this area is “What makes some problems com-\\nputationally hard and other problems easy?”\\nInformally, a problem is called “easy”, if it is eﬃciently solvable. Exam-\\nples of “easy” problems are (i) sorting a sequence of, say, 1,000,000 numbers,\\n(ii) searching for a name in a telephone directory, and (iii) computing the\\nfastest way to drive from Ottawa to Miami. On the other hand, a problem is\\ncalled “hard”, if it cannot be solved eﬃciently, or if we don’t know whether\\nit can be solved eﬃciently. Examples of “hard” problems are (i) time table\\nscheduling for all courses at Carleton, (ii) factoring a 300-digit integer into\\nits prime factors, and (iii) computing a layout for chips in VLSI.\\nCentral Question in Complexity Theory: Classify problems ac-\\ncording to their degree of “diﬃculty”. Give a rigorous proof that\\nproblems that seem to be “hard” are really “hard”.\\n1.1.2\\nComputability theory\\nIn the 1930’s, G¨odel, Turing, and Church discovered that some of the fun-\\ndamental mathematical problems cannot be solved by a “computer”. (This\\nmay sound strange, because computers were invented only in the 1940’s).\\nAn example of such a problem is “Is an arbitrary mathematical statement\\ntrue or false?” To attack such a problem, we need formal deﬁnitions of the\\nnotions of\\n• computer,\\n• algorithm, and\\n• computation.\\nThe theoretical models that were proposed in order to understand solvable\\nand unsolvable problems led to the development of real computers.\\nCentral Question in Computability Theory: Classify problems\\nas being solvable or unsolvable.\\n1.1.\\nPurpose and motivation\\n3\\n1.1.3\\nAutomata theory\\nAutomata Theory deals with deﬁnitions and properties of diﬀerent types of\\n“computation models”. Examples of such models are:\\n• Finite Automata. These are used in text processing, compilers, and\\nhardware design.\\n• Context-Free Grammars. These are used to deﬁne programming lan-\\nguages and in Artiﬁcial Intelligence.\\n• Turing Machines.\\nThese form a simple abstract model of a “real”\\ncomputer, such as your PC at home.\\nCentral Question in Automata Theory: Do these models have\\nthe same power, or can one model solve more problems than the\\nother?\\n1.1.4\\nThis course\\nIn this course, we will study the last two areas in reverse order: We will start\\nwith Automata Theory, followed by Computability Theory. The ﬁrst area,\\nComplexity Theory, will be covered in COMP 3804.\\nActually, before we start, we will review some mathematical proof tech-\\nniques. As you may guess, this is a fairly theoretical course, with lots of\\ndeﬁnitions, theorems, and proofs. You may guess this course is fun stuﬀfor\\nmath lovers, but boring and irrelevant for others. You guessed it wrong, and\\nhere are the reasons:\\n1. This course is about the fundamental capabilities and limitations of\\ncomputers. These topics form the core of computer science.\\n2. It is about mathematical properties of computer hardware and software.\\n3. This theory is very much relevant to practice, for example, in the design\\nof new programming languages, compilers, string searching, pattern\\nmatching, computer security, artiﬁcial intelligence, etc., etc.\\n4. This course helps you to learn problem solving skills. Theory teaches\\nyou how to think, prove, argue, solve problems, express, and abstract.\\n4\\nChapter 1.\\nIntroduction\\n5. This theory simpliﬁes the complex computers to an abstract and simple\\nmathematical model, and helps you to understand them better.\\n6. This course is about rigorously analyzing capabilities and limitations\\nof systems.\\nWhere does this course ﬁt in the Computer Science Curriculum at Car-\\nleton University? It is a theory course that is the third part in the series\\nCOMP 1805, COMP 2804, COMP 3803, COMP 3804, and COMP 4804.\\nThis course also widens your understanding of computers and will inﬂuence\\nother courses including Compilers, Programming Languages, and Artiﬁcial\\nIntelligence.\\n1.2\\nMathematical preliminaries\\nThroughout this course, we will assume that you know the following mathe-\\nmatical concepts:\\n1. A set is a collection of well-deﬁned objects. Examples are (i) the set of\\nall Dutch Olympic Gold Medallists, (ii) the set of all pubs in Ottawa,\\nand (iii) the set of all even natural numbers.\\n2. The set of natural numbers is N = {1, 2, 3, . . .}.\\n3. The set of integers is Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n4. The set of rational numbers is Q = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\n5. The set of real numbers is denoted by R.\\n6. If A and B are sets, then A is a subset of B, written as A ⊆B, if every\\nelement of A is also an element of B. For example, the set of even\\nnatural numbers is a subset of the set of all natural numbers. Every\\nset A is a subset of itself, i.e., A ⊆A. The empty set is a subset of\\nevery set A, i.e., ∅⊆A.\\n7. If B is a set, then the power set P(B) of B is deﬁned to be the set of\\nall subsets of B:\\nP(B) = {A : A ⊆B}.\\nObserve that ∅∈P(B) and B ∈P(B).\\n1.2.\\nMathematical preliminaries\\n5\\n8. If A and B are two sets, then\\n(a) their union is deﬁned as\\nA ∪B = {x : x ∈A or x ∈B},\\n(b) their intersection is deﬁned as\\nA ∩B = {x : x ∈A and x ∈B},\\n(c) their diﬀerence is deﬁned as\\nA \\\\ B = {x : x ∈A and x ̸∈B},\\n(d) the Cartesian product of A and B is deﬁned as\\nA × B = {(x, y) : x ∈A and y ∈B},\\n(e) the complement of A is deﬁned as\\nA = {x : x ̸∈A}.\\n9. A binary relation on two sets A and B is a subset of A × B.\\n10. A function f from A to B, denoted by f : A →B, is a binary relation\\nR, having the property that for each element a ∈A, there is exactly\\none ordered pair in R, whose ﬁrst component is a. We will also say\\nthat f(a) = b, or f maps a to b, or the image of a under f is b. The\\nset A is called the domain of f, and the set\\n{b ∈B : there is an a ∈A with f(a) = b}\\nis called the range of f.\\n11. A function f : A →B is one-to-one (or injective), if for any two distinct\\nelements a and a′ in A, we have f(a) ̸= f(a′). The function f is onto\\n(or surjective), if for each element b ∈B, there exists an element a ∈A,\\nsuch that f(a) = b; in other words, the range of f is equal to the set\\nB. A function f is a bijection, if f is both injective and surjective.\\n12. A binary relation R ⊆A × A is an equivalence relation, if it satisﬁes\\nthe following three conditions:\\n6\\nChapter 1.\\nIntroduction\\n(a) R is reﬂexive: For every element in a ∈A, we have (a, a) ∈R.\\n(b) R is symmetric: For all a and b in A, if (a, b) ∈R, then also\\n(b, a) ∈R.\\n(c) R is transitive: For all a, b, and c in A, if (a, b) ∈R and (b, c) ∈R,\\nthen also (a, c) ∈R.\\n13. A graph G = (V, E) is a pair consisting of a set V , whose elements are\\ncalled vertices, and a set E, where each element of E is a pair of distinct\\nvertices. The elements of E are called edges. The ﬁgure below shows\\nsome well-known graphs: K5 (the complete graph on ﬁve vertices), K3,3\\n(the complete bipartite graph on 2 × 3 = 6 vertices), and the Peterson\\ngraph.\\nK5\\nK3,3\\nPeterson graph\\nThe degree of a vertex v, denoted by deg(v), is deﬁned to be the number\\nof edges that are incident on v.\\nA path in a graph is a sequence of vertices that are connected by edges.\\nA path is a cycle, if it starts and ends at the same vertex. A simple\\npath is a path without any repeated vertices. A graph is connected, if\\nthere is a path between every pair of vertices.\\n14. In the context of strings, an alphabet is a ﬁnite set, whose elements\\nare called symbols. Examples of alphabets are Σ = {0, 1} and Σ =\\n{a, b, c, . . . , z}.\\n15. A string over an alphabet Σ is a ﬁnite sequence of symbols, where each\\nsymbol is an element of Σ. The length of a string w, denoted by |w|, is\\nthe number of symbols contained in w. The empty string, denoted by\\n1.3.\\nProof techniques\\n7\\nϵ, is the string having length zero. For example, if the alphabet Σ is\\nequal to {0, 1}, then 10, 1000, 0, 101, and ϵ are strings over Σ, having\\nlengths 2, 4, 1, 3, and 0, respectively.\\n16. A language is a set of strings.\\n17. The Boolean values are 1 and 0, that represent true and false, respec-\\ntively. The basic Boolean operations include\\n(a) negation (or NOT), represented by ¬,\\n(b) conjunction (or AND), represented by ∧,\\n(c) disjunction (or OR), represented by ∨,\\n(d) exclusive-or (or XOR), represented by ⊕,\\n(e) equivalence, represented by ↔or ⇔,\\n(f) implication, represented by →or ⇒.\\nThe following table explains the meanings of these operations.\\nNOT\\nAND\\nOR\\nXOR\\nequivalence\\nimplication\\n¬0 = 1\\n0 ∧0 = 0\\n0 ∨0 = 0\\n0 ⊕0 = 0\\n0 ↔0 = 1\\n0 →0 = 1\\n¬1 = 0\\n0 ∧1 = 0\\n0 ∨1 = 1\\n0 ⊕1 = 1\\n0 ↔1 = 0\\n0 →1 = 1\\n1 ∧0 = 0\\n1 ∨0 = 1\\n1 ⊕0 = 1\\n1 ↔0 = 0\\n1 →0 = 0\\n1 ∧1 = 1\\n1 ∨1 = 1\\n1 ⊕1 = 0\\n1 ↔1 = 1\\n1 →1 = 1\\n1.3\\nProof techniques\\nIn mathematics, a theorem is a statement that is true. A proof is a sequence\\nof mathematical statements that form an argument to show that a theorem is\\ntrue. The statements in the proof of a theorem include axioms (assumptions\\nabout the underlying mathematical structures), hypotheses of the theorem\\nto be proved, and previously proved theorems. The main question is “How\\ndo we go about proving theorems?” This question is similar to the question\\nof how to solve a given problem. Of course, the answer is that ﬁnding proofs,\\nor solving problems, is not easy; otherwise life would be dull! There is no\\nspeciﬁed way of coming up with a proof, but there are some generic strategies\\nthat could be of help. In this section, we review some of these strategies,\\nthat will be suﬃcient for this course. The best way to get a feeling of how\\nto come up with a proof is by solving a large number of problems. Here are\\n8\\nChapter 1.\\nIntroduction\\nsome useful tips. (You may take a look at the book How to Solve It, by G.\\nP´olya).\\n1. Read and completely understand the statement of the theorem to be\\nproved. Most often this is the hardest part.\\n2. Sometimes, theorems contain theorems inside them.\\nFor example,\\n“Property A if and only if property B”, requires showing two state-\\nments:\\n(a) If property A is true, then property B is true (A ⇒B).\\n(b) If property B is true, then property A is true (B ⇒A).\\nAnother example is the theorem “Set A equals set B.” To prove this,\\nwe need to prove that A ⊆B and B ⊆A. That is, we need to show\\nthat each element of set A is in set B, and that each element of set B\\nis in set A.\\n3. Try to work out a few simple cases of the theorem just to get a grip on\\nit (i.e., crack a few simple cases ﬁrst).\\n4. Try to write down the proof once you have it. This is to ensure the\\ncorrectness of your proof. Often, mistakes are found at the time of\\nwriting.\\n5. Finding proofs takes time, we do not come prewired to produce proofs.\\nBe patient, think, express and write clearly and try to be precise as\\nmuch as possible.\\nIn the next sections, we will go through some of the proof strategies.\\n1.3.1\\nDirect proofs\\nAs the name suggests, in a direct proof of a theorem, we just approach the\\ntheorem directly.\\nTheorem 1.3.1 If n is an odd positive integer, then n2 is odd as well.\\n1.3.\\nProof techniques\\n9\\nProof. An odd positive integer n can be written as n = 2k + 1, for some\\ninteger k ≥0. Then\\nn2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1.\\nSince 2(2k2 + 2k) is even, and “even plus one is odd”, we can conclude that\\nn2 is odd.\\nTheorem 1.3.2 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is an even integer, i.e.,\\nX\\nv∈V\\ndeg(v)\\nis even.\\nProof. If you do not see the meaning of this statement, then ﬁrst try it out\\nfor a few graphs. The reason why the statement holds is very simple: Each\\nedge contributes 2 to the summation (because an edge is incident on exactly\\ntwo distinct vertices).\\nActually, the proof above proves the following theorem.\\nTheorem 1.3.3 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2|E|.\\n1.3.2\\nConstructive proofs\\nThis technique not only shows the existence of a certain object, it actually\\ngives a method of creating it. Here is how a constructive proof looks like:\\nTheorem 1.3.4 There exists an object with property P.\\nProof. Here is the object: [. . .]\\nAnd here is the proof that the object satisﬁes property P: [. . .]\\nHere is an example of a constructive proof. A graph is called 3-regular, if\\neach vertex has degree three.\\n10\\nChapter 1.\\nIntroduction\\nTheorem 1.3.5 For every even integer n ≥4, there exists a 3-regular graph\\nwith n vertices.\\nProof. Deﬁne\\nV = {0, 1, 2, . . . , n −1},\\nand\\nE = {{i, i+1} : 0 ≤i ≤n−2}∪{{n−1, 0}}∪{{i, i+n/2} : 0 ≤i ≤n/2−1}.\\nThen the graph G = (V, E) is 3-regular.\\nConvince yourself that this graph is indeed 3-regular. It may help to draw\\nthe graph for, say, n = 8.\\n1.3.3\\nNonconstructive proofs\\nIn a nonconstructive proof, we show that a certain object exists, without\\nactually creating it. Here is an example of such a proof:\\nTheorem 1.3.6 There exist irrational numbers x and y such that xy is ra-\\ntional.\\nProof. There are two possible cases.\\nCase 1:\\n√\\n2\\n√\\n2 ∈Q.\\nIn this case, we take x = y =\\n√\\n2. In Theorem 1.3.9 below, we will prove\\nthat\\n√\\n2 is irrational.\\nCase 2:\\n√\\n2\\n√\\n2 ̸∈Q.\\nIn this case, we take x =\\n√\\n2\\n√\\n2 and y =\\n√\\n2. Since\\nxy =\\n\\x12√\\n2\\n√\\n2\\x13√\\n2\\n=\\n√\\n2\\n2 = 2,\\nthe claim in the theorem follows.\\nObserve that this proof indeed proves the theorem, but it does not give\\nan example of a pair of irrational numbers x and y such that xy is rational.\\n1.3.\\nProof techniques\\n11\\n1.3.4\\nProofs by contradiction\\nThis is how a proof by contradiction looks like:\\nTheorem 1.3.7 Statement S is true.\\nProof. Assume that statement S is false. Then, derive a contradiction (such\\nas 1 + 1 = 3).\\nIn other words, show that the statement “¬S ⇒false” is true. This is\\nsuﬃcient, because the contrapositive of the statement “¬S ⇒false” is the\\nstatement “true ⇒S”. The latter logical formula is equivalent to S, and\\nthat is what we wanted to show.\\nBelow, we give two examples of proofs by contradiction.\\nTheorem 1.3.8 Let n be a positive integer. If n2 is even, then n is even.\\nProof. We will prove the theorem by contradiction. So we assume that n2\\nis even, but n is odd. Since n is odd, we know from Theorem 1.3.1 that n2\\nis odd. This is a contradiction, because we assumed that n2 is even.\\nTheorem 1.3.9\\n√\\n2 is irrational, i.e.,\\n√\\n2 cannot be written as a fraction of\\ntwo integers m and n.\\nProof. We will prove the theorem by contradiction. So we assume that\\n√\\n2\\nis rational. Then\\n√\\n2 can be written as a fraction of two integers,\\n√\\n2 = m/n,\\nwhere m ≥1 and n ≥1. We may assume that m and n do not share any\\ncommon factors, i.e., the greatest common divisor of m and n is equal to\\none; if this is not the case, then we can get rid of the common factors. By\\nsquaring\\n√\\n2 = m/n, we get 2n2 = m2. This implies that m2 is even. Then,\\nby Theorem 1.3.8, m is even, which means that we can write m as m = 2k,\\nfor some positive integer k. It follows that 2n2 = m2 = 4k2, which implies\\nthat n2 = 2k2. Hence, n2 is even. Again by Theorem 1.3.8, it follows that n\\nis even.\\nWe have shown that m and n are both even. But we know that m and\\nn are not both even. Hence, we have a contradiction. Our assumption that\\n√\\n2 is rational is wrong. Thus, we can conclude that\\n√\\n2 is irrational.\\nThere is a nice discussion of this proof in the book My Brain is Open:\\nThe Mathematical Journeys of Paul Erd˝os by B. Schechter.\\n12\\nChapter 1.\\nIntroduction\\n1.3.5\\nThe pigeon hole principle\\nThis is a simple principle with surprising consequences.\\nPigeon Hole Principle: If n + 1 or more objects are placed into n\\nboxes, then there is at least one box containing two or more objects.\\nIn other words, if A and B are two sets such that |A| > |B|, then\\nthere is no one-to-one function from A to B.\\nTheorem 1.3.10 Let n be a positive integer. Every sequence of n2 + 1 dis-\\ntinct real numbers contains a subsequence of length n + 1 that is either in-\\ncreasing or decreasing.\\nProof. For example consider the sequence (20, 10, 9, 7, 11, 2, 21, 1, 20, 31) of\\n10 = 32 + 1 numbers. This sequence contains an increasing subsequence of\\nlength 4 = 3 + 1, namely (10, 11, 21, 31).\\nThe proof of this theorem is by contradiction, and uses the pigeon hole\\nprinciple.\\nLet (a1, a2, . . . , an2+1) be an arbitrary sequence of n2 + 1 distinct real\\nnumbers. For each i with 1 ≤i ≤n2 + 1, let inci denote the length of\\nthe longest increasing subsequence that starts at ai, and let deci denote the\\nlength of the longest decreasing subsequence that starts at ai.\\nUsing this notation, the claim in the theorem can be formulated as follows:\\nThere is an index i such that inci ≥n + 1 or deci ≥n + 1.\\nWe will prove the claim by contradiction. So we assume that inci ≤n\\nand deci ≤n for all i with 1 ≤i ≤n2 + 1.\\nConsider the set\\nB = {(b, c) : 1 ≤b ≤n, 1 ≤c ≤n},\\nand think of the elements of B as being boxes. For each i with 1 ≤i ≤n2+1,\\nthe pair (inci, deci) is an element of B. So we have n2+1 elements (inci, deci),\\nwhich are placed in the n2 boxes of B. By the pigeon hole principle, there\\nmust be a box that contains two (or more) elements. In other words, there\\nexist two integers i and j such that i < j and\\n(inci, deci) = (incj, decj).\\nRecall that the elements in the sequence are distinct. Hence, ai ̸= aj. We\\nconsider two cases.\\n1.3.\\nProof techniques\\n13\\nFirst assume that ai < aj. Then the length of the longest increasing\\nsubsequence starting at ai must be at least 1+incj, because we can append ai\\nto the longest increasing subsequence starting at aj. Therefore, inci ̸= incj,\\nwhich is a contradiction.\\nThe second case is when ai > aj. Then the length of the longest decreasing\\nsubsequence starting at ai must be at least 1+decj, because we can append ai\\nto the longest decreasing subsequence starting at aj. Therefore, deci ̸= decj,\\nwhich is again a contradiction.\\n1.3.6\\nProofs by induction\\nThis is a very powerful and important technique for proving theorems.\\nFor each positive integer n, let P(n) be a mathematical statement that\\ndepends on n. Assume we wish to prove that P(n) is true for all positive\\nintegers n. A proof by induction of such a statement is carried out as follows:\\nBasis: Prove that P(1) is true.\\nInduction step: Prove that for all n ≥1, the following holds: If P(n) is\\ntrue, then P(n + 1) is also true.\\nIn the induction step, we choose an arbitrary integer n ≥1 and assume\\nthat P(n) is true; this is called the induction hypothesis. Then we prove that\\nP(n + 1) is also true.\\nTheorem 1.3.11 For all positive integers n, we have\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\nProof. We start with the basis of the induction. If n = 1, then the left-hand\\nside is equal to 1, and so is the right-hand side. So the theorem is true for\\nn = 1.\\nFor the induction step, let n ≥1 and assume that the theorem is true for\\nn, i.e., assume that\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\n14\\nChapter 1.\\nIntroduction\\nWe have to prove that the theorem is true for n + 1, i.e., we have to prove\\nthat\\n1 + 2 + 3 + . . . + (n + 1) = (n + 1)(n + 2)\\n2\\n.\\nHere is the proof:\\n1 + 2 + 3 + . . . + (n + 1)\\n=\\n1 + 2 + 3 + . . . + n\\n|\\n{z\\n}\\n= n(n+1)\\n2\\n+(n + 1)\\n=\\nn(n + 1)\\n2\\n+ (n + 1)\\n=\\n(n + 1)(n + 2)\\n2\\n.\\nBy the way, here is an alternative proof of the theorem above: Let S =\\n1 + 2 + 3 + . . . + n. Then,\\nS\\n=\\n1\\n+\\n2\\n+\\n3\\n+\\n. . .\\n+\\n(n −2)\\n+\\n(n −1)\\n+\\nn\\nS\\n=\\nn\\n+\\n(n −1)\\n+\\n(n −2)\\n+\\n. . .\\n+\\n3\\n+\\n2\\n+\\n1\\n2S\\n=\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n. . .\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\nSince there are n terms on the right-hand side, we have 2S = n(n + 1). This\\nimplies that S = n(n + 1)/2.\\nTheorem 1.3.12 For every positive integer n, a −b is a factor of an −bn.\\nProof. A direct proof can be given by providing a factorization of an −bn:\\nan −bn = (a −b)(an−1 + an−2b + an−3b2 + . . . + abn−2 + bn−1).\\nWe now prove the theorem by induction. For the basis, let n = 1. The claim\\nin the theorem is “a −b is a factor of a −b”, which is obviously true.\\nLet n ≥1 and assume that a −b is a factor of an −bn. We have to prove\\nthat a −b is a factor of an+1 −bn+1. We have\\nan+1 −bn+1 = an+1 −anb + anb −bn+1 = an(a −b) + (an −bn)b.\\nThe ﬁrst term on the right-hand side is divisible by a −b. By the induction\\nhypothesis, the second term on the right-hand side is divisible by a −b as\\nwell. Therefore, the entire right-hand side is divisible by a −b. Since the\\nright-hand side is equal to an+1 −bn+1, it follows that a −b is a factor of\\nan+1 −bn+1.\\nWe now give an alternative proof of Theorem 1.3.3:\\n1.3.\\nProof techniques\\n15\\nTheorem 1.3.13 Let G = (V, E) be a graph with m edges. Then the sum\\nof the degrees of all vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2m.\\nProof. The proof is by induction on the number m of edges. For the basis of\\nthe induction, assume that m = 0. Then the graph G does not contain any\\nedges and, therefore, P\\nv∈V deg(v) = 0. Thus, the theorem is true if m = 0.\\nLet m ≥0 and assume that the theorem is true for every graph with m\\nedges. Let G be an arbitrary graph with m+1 edges. We have to prove that\\nP\\nv∈V deg(v) = 2(m + 1).\\nLet {a, b} be an arbitrary edge in G, and let G′ be the graph obtained\\nfrom G by removing the edge {a, b}. Since G′ has m edges, we know from\\nthe induction hypothesis that the sum of the degrees of all vertices in G′ is\\nequal to 2m. Using this, we obtain\\nX\\nv∈G\\ndeg(v) =\\nX\\nv∈G′\\ndeg(v) + 2 = 2m + 2 = 2(m + 1).\\n1.3.7\\nMore examples of proofs\\nRecall Theorem 1.3.5, which states that for every even integer n ≥4, there\\nexists a 3-regular graph with n vertices. The following theorem explains why\\nwe stated this theorem for even values of n.\\nTheorem 1.3.14 Let n ≥5 be an odd integer. There is no 3-regular graph\\nwith n vertices.\\nProof. The proof is by contradiction. So we assume that there exists a\\ngraph G = (V, E) with n vertices that is 3-regular. Let m be the number of\\nedges in G. Since deg(v) = 3 for every vertex, we have\\nX\\nv∈V\\ndeg(v) = 3n.\\nOn the other hand, by Theorem 1.3.3, we have\\nX\\nv∈V\\ndeg(v) = 2m.\\n16\\nChapter 1.\\nIntroduction\\nIt follows that 3n = 2m, which can be rewritten as m = 3n/2. Since m is an\\ninteger, and since gcd(2, 3) = 1, n/2 must be an integer. Hence, n is even,\\nwhich is a contradiction.\\nLet Kn be the complete graph on n vertices. This graph has a vertex set\\nof size n, and every pair of distinct vertices is joined by an edge.\\nIf G = (V, E) is a graph with n vertices, then the complement G of G is\\nthe graph with vertex set V that consists of those edges of Kn that are not\\npresent in G.\\nTheorem 1.3.15 Let n ≥2 and let G be a graph on n vertices. Then G is\\nconnected or G is connected.\\nProof. We prove the theorem by induction on the number n of vertices. For\\nthe basis, assume that n = 2. There are two possibilities for the graph G:\\n1. G contains one edge. In this case, G is connected.\\n2. G does not contain an edge. In this case, the complement G contains\\none edge and, therefore, G is connected.\\nSo for n = 2, the theorem is true.\\nLet n ≥2 and assume that the theorem is true for every graph with n\\nvertices. Let G be graph with n + 1 vertices. We have to prove that G is\\nconnected or G is connected. We consider three cases.\\nCase 1: There is a vertex v whose degree in G is equal to n.\\nSince G has n+1 vertices, v is connected by an edge to every other vertex\\nof G. Therefore, G is connected.\\nCase 2: There is a vertex v whose degree in G is equal to 0.\\nIn this case, the degree of v in the graph G is equal to n. Since G has n+1\\nvertices, v is connected by an edge to every other vertex of G. Therefore, G\\nis connected.\\nCase 3: For every vertex v, the degree of v in G is in {1, 2, . . . , n −1}.\\nLet v be an arbitrary vertex of G.\\nLet G′ be the graph obtained by\\ndeleting from G the vertex v, together with all edges that are incident on v.\\nSince G′ has n vertices, we know from the induction hypothesis that G′ is\\nconnected or G′ is connected.\\n1.3.\\nProof techniques\\n17\\nLet us ﬁrst assume that G′ is connected. Then the graph G is connected\\nas well, because there is at least one edge in G between v and some vertex\\nof G′.\\nIf G′ is not connected, then G′ must be connected. Since we are in Case 3,\\nwe know that the degree of v in G is in the set {1, 2, . . . , n −1}. It follows\\nthat the degree of v in the graph G is in this set as well. Hence, there is at\\nleast one edge in G between v and some vertex in G′. This implies that G is\\nconnected.\\nThe previous theorem can be rephrased as follows:\\nTheorem 1.3.16 Let n ≥2 and consider the complete graph Kn on n ver-\\ntices. Color each edge of this graph as either red or blue. Let R be the graph\\nconsisting of all the red edges, and let B be the graph consisting of all the\\nblue edges. Then R is connected or B is connected.\\nA graph is said to be planar, if it can be drawn (a better term is “embed-\\nded”) in the plane in such a way that no two edges intersect, except possibly\\nat their endpoints.\\nAn embedding of a planar graph consists of vertices,\\nedges, and faces. In the example below, there are 11 vertices, 18 edges, and\\n9 faces (including the unbounded face).\\nThe following theorem is known as Euler’s theorem for planar graphs.\\nApparently, this theorem was discovered by Euler around 1750. Legendre\\ngave the ﬁrst proof in 1794, see\\nhttp://www.ics.uci.edu/~eppstein/junkyard/euler/\\nTheorem 1.3.17 (Euler) Consider an embedding of a planar graph G. Let\\nv, e, and f be the number of vertices, edges, and faces (including the single\\n18\\nChapter 1.\\nIntroduction\\nunbounded face) of this embedding, respectively. Moreover, let c be the number\\nof connected components of G. Then\\nv −e + f = c + 1.\\nProof. The proof is by induction on the number of edges of G. To be more\\nprecise, we start with a graph having no edges, and prove that the theorem\\nholds for this case. Then, we add the edges one by one, and show that the\\nrelation v −e + f = c + 1 is maintained.\\nSo we ﬁrst assume that G has no edges, i.e., e = 0. Then the embedding\\nconsists of a collection of v points. In this case, we have f = 1 and c = v.\\nHence, the relation v −e + f = c + 1 holds.\\nLet e > 0 and assume that Euler’s formula holds for a subgraph of G\\nhaving e −1 edges. Let {u, v} be an edge of G that is not in the subgraph,\\nand add this edge to the subgraph. There are two cases depending on whether\\nthis new edge joins two connected components or joins two vertices in the\\nsame connected component.\\nCase 1: The new edge {u, v} joins two connected components.\\nIn this case, the number of vertices and the number of faces do not change,\\nthe number of connected components goes down by 1, and the number of\\nedges increases by 1. It follows that the relation in the theorem is still valid.\\nCase 2: The new edge {u, v} joins two vertices in the same connected com-\\nponent.\\nIn this case, the number of vertices and the number of connected com-\\nponents do not change, the number of edges increases by 1, and the number\\nof faces increases by 1 (because the new edge splits one face into two faces).\\nTherefore, the relation in the theorem is still valid.\\nEuler’s theorem is usually stated as follows:\\nTheorem 1.3.18 (Euler) Consider an embedding of a connected planar\\ngraph G. Let v, e, and f be the number of vertices, edges, and faces (in-\\ncluding the single unbounded face) of this embedding, respectively. Then\\nv −e + f = 2.\\nIf you like surprising proofs of various mathematical results, you should\\nread the book Proofs from THE BOOK by Aigner and Ziegler.\\nExercises\\n19\\nExercises\\n1.1 Use induction to prove that every integer n ≥2 can be written as a\\nproduct of prime numbers.\\n1.2 For every prime number p, prove that √p is irrational.\\n1.3 Let n be a positive integer that is not a perfect square. Prove that √n\\nis irrational.\\n1.4 Prove by induction that n4 −4n2 is divisible by 3, for all integers n ≥1.\\n1.5 Prove that\\nn\\nX\\ni=1\\n1\\ni2 < 2 −1/n,\\nfor every integer n ≥2.\\n1.6 Prove that 9 divides n3 + (n + 1)3 + (n + 2)3, for every integer n ≥0.\\n1.7 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers that are consecutive.\\n1.8 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers such that one divides the other.\\n20\\nChapter 1.\\nIntroduction\\nChapter 2\\nFinite Automata and Regular\\nLanguages\\nIn this chapter, we introduce and analyze the class of languages that are\\nknown as regular languages. Informally, these languages can be “processed”\\nby computers having a very small amount of memory.\\n2.1\\nAn example: Controling a toll gate\\nBefore we give a formal deﬁnition of a ﬁnite automaton, we consider an\\nexample in which such an automaton shows up in a natural way. We consider\\nthe problem of designing a “computer” that controls a toll gate.\\nWhen a car arrives at the toll gate, the gate is closed. The gate opens as\\nsoon as the driver has payed 25 cents. We assume that we have only three\\ncoin denominations: 5, 10, and 25 cents. We also assume that no excess\\nchange is returned.\\nAfter having arrived at the toll gate, the driver inserts a sequence of coins\\ninto the machine. At any moment, the machine has to decide whether or not\\nto open the gate, i.e., whether or not the driver has paid 25 cents (or more).\\nIn order to decide this, the machine is in one of the following six states, at\\nany moment during the process:\\n• The machine is in state q0, if it has not collected any money yet.\\n• The machine is in state q1, if it has collected exactly 5 cents.\\n• The machine is in state q2, if it has collected exactly 10 cents.\\n22\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The machine is in state q3, if it has collected exactly 15 cents.\\n• The machine is in state q4, if it has collected exactly 20 cents.\\n• The machine is in state q5, if it has collected 25 cents or more.\\nInitially (when a car arrives at the toll gate), the machine is in state q0.\\nAssume, for example, that the driver presents the sequence (10,5,5,10) of\\ncoins.\\n• After receiving the ﬁrst 10 cents coin, the machine switches from state\\nq0 to state q2.\\n• After receiving the ﬁrst 5 cents coin, the machine switches from state\\nq2 to state q3.\\n• After receiving the second 5 cents coin, the machine switches from state\\nq3 to state q4.\\n• After receiving the second 10 cents coin, the machine switches from\\nstate q4 to state q5. At this moment, the gate opens. (Remember that\\nno change is given.)\\nThe ﬁgure below represents the behavior of the machine for all possible\\nsequences of coins. State q5 is represented by two circles, because it is a\\nspecial state: As soon as the machine reaches this state, the gate opens.\\nq0\\nq1\\nq2\\nq3\\nq4\\nq5\\n5\\n5\\n5\\n5\\n10\\n10\\n10\\n25\\n25\\n25\\n10, 25\\n5, 10, 25\\n5, 10\\n25\\nstart\\nObserve that the machine (or computer) only has to remember which\\nstate it is in at any given time. Thus, it needs only a very small amount\\nof memory: It has to be able to distinguish between any one of six possible\\ncases and, therefore, it only needs a memory of ⌈log 6⌉= 3 bits.\\n2.2.\\nDeterministic ﬁnite automata\\n23\\n2.2\\nDeterministic ﬁnite automata\\nLet us look at another example. Consider the following state diagram:\\nq1\\nq2\\nq3\\n0\\n0\\n1\\n1\\n0,1\\nWe say that q1 is the start state and q2 is an accept state. Consider the\\ninput string 1101. This string is processed in the following way:\\n• Initially, the machine is in the start state q1.\\n• After having read the ﬁrst 1, the machine switches from state q1 to\\nstate q2.\\n• After having read the second 1, the machine switches from state q2 to\\nstate q2. (So actually, it does not switch.)\\n• After having read the ﬁrst 0, the machine switches from state q2 to\\nstate q3.\\n• After having read the third 1, the machine switches from state q3 to\\nstate q2.\\nAfter the entire string 1101 has been processed, the machine is in state q2,\\nwhich is an accept state. We say that the string 1101 is accepted by the\\nmachine.\\nConsider now the input string 0101010. After having read this string\\nfrom left to right (starting in the start state q1), the machine is in state q3.\\nSince q3 is not an accept state, we say that the machine rejects the string\\n0101010.\\nWe hope you are able to see that this machine accepts every binary string\\nthat ends with a 1. In fact, the machine accepts more strings:\\n• Every binary string having the property that there are an even number\\nof 0s following the rightmost 1, is accepted by this machine.\\n24\\nChapter 2.\\nFinite Automata and Regular Languages\\n• Every other binary string is rejected by the machine. Observe that each\\nsuch string is either empty, consists of 0s only, or has an odd number\\nof 0s following the rightmost 1.\\nWe now come to the formal deﬁnition of a ﬁnite automaton:\\nDeﬁnition 2.2.1 A ﬁnite automaton is a 5-tuple M = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σ →Q is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\nYou can think of the transition function δ as being the “program” of the\\nﬁnite automaton M = (Q, Σ, δ, q, F). This function tells us what M can do\\nin “one step”:\\n• Let r be a state of Q and let a be a symbol of the alphabet Σ. If\\nthe ﬁnite automaton M is in state r and reads the symbol a, then it\\nswitches from state r to state δ(r, a). (In fact, δ(r, a) may be equal to\\nr.)\\nThe “computer” that we designed in the toll gate example in Section 2.1\\nis a ﬁnite automaton. For this example, we have Q = {q0, q1, q2, q3, q4, q5},\\nΣ = {5, 10, 25}, the start state is q0, F = {q5}, and δ is given by the following\\ntable:\\n5\\n10\\n25\\nq0\\nq1\\nq2\\nq5\\nq1\\nq2\\nq3\\nq5\\nq2\\nq3\\nq4\\nq5\\nq3\\nq4\\nq5\\nq5\\nq4\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nThe example given in the beginning of this section is also a ﬁnite automa-\\nton. For this example, we have Q = {q1, q2, q3}, Σ = {0, 1}, the start state\\nis q1, F = {q2}, and δ is given by the following table:\\n2.2.\\nDeterministic ﬁnite automata\\n25\\n0\\n1\\nq1\\nq1\\nq2\\nq2\\nq3\\nq2\\nq3\\nq2\\nq2\\nLet us denote this ﬁnite automaton by M. The language of M, denoted\\nby L(M), is the set of all binary strings that are accepted by M. As we have\\nseen before, we have\\nL(M) = {w : w contains at least one 1 and ends with an even number of 0s}.\\nWe now give a formal deﬁnition of the language of a ﬁnite automaton:\\nDeﬁnition 2.2.2 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton and let w =\\nw1w2 . . . wn be a string over Σ. Deﬁne the sequence r0, r1, . . . , rn of states, in\\nthe following way:\\n• r0 = q,\\n• ri+1 = δ(ri, wi+1), for i = 0, 1, . . . , n −1.\\n1. If rn ∈F, then we say that M accepts w.\\n2. If rn ̸∈F, then we say that M rejects w.\\nIn this deﬁnition, w may be the empty string, which we denote by ϵ, and\\nwhose length is zero; thus in the deﬁnition above, n = 0. In this case, the\\nsequence r0, r1, . . . , rn of states has length one; it consists of just the state\\nr0 = q. The empty string is accepted by M if and only if the start state q\\nbelongs to F.\\nDeﬁnition 2.2.3 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton. The lan-\\nguage L(M) accepted by M is deﬁned to be the set of all strings that are\\naccepted by M:\\nL(M) = {w : w is a string over Σ and M accepts w }.\\nDeﬁnition 2.2.4 A language A is called regular, if there exists a ﬁnite au-\\ntomaton M such that A = L(M).\\n26\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe ﬁnish this section by presenting an equivalent way of deﬁning the\\nlanguage accepted by a ﬁnite automaton. Let M = (Q, Σ, δ, q, F) be a ﬁnite\\nautomaton. The transition function δ : Q × Σ →Q tells us that, when M\\nis in state r ∈Q and reads symbol a ∈Σ, it switches from state r to state\\nδ(r, a). Let Σ∗denote the set of all strings over the alphabet Σ. (Σ∗includes\\nthe empty string ϵ.) We extend the function δ to a function\\nδ : Q × Σ∗→Q,\\nthat is deﬁned as follows. For any state r ∈Q and for any string w over the\\nalphabet Σ,\\nδ(r, w) =\\n\\x1a r\\nif w = ϵ,\\nδ(δ(r, v), a)\\nif w = va, where v is a string and a ∈Σ.\\nWhat is the meaning of this function δ? Let r be a state of Q and let w be\\na string over the alphabet Σ. Then\\n• δ(r, w) is the state that M reaches, when it starts in state r, reads the\\nstring w from left to right, and uses δ to switch from state to state.\\nUsing this notation, we have\\nL(M) = {w : w is a string over Σ and δ(q, w) ∈F}.\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton\\nLet\\nA = {w : w is a binary string containing an odd number of 1s}.\\nWe claim that this language A is regular. In order to prove this, we have to\\nconstruct a ﬁnite automaton M such that A = L(M).\\nHow to construct M? Here is a ﬁrst idea: The ﬁnite automaton reads the\\ninput string w from left to right and keeps track of the number of 1s it has\\nseen. After having read the entire string w, it checks whether this number\\nis odd (in which case w is accepted) or even (in which case w is rejected).\\nUsing this approach, the ﬁnite automaton needs a state for every integer\\ni ≥0, indicating that the number of 1s read so far is equal to i. Hence,\\nto design a ﬁnite automaton that follows this approach, we need an inﬁnite\\n2.2.\\nDeterministic ﬁnite automata\\n27\\nnumber of states. But, the deﬁnition of ﬁnite automaton requires the number\\nof states to be ﬁnite.\\nA better, and correct approach, is to keep track of whether the number\\nof 1s read so far is even or odd. This leads to the following ﬁnite automaton:\\n• The set of states is Q = {qe, qo}. If the ﬁnite automaton is in state qe,\\nthen it has read an even number of 1s; if it is in state qo, then it has\\nread an odd number of 1s.\\n• The alphabet is Σ = {0, 1}.\\n• The start state is qe, because at the start, the number of 1s read by the\\nautomaton is equal to 0, and 0 is even.\\n• The set F of accept states is F = {qo}.\\n• The transition function δ is given by the following table:\\n0\\n1\\nqe\\nqe\\nqo\\nqo\\nqo\\nqe\\nThis ﬁnite automaton M = (Q, Σ, δ, qe, F) can also be described by its state\\ndiagram, which is given in the ﬁgure below. The arrow that comes “out of\\nthe blue” and enters the state qe, indicates that qe is the start state. The\\nstate depicted with double circles indicates the accept state.\\nqe\\nqo\\n0\\n0\\n1\\n1\\nWe have constructed a ﬁnite automaton M that accepts the language A.\\nTherefore, A is a regular language.\\n28\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.2.2\\nA second example of a ﬁnite automaton\\nDeﬁne the language A as\\nA = {w : w is a binary string containing 101 as a substring}.\\nAgain, we claim that A is a regular language. In other words, we claim that\\nthere exists a ﬁnite automaton M that accepts A, i.e., A = L(M).\\nThe ﬁnite automaton M will do the following, when reading an input\\nstring from left to right:\\n• It skips over all 0s, and stays in the start state.\\n• At the ﬁrst 1, it switches to the state “maybe the next two symbols are\\n01”.\\n– If the next symbol is 1, then it stays in the state “maybe the next\\ntwo symbols are 01”.\\n– On the other hand, if the next symbol is 0, then it switches to the\\nstate “maybe the next symbol is 1”.\\n∗If the next symbol is indeed 1, then it switches to the accept\\nstate (but keeps on reading until the end of the string).\\n∗On the other hand, if the next symbol is 0, then it switches\\nto the start state, and skips 0s until it reads 1 again.\\nBy deﬁning the following four states, this process will become clear:\\n• q1: M is in this state if the last symbol read was 1, but the substring\\n101 has not been read.\\n• q10: M is in this state if the last two symbols read were 10, but the\\nsubstring 101 has not been read.\\n• q101: M is in this state if the substring 101 has been read in the input\\nstring.\\n• q: In all other cases, M is in this state.\\nHere is the formal description of the ﬁnite automaton that accepts the\\nlanguage A:\\n• Q = {q, q1, q10, q101},\\n2.2.\\nDeterministic ﬁnite automata\\n29\\n• Σ = {0, 1},\\n• the start state is q,\\n• the set F of accept states is equal to F = {q101}, and\\n• the transition function δ is given by the following table:\\n0\\n1\\nq\\nq\\nq1\\nq1\\nq10\\nq1\\nq10\\nq\\nq101\\nq101\\nq101\\nq101\\nThe ﬁgure below gives the state diagram of the ﬁnite automaton M =\\n(Q, Σ, δ, q, F).\\nq\\nq1\\nq10\\nq101\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nThis ﬁnite automaton accepts the language A consisting of all binary\\nstrings that contain the substring 101. As an exercise, how would you obtain\\na ﬁnite automaton that accepts the complement of A, i.e., the language\\nconsisting of all binary strings that do not contain the substring 101?\\n2.2.3\\nA third example of a ﬁnite automaton\\nThe ﬁnite automata we have seen so far have exactly one accept state. In\\nthis section, we will see an example of a ﬁnite automaton having more accept\\nstates.\\n30\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right},\\nwhere {0, 1}∗is the set of all binary strings, including the empty string ϵ. We\\nclaim that A is a regular language. To prove this, we have to construct a ﬁnite\\nautomaton M such that A = L(M). At ﬁrst sight, it seems diﬃcult (or even\\nimpossible?) to construct such a ﬁnite automaton: How does the automaton\\n“know” that it has reached the third symbol from the right? It is, however,\\npossible to construct such an automaton. The main idea is to remember the\\nlast three symbols that have been read. Thus, the ﬁnite automaton has eight\\nstates qijk, where i, j, and k range over the two elements of {0, 1}. If the\\nautomaton is in state qijk, then the following hold:\\n• If M has read at least three symbols, then the three most recently read\\nsymbols are ijk.\\n• If M has read only two symbols, then these two symbols are jk; more-\\nover, i = 0.\\n• If M has read only one symbol, then this symbol is k; moreover, i =\\nj = 0.\\n• If M has not read any symbol, then i = j = k = 0.\\nThe start state is q000 and the set of accept states is {q100, q110, q101, q111}.\\nThe transition function of M is given by the following state diagram.\\nq000\\nq100\\nq010\\nq110\\nq001\\nq101\\nq011\\nq111\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n2.3.\\nRegular operations\\n31\\n2.3\\nRegular operations\\nIn this section, we deﬁne three operations on languages. Later, we will answer\\nthe question whether the set of all regular languages is closed under these\\noperations. Let A and B be two languages over the same alphabet.\\n1. The union of A and B is deﬁned as\\nA ∪B = {w : w ∈A or w ∈B}.\\n2. The concatenation of A and B is deﬁned as\\nAB = {ww′ : w ∈A and w′ ∈B}.\\nIn words, AB is the set of all strings obtained by taking an arbitrary\\nstring w in A and an arbitrary string w′ in B, and gluing them together\\n(such that w is to the left of w′).\\n3. The star of A is deﬁned as\\nA∗= {u1u2 . . . uk : k ≥0 and ui ∈A for all i = 1, 2, . . . , k}.\\nIn words, A∗is obtained by taking any ﬁnite number of strings in A, and\\ngluing them together. Observe that k = 0 is allowed; this corresponds\\nto the empty string ϵ. Thus, ϵ ∈A∗.\\nTo give an example, let A = {0, 01} and B = {1, 10}. Then\\nA ∪B = {0, 01, 1, 10},\\nAB = {01, 010, 011, 0110},\\nand\\nA∗= {ϵ, 0, 01, 00, 001, 010, 0101, 000, 0001, 00101, . . .}.\\nAs another example, if Σ = {0, 1}, then Σ∗is the set of all binary strings\\n(including the empty string). Observe that a string always has a ﬁnite length.\\nBefore we proceed, we give an alternative (and equivalent) deﬁnition of\\nthe star of the language A: Deﬁne\\nA0 = {ϵ}\\n32\\nChapter 2.\\nFinite Automata and Regular Languages\\nand, for k ≥1,\\nAk = AAk−1,\\ni.e., Ak is the concatenation of the two languages A and Ak−1. Then we have\\nA∗=\\n∞\\n[\\nk=0\\nAk.\\nTheorem 2.3.1 The set of regular languages is closed under the union op-\\neration, i.e., if A and B are regular languages over the same alphabet Σ, then\\nA ∪B is also a regular language.\\nProof.\\nSince A and B are regular languages, there are ﬁnite automata\\nM1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2) that accept A and B,\\nrespectively. In order to prove that A ∪B is regular, we have to construct a\\nﬁnite automaton M that accepts A ∪B. In other words, M must have the\\nproperty that for every string w ∈Σ∗,\\nM accepts w ⇔M1 accepts w or M2 accepts w.\\nAs a ﬁrst idea, we may think that M could do the following:\\n• Starting in the start state q1 of M1, M “runs” M1 on w.\\n• If, after having read w, M1 is in a state of F1, then w ∈A, thus\\nw ∈A ∪B and, therefore, M accepts w.\\n• On the other hand, if, after having read w, M1 is in a state that is not\\nin F1, then w ̸∈A and M “runs” M2 on w, starting in the start state\\nq2 of M2. If, after having read w, M2 is in a state of F2, then we know\\nthat w ∈B, thus w ∈A ∪B and, therefore, M accepts w. Otherwise,\\nwe know that w ̸∈A ∪B, and M rejects w.\\nThis idea does not work, because the ﬁnite automaton M can read the input\\nstring w only once. The correct approach is to run M1 and M2 simulta-\\nneously. We deﬁne the set Q of states of M to be the Cartesian product\\nQ1 × Q2. If M is in state (r1, r2), this means that\\n• if M1 would have read the input string up to this point, then it would\\nbe in state r1, and\\n2.3.\\nRegular operations\\n33\\n• if M2 would have read the input string up to this point, then it would\\nbe in state r2.\\nThis leads to the ﬁnite automaton M = (Q, Σ, δ, q, F), where\\n• Q = Q1 × Q2 = {(r1, r2) : r1 ∈Q1 and r2 ∈Q2}.\\nObserve that\\n|Q| = |Q1| × |Q2|, which is ﬁnite.\\n• Σ is the alphabet of A and B (recall that we assume that A and B are\\nlanguages over the same alphabet).\\n• The start state q of M is equal to q = (q1, q2).\\n• The set F of accept states of M is given by\\nF = {(r1, r2) : r1 ∈F1 or r2 ∈F2} = (F1 × Q2) ∪(Q1 × F2).\\n• The transition function δ : Q × Σ →Q is given by\\nδ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)),\\nfor all r1 ∈Q1, r2 ∈Q2, and a ∈Σ.\\nTo ﬁnish the proof, we have to show that this ﬁnite automaton M indeed\\naccepts the language A∪B. Intuitively, this should be clear from the discus-\\nsion above. The easiest way to give a formal proof is by using the extended\\ntransition functions δ1 and δ2. (The extended transition function has been\\ndeﬁned after Deﬁnition 2.2.4.) Here we go: Recall that we have to prove that\\nM accepts w ⇔M1 accepts w or M2 accepts w,\\ni.e,\\nM accepts w ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\nIn terms of the extended transition function δ of the transition function δ of\\nM, this becomes\\nδ((q1, q2), w) ∈F ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\n(2.1)\\nBy applying the deﬁnition of the extended transition function, as given after\\nDeﬁnition 2.2.4, to δ, it can be seen that\\nδ((q1, q2), w) = (δ1(q1, w), δ2(q2, w)).\\n34\\nChapter 2.\\nFinite Automata and Regular Languages\\nThe latter equality implies that (2.1) is true and, therefore, M indeed accepts\\nthe language A ∪B.\\nWhat about the closure of the regular languages under the concatenation\\nand star operations? It turns out that the regular languages are closed under\\nthese operations. But how do we prove this?\\nLet A and B be two regular languages, and let M1 and M2 be ﬁnite\\nautomata that accept A and B, respectively. How do we construct a ﬁnite\\nautomaton M that accepts the concatenation AB? Given an input string\\nu, M has to decide whether or not u can be broken into two strings w and\\nw′ (i.e., write u as u = ww′), such that w ∈A and w′ ∈B. In words, M\\nhas to decide whether or not u can be broken into two substrings, such that\\nthe ﬁrst substring is accepted by M1 and the second substring is accepted by\\nM2. The diﬃculty is caused by the fact that M has to make this decision by\\nscanning the string u only once. If u ∈AB, then M has to decide, during\\nthis single scan, where to break u into two substrings. Similarly, if u ̸∈AB,\\nthen M has to decide, during this single scan, that u cannot be broken into\\ntwo substrings such that the ﬁrst substring is in A and the second substring\\nis in B.\\nIt seems to be even more diﬃcult to prove that A∗is a regular language,\\nif A itself is regular. In order to prove this, we need a ﬁnite automaton that,\\nwhen given an arbitrary input string u, decides whether or not u can be\\nbroken into substrings such that each substring is in A. The problem is that,\\nif u ∈A∗, the ﬁnite automaton has to determine into how many substrings,\\nand where, the string u has to be broken; it has to do this during one single\\nscan of the string u.\\nAs we mentioned already, if A and B are regular languages, then both\\nAB and A∗are also regular. In order to prove these claims, we will introduce\\na more general type of ﬁnite automaton.\\nThe ﬁnite automata that we have seen so far are deterministic.\\nThis\\nmeans the following:\\n• If the ﬁnite automaton M is in state r and if it reads the symbol a,\\nthen M switches from state r to the uniquely deﬁned state δ(r, a).\\nFrom now on, we will call such a ﬁnite automaton a deterministic ﬁnite\\nautomaton (DFA). In the next section, we will deﬁne the notion of a nonde-\\nterministic ﬁnite automaton (NFA). For such an automaton, there are zero\\nor more possible states to switch to. At ﬁrst sight, nondeterministic ﬁnite\\n2.4.\\nNondeterministic ﬁnite automata\\n35\\nautomata seem to be more powerful than their deterministic counterparts.\\nWe will prove, however, that DFAs have the same power as NFAs. As we will\\nsee, using this fact, it will be easy to prove that the class of regular languages\\nis closed under the concatenation and star operations.\\n2.4\\nNondeterministic ﬁnite automata\\nWe start by giving three examples of nondeterministic ﬁnite automata. These\\nexamples will show the diﬀerence between this type of automata and the\\ndeterministic versions that we have considered in the previous sections. After\\nthese examples, we will give a formal deﬁnition of a nondeterministic ﬁnite\\nautomaton.\\n2.4.1\\nA ﬁrst example\\nConsider the following state diagram:\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,ε\\n1\\n0,1\\nYou will notice three diﬀerences with the ﬁnite automata that we have\\nseen until now. First, if the automaton is in state q1 and reads the symbol 1,\\nthen it has two options: Either it stays in state q1, or it switches to state q2.\\nSecond, if the automaton is in state q2, then it can switch to state q3 without\\nreading a symbol; this is indicated by the edge having the empty string ϵ as\\nlabel. Third, if the automaton is in state q3 and reads the symbol 0, then it\\ncannot continue.\\nLet us see what this automaton can do when it gets the string 010110 as\\ninput. Initially, the automaton is in the start state q1.\\n• Since the ﬁrst symbol in the input string is 0, the automaton stays in\\nstate q1 after having read this symbol.\\n• The second symbol is 1, and the automaton can either stay in state q1\\nor switch to state q2.\\n36\\nChapter 2.\\nFinite Automata and Regular Languages\\n– If the automaton stays in state q1, then it is still in this state after\\nhaving read the third symbol.\\n– If the automaton switches to state q2, then it again has two op-\\ntions:\\n∗Either read the third symbol in the input string, which is 0,\\nand switch to state q3,\\n∗or switch to state q3, without reading the third symbol.\\nIf we continue in this way, then we see that, for the input string 010110,\\nthere are seven possible computations. All these computations are given in\\nthe ﬁgure below.\\nq1\\nq1\\n0\\n1\\nq1\\nq1\\n0\\n1\\n1\\nq1\\nq2\\n1\\n1\\nq1\\nq2\\nq1\\n0\\n0\\nε\\nq3\\nq3\\nhang\\nhang\\nε\\nq3\\nq4\\n1\\n0\\nq4\\n1\\nq2\\n0\\nε\\nq3\\nq3\\nhang\\n1\\nq4\\n1\\nq4\\nq4\\n0\\nConsider the lowest path in the ﬁgure above:\\n• When reading the ﬁrst symbol, the automaton stays in state q1.\\n• When reading the second symbol, the automaton switches to state q2.\\n• The automaton does not read the third symbol (equivalently, it “reads”\\nthe empty string ϵ), and switches to state q3. At this moment, the\\n2.4.\\nNondeterministic ﬁnite automata\\n37\\nautomaton cannot continue: The third symbol is 0, but there is no\\nedge leaving q3 that is labeled 0, and there is no edge leaving q3 that\\nis labeled ϵ. Therefore, the computation hangs at this point.\\nFrom the ﬁgure, you can see that, out of the seven possible computations,\\nexactly two end in the accept state q4 (after the entire input string 010110 has\\nbeen read). We say that the automaton accepts the string 010110, because\\nthere is at least one computation that ends in the accept state.\\nNow consider the input string 010. In this case, there are three possible\\ncomputations:\\n1. q1\\n0→q1\\n1→q1\\n0→q1\\n2. q1\\n0→q1\\n1→q2\\n0→q3\\n3. q1\\n0→q1\\n1→q2\\nϵ→q3 →hang\\nNone of these computations ends in the accept state (after the entire input\\nstring 010 has been read). Therefore, we say that the automaton rejects the\\ninput string 010.\\nThe state diagram given above is an example of a nondeterministic ﬁnite\\nautomaton (NFA). Informally, an NFA accepts a string, if there exists at least\\none path in the state diagram that (i) starts in the start state, (ii) does not\\nhang before the entire string has been read, and (iii) ends in an accept state.\\nA string for which (i), (ii), and (iii) does not hold is rejected by the NFA.\\nThe NFA given above accepts all binary strings that contain 101 or 11 as\\na substring. All other binary strings are rejected.\\n2.4.2\\nA second example\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right}.\\nThe following state diagram deﬁnes an NFA that accepts all strings that are\\nin A, and rejects all strings that are not in A.\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,1\\n0,1\\n38\\nChapter 2.\\nFinite Automata and Regular Languages\\nThis NFA does the following. If it is in the start state q1 and reads the\\nsymbol 1, then it either stays in state q1 or it “guesses” that this symbol\\nis the third symbol from the right in the input string. In the latter case,\\nthe NFA switches to state q2, and then it “veriﬁes” that there are indeed\\nexactly two remaining symbols in the input string. If there are more than\\ntwo remaining symbols, then the NFA hangs (in state q4) after having read\\nthe next two symbols.\\nObserve how this guessing mechanism is used: The automaton can only\\nread the input string once, from left to right. Hence, it does not know when\\nit reaches the third symbol from the right. When the NFA reads a 1, it can\\nguess that this is the third symbol from the right; after having made this\\nguess, it veriﬁes whether or not the guess was correct.\\nIn Section 2.2.3, we have seen a DFA for the same language A. Observe\\nthat the NFA has a much simpler structure than the DFA.\\n2.4.3\\nA third example\\nConsider the following state diagram, which deﬁnes an NFA whose alphabet\\nis {0}.\\nε\\nε\\n0\\n0\\n0\\n0\\n0\\nThis NFA accepts the language\\nA = {0k : k ≡0 mod 2 or k ≡0 mod 3},\\nwhere 0k is the string consisting of k many 0s. (If k = 0, then 0k = ϵ.)\\nObserve that A is the union of the two languages\\nA1 = {0k : k ≡0 mod 2}\\n2.4.\\nNondeterministic ﬁnite automata\\n39\\nand\\nA2 = {0k : k ≡0 mod 3}.\\nThe NFA basically consists of two DFAs: one of these accepts A1, whereas the\\nother accepts A2. Given an input string w, the NFA has to decide whether\\nor not w ∈A, which is equivalent to deciding whether or not w ∈A1 or\\nw ∈A2. The NFA makes this decision in the following way: At the start, it\\n“guesses” whether (i) it is going to check whether or not w ∈A1 (i.e., the\\nlength of w is even), or (ii) it is going to check whether or not w ∈A2 (i.e.,\\nthe length of w is a multiple of 3). After having made the guess, it veriﬁes\\nwhether or not the guess was correct. If w ∈A, then there exists a way of\\nmaking the correct guess and verifying that w is indeed an element of A (by\\nending in an accept state). If w ̸∈A, then no matter which guess is made,\\nthe NFA will never end in an accept state.\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\nThe previous examples give you an idea what nondeterministic ﬁnite au-\\ntomata are and how they work. In this section, we give a formal deﬁnition\\nof these automata.\\nFor any alphabet Σ, we deﬁne Σϵ to be the set\\nΣϵ = Σ ∪{ϵ}.\\nRecall the notion of a power set: For any set Q, the power set of Q, denoted\\nby P(Q), is the set of all subsets of Q, i.e.,\\nP(Q) = {R : R ⊆Q}.\\nDeﬁnition 2.4.1 A nondeterministic ﬁnite automaton (NFA) is a 5-tuple\\nM = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σϵ →P(Q) is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\n40\\nChapter 2.\\nFinite Automata and Regular Languages\\nAs for DFAs, the transition function δ can be thought of as the “program”\\nof the ﬁnite automaton M = (Q, Σ, δ, q, F):\\n• Let r ∈Q, and let a ∈Σϵ. Then δ(r, a) is a (possibly empty) subset of\\nQ. If the NFA M is in state r, and if it reads a (where a may be the\\nempty string ϵ), then M can switch from state r to any state in δ(r, a).\\nIf δ(r, a) = ∅, then M cannot continue and the computation hangs.\\nThe example given in Section 2.4.1 is an NFA, where Q = {q1, q2, q3, q4},\\nΣ = {0, 1}, the start state is q1, the set of accept states is F = {q4}, and the\\ntransition function δ is given by the following table:\\n0\\n1\\nϵ\\nq1\\n{q1}\\n{q1, q2}\\n∅\\nq2\\n{q3}\\n∅\\n{q3}\\nq3\\n∅\\n{q4}\\n∅\\nq4\\n{q4}\\n{q4}\\n∅\\nDeﬁnition 2.4.2 Let M = (Q, Σ, δ, q, F) be an NFA, and let w ∈Σ∗. We\\nsay that M accepts w, if1\\n• w = ϵ and the start state q is an accept state, or\\n• there exists an integer m ≥1, such that w can be written as w =\\ny1y2 . . . ym, where yi ∈Σϵ for all i with 1 ≤i ≤m, and there exists a\\nsequence r0, r1, . . . , rm of states in Q, such that\\n– r0 = q,\\n– ri+1 ∈δ(ri, yi+1), for i = 0, 1, . . . , m −1, and\\n– rm ∈F.\\nOtherwise, we say that M rejects the string w.\\nThe NFA in the example in Section 2.4.1 accepts the string 01100. This\\ncan be seen by taking\\n• m = 6,\\n1Thanks to Antoine Vigneron for pointing out an error in a previous version of this\\ndeﬁnition.\\n2.5.\\nEquivalence of DFAs and NFAs\\n41\\n• w = 01ϵ100 = y1y2y3y4y5y6, and\\n• r0 = q1, r1 = q1, r2 = q2, r3 = q3, r4 = q4, r5 = q4, and r6 = q4.\\nDeﬁnition 2.4.3 Let M = (Q, Σ, δ, q, F) be an NFA. The language L(M)\\naccepted by M is deﬁned as\\nL(M) = {w ∈Σ∗: M accepts w }.\\n2.5\\nEquivalence of DFAs and NFAs\\nYou may have the impression that nondeterministic ﬁnite automata are more\\npowerful than deterministic ﬁnite automata. In this section, we will show\\nthat this is not the case.\\nThat is, we will prove that a language can be\\naccepted by a DFA if and only if it can be accepted by an NFA. In order\\nto prove this, we will show how to convert an arbitrary NFA to a DFA that\\naccepts the same language.\\nWhat about converting a DFA to an NFA? Well, there is (almost) nothing\\nto do, because a DFA is also an NFA. This is not quite true, because\\n• the transition function of a DFA maps a state and a symbol to a state,\\nwhereas\\n• the transition function of an NFA maps a state and a symbol to a set\\nof zero or more states.\\nThe formal conversion of a DFA to an NFA is done as follows: Let M =\\n(Q, Σ, δ, q, F) be a DFA. Recall that δ is a function δ : Q × Σ →Q. We\\ndeﬁne the function δ′ : Q × Σϵ →P(Q) as follows. For any r ∈Q and for\\nany a ∈Σϵ,\\nδ′(r, a) =\\n\\x1a {δ(r, a)}\\nif a ̸= ϵ,\\n∅\\nif a = ϵ.\\nThen N = (Q, Σ, δ′, q, F) is an NFA, whose behavior is exactly the same as\\nthat of the DFA M; the easiest way to see this is by observing that the state\\ndiagrams of M and N are equal. Therefore, we have L(M) = L(N).\\nIn the rest of this section, we will show how to convert an NFA to a DFA:\\nTheorem 2.5.1 Let N = (Q, Σ, δ, q, F) be a nondeterministic ﬁnite automa-\\nton. There exists a deterministic ﬁnite automaton M, such that L(M) =\\nL(N).\\n42\\nChapter 2.\\nFinite Automata and Regular Languages\\nProof.\\nRecall that the NFA N can (in general) perform more than one\\ncomputation on a given input string. The idea of the proof is to construct a\\nDFA M that runs all these diﬀerent computations simultaneously. (We have\\nseen this idea already in the proof of Theorem 2.3.1.) To be more precise,\\nthe DFA M will have the following property:\\n• the state that M is in after having read an initial part of the input\\nstring corresponds exactly to the set of all states that N can reach\\nafter having read the same part of the input string.\\nWe start by presenting the conversion for the case when N does not\\ncontain ϵ-transitions. In other words, the state diagram of N does not contain\\nany edge that has ϵ as a label. (Later, we will extend the conversion to the\\ngeneral case.) Let the DFA M be deﬁned as M = (Q′, Σ, δ′, q′, F ′), where\\n• the set Q′ of states is equal to Q′ = P(Q); observe that |Q′| = 2|Q|,\\n• the start state q′ is equal to q′ = {q}; so M has the “same” start state\\nas N,\\n• the set F ′ of accept states is equal to the set of all elements R of Q′\\nhaving the property that R contains at least one accept state of N, i.e.,\\nF ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• the transition function δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each\\nR ∈Q′ and for each a ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nδ(r, a).\\nLet us see what the transition function δ′ of M does. First observe that,\\nsince N is an NFA, δ(r, a) is a subset of Q. This implies that δ′(R, a) is the\\nunion of subsets of Q and, therefore, also a subset of Q. Hence, δ′(R, a) is\\nan element of Q′.\\nThe set δ(r, a) is equal to the set of all states of the NFA N that can be\\nreached from state r by reading the symbol a. We take the union of these\\nsets δ(r, a), where r ranges over all elements of R, to obtain the new set\\nδ′(R, a). This new set is the state that the DFA M reaches from state R, by\\nreading the symbol a.\\n2.5.\\nEquivalence of DFAs and NFAs\\n43\\nIn this way, we obtain the correspondence that was given in the beginning\\nof this proof.\\nAfter this warming-up, we can consider the general case. In other words,\\nfrom now on, we allow ϵ-transitions in the NFA N. The DFA M is deﬁned as\\nabove, except that the start state q′ and the transition function δ′ have to be\\nmodiﬁed. Recall that a computation of the NFA N consists of the following:\\n1. Start in the start state q and make zero or more ϵ-transitions.\\n2. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n3. Make zero or more ϵ-transitions.\\n4. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n5. Make zero or more ϵ-transitions.\\n6. Etc.\\nThe DFA M will simulate this computation in the following way:\\n• Simulate 1. in one single step. As we will see below, this simulation is\\nimplicitly encoded in the deﬁnition of the start state q′ of M.\\n• Simulate 2. and 3. in one single step.\\n• Simulate 4. and 5. in one single step.\\n• Etc.\\nThus, in one step, the DFA M simulates the reading of one “real” symbol of\\nΣ, followed by making zero or more ϵ-transitions.\\nTo formalize this, we need the notion of ϵ-closure. For any state r of the\\nNFA N, the ϵ-closure of r, denoted by Cϵ(r), is deﬁned to be the set of all\\nstates of N that can be reached from r, by making zero or more ϵ-transitions.\\nFor any state R of the DFA M (hence, R ⊆Q), we deﬁne\\nCϵ(R) =\\n[\\nr∈R\\nCϵ(r).\\n44\\nChapter 2.\\nFinite Automata and Regular Languages\\nHow do we deﬁne the start state q′ of the DFA M? Before the NFA N\\nreads its ﬁrst “real” symbol of Σ, it makes zero or more ϵ-transitions. In\\nother words, at the moment when N reads the ﬁrst symbol of Σ, it can be\\nin any state of Cϵ(q). Therefore, we deﬁne q′ to be\\nq′ = Cϵ(q) = Cϵ({q}).\\nHow do we deﬁne the transition function δ′ of the DFA M? Assume that\\nM is in state R, and reads the symbol a. At this moment, the NFA N would\\nhave been in any state r of R. By reading the symbol a, N can switch to\\nany state in δ(r, a), and then make zero or more ϵ-transitions. Hence, the\\nNFA can switch to any state in the set Cϵ(δ(r, a)). Based on this, we deﬁne\\nδ′(R, a) to be\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nTo summarize, the NFA N = (Q, Σ, δ, q, F) is converted to the DFA\\nM = (Q′, Σ, δ′, q′, F ′), where\\n• Q′ = P(Q),\\n• q′ = Cϵ({q}),\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nThe results proved until now can be summarized in the following theorem.\\nTheorem 2.5.2 Let A be a language. Then A is regular if and only if there\\nexists a nondeterministic ﬁnite automaton that accepts A.\\n2.5.1\\nAn example\\nConsider the NFA N = (Q, Σ, δ, q, F), where Q = {1, 2, 3}, Σ = {a, b}, q = 1,\\nF = {2}, and δ is given by the following table:\\n2.5.\\nEquivalence of DFAs and NFAs\\n45\\na\\nb\\nϵ\\n1\\n{3}\\n∅\\n{2}\\n2\\n{1}\\n∅\\n∅\\n3\\n{2}\\n{2, 3}\\n∅\\nThe state diagram of N is as follows:\\n1\\n2\\n3\\na\\na\\nǫ\\nb\\na, b\\nWe will show how to convert this NFA N to a DFA M that accepts the\\nsame language. Following the proof of Theorem 2.5.1, the DFA M is speciﬁed\\nby M = (Q′, Σ, δ′, q′, F ′), where each of the components is deﬁned below.\\n• Q′ = P(Q). Hence,\\nQ′ = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}.\\n• q′ = Cϵ({q}). Hence, the start state q′ of M is the set of all states of\\nN that can be reached from N’s start state q = 1, by making zero or\\nmore ϵ-transitions. We obtain\\nq′ = Cϵ({q}) = Cϵ({1}) = {1, 2}.\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅}. Hence, the accept states of M are those\\nstates that contain the accept state 2 of N. We obtain\\nF ′ = {{2}, {1, 2}, {2, 3}, {1, 2, 3}}.\\n46\\nChapter 2.\\nFinite Automata and Regular Languages\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nIn this example δ′ is given by\\nδ′(∅, a) = ∅\\nδ′(∅, b) = ∅\\nδ′({1}, a) = {3}\\nδ′({1}, b) = ∅\\nδ′({2}, a) = {1, 2}\\nδ′({2}, b) = ∅\\nδ′({3}, a) = {2}\\nδ′({3}, b) = {2, 3}\\nδ′({1, 2}, a) = {1, 2, 3}\\nδ′({1, 2}, b) = ∅\\nδ′({1, 3}, a) = {2, 3}\\nδ′({1, 3}, b) = {2, 3}\\nδ′({2, 3}, a) = {1, 2}\\nδ′({2, 3}, b) = {2, 3}\\nδ′({1, 2, 3}, a) = {1, 2, 3}\\nδ′({1, 2, 3}, b) = {2, 3}\\nThe state diagram of the DFA M is as follows:\\n2.5.\\nEquivalence of DFAs and NFAs\\n47\\n/0\\n{1}\\n{2}\\n{3}\\n{1,2}\\n{2,3}\\n{1,3}\\n{1,2,3}\\na,b\\nb\\na\\nb\\na\\na\\nb\\na,b\\na\\nb\\nb\\na\\nb\\na\\nWe make the following observations:\\n• The states {1} and {1, 3} do not have incoming edges. Therefore, these\\ntwo states cannot be reached from the start state {1, 2}.\\n• The state {3} has only one incoming edge; it comes from the state\\n{1}. Since {1} cannot be reached from the start state, {3} cannot be\\nreached from the start state.\\n• The state {2} has only one incoming edge; it comes from the state\\n{3}. Since {3} cannot be reached from the start state, {2} cannot be\\nreached from the start state.\\nHence, we can remove the four states {1}, {2}, {3}, and {1, 3}. The\\nresulting DFA accepts the same language as the DFA above.\\nThis leads\\nto the following state diagram, which depicts a DFA that accepts the same\\nlanguage as the NFA N:\\n48\\nChapter 2.\\nFinite Automata and Regular Languages\\n/0\\n{1,2}\\n{2,3}\\n{1,2,3}\\na,b\\na\\nb\\nb\\na\\nb\\na\\n2.6\\nClosure under the regular operations\\nIn Section 2.3, we have deﬁned the regular operations union, concatenation,\\nand star. We proved in Theorem 2.3.1 that the union of two regular lan-\\nguages is a regular language. We also explained why it is not clear that the\\nconcatenation of two regular languages is regular, and that the star of a reg-\\nular language is regular. In this section, we will see that the concept of NFA,\\ntogether with Theorem 2.5.2, can be used to give a simple proof of the fact\\nthat the regular languages are indeed closed under the regular operations.\\nWe start by giving an alternative proof of Theorem 2.3.1:\\nTheorem 2.6.1 The set of regular languages is closed under the union op-\\neration, i.e., if A1 and A2 are regular languages over the same alphabet Σ,\\nthen A1 ∪A2 is also a regular language.\\n2.6.\\nClosure under the regular operations\\n49\\nq1\\nM1\\nM2\\nq2\\nq0\\nq1\\nq2\\nε\\nε\\nM\\nFigure 2.1: The NFA M accepts L(M1) ∪L(M2).\\nProof.\\nSince A1 is regular, there is, by Theorem 2.5.2, an NFA M1 =\\n(Q1, Σ, δ1, q1, F1), such that A1 = L(M1). Similarly, there is an NFA M2 =\\n(Q2, Σ, δ2, q2, F2), such that A2 = L(M2). We may assume that Q1 ∩Q2 = ∅,\\nbecause otherwise, we can give new “names” to the states of Q1 and Q2.\\nFrom these two NFAs, we will construct an NFA M = (Q, Σ, δ, q0, F), such\\nthat L(M) = A1 ∪A2. The construction is illustrated in Figure 2.1. The\\nNFA M is deﬁned as follows:\\n1. Q = {q0} ∪Q1 ∪Q2, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = F1 ∪F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\n50\\nChapter 2.\\nFinite Automata and Regular Languages\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1,\\nδ2(r, a)\\nif r ∈Q2,\\n{q1, q2}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nTheorem 2.6.2 The set of regular languages is closed under the concatena-\\ntion operation, i.e., if A1 and A2 are regular languages over the same alphabet\\nΣ, then A1A2 is also a regular language.\\nProof.\\nLet M1 = (Q1, Σ, δ1, q1, F1) be an NFA, such that A1 = L(M1).\\nSimilarly, let M2 = (Q2, Σ, δ2, q2, F2) be an NFA, such that A2 = L(M2).\\nAs in the proof of Theorem 2.6.1, we may assume that Q1 ∩Q2 = ∅. We\\nwill construct an NFA M = (Q, Σ, δ, q0, F), such that L(M) = A1A2. The\\nconstruction is illustrated in Figure 2.2. The NFA M is deﬁned as follows:\\n1. Q = Q1 ∪Q2.\\n2. q0 = q1.\\n3. F = F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q2}\\nif r ∈F1 and a = ϵ,\\nδ2(r, a)\\nif r ∈Q2.\\nTheorem 2.6.3 The set of regular languages is closed under the star oper-\\nation, i.e., if A is a regular language, then A∗is also a regular language.\\n2.6.\\nClosure under the regular operations\\n51\\nq1\\nM1\\nM2\\nq2\\nq2\\nε\\nε\\nε\\nq0\\nM\\nFigure 2.2: The NFA M accepts L(M1)L(M2).\\nq1\\nN\\nq1\\nq0\\nε\\nε\\nε\\nε\\nM\\nFigure 2.3: The NFA M accepts (L(N))∗.\\nProof. Let Σ be the alphabet of A and let N = (Q1, Σ, δ1, q1, F1) be an\\nNFA, such that A = L(N). We will construct an NFA M = (Q, Σ, δ, q0, F),\\nsuch that L(M) = A∗. The construction is illustrated in Figure 2.3. The\\nNFA M is deﬁned as follows:\\n52\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. Q = {q0} ∪Q1, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = {q0} ∪F1. (Since ϵ ∈A∗, q0 has to be an accept state.)\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ,\\n{q1}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nIn the ﬁnal theorem of this section, we mention (without proof) two more\\nclosure properties of the regular languages:\\nTheorem 2.6.4 The set of regular languages is closed under the complement\\nand intersection operations:\\n1. If A is a regular language over the alphabet Σ, then the complement\\nA = {w ∈Σ∗: w ̸∈A}\\nis also a regular language.\\n2. If A1 and A2 are regular languages over the same alphabet Σ, then the\\nintersection\\nA1 ∩A2 = {w ∈Σ∗: w ∈A1 and w ∈A2}\\nis also a regular language.\\n2.7\\nRegular expressions\\nIn this section, we present regular expressions, which are a means to describe\\nlanguages. As we will see, the class of languages that can be described by\\nregular expressions coincides with the class of regular languages.\\n2.7.\\nRegular expressions\\n53\\nBefore formally deﬁning the notion of a regular expression, we give some\\nexamples. Consider the expression\\n(0 ∪1)01∗.\\nThe language described by this expression is the set of all binary strings\\n1. that start with either 0 or 1 (this is indicated by (0 ∪1)),\\n2. for which the second symbol is 0 (this is indicated by 0), and\\n3. that end with zero or more 1s (this is indicated by 1∗).\\nThat is, the language described by this expression is\\n{00, 001, 0011, 00111, . . . , 10, 101, 1011, 10111, . . .}.\\nHere are some more examples (in all cases, the alphabet is {0, 1}):\\n• The language {w : w contains exactly two 0s} is described by the ex-\\npression\\n1∗01∗01∗.\\n• The language {w : w contains at least two 0s} is described by the ex-\\npression\\n(0 ∪1)∗0(0 ∪1)∗0(0 ∪1)∗.\\n• The language {w : 1011 is a substring of w} is described by the ex-\\npression\\n(0 ∪1)∗1011(0 ∪1)∗.\\n• The language {w : the length of w is even} is described by the expres-\\nsion\\n((0 ∪1)(0 ∪1))∗.\\n• The language {w : the length of w is odd} is described by the expres-\\nsion\\n(0 ∪1) ((0 ∪1)(0 ∪1))∗.\\n• The language {1011, 0} is described by the expression\\n1011 ∪0.\\n54\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The language {w :\\nthe ﬁrst and last symbols of w are equal} is de-\\nscribed by the expression\\n0(0 ∪1)∗0 ∪1(0 ∪1)∗1 ∪0 ∪1.\\nAfter these examples, we give a formal (and inductive) deﬁnition of regular\\nexpressions:\\nDeﬁnition 2.7.1 Let Σ be a non-empty alphabet.\\n1. ϵ is a regular expression.\\n2. ∅is a regular expression.\\n3. For each a ∈Σ, a is a regular expression.\\n4. If R1 and R2 are regular expressions, then R1 ∪R2 is a regular expres-\\nsion.\\n5. If R1 and R2 are regular expressions, then R1R2 is a regular expression.\\n6. If R is a regular expression, then R∗is a regular expression.\\nYou can regard 1., 2., and 3. as being the “building blocks” of regular\\nexpressions.\\nItems 4., 5., and 6. give rules that can be used to combine\\nregular expressions into new (and “larger”) regular expressions. To give an\\nexample, we claim that\\n(0 ∪1)∗101(0 ∪1)∗\\nis a regular expression (where the alphabet Σ is equal to {0, 1}). In order\\nto prove this, we have to show that this expression can be “built” using the\\n“rules” given in Deﬁnition 2.7.1. Here we go:\\n• By 3., 0 is a regular expression.\\n• By 3., 1 is a regular expression.\\n• Since 0 and 1 are regular expressions, by 4., 0∪1 is a regular expression.\\n• Since 0∪1 is a regular expression, by 6., (0∪1)∗is a regular expression.\\n• Since 1 and 0 are regular expressions, by 5., 10 is a regular expression.\\n2.7.\\nRegular expressions\\n55\\n• Since 10 and 1 are regular expressions, by 5., 101 is a regular expression.\\n• Since (0 ∪1)∗and 101 are regular expressions, by 5., (0 ∪1)∗101 is a\\nregular expression.\\n• Since (0 ∪1)∗101 and (0 ∪1)∗are regular expressions, by 5., (0 ∪\\n1)∗101(0 ∪1)∗is a regular expression.\\nNext we deﬁne the language that is described by a regular expression:\\nDeﬁnition 2.7.2 Let Σ be a non-empty alphabet.\\n1. The regular expression ϵ describes the language {ϵ}.\\n2. The regular expression ∅describes the language ∅.\\n3. For each a ∈Σ, the regular expression a describes the language {a}.\\n4. Let R1 and R2 be regular expressions and let L1 and L2 be the lan-\\nguages described by them, respectively. The regular expression R1∪R2\\ndescribes the language L1 ∪L2.\\n5. Let R1 and R2 be regular expressions and let L1 and L2 be the languages\\ndescribed by them, respectively. The regular expression R1R2 describes\\nthe language L1L2.\\n6. Let R be a regular expression and let L be the language described by\\nit. The regular expression R∗describes the language L∗.\\nWe consider some examples:\\n• The regular expression (0∪ϵ)(1∪ϵ) describes the language {01, 0, 1, ϵ}.\\n• The regular expression 0 ∪ϵ describes the language {0, ϵ}, whereas the\\nregular expression 1∗describes the language {ϵ, 1, 11, 111, . . .}. There-\\nfore, the regular expression (0 ∪ϵ)1∗describes the language\\n{0, 01, 011, 0111, . . . , ϵ, 1, 11, 111, . . .}.\\nObserve that this language is also described by the regular expression\\n01∗∪1∗.\\n56\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The regular expression 1∗∅describes the empty language, i.e., the lan-\\nguage ∅. (You should convince yourself that this is correct.)\\n• The regular expression ∅∗describes the language {ϵ}.\\nDeﬁnition 2.7.3 Let R1 and R2 be regular expressions and let L1 and L2\\nbe the languages described by them, respectively. If L1 = L2 (i.e., R1 and\\nR2 describe the same language), then we will write R1 = R2.\\nHence, even though (0∪ϵ)1∗and 01∗∪1∗are diﬀerent regular expressions,\\nwe write\\n(0 ∪ϵ)1∗= 01∗∪1∗,\\nbecause they describe the same language.\\nIn Section 2.8.2, we will show that every regular language can be described\\nby a regular expression. The proof of this fact is purely algebraic and uses\\nthe following algebraic identities involving regular expressions.\\nTheorem 2.7.4 Let R1, R2, and R3 be regular expressions. The following\\nidentities hold:\\n1. R1∅= ∅R1 = ∅.\\n2. R1ϵ = ϵR1 = R1.\\n3. R1 ∪∅= ∅∪R1 = R1.\\n4. R1 ∪R1 = R1.\\n5. R1 ∪R2 = R2 ∪R1.\\n6. R1(R2 ∪R3) = R1R2 ∪R1R3.\\n7. (R1 ∪R2)R3 = R1R3 ∪R2R3.\\n8. R1(R2R3) = (R1R2)R3.\\n9. ∅∗= ϵ.\\n10. ϵ∗= ϵ.\\n11. (ϵ ∪R1)∗= R∗\\n1.\\n2.8.\\nEquivalence of regular expressions and regular languages 57\\n12. (ϵ ∪R1)(ϵ ∪R1)∗= R∗\\n1.\\n13. R∗\\n1(ϵ ∪R1) = (ϵ ∪R1)R∗\\n1 = R∗\\n1.\\n14. R∗\\n1R2 ∪R2 = R∗\\n1R2.\\n15. R1(R2R1)∗= (R1R2)∗R1.\\n16. (R1 ∪R2)∗= (R∗\\n1R2)∗R∗\\n1 = (R∗\\n2R1)∗R∗\\n2.\\nWe will not present the (boring) proofs of these identities, but urge you\\nto convince yourself informally that they make perfect sense. To give an\\nexample, we mentioned above that\\n(0 ∪ϵ)1∗= 01∗∪1∗.\\nWe can verify this identity in the following way:\\n(0 ∪ϵ)1∗\\n=\\n01∗∪ϵ1∗\\n(by identity 7)\\n=\\n01∗∪1∗\\n(by identity 2)\\n2.8\\nEquivalence of regular expressions and reg-\\nular languages\\nIn the beginning of Section 2.7, we mentioned the following result:\\nTheorem 2.8.1 Let L be a language. Then L is regular if and only if there\\nexists a regular expression that describes L.\\nThe proof of this theorem consists of two parts:\\n• In Section 2.8.1, we will prove that every regular expression describes\\na regular language.\\n• In Section 2.8.2, we will prove that every DFA M can be converted to\\na regular expression that describes the language L(M).\\nThese two results will prove Theorem 2.8.1.\\n58\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.8.1\\nEvery regular expression describes a regular lan-\\nguage\\nLet R be an arbitrary regular expression over the alphabet Σ. We will prove\\nthat the language described by R is a regular language. The proof is by\\ninduction on the structure of R (i.e., by induction on the way R is “built”\\nusing the “rules” given in Deﬁnition 2.7.1).\\nThe ﬁrst base case: Assume that R = ϵ.\\nThen R describes the lan-\\nguage {ϵ}. In order to prove that this language is regular, it suﬃces, by\\nTheorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q, F) that accepts this\\nlanguage. This NFA is obtained by deﬁning Q = {q}, q is the start state,\\nF = {q}, and δ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state\\ndiagram of M:\\nq\\nThe second base case: Assume that R = ∅. Then R describes the language\\n∅. In order to prove that this language is regular, it suﬃces, by Theorem 2.5.2,\\nto construct an NFA M = (Q, Σ, δ, q, F) that accepts this language. This\\nNFA is obtained by deﬁning Q = {q}, q is the start state, F = ∅, and\\nδ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state diagram of M:\\nq\\nThe third base case: Let a ∈Σ and assume that R = a. Then R describes\\nthe language {a}. In order to prove that this language is regular, it suﬃces,\\nby Theorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q1, F) that accepts\\nthis language. This NFA is obtained by deﬁning Q = {q1, q2}, q1 is the start\\nstate, F = {q2}, and\\nδ(q1, a)\\n=\\n{q2},\\nδ(q1, b)\\n=\\n∅for all b ∈Σϵ \\\\ {a},\\nδ(q2, b)\\n=\\n∅for all b ∈Σϵ.\\nThe ﬁgure below gives the state diagram of M:\\n2.8.\\nEquivalence of regular expressions and regular languages 59\\nq1\\nq2\\na\\nThe ﬁrst case of the induction step: Assume that R = R1 ∪R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1 ∪L2, which, by Theorem 2.6.1, is regular.\\nThe second case of the induction step: Assume that R = R1R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1L2, which, by Theorem 2.6.2, is regular.\\nThe third case of the induction step: Assume that R = (R1)∗, where\\nR1 is a regular expression.\\nLet L1 be the language described by R1 and\\nassume that L1 is regular. Then R describes the language (L1)∗, which, by\\nTheorem 2.6.3, is regular.\\nThis concludes the proof of the claim that every regular expression de-\\nscribes a regular language.\\nTo give an example, consider the regular expression\\n(ab ∪a)∗,\\nwhere the alphabet is {a, b}. We will prove that this regular expression de-\\nscribes a regular language, by constructing an NFA that accepts the language\\ndescribed by this regular expression. Observe how the regular expression is\\n“built”:\\n• Take the regular expressions a and b, and combine them into the regular\\nexpression ab.\\n• Take the regular expressions ab and a, and combine them into the\\nregular expression ab ∪a.\\n• Take the regular expression ab ∪a, and transform it into the regular\\nexpression (ab ∪a)∗.\\nFirst, we construct an NFA M1 that accepts the language described by\\nthe regular expression a:\\n60\\nChapter 2.\\nFinite Automata and Regular Languages\\na\\nM1\\nNext, we construct an NFA M2 that accepts the language described by\\nthe regular expression b:\\nM2\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.2 to\\nM1 and M2. This gives an NFA M3 that accepts the language described by\\nthe regular expression ab:\\nM3\\na\\nε\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.1 to\\nM3 and M1. This gives an NFA M4 that accepts the language described by\\nthe regular expression ab ∪a:\\na\\nε\\nb\\na\\nε\\nε\\nM4\\nFinally, we apply the construction given in the proof of Theorem 2.6.3\\nto M4. This gives an NFA M5 that accepts the language described by the\\nregular expression (ab ∪a)∗:\\n2.8.\\nEquivalence of regular expressions and regular languages 61\\na\\nε\\nb\\na\\nε\\nε\\nε\\nε\\nε\\nM5\\n2.8.2\\nConverting a DFA to a regular expression\\nIn this section, we will prove that every DFA M can be converted to a regular\\nexpression that describes the language L(M). In order to prove this result,\\nwe need to solve recurrence relations involving languages.\\nSolving recurrence relations\\nLet Σ be an alphabet, let B and C be “known” languages in Σ∗such that\\nϵ ̸∈B, and let L be an “unknown” language such that\\nL = BL ∪C.\\nCan we “solve” this equation for L? That is, can we express L in terms of\\nB and C?\\nConsider an arbitrary string u in L. We are going to determine how u\\nlooks like. Since u ∈L and L = BL ∪C, we know that u is a string in\\nBL ∪C. Hence, there are two possibilities for u.\\n1. u is an element of C.\\n2. u is an element of BL. In this case, there are strings b ∈B and v ∈L\\nsuch that u = bv. Since ϵ ̸∈B, we have b ̸= ϵ and, therefore, |v| < |u|.\\n(Recall that |v| denotes the length, i.e., the number of symbols, of the\\nstring v.) Since v is a string in L, which is equal to BL ∪C, v is a\\nstring in BL ∪C. Hence, there are two possibilities for v.\\n62\\nChapter 2.\\nFinite Automata and Regular Languages\\n(a) v is an element of C. In this case,\\nu = bv, where b ∈B and v ∈C; thus, u ∈BC.\\n(b) v is an element of BL. In this case, there are strings b′ ∈B and\\nw ∈L such that v = b′w. Since ϵ ̸∈B, we have b′ ̸= ϵ and,\\ntherefore, |w| < |v|. Since w is a string in L, which is equal to\\nBL∪C, w is a string in BL∪C. Hence, there are two possibilities\\nfor w.\\ni. w is an element of C. In this case,\\nu = bb′w, where b, b′ ∈B and w ∈C; thus, u ∈BBC.\\nii. w is an element of BL. In this case, there are strings b′′ ∈B\\nand x ∈L such that w = b′′x. Since ϵ ̸∈B, we have b′′ ̸= ϵ\\nand, therefore, |x| < |w|. Since x is a string in L, which is\\nequal to BL ∪C, x is a string in BL ∪C. Hence, there are\\ntwo possibilities for x.\\nA. x is an element of C. In this case,\\nu = bb′b′′x, where b, b′, b′′ ∈B and x ∈C; thus, u ∈BBBC.\\nB. x is an element of BL. Etc., etc.\\nThis process hopefully convinces you that any string u in L can be written\\nas the concatenation of zero or more strings in B, followed by one string in\\nC. In fact, L consists of exactly those strings having this property:\\nLemma 2.8.2 Let Σ be an alphabet, and let B, C, and L be languages in\\nΣ∗such that ϵ ̸∈B and\\nL = BL ∪C.\\nThen\\nL = B∗C.\\nProof. First, we show that B∗C ⊆L. Let u be an arbitrary string in B∗C.\\nThen u is the concatenation of k strings of B, for some k ≥0, followed by\\none string of C. We proceed by induction on k.\\nThe base case is when k = 0. In this case, u is a string in C. Hence, u is\\na string in BL ∪C. Since BL ∪C = L, it follows that u is a string in L.\\n2.8.\\nEquivalence of regular expressions and regular languages 63\\nNow let k ≥1. Then we can write u = vwc, where v is a string in B,\\nw is the concatenation of k −1 strings of B, and c is a string of C. Deﬁne\\ny = wc. Observe that y is the concatenation of k −1 strings of B followed\\nby one string of C. Therefore, by induction, the string y is an element of L.\\nHence, u = vy, where v is a string in B and y is a string in L. This shows\\nthat u is a string in BL. Hence, u is a string in BL ∪C. Since BL ∪C = L,\\nit follows that u is a string in L. This completes the proof that B∗C ⊆L.\\nIt remains to show that L ⊆B∗C. Let u be an arbitrary string in L,\\nand let ℓbe its length (i.e., ℓis the number of symbols in u). We prove by\\ninduction on ℓthat u is a string in B∗C.\\nThe base case is when ℓ= 0. Then u = ϵ. Since u ∈L and L = BL ∪C,\\nu is a string in BL ∪C. Since ϵ ̸∈B, u cannot be a string in BL. Hence, u\\nmust be a string in C. Since C ⊆B∗C, it follows that u is a string in B∗C.\\nLet ℓ≥1. If u is a string in C, then u is a string in B∗C and we are done.\\nSo assume that u is not a string in C. Since u ∈L and L = BL ∪C, u is a\\nstring in BL. Hence, there are strings b ∈B and v ∈L such that u = bv.\\nSince ϵ ̸∈B, the length of b is at least one; hence, the length of v is less than\\nthe length of u. By induction, v is a string in B∗C. Hence, u = bv, where\\nb ∈B and v ∈B∗C. This shows that u ∈B(B∗C). Since B(B∗C) ⊆B∗C,\\nit follows that u ∈B∗C.\\nNote that Lemma 2.8.2 holds for any language B that does not contain\\nthe empty string ϵ. As an example, assume that B = ∅. Then the language\\nL satisﬁes the equation\\nL = BL ∪C = ∅L ∪C.\\nUsing Theorem 2.7.4, this equation becomes\\nL = ∅∪C = C.\\nWe now show that Lemma 2.8.2 also implies that L = C: Since ϵ ̸∈B,\\nLemma 2.8.2 implies that L = B∗C, which, using Theorem 2.7.4, becomes\\nL = B∗C = ∅∗C = ϵC = C.\\nThe conversion\\nWe will now use Lemma 2.8.2 to prove that every DFA can be converted to\\na regular expression.\\n64\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet M = (Q, Σ, δ, q, F) be an arbitrary deterministic ﬁnite automaton.\\nWe will show that there exists a regular expression that describes the lan-\\nguage L(M).\\nFor each state r ∈Q, we deﬁne\\nLr = {w ∈Σ∗:\\nthe path in the state diagram of M that starts\\nin state r and that corresponds to w ends in a\\nstate of F }.\\nIn words, Lr is the language accepted by M, if r were the start state.\\nWe will show that each such language Lr can be described by a regular\\nexpression. Since L(M) = Lq, this will prove that L(M) can be described by\\na regular expression.\\nThe basic idea is to set up equations for the languages Lr, which we then\\nsolve using Lemma 2.8.2. We claim that\\nLr =\\n[\\na∈Σ\\na · Lδ(r,a)\\nif r ̸∈F.\\n(2.2)\\nWhy is this true? Let w be a string in Lr. Then the path P in the state\\ndiagram of M that starts in state r and that corresponds to w ends in a\\nstate of F. Since r ̸∈F, this path contains at least one edge. Let r′ be the\\nstate that follows the ﬁrst state (i.e., r) of P. Then r′ = δ(r, b) for some\\nsymbol b ∈Σ. Hence, b is the ﬁrst symbol of w. Write w = bv, where v is\\nthe remaining part of w. Then the path P ′ = P \\\\ {r} in the state diagram\\nof M that starts in state r′ and that corresponds to v ends in a state of F.\\nTherefore, v ∈Lr′ = Lδ(r,b). Hence,\\nw ∈b · Lδ(r,b) ⊆\\n[\\na∈Σ\\na · Lδ(r,a).\\nConversely, let w be a string in S\\na∈Σ a · Lδ(r,a). Then there is a symbol b ∈Σ\\nand a string v ∈Lδ(r,b) such that w = bv. Let P ′ be the path in the state\\ndiagram of M that starts in state δ(r, b) and that corresponds to v. Since\\nv ∈Lδ(r,b), this path ends in a state of F. Let P be the path in the state\\ndiagram of M that starts in r, follows the edge to δ(r, b), and then follows P ′.\\nThis path P corresponds to w and ends in a state of F. Therefore, w ∈Lr.\\nThis proves the correctness of (2.2).\\n2.8.\\nEquivalence of regular expressions and regular languages 65\\nSimilarly, we can prove that\\nLr = ϵ ∪\\n [\\na∈Σ\\na · Lδ(r,a)\\n!\\nif r ∈F.\\n(2.3)\\nSo we now have a set of equations in the “unknowns” Lr, for r ∈Q. The\\nnumber of equations is equal to the size of Q. In other words, the number\\nof equations is equal to the number of unknowns. The regular expression for\\nL(M) = Lq is obtained by solving these equations using Lemma 2.8.2.\\nOf course, we have to convince ourselves that these equations have a so-\\nlution for any given DFA. Before we deal with this issue, we give an example.\\nAn example\\nConsider the deterministic ﬁnite automaton M = (Q, Σ, δ, q0, F), where Q =\\n{q0, q1, q2}, Σ = {a, b}, q0 is the start state, F = {q2}, and δ is given in the\\nstate diagram below. We show how to obtain the regular expression that\\ndescribes the language accepted by M.\\nq0\\nq1\\nq2\\na\\na\\na\\nb\\nb\\nb\\nFor this case, (2.2) and (2.3) give the following equations:\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nLq0\\n=\\na · Lq0 ∪b · Lq2\\nLq1\\n=\\na · Lq0 ∪b · Lq1\\nLq2\\n=\\nϵ ∪a · Lq1 ∪b · Lq0\\n66\\nChapter 2.\\nFinite Automata and Regular Languages\\nIn the third equation, Lq2 is expressed in terms of Lq0 and Lq1. Hence, if we\\nsubstitute the third equation into the ﬁrst one, and use Theorem 2.7.4, then\\nwe get\\nLq0\\n=\\na · Lq0 ∪b · (ϵ ∪a · Lq1 ∪b · Lq0)\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b.\\nWe obtain the following set of equations.\\n\\x1a Lq0\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b\\nLq1\\n=\\nb · Lq1 ∪a · Lq0\\nLet L = Lq1, B = b, and C = a · Lq0. Then ϵ ̸∈B and the second equation\\nreads L = BL ∪C. Hence, by Lemma 2.8.2,\\nLq1 = L = B∗C = b∗a · Lq0.\\nIf we substitute Lq1 into the ﬁrst equation, then we get (again using Theo-\\nrem 2.7.4)\\nLq0\\n=\\n(a ∪bb) · Lq0 ∪ba · b∗a · Lq0 ∪b\\n=\\n(a ∪bb ∪bab∗a)Lq0 ∪b.\\nAgain applying Lemma 2.8.2, this time with L = Lq0, B = a∪bb∪bab∗a and\\nC = b, gives\\nLq0 = (a ∪bb ∪bab∗a)∗b.\\nThus, the regular expression that describes the language accepted by M is\\n(a ∪bb ∪bab∗a)∗b.\\nCompleting the correctness of the conversion\\nIt remains to prove that, for any DFA, the system of equations (2.2) and (2.3)\\ncan be solved. This will follow from the following (more general) lemma.\\n(You should verify that the equations (2.2) and (2.3) are in the form as\\nspeciﬁed in this lemma.)\\n2.8.\\nEquivalence of regular expressions and regular languages 67\\nLemma 2.8.3 Let n ≥1 be an integer and, for 1 ≤i ≤n and 1 ≤j ≤n,\\nlet Bij and Ci be regular expressions such that ϵ ̸∈Bij. Let L1, L2, . . . , Ln be\\nlanguages that satisfy\\nLi =\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci for 1 ≤i ≤n.\\nThen L1 can be expressed as a regular expression only involving the regular\\nexpressions Bij and Ci.\\nProof. The proof is by induction on n. The base case is when n = 1. In\\nthis case, we have\\nL1 = B11L1 ∪C1.\\nSince ϵ ̸∈B11, it follows from Lemma 2.8.2 that L1 = B∗\\n11C1. This proves\\nthe base case.\\nLet n ≥2 and assume the lemma is true for n −1. We have\\nLn\\n=\\n n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n=\\nBnnLn ∪\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn.\\nSince ϵ ̸∈Bnn, it follows from Lemma 2.8.2 that\\nLn\\n=\\nB∗\\nnn\\n  n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n!\\n=\\nB∗\\nnn\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪B∗\\nnnCn\\n=\\n n−1\\n[\\nj=1\\nB∗\\nnnBnjLj\\n!\\n∪B∗\\nnnCn\\nBy substituting this equation for Ln into the equations for Li, 1 ≤i ≤n −1,\\n68\\nChapter 2.\\nFinite Automata and Regular Languages\\nwe obtain\\nLi\\n=\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\nBinLn ∪\\n n−1\\n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\n n−1\\n[\\nj=1\\n(BinB∗\\nnnBnj ∪Bij) Lj\\n!\\n∪BinB∗\\nnnCn ∪Ci.\\nThus, we have obtained n −1 equations in L1, L2, . . . , Ln−1.\\nSince ϵ ̸∈\\nBinB∗\\nnnBnj ∪Bij, it follows from the induction hypothesis that L1 can be\\nexpressed as a regular expression only involving the regular expressions Bij\\nand Ci.\\n2.9\\nThe pumping lemma and nonregular lan-\\nguages\\nIn the previous sections, we have seen that the class of regular languages is\\nclosed under various operations, and that these languages can be described by\\n(deterministic or nondeterministic) ﬁnite automata and regular expressions.\\nThese properties helped in developing techniques for showing that a language\\nis regular. In this section, we will present a tool that can be used to prove\\nthat certain languages are not regular. Observe that for a regular language,\\n1. the amount of memory that is needed to determine whether or not a\\ngiven string is in the language is ﬁnite and independent of the length\\nof the string, and\\n2. if the language consists of an inﬁnite number of strings, then this lan-\\nguage should contain inﬁnite subsets having a fairly repetitive struc-\\nture.\\nIntuitively, languages that do not follow 1. or 2. should be nonregular. For\\nexample, consider the language\\n{0n1n : n ≥0}.\\n2.9.\\nThe pumping lemma and nonregular languages\\n69\\nThis language should be nonregular, because it seems unlikely that a DFA can\\nremember how many 0s it has seen when it has reached the border between\\nthe 0s and the 1s. Similarly the language\\n{0n : n is a prime number}\\nshould be nonregular, because the prime numbers do not seem to have any\\nrepetitive structure that can be used by a DFA. To be more rigorous about\\nthis, we will establish a property that all regular languages must possess.\\nThis property is called the pumping lemma. If a language does not have this\\nproperty, then it must be nonregular.\\nThe pumping lemma states that any suﬃciently long string in a regular\\nlanguage can be pumped, i.e., there is a section in that string that can be\\nrepeated any number of times, so that the resulting strings are all in the\\nlanguage.\\nTheorem 2.9.1 (Pumping Lemma for Regular Languages) Let A be\\na regular language. Then there exists an integer p ≥1, called the pumping\\nlength, such that the following holds: Every string s in A, with |s| ≥p, can\\nbe written as s = xyz, such that\\n1. y ̸= ϵ (i.e., |y| ≥1),\\n2. |xy| ≤p, and\\n3. for all i ≥0, xyiz ∈A.\\nIn words, the pumping lemma states that by replacing the portion y in s\\nby zero or more copies of it, the resulting string is still in the language A.\\nProof. Let Σ be the alphabet of A. Since A is a regular language, there\\nexists a DFA M = (Q, Σ, δ, q, F) that accepts A. We deﬁne p to be the\\nnumber of states in Q.\\nLet s = s1s2 . . . sn be an arbitrary string in A such that n ≥p. Deﬁne\\nr1 = q, r2 = δ(r1, s1), r3 = δ(r2, s2), . . ., rn+1 = δ(rn, sn). Thus, when the\\nDFA M reads the string s from left to right, it visits the states r1, r2, . . . , rn+1.\\nSince s is a string in A, we know that rn+1 belongs to F.\\nConsider the ﬁrst p + 1 states r1, r2, . . . , rp+1 in this sequence. Since the\\nnumber of states of M is equal to p, the pigeonhole principle implies that\\nthere must be a state that occurs twice in this sequence. That is, there are\\nindices j and ℓsuch that 1 ≤j < ℓ≤p + 1 and rj = rℓ.\\n70\\nChapter 2.\\nFinite Automata and Regular Languages\\nq = r1\\nrn+1\\nr j = rℓ\\nread x\\nread y\\nread z\\nWe deﬁne x = s1s2 . . . sj−1, y = sj . . . sℓ−1, and z = sℓ. . . sn. Since j < ℓ,\\nwe have y ̸= ϵ, proving the ﬁrst claim in the theorem. Since ℓ≤p + 1, we\\nhave |xy| = ℓ−1 ≤p, proving the second claim in the theorem. To see that\\nthe third claim also holds, recall that the string s = xyz is accepted by M.\\nWhile reading x, M moves from the start state q to state rj. While reading\\ny, it moves from state rj to state rℓ= rj, i.e., after having read y, M is again\\nin state rj. While reading z, M moves from state rj to the accept state rn+1.\\nTherefore, the substring y can be repeated any number i ≥0 of times, and\\nthe corresponding string xyiz will still be accepted by M. It follows that\\nxyiz ∈A for all i ≥0.\\n2.9.1\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {0n1n : n ≥0}.\\nWe will prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. It is clear\\nthat s ∈A and |s| = 2p ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that, since |xy| ≤p, the string y contains only 0s. Moreover,\\nsince y ̸= ϵ, y contains at least one 0. But now we are in trouble: None of\\nthe strings xy0z = xz, xy2z = xyyz, xy3z = xyyyz, . . . , is contained in A.\\nHowever, by the pumping lemma, all these strings must be in A. Hence, we\\nhave a contradiction and we conclude that A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n71\\nSecond example\\nConsider the language\\nA = {w ∈{0, 1}∗: the number of 0s in w equals the number of 1s in w}.\\nAgain, we prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. Then s ∈A\\nand |s| = 2p ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz contains more 0s than 1s,\\nwhich implies that this string is not contained in A. But, by the pumping\\nlemma, this string is contained in A. This is a contradiction and, therefore,\\nA is not a regular language.\\nThird example\\nConsider the language\\nA = {ww : w ∈{0, 1}∗}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p10p1. Then s ∈A\\nand |s| = 2p + 2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz is not contained in A. But,\\nby the pumping lemma, this string is contained in A. This is a contradiction\\nand, therefore, A is not a regular language.\\nYou should convince yourself that by choosing s = 02p (which is a string\\nin A whose length is at least p), we do not obtain a contradiction. The reason\\nis that the string y may have an even length. Thus, 02p is the “wrong” string\\nfor showing that A is not regular. By choosing s = 0p10p1, we do obtain\\na contradiction; thus, this is the “correct” string for showing that A is not\\nregular.\\n72\\nChapter 2.\\nFinite Automata and Regular Languages\\nFourth example\\nConsider the language\\nA = {0m1n : m > n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p+11p. Then s ∈A\\nand |s| = 2p + 1 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Consider the string xy0z = xz. The number of 1s in this string\\nis equal to p, whereas the number of 0s is at most equal to p. Therefore, the\\nstring xy0z is not contained in A. But, by the pumping lemma, this string\\nis contained in A. This is a contradiction and, therefore, A is not a regular\\nlanguage.\\nFifth example\\nConsider the language\\nA = {1n2 : n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 1p2. Then s ∈A\\nand |s| = p2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that\\n|s| = |xyz| = p2\\nand\\n|xy2z| = |xyyz| = |xyz| + |y| = p2 + |y|.\\nSince |xy| ≤p, we have |y| ≤p. Since y ̸= ϵ, we have |y| ≥1. It follows that\\np2 < |xy2z| ≤p2 + p < (p + 1)2.\\nHence, the length of the string xy2z is strictly between two consecutive\\nsquares.\\nIt follows that this length is not a square and, therefore, xy2z\\nis not contained in A. But, by the pumping lemma, this string is contained\\nin A. This is a contradiction and, therefore, A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n73\\nSixth example\\nConsider the language\\nA = {1n : n is a prime number}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Let n ≥p be a prime number, and consider\\nthe string s = 1n. Then s ∈A and |s| = n ≥p. By the pumping lemma, s\\ncan be written as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nLet k be the integer such that y = 1k. Since y ̸= ϵ, we have k ≥1. For\\neach i ≥0, n + (i −1)k is a prime number, because xyiz = 1n+(i−1)k ∈A.\\nFor i = n + 1, however, we have\\nn + (i −1)k = n + nk = n(1 + k),\\nwhich is not a prime number, because n ≥2 and 1 + k ≥2.\\nThis is a\\ncontradiction and, therefore, A is not a regular language.\\nSeventh example\\nConsider the language\\nA = {w ∈{0, 1}∗:\\nthe number of occurrences of 01 in w is equal to\\nthe number of occurrences of 10 in w }.\\nSince this language has the same ﬂavor as the one in the second example,\\nwe may suspect that A is not a regular language. This is, however, not true:\\nAs we will show, A is a regular language.\\nThe key property is the following one: Let w be an arbitrary string in\\n{0, 1}∗. Then\\nthe absolute value of the number of occurrences of 01 in w minus\\nthe number of occurrences of 10 in w is at most one.\\nThis property holds, because between any two consecutive occurrences of\\n01, there must be exactly one occurrence of 10. Similarly, between any two\\nconsecutive occurrences of 10, there must be exactly one occurrence of 01.\\nWe will construct a DFA that accepts A. This DFA uses the following\\nﬁve states:\\n74\\nChapter 2.\\nFinite Automata and Regular Languages\\n• q: start state; no symbol has been read.\\n• q01: the last symbol read was 1; in the part of the string read so far, the\\nnumber of occurrences of 01 is one more than the number of occurrences\\nof 10.\\n• q10: the last symbol read was 0; in the part of the string read so far, the\\nnumber of occurrences of 10 is one more than the number of occurrences\\nof 01.\\n• q0\\nequal: the last symbol read was 0; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\n• q1\\nequal: the last symbol read was 1; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\nThe set of accept states is equal to {q, q0\\nequal, q1\\nequal}. The state diagram of\\nthe DFA is given below.\\nq0\\nequal\\nq1\\nequal\\nq01\\nq10\\nq\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\nIn fact, the key property mentioned above implies that the language A\\nconsists of the empty string ϵ and all non-empty binary strings that start\\n2.9.\\nThe pumping lemma and nonregular languages\\n75\\nand end with the same symbol. As a result, A is the language described by\\nthe regular expression\\nϵ ∪0 ∪1 ∪0(0 ∪1)∗0 ∪1(0 ∪1)∗1.\\nThis gives an alternative proof for the fact that A is a regular language.\\nEighth example\\nConsider the language\\nL = {w ∈{0, 1}∗: w is the binary representation of a prime number}.\\nWe assume that for any positive integer, the leftmost bit in its binary repre-\\nsentation is 1. In other words, we assume that there are no 0’s added to the\\nleft of such a binary representation. Thus,\\nL = {10, 11, 101, 111, 1011, 1101, 10001, . . .}.\\nWe will prove that L is not a regular language.\\nAssume that L is a regular language. Let p ≥1 be the pumping length.\\nLet N > 2p be a prime number and let s ∈{0, 1}∗be the binary representa-\\ntion of N. Observe that |s| ≥p + 1. Also, the leftmost and rightmost bits of\\ns are 1.\\nSince s ∈L and |s| ≥p + 1 ≥p, the Pumping Lemma implies that we\\ncan write s = xyz, such that\\n1. |y| ≥1,\\n2. |xy| ≤p (and, thus, |z| ≥1), and\\n3. for all i ≥0, xyiz ∈L, i.e., xyiz is the binary representation of a prime\\nnumber.\\nDeﬁne A, B, and C to be the integers whose binary representations are\\nx, y, and z, respectively. Note that both y and z may have leading 0’s. In\\nfact, y may be a string consisting of 0’s only, in which case B = 0. However,\\nsince the rightmost bit of z is 1, we have C ≥1. Observe that\\nN = C + B · 2|z| + A · 2|z|+|y|.\\n(2.4)\\n76\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet i = N, consider the bitstring xyiz = xyNz, and let M be the prime\\nnumber whose binary representation is given by this bitstring. Then,\\nM\\n=\\nC +\\nN−1\\nX\\nk=0\\nB · 2|z|+k|y| + A · 2|z|+N|y|\\n=\\nC + B · 2|z|\\nN−1\\nX\\nk=0\\n2k|y| + A · 2|z|+N|y|.\\nLet\\nT =\\nN−1\\nX\\nk=0\\n2k|y|.\\nThen\\n\\x002|y| −1\\n\\x01\\nT = 2N|y| −1.\\n(2.5)\\nBy Fermat’s Little Theorem, we have\\n2N ≡2\\n(mod N),\\nimplying that\\n2N|y| −1 =\\n\\x002N\\x01|y| −1 ≡2|y| −1\\n(mod N).\\nThus, (2.5) implies that\\n\\x002|y| −1\\n\\x01\\nT ≡2|y| −1\\n(mod N).\\n(2.6)\\nObserve that 2|y| ≤2p < N, because |y| ≤|xy| ≤p. Also, 2|y| ≥2, because\\ny ̸= ϵ. It follows that\\n1 ≤2|y| −1 < N,\\nimplying that\\n2|y| −1 ̸≡0\\n(mod N).\\nThis, together with (2.6), implies that\\nT ≡1\\n(mod N).\\nSince\\nM = C + B · 2|z| · T + A · 2|z|+N|y|,\\n2.10.\\nHigman’s Theorem\\n77\\nit follows that\\nM ≡C + B · 2|z| + A · 2|z|+|y|\\n(mod N).\\nThis, together with (2.4), implies that\\nM ≡0\\n(mod N),\\ni.e., N divides M. Since M > N, we conclude that M is not a prime number,\\nwhich is a contradiction. Thus, the language L is not regular.\\n2.10\\nHigman’s Theorem\\nLet Σ be a ﬁnite alphabet. For any two strings x and y in Σ∗, we say that x\\nis a subsequence of y, if x can be obtained by deleting zero or more symbols\\nfrom y. For example, 10110 is a subsequence of 0010010101010001. For any\\nlanguage L ⊆Σ∗, we deﬁne\\nSUBSEQ(L) := {x : there exists a y ∈L such that x is a subsequence of y}.\\nThat is, SUBSEQ(L) is the language consisting of the subsequences of all\\nstrings in L. In 1952, Higman proved the following result:\\nTheorem 2.10.1 (Higman) For any ﬁnite alphabet Σ and for any lan-\\nguage L ⊆Σ∗, the language SUBSEQ(L) is regular.\\n2.10.1\\nDickson’s Theorem\\nOur proof of Higman’s Theorem will use a theorem that was proved in 1913\\nby Dickson.\\nRecall that N denotes the set of positive integers. Let n ∈N. For any\\ntwo points p = (p1, p2, . . . , pn) and q = (q1, q2, . . . , qn) in Nn, we say that p is\\ndominated by q, if pi ≤qi for all i with 1 ≤i ≤n.\\nTheorem 2.10.2 (Dickson) Let S ⊆Nn, and let M be the set consisting of\\nall elements of S that are minimal in the relation “is dominated by”. Thus,\\nM = {q ∈S : there is no p in S \\\\ {q} such that p is dominated by q}.\\nThen, the set M is ﬁnite.\\n78\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe will prove this theorem by induction on the dimension n. If n = 1,\\nthen either M = ∅(if S = ∅) or M consists of exactly one element (if S ̸= ∅).\\nTherefore, the theorem holds if n = 1. Let n ≥2 and assume the theorem\\nholds for all subsets of Nn−1. Let S be a subset of Nn and consider the set\\nM of minimal elements in S. If S = ∅, then M = ∅and, thus, M is ﬁnite.\\nAssume that S ̸= ∅. We ﬁx an arbitrary element q in M. If p ∈M \\\\ {q},\\nthen q is not dominated by p. Therefore, there exists an index i such that\\npi ≤qi −1. It follows that\\nM \\\\ {q} ⊆\\nn[\\ni=1\\n\\x00Ni−1 × [1, qi −1] × Nn−i\\x01\\n.\\nFor all i and k with 1 ≤i ≤n and 1 ≤k ≤qi −1, we deﬁne\\nSik = {p ∈S : pi = k}\\nand\\nMik = {p ∈M : pi = k}.\\nThen,\\nM \\\\ {q} =\\nn[\\ni=1\\nqi−1\\n[\\nk=1\\nMik.\\n(2.7)\\nLemma 2.10.3 Mik is a subset of the set of all elements of Sik that are\\nminimal in the relation “is dominated by”.\\nProof. Let p be an element of Mik, and assume that p is not minimal in\\nSik. Then there is an element r in Sik, such that r ̸= p and r is dominated\\nby p. Since p and r are both elements of S, it follows that p ̸∈M. This is a\\ncontradiction.\\nSince the set Sik is basically a subset of Nn−1, it follows from the induction\\nhypothesis that Sik contains ﬁnitely many minimal elements. This, combined\\nwith Lemma 2.10.3, implies that Mik is a ﬁnite set. Thus, by (2.7), M \\\\ {q}\\nis the union of ﬁnitely many ﬁnite sets. Therefore, the set M is ﬁnite.\\n2.10.2\\nProof of Higman’s Theorem\\nWe give the proof of Theorem 2.10.1 for the case when Σ = {0, 1}. If L = ∅\\nor SUBSEQ(L) = {0, 1}∗, then SUBSEQ(L) is obviously a regular language.\\n2.10.\\nHigman’s Theorem\\n79\\nHence, we may assume that L is non-empty and SUBSEQ(L) is a proper\\nsubset of {0, 1}∗.\\nWe ﬁx a string z of length at least two in the complement SUBSEQ(L) of\\nthe language SUBSEQ(L). Observe that this is possible, because SUBSEQ(L)\\nis an inﬁnite language. We insert 0s and 1s into z, such that, in the result-\\ning string z′, 0s and 1s alternate. For example, if z = 0011101011, then\\nz′ = 01010101010101. Let n = |z′| −1, where |z′| denotes the length of z′.\\nThen, n ≥|z| −1 ≥1.\\nA (0, 1)-alternation in a binary string x is any occurrence of 01 or 10 in x.\\nFor example, the string 1101001 contains four (0, 1)-alternations. We deﬁne\\nA = {x ∈{0, 1}∗: x has at most n many (0, 1)-alternations}.\\nLemma 2.10.4 SUBSEQ(L) ⊆A.\\nProof. Let x ∈SUBSEQ(L) and assume that x ̸∈A. Then, x has at least\\nn + 1 = |z′| many (0, 1)-alternations and, therefore, z′ is a subsequence of x.\\nIn particular, z is a subsequence of x. Since x ∈SUBSEQ(L), it follows that\\nz ∈SUBSEQ(L), which is a contradiction.\\nLemma 2.10.5 SUBSEQ(L) =\\n\\x10\\nA ∩SUBSEQ(L)\\n\\x11\\n∪A.\\nProof. Follows from Lemma 2.10.4.\\nLemma 2.10.6 The language A is regular.\\nProof.\\nThe complement A of A is the language consisting of all binary\\nstrings with at least n + 1 many (0, 1)-alternations. If, for example, n = 3,\\nthen A is described by the regular expression\\n(00∗11∗00∗11∗0(0 ∪1)∗) ∪(11∗00∗11∗00∗1(0 ∪1)∗) .\\nThis should convince you that the claim is true for any value of n.\\nFor any b ∈{0, 1} and for any k ≥0, we deﬁne Abk to be the language\\nconsisting of all binary strings that start with a b and have exactly k many\\n(0, 1)-alternations. Then, we have\\nA = {ϵ} ∪\\n 1[\\nb=0\\nn[\\nk=0\\nAbk\\n!\\n.\\n80\\nChapter 2.\\nFinite Automata and Regular Languages\\nThus, if we deﬁne\\nFbk = Abk ∩SUBSEQ(L),\\nand use the fact that ϵ ∈SUBSEQ(L) (which is true because L ̸= ∅), then\\nA ∩SUBSEQ(L) =\\n1[\\nb=0\\nn[\\nk=0\\nFbk.\\n(2.8)\\nFor any b ∈{0, 1} and for any k ≥0, consider the relation “is a subse-\\nquence of” on the language Fbk. We deﬁne Mbk to be the language consisting\\nof all strings in Fbk that are minimal in this relation. Thus,\\nMbk = {x ∈Fbk : there is no x′ in Fbk \\\\ {x} such that x′ is a subsequence of x}.\\nIt is clear that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Fbk : x is a subsequence of y}.\\nIf x ∈Mbk, y ∈Abk, and x is a subsequence of y, then y must be in\\nSUBSEQ(L) and, therefore, y must be in Fbk. To prove this, assume that\\ny ∈SUBSEQ(L).\\nThen, x ∈SUBSEQ(L), contradicting the fact that\\nx ∈Mbk ⊆Fbk ⊆SUBSEQ(L). It follows that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Abk : x is a subsequence of y}.\\n(2.9)\\nLemma 2.10.7 Let b ∈{0, 1} and 0 ≤k ≤n, and let x be an element of\\nMbk. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis regular.\\nProof. We will prove the claim by means of an example. Assume that b = 1,\\nk = 3, and x = 11110001000. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis described by the regular expression\\n11111∗0000∗11∗0000∗.\\nThis should convince you that the claim is true in general.\\nExercises\\n81\\nLemma 2.10.8 For each b ∈{0, 1} and each 0 ≤k ≤n, the set Mbk is\\nﬁnite.\\nProof. Again, we will prove the claim by means of an example. Assume\\nthat b = 1 and k = 3. Any string in Fbk can be written as 1a0b1c0d, for some\\nintegers a, b, c, d ≥1. Consider the function ϕ : Fbk →N4 that is deﬁned by\\nϕ(1a0b1c0d) = (a, b, c, d). Then, ϕ is an injective function, and the following\\nis true, for any two strings x and x′ in Fbk:\\nx is a subsequence of x′ if and only if ϕ(x) is dominated by ϕ(x′).\\nIt follows that the elements of Mbk are in one-to-one correspondence with\\nthose elements of ϕ(Fbk) that are minimal in the relation “is dominated by”.\\nThe lemma thus follows from Dickson’s Theorem.\\nNow we can complete the proof of Higman’s Theorem:\\n• It follows from (2.9) and Lemmas 2.10.7 and 2.10.8, that Fbk is the\\nunion of ﬁnitely many regular languages. Therefore, by Theorem 2.3.1,\\nFbk is a regular language.\\n• It follows from (2.8) that A∩SUBSEQ(L) is the union of ﬁnitely many\\nregular languages. Therefore, again by Theorem 2.3.1, A∩SUBSEQ(L)\\nis a regular language.\\n• Since A ∩SUBSEQ(L) is regular and, by Lemma 2.10.6, A is regular,\\nit follows from Lemma 2.10.5 that SUBSEQ(L) is the union of two reg-\\nular languages. Therefore, by Theorem 2.3.1, SUBSEQ(L) is a regular\\nlanguage.\\n• Since SUBSEQ(L) is regular, it follows from Theorem 2.6.4 that the\\nlanguage SUBSEQ(L) is regular as well.\\nExercises\\n2.1 For each of the following languages, construct a DFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : the length of w is divisible by three}\\n82\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. {w : 110 is not a substring of w}\\n3. {w : w contains at least ﬁve 1s}\\n4. {w : w contains the substring 1011}\\n5. {w : w contains at least two 1s and at most two 0s}\\n6. {w : w contains an odd number of 1s or exactly two 0s}\\n7. {w : w begins with 1 and ends with 0}\\n8. {w : every odd position in w is 1}\\n9. {w : w has length at least 3 and its third symbol is 0}\\n10. {ϵ, 0}\\n2.2 For each of the following languages, construct an NFA, with the speciﬁed\\nnumber of states, that accepts the language. In all cases, the alphabet is\\n{0, 1}.\\n1. The language {w : w ends with 10} with three states.\\n2. The language {w : w contains the substring 1011} with ﬁve states.\\n3. The language {w : w contains an odd number of 1s or exactly two 0s}\\nwith six states.\\n2.3 For each of the following languages, construct an NFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : w contains the substring 11001}\\n2. {w : w has length at least 2 and does not end with 10}\\n3. {w : w begins with 1 or ends with 0}\\n2.4 Convert the following NFA to an equivalent DFA.\\nExercises\\n83\\n1\\n2\\na\\nb\\na, b\\n2.5 Convert the following NFA to an equivalent DFA.\\n1\\n3\\n2\\na\\na\\nb\\na\\nε,b\\n2.6 Convert the following NFA to an equivalent DFA.\\n0\\n1\\n2\\n3\\na, ǫ\\nb\\na\\nǫ\\nb\\n2.7 In the proof of Theorem 2.6.3, we introduced a new start state q0, which\\nis also an accept state. Explain why the following is not a valid proof of\\nTheorem 2.6.3:\\nLet N = (Q1, Σ, δ1, q1, F1) be an NFA, such that A = L(N). Deﬁne the\\nNFA M = (Q1, Σ, δ, q1, F), where\\n84\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. F = {q1} ∪F1.\\n2. δ : Q1 × Σϵ →P(Q1) is deﬁned as follows: For any r ∈Q1 and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ.\\nThen L(M) = A∗.\\n2.8 Prove Theorem 2.6.4.\\n2.9 Let A be a language over the alphabet Σ = {0, 1} and let A be the\\ncomplement of A. Thus, A is the language consisting of all binary strings\\nthat are not in A.\\nAssume that A is a regular language. Let M = (Q, Σ, δ, q, F) be a non-\\ndeterministic ﬁnite automaton (NFA) that accepts A.\\nConsider the NFA N = (Q, Σ, δ, q, F), where F = Q\\\\F is the complement\\nof F. Thus, N is obtained from M by turning all accept states into nonaccept\\nstates, and turning all nonaccept states into accept states.\\n1. Is it true that the language accepted by N is equal to A?\\n2. Assume now that M is a deterministic ﬁnite automaton (DFA) that\\naccepts A. Deﬁne N as above; thus, turn all accept states into nonac-\\ncept states, and turn all nonaccept states into accept states. Is it true\\nthat the language accepted by N is equal to A?\\n2.10 Recall the alternative deﬁnition for the star of a language A that we\\ngave just before Theorem 2.3.1.\\nIn Theorems 2.3.1 and 2.6.2, we have shown that the class of regular\\nlanguages is closed under the union and concatenation operations.\\nSince\\nA∗= S∞\\nk=0 Ak, why doesn’t this imply that the class of regular languages is\\nclosed under the star operation?\\n2.11 Let A and B be two regular languages over the same alphabet Σ. Prove\\nthat the diﬀerence of A and B, i.e., the language\\nA \\\\ B = {w : w ∈A and w ̸∈B}\\nis a regular language.\\nExercises\\n85\\n2.12 For each of the following regular expressions, give two strings that are\\nmembers and two strings that are not members of the language described by\\nthe expression. The alphabet is Σ = {a, b}.\\n1. a(ba)∗b.\\n2. (a ∪b)∗a(a ∪b)∗b(a ∪b)∗a(a ∪b)∗.\\n3. (a ∪ba ∪bb)(a ∪b)∗.\\n2.13 Give regular expressions describing the following languages.\\nIn all\\ncases, the alphabet is {0, 1}.\\n1. {w : w contains at least three 1s}.\\n2. {w : w contains at least two 1s and at most one 0},\\n3. {w : w contains an even number of 0s and exactly two 1s}.\\n4. {w : w contains exactly two 0s and at least two 1s}.\\n5. {w : w contains an even number of 0s and each 0 is followed by at least one 1}.\\n6. {w : every odd position in w is 1}.\\n2.14 Convert each of the following regular expressions to an NFA.\\n1. (0 ∪1)∗000(0 ∪1)∗\\n2. (((10)∗(00)) ∪10)∗\\n3. ((0 ∪1)(11)∗∪0)∗\\n2.15 Convert the following DFA to a regular expression.\\n86\\nChapter 2.\\nFinite Automata and Regular Languages\\n1\\n2\\n3\\na\\na\\nb\\nb\\na\\nb\\n2.16 Convert the following DFA to a regular expression.\\n1\\n2\\n3\\na, b\\na\\na\\nb\\nb\\n2.17 Convert the following DFA to a regular expression.\\na, b\\n2.18\\n1. Let A be a non-empty regular language. Prove that there exists\\nan NFA that accepts A and that has exactly one accept state.\\nExercises\\n87\\n2. For any string w = w1w2 . . . wn, we denote by wR the string obtained\\nby reading w backwards, i.e., wR = wnwn−1 . . . w2w1. For any language\\nA, we deﬁne AR to be the language obtained by reading all strings in\\nA backwards, i.e.,\\nAR = {wR : w ∈A}.\\nLet A be a non-empty regular language. Prove that the language AR\\nis also regular.\\n2.19 If n ≥1 is an integer and w = a1a2 . . . an is a string, then for any i\\nwith 0 ≤i < n, the string a1a2 . . . ai is called a proper preﬁx of w. (If i = 0,\\nthen a1a2 . . . ai = ϵ.)\\nFor any language L, we deﬁne MIN(L) to be the language\\nMIN(L) = {w ∈L : no proper preﬁx of w belongs to L}.\\nProve the following claim: If L is a regular language, then MIN(L) is regular\\nas well.\\n2.20 Use the pumping lemma to prove that the following languages are not\\nregular.\\n1. {anbmcn+m : n ≥0, m ≥0}.\\n2. {anbnc2n : n ≥0}.\\n3. {anbman : n ≥0, m ≥0}.\\n4. {a2n : n ≥0}. (Remark: a2n is the string consisting of 2n many a’s.)\\n5. {anbmck : n ≥0, m ≥0, k ≥0, n2 + m2 = k2}.\\n6. {uvu : u ∈{a, b}∗, u ̸= ϵ, v ∈{a, b}∗}.\\n2.21 Prove that the language\\n{ambn : m ≥0, n ≥0, m ̸= n}\\nis not regular. (Using the pumping lemma for this one is a bit tricky. You\\ncan avoid using the pumping lemma by combining results about the closure\\nunder regular operations.)\\n88\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.22\\n1. Give an example of a regular language A and a non-regular lan-\\nguage B for which A ⊆B.\\n2. Give an example of a non-regular language A and a regular language\\nB for which A ⊆B.\\n2.23 Let A be a language consisting of ﬁnitely many strings.\\n1. Prove that A is a regular language.\\n2. Let n be the maximum length of any string in A. Prove that every\\ndeterministic ﬁnite automaton (DFA) that accepts A has at least n+1\\nstates. (Hint: How is the pumping length chosen in the proof of the\\npumping lemma?)\\n2.24 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L ̸= ∅if and only\\nif L contains a string of length less than p.\\n2.25 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L is an inﬁnite\\nlanguage if and only if L contains a string w with p ≤|w| ≤2p −1.\\n2.26 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. We deﬁne a binary relation RL on Σ∗× Σ∗, in the following way:\\nFor any two strings u and u′ in Σ∗,\\nuRLu′ if and only if (∀v ∈Σ∗: uv ∈L ⇔u′v ∈L) .\\nProve that RL is an equivalence relation.\\n2.27 Let Σ = {0, 1}, let\\nL = {w ∈Σ∗: |w| is odd},\\nand consider the relation RL deﬁned in Exercise 2.26.\\n1. Prove that for any two strings u and u′ in Σ∗,\\nuRLu′ ⇔|u| −|u′| is even.\\nExercises\\n89\\n2. Determine all equivalence classes of the relation RL.\\n2.28 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. Recall the equivalence relation RL that was deﬁned in Exercise 2.26.\\n1. Assume that L is a regular language, and let M = (Q, Σ, δ, q0, F) be\\na DFA that accepts L. Let u and u′ be strings in Σ∗. Let q be the\\nstate reached, when following the path in the state diagram of M, that\\nstarts in q0 and that is obtained by reading the string u. Similarly, let\\nq′ be the state reached, when following the path in the state diagram\\nof M, that starts in q0 and that is obtained by reading the string u′.\\nProve the following: If q = q′, then uRLu′.\\n2. Prove the following claim: If L is a regular language, then the equiva-\\nlence relation RL has a ﬁnite number of equivalence classes.\\n2.29 Let L be the language deﬁned by\\nL = {uuR : u ∈{0, 1}∗}.\\nIn words, a string is in L if and only if its length is even, and the second half\\nis the reverse of the ﬁrst half. Consider the equivalence relation RL that was\\ndeﬁned in Exercise 2.26.\\n1. Let m and n be two distinct positive integers and consider the two\\nstrings u = 0m1 and u′ = 0n1. Prove that ¬(uRLu′).\\n2. Prove that L is not a regular language, without using the pumping\\nlemma.\\n3. Use the pumping lemma to prove that L is not a regular language.\\n2.30 In this exercise, we will show that the converse of the pumping lemma\\ndoes, in general, not hold. Consider the language\\nA = {ambncn : m ≥1, n ≥0} ∪{bnck : n ≥0, k ≥0}.\\n1. Show that A satisﬁes the conclusion of the pumping lemma for p = 1.\\nThus, show that every string s in A whose length is at least p can be\\nwritten as s = xyz, such that y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all\\ni ≥0.\\n90\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. Consider the equivalence relation RA that was deﬁned in Exercise 2.26.\\nLet n and n′ be two distinct non-negative integers and consider the two\\nstrings u = abn and u′ = abn′. Prove that ¬(uRAu′).\\n3. Prove that A is not a regular language.\\nChapter 3\\nContext-Free Languages\\nIn this chapter, we introduce the class of context-free languages.\\nAs we\\nwill see, this class contains all regular languages, as well as some nonregular\\nlanguages such as {0n1n : n ≥0}.\\nThe class of context-free languages consists of languages that have some\\nsort of recursive structure. We will see two equivalent methods to obtain this\\nclass. We start with context-free grammars, which are used for deﬁning the\\nsyntax of programming languages and their compilation. Then we introduce\\nthe notion of (nondeterministic) pushdown automata, and show that these\\nautomata have the same power as context-free grammars.\\n3.1\\nContext-free grammars\\nWe start with an example. Consider the following ﬁve (substitution) rules:\\nS\\n→\\nAB\\nA\\n→\\na\\nA\\n→\\naA\\nB\\n→\\nb\\nB\\n→\\nbB\\nHere, S, A, and B are variables, S is the start variable, and a and b are\\nterminals. We use these rules to derive strings consisting of terminals (i.e.,\\nelements of {a, b}∗), in the following manner:\\n1. Initialize the current string to be the string consisting of the start\\nvariable S.\\n92\\nChapter 3.\\nContext-Free Languages\\n2. Take any variable in the current string and take any rule that has this\\nvariable on the left-hand side. Then, in the current string, replace this\\nvariable by the right-hand side of the rule.\\n3. Repeat 2. until the current string only contains terminals.\\nFor example, the string aaaabb can be derived in the following way:\\nS\\n⇒\\nAB\\n⇒\\naAB\\n⇒\\naAbB\\n⇒\\naaAbB\\n⇒\\naaaAbB\\n⇒\\naaaabB\\n⇒\\naaaabb\\nThis derivation can also be represented using a parse tree, as in the ﬁgure\\nbelow:\\nS\\nA\\nA\\nA\\nA\\na\\na\\na\\na\\nb\\nb\\nB\\nB\\nThe ﬁve rules in this example constitute a context-free grammar. The\\nlanguage of this grammar is the set of all strings that\\n3.1.\\nContext-free grammars\\n93\\n• can be derived from the start variable and\\n• only contain terminals.\\nFor this example, the language is\\n{ambn : m ≥1, n ≥1},\\nbecause every string of the form ambn, for some m ≥1 and n ≥1, can be\\nderived from the start variable, whereas no other string over the alphabet\\n{a, b} can be derived from the start variable.\\nDeﬁnition 3.1.1 A context-free grammar is a 4-tuple G = (V, Σ, R, S),\\nwhere\\n1. V is a ﬁnite set, whose elements are called variables,\\n2. Σ is a ﬁnite set, whose elements are called terminals,\\n3. V ∩Σ = ∅,\\n4. S is an element of V ; it is called the start variable,\\n5. R is a ﬁnite set, whose elements are called rules. Each rule has the\\nform A →w, where A ∈V and w ∈(V ∪Σ)∗.\\nIn our example, we have V = {S, A, B}, Σ = {a, b}, and\\nR = {S →AB, A →a, A →aA, B →b, B →bB}.\\nDeﬁnition 3.1.2 Let G = (V, Σ, R, S) be a context-free grammar. Let A be\\nan element in V and let u, v, and w be strings in (V ∪Σ)∗such that A →w\\nis a rule in R. We say that the string uwv can be derived in one step from\\nthe string uAv, and write this as\\nuAv ⇒uwv.\\nIn other words, by applying the rule A →w to the string uAv, we obtain\\nthe string uwv. In our example, we see that aaAbb ⇒aaaAbb.\\nDeﬁnition 3.1.3 Let G = (V, Σ, R, S) be a context-free grammar. Let u\\nand v be strings in (V ∪Σ)∗. We say that v can be derived from u, and write\\nthis as u\\n∗⇒v, if one of the following two conditions holds:\\n94\\nChapter 3.\\nContext-Free Languages\\n1. u = v or\\n2. there exist an integer k ≥2 and a sequence u1, u2, . . . , uk of strings in\\n(V ∪Σ)∗, such that\\n(a) u = u1,\\n(b) v = uk, and\\n(c) u1 ⇒u2 ⇒. . . ⇒uk.\\nIn other words, by starting with the string u and applying rules zero or\\nmore times, we obtain the string v. In our example, we see that aaAbB\\n∗⇒\\naaaabbbB.\\nDeﬁnition 3.1.4 Let G = (V, Σ, R, S) be a context-free grammar.\\nThe\\nlanguage of G is deﬁned to be the set of all strings in Σ∗that can be derived\\nfrom the start variable S:\\nL(G) = {w ∈Σ∗: S\\n∗⇒w}.\\nDeﬁnition 3.1.5 A language L is called context-free, if there exists a context-\\nfree grammar G such that L(G) = L.\\n3.2\\nExamples of context-free grammars\\n3.2.1\\nProperly nested parentheses\\nConsider the context-free grammar G = (V, Σ, R, S), where V = {S}, Σ =\\n{a, b}, and\\nR = {S →ϵ, S →aSb, S →SS}.\\nWe write the three rules in R as\\nS →ϵ|aSb|SS,\\nwhere you can think of “|” as being a short-hand for “or”.\\n3.2.\\nExamples of context-free grammars\\n95\\nBy applying the rules in R, starting with the start variable S, we obtain,\\nfor example,\\nS\\n⇒\\nSS\\n⇒\\naSbS\\n⇒\\naSbSS\\n⇒\\naSSbSS\\n⇒\\naaSbSbSS\\n⇒\\naabSbSS\\n⇒\\naabbSS\\n⇒\\naabbaSbS\\n⇒\\naabbabS\\n⇒\\naabbabaSb\\n⇒\\naabbabab\\nWhat is the language L(G) of this context-free grammar G? If we think\\nof a as being a left-parenthesis “(”, and of b as being a right-parenthesis “)”,\\nthen L(G) is the language consisting of all strings of properly nested paren-\\ntheses. Here is the explanation: Any string of properly nested parentheses is\\neither\\n• empty (which we derive from S by the rule S →ϵ),\\n• consists of a left-parenthesis, followed by an arbitrary string of properly\\nnested parentheses, followed by a right-parenthesis (these are derived\\nfrom S by ﬁrst applying the rule S →aSb), or\\n• consists of an arbitrary string of properly nested parentheses, followed\\nby an arbitrary string of properly nested parentheses (these are derived\\nfrom S by ﬁrst applying the rule S →SS).\\n3.2.2\\nA context-free grammar for a nonregular lan-\\nguage\\nConsider the language L1 = {0n1n : n ≥0}. We have seen in Section 2.9.1\\nthat L1 is not a regular language. We claim that L1 is a context-free language.\\n96\\nChapter 3.\\nContext-Free Languages\\nIn order to prove this claim, we have to construct a context-free grammar\\nG1 such that L(G1) = L1.\\nObserve that any string in L1 is either\\n• empty or\\n• consists of a 0, followed by an arbitrary string in L1, followed by a 1.\\nThis leads to the context-free grammar G1 = (V1, Σ, R1, S1), where V1 =\\n{S1}, Σ = {0, 1}, and R1 consists of the rules\\nS1 →ϵ|0S11.\\nHence, R1 = {S1 →ϵ, S1 →0S11}.\\nTo derive the string 0n1n from the start variable S1, we do the following:\\n• Starting with S1, apply the rule S1 →0S11 exactly n times. This gives\\nthe string 0nS11n.\\n• Apply the rule S1 →ϵ. This gives the string 0n1n.\\nIt is not diﬃcult to see that these are the only strings that can be derived\\nfrom the start variable S1. Thus, L(G1) = L1.\\nIn a symmetric way, we see that the context-free grammar G2 = (V2, Σ, R2, S2),\\nwhere V2 = {S2}, Σ = {0, 1}, and R2 consists of the rules\\nS2 →ϵ|1S20,\\nhas the property that L(G2) = L2, where L2 = {1n0n : n ≥0}. Thus, L2 is\\na context-free language.\\nDeﬁne L = L1 ∪L2, i.e.,\\nL = {0n1n : n ≥0} ∪{1n0n : n ≥0}.\\nThe context-free grammar G = (V, Σ, R, S), where V = {S, S1, S2}, Σ =\\n{0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2\\nS1\\n→\\nϵ|0S11\\nS2\\n→\\nϵ|1S20,\\nhas the property that L(G) = L. Hence, L is a context-free language.\\n3.2.\\nExamples of context-free grammars\\n97\\n3.2.3\\nA context-free grammar for the complement of\\na nonregular language\\nLet L be the (nonregular) language L = {0n1n : n ≥0}. We want to prove\\nthat the complement L of L is a context-free language. Hence, we want to\\nconstruct a context-free grammar G whose language is equal to L. Observe\\nthat a binary string w is in L if and only if\\n1. w = 0m1n, for some integers m and n with 0 ≤m < n, or\\n2. w = 0m1n, for some integers m and n with 0 ≤n < m, or\\n3. w contains 10 as a substring.\\nThus, we can write L as the union of the languages of all strings of type 1.,\\ntype 2., and type 3.\\nAny string of type 1. is either\\n• the string 1,\\n• consists of a string of type 1., followed by one 1, or\\n• consists of one 0, followed by an arbitrary string of type 1., followed by\\none 1.\\nThus, using the rules\\nS1 →1|S11|0S11,\\nwe can derive, from S1, all strings of type 1.\\nSimilarly, using the rules\\nS2 →0|0S2|0S21,\\nwe can derive, from S2, all strings of type 2.\\nAny string of type 3.\\n• consists of an arbitrary binary string, followed by the string 10, followed\\nby an arbitrary binary string.\\nUsing the rules\\nX →ϵ|0X|1X,\\n98\\nChapter 3.\\nContext-Free Languages\\nwe can derive, from X, all binary strings. Thus, by combining these with\\nthe rule\\nS3 →X10X,\\nwe can derive, from S3, all strings of type 3.\\nWe arrive at the context-free grammar G = (V, Σ, R, S), where V =\\n{S, S1, S2, S3, X}, Σ = {0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2|S3\\nS1\\n→\\n1|S11|0S11\\nS2\\n→\\n0|0S2|0S21\\nS3\\n→\\nX10X\\nX\\n→\\nϵ|0X|1X\\nTo summarize, we have\\nS1\\n∗⇒0m1n, for all integers m and n with 0 ≤m < n,\\nS2\\n∗⇒0m1n, for all integers m and n with 0 ≤n < m,\\nX\\n∗⇒u, for each string u in {0, 1}∗,\\nand\\nS3\\n∗⇒w, for every binary string w that contains 10 as a substring.\\nFrom these observations, it follows that that L(G) = L.\\n3.2.4\\nA context-free grammar that veriﬁes addition\\nConsider the language\\nL = {anbmcn+m : n ≥0, m ≥0}.\\nUsing the pumping lemma for regular languages (Theorem 2.9.1), it can\\nbe shown that L is not a regular language. We will construct a context-\\nfree grammar G whose language is equal to L, thereby proving that L is a\\ncontext-free language.\\nFirst observe that ϵ ∈L. Therefore, we will take S →ϵ to be one of the\\nrules in the grammar.\\nLet us see how we can derive all strings in L from the start variable S:\\n3.3.\\nRegular languages are context-free\\n99\\n1. Every time we add an a, we also add a c. In this way, we obtain all\\nstrings of the form ancn, where n ≥0.\\n2. Given a string of the form ancn, we start adding bs. Every time we add\\na b, we also add a c. Observe that every b has to be added between\\nthe as and the cs. Therefore, we use a variable B as a “pointer” to\\nthe position in the current string where a b can be added: Instead of\\nderiving ancn from S, we derive the string anBcn. Then, from B, we\\nderive all strings of the form bmcm, where m ≥0.\\nWe obtain the context-free grammar G = (V, Σ, R, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R consists of the rules\\nS\\n→\\nϵ|A\\nA\\n→\\nϵ|aAc|B\\nB\\n→\\nϵ|bBc\\nThe facts that\\n• A\\n∗⇒anBcn, for every n ≥0,\\n• B\\n∗⇒bmcm, for every m ≥0,\\nimply that the following strings can be derived from the start variable S:\\n• S\\n∗⇒anBcn\\n∗⇒anbmcmcn = anbmcn+m, for all n ≥0 and m ≥0.\\nIn fact, no other strings in {a, b, c}∗can be derived from S. Therefore, we\\nhave L(G) = L. Since\\nS ⇒A ⇒B ⇒ϵ,\\nwe can simplify this grammar G, by eliminating the rules S →ϵ and A →ϵ.\\nThis gives the context-free grammar G′ = (V, Σ, R′, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R′ consists of the rules\\nS\\n→\\nA\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\nFinally, observe that we do not need S; instead, we can use A as start\\nvariable. This gives our ﬁnal context-free grammar G′′ = (V, Σ, R′′, A), where\\nV = {A, B}, Σ = {a, b, c}, and R′′ consists of the rules\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\n100\\nChapter 3.\\nContext-Free Languages\\n3.3\\nRegular languages are context-free\\nWe mentioned already that the class of context-free languages includes the\\nclass of regular languages. In this section, we will prove this claim.\\nTheorem 3.3.1 Let Σ be an alphabet and let L ⊆Σ∗be a regular language.\\nThen L is a context-free language.\\nProof.\\nSince L is a regular language, there exists a deterministic ﬁnite\\nautomaton M = (Q, Σ, δ, q, F) that accepts L.\\nTo prove that L is context-free, we have to deﬁne a context-free grammar\\nG = (V, Σ, R, S), such that L = L(M) = L(G). Thus, G must have the\\nfollowing property: For every string w ∈Σ∗,\\nw ∈L(M) if and only if w ∈L(G),\\nwhich can be reformulated as\\nM accepts w if and only if S\\n∗⇒w.\\nWe will deﬁne the context-free grammar G in such a way that the following\\ncorrespondence holds for any string w = w1w2 . . . wn:\\n• Assume that M is in state A just after it has read the substring\\nw1w2 . . . wi.\\n• Then in the context-free grammar G, we have S\\n∗⇒w1w2 . . . wiA.\\nIn the next step, M reads the symbol wi+1 and switches from state A to,\\nsay, state B; thus, δ(A, wi+1) = B. In order to guarantee that the above\\ncorrespondence still holds, we have to add the rule A →wi+1B to G.\\nConsider the moment when M has read the entire string w. Let A be the\\nstate M is in at that moment. By the above correspondence, we have\\nS\\n∗⇒w1w2 . . . wnA = wA.\\nRecall that G must have the property that\\nM accepts w if and only if S\\n∗⇒w,\\nwhich is equivalent to\\nA ∈F if and only if S\\n∗⇒w.\\n3.3.\\nRegular languages are context-free\\n101\\nWe guarantee this property by adding to G the rule A →ϵ for every accept\\nstate A of M.\\nWe are now ready to give the formal deﬁnition of the context-free gram-\\nmar G = (V, Σ, R, S):\\n• V = Q, i.e., the variables of G are the states of M.\\n• S = q, i.e., the start variable of G is the start state of M.\\n• R consists of the rules\\nA →aB, where A ∈Q, a ∈Σ, B ∈Q, and δ(A, a) = B,\\nand\\nA →ϵ, where A ∈F.\\nIn words,\\n• every transition δ(A, a) = B of M (i.e., when M is in the state A and\\nreads the symbol a, it switches to the state B) corresponds to a rule\\nA →aB in the grammar G,\\n• every accept state A of M corresponds to a rule A →ϵ in the grammar\\nG.\\nWe claim that L(G) = L. In order to prove this, we have to show that\\nL(G) ⊆L and L ⊆L(G).\\nWe prove that L ⊆L(G). Let w = w1w2 . . . wn be an arbitrary string\\nin L. When the ﬁnite automaton M reads the string w, it visits the states\\nr0, r1, . . . , rn, where\\n• r0 = q, and\\n• ri+1 = δ(ri, wi+1) for i = 0, 1, . . . , n −1.\\nSince w ∈L = L(M), we know that rn ∈F.\\nIt follows from the way we deﬁned the grammar G that\\n• for each i = 0, 1, . . . , n −1, ri →wi+1ri+1 is a rule in R, and\\n• rn →ϵ is a rule in R.\\n102\\nChapter 3.\\nContext-Free Languages\\nTherefore, we have\\nS = q = r0 ⇒w1r1 ⇒w1w2r2 ⇒. . . ⇒w1w2 . . . wnrn ⇒w1w2 . . . wn = w.\\nThis proves that w ∈L(G).\\nThe proof of the claim that L(G) ⊆L is left as an exercise.\\nIn Sections 2.9.1 and 3.2.2, we have seen that the language {0n1n : n ≥\\n0} is not regular, but context-free. Therefore, the class of all context-free\\nlanguages properly contains the class of regular languages.\\n3.3.1\\nAn example\\nLet L be the language deﬁned as\\nL = {w ∈{0, 1}∗: 101 is a substring of w}.\\nIn Section 2.2.2, we have seen that L is a regular language. In that section,\\nwe constructed the following deterministic ﬁnite automaton M that accepts\\nL (we have renamed the states):\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nS\\nA\\nB\\nC\\nWe apply the construction given in the proof of Theorem 3.3.1 to convert\\nM to a context-free grammar G whose language is equal to L. According\\nto this construction, we have G = (V, Σ, R, S), where V = {S, A, B, C},\\nΣ = {0, 1}, the start variable S is the start state of M, and R consists of the\\nrules\\nS\\n→\\n0S|1A\\nA\\n→\\n0B|1A\\nB\\n→\\n0S|1C\\nC\\n→\\n0C|1C|ϵ\\n3.4.\\nChomsky normal form\\n103\\nConsider the string 010011011, which is an element of L. When the ﬁnite\\nautomaton M reads this string, it visits the states\\nS, S, A, B, S, A, A, B, C, C.\\nIn the grammar G, this corresponds to the derivation\\nS\\n⇒\\n0S\\n⇒\\n01A\\n⇒\\n010B\\n⇒\\n0100S\\n⇒\\n01001A\\n⇒\\n010011A\\n⇒\\n0100110B\\n⇒\\n01001101C\\n⇒\\n010011011C\\n⇒\\n010011011.\\nHence,\\nS\\n∗⇒010011011,\\nimplying that the string 010011011 is in the language L(G) of the context-free\\ngrammar G.\\nThe string 10011 is not in the language L. When the ﬁnite automaton\\nM reads this string, it visits the states\\nS, A, B, S, A, A,\\ni.e., after the string has been read, M is in the non-accept state A. In the\\ngrammar G, reading the string 10011 corresponds to the derivation\\nS\\n⇒\\n1A\\n⇒\\n10B\\n⇒\\n100S\\n⇒\\n1001A\\n⇒\\n10011A.\\nSince A is not an accept state in M, the grammar G does not contain the\\nrule A →ϵ. This implies that the string 10011 cannot be derived from the\\nstart variable S. Thus, 10011 is not in the language L(G) of G.\\n104\\nChapter 3.\\nContext-Free Languages\\n3.4\\nChomsky normal form\\nThe rules in a context-free grammar G = (V, Σ, R, S) are of the form\\nA →w,\\nwhere A is a variable and w is a string over the alphabet V ∪Σ. In this\\nsection, we show that every context-free grammar G can be converted to a\\ncontext-free grammar G′, such that L(G) = L(G′), and the rules of G′ are of\\na restricted form, as speciﬁed in the following deﬁnition:\\nDeﬁnition 3.4.1 A context-free grammar G = (V, Σ, R, S) is said to be in\\nChomsky normal form, if every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are elements of V , B ̸= S, and C ̸= S.\\n2. A →a, where A is an element of V and a is an element of Σ.\\n3. S →ϵ, where S is the start variable.\\nYou should convince yourself that, for such a grammar, R contains the\\nrule S →ϵ if and only if ϵ ∈L(G).\\nTheorem 3.4.2 Let Σ be an alphabet and let L ⊆Σ∗be a context-free lan-\\nguage. There exists a context-free grammar in Chomsky normal form, whose\\nlanguage is L.\\nProof. Since L is a context-free language, there exists a context-free gram-\\nmar G = (V, Σ, R, S), such that L(G) = L. We will transform G into a\\ngrammar that is in Chomsky normal form and whose language is equal to\\nL(G). The transformation consists of ﬁve steps.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe deﬁne G1 = (V1, Σ, R1, S1), where S1 is the start variable (which is a\\nnew variable), V1 = V ∪{S1}, and R1 = R ∪{S1 →S}. This grammar has\\nthe property that\\n• the start variable S1 does not occur on the right-hand side of any rule\\nin R1, and\\n• L(G1) = L(G).\\n3.4.\\nChomsky normal form\\n105\\nStep 2: An ϵ-rule is a rule that is of the form A →ϵ, where A is a variable\\nthat is not equal to the start variable. In the second step, we eliminate all\\nϵ-rules from G1.\\nWe consider all ϵ-rules, one after another. Let A →ϵ be one such rule,\\nwhere A ∈V1 and A ̸= S1. We modify G1 as follows:\\n1. Remove the rule A →ϵ from the current set R1.\\n2. For each rule in the current set R1 that is of the form\\n(a) B →A, add the rule B →ϵ to R1, unless this rule has already\\nbeen deleted from R1; observe that in this way, we replace the two-\\nstep derivation B ⇒A ⇒ϵ by the one-step derivation B ⇒ϵ;\\n(b) B →uAv (where u and v are strings that are not both empty),\\nadd the rule B →uv to R1; observe that in this way, we replace\\nthe two-step derivation B ⇒uAv ⇒uv by the one-step derivation\\nB ⇒uv;\\n(c) B →uAvAw (where u, v, and w are strings), add the rules B →\\nuvw, B →uAvw, and B →uvAw to R1; if u = v = w = ϵ and\\nthe rule B →ϵ has already been deleted from R1, then we do not\\nadd the rule B →ϵ;\\n(d) treat rules in which A occurs more than twice on the right-hand\\nside in a similar fashion.\\nWe repeat this process until all ϵ-rules have been eliminated.\\nLet R2\\nbe the set of rules, after all ϵ-rules have been eliminated. We deﬁne G2 =\\n(V2, Σ, R2, S2), where V2 = V1 and S2 = S1. This grammar has the property\\nthat\\n• the start variable S2 does not occur on the right-hand side of any rule\\nin R2,\\n• R2 does not contain any ϵ-rule (it may contain the rule S2 →ϵ), and\\n• L(G2) = L(G1) = L(G).\\nStep 3: A unit-rule is a rule that is of the form A →B, where A and B are\\nvariables. In the third step, we eliminate all unit-rules from G2.\\n106\\nChapter 3.\\nContext-Free Languages\\nWe consider all unit-rules, one after another. Let A →B be one such\\nrule, where A and B are elements of V2. We know that B ̸= S2. We modify\\nG2 as follows:\\n1. Remove the rule A →B from the current set R2.\\n2. For each rule in the current set R2 that is of the form B →u, where\\nu ∈(V2 ∪Σ)∗, add the rule A →u to the current set R2, unless this is\\na unit-rule that has already been eliminated.\\nObserve that in this way, we replace the two-step derivation A ⇒B ⇒\\nu by the one-step derivation A ⇒u.\\nWe repeat this process until all unit-rules have been eliminated.\\nLet\\nR3 be the set of rules, after all unit-rules have been eliminated. We deﬁne\\nG3 = (V3, Σ, R3, S3), where V3 = V2 and S3 = S2. This grammar has the\\nproperty that\\n• the start variable S3 does not occur on the right-hand side of any rule\\nin R3,\\n• R3 does not contain any ϵ-rule (it may contain the rule S3 →ϵ),\\n• R3 does not contain any unit-rule, and\\n• L(G3) = L(G2) = L(G1) = L(G).\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside.\\nFor each rule in the current set R3 that is of the form A →u1u2 . . . uk,\\nwhere k ≥3 and each ui is an element of V3 ∪Σ, we modify G3 as follows:\\n1. Remove the rule A →u1u2 . . . uk from the current set R3.\\n2. Add the following rules to the current set R3:\\nA\\n→\\nu1A1\\nA1\\n→\\nu2A2\\nA2\\n→\\nu3A3\\n...\\nAk−3\\n→\\nuk−2Ak−2\\nAk−2\\n→\\nuk−1uk\\n3.4.\\nChomsky normal form\\n107\\nwhere A1, A2, . . . , Ak−2 are new variables that are added to the current\\nset V3.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 . . . uk by the (k −1)-step derivation\\nA ⇒u1A1 ⇒u1u2A2 ⇒. . . ⇒u1u2 . . . uk−2Ak−2 ⇒u1u2 . . . uk.\\nLet R4 be the set of rules, and let V4 be the set of variables, after all rules\\nwith more than two symbols on the right-hand side have been eliminated. We\\ndeﬁne G4 = (V4, Σ, R4, S4), where S4 = S3. This grammar has the property\\nthat\\n• the start variable S4 does not occur on the right-hand side of any rule\\nin R4,\\n• R4 does not contain any ϵ-rule (it may contain the rule S4 →ϵ),\\n• R4 does not contain any unit-rule,\\n• R4 does not contain any rule with more than two symbols on the right-\\nhand side, and\\n• L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nStep 5: Eliminate all rules of the form A →u1u2, where u1 and u2 are not\\nboth variables.\\nFor each rule in the current set R4 that is of the form A →u1u2, where\\nu1 and u2 are elements of V4 ∪Σ, but u1 and u2 are not both contained in\\nV4, we modify G3 as follows:\\n1. If u1 ∈Σ and u2 ∈V4, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →U1u2 and U1 →u1, where U1 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒U1u2 ⇒u1u2.\\n2. If u1 ∈V4 and u2 ∈Σ, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →u1U2 and U2 →u2, where U2 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒u1U2 ⇒u1u2.\\n108\\nChapter 3.\\nContext-Free Languages\\n3. If u1 ∈Σ, u2 ∈Σ, and u1 ̸= u2, then replace the rule A →u1u2 in the\\ncurrent set R4 by the three rules A →U1U2, U1 →u1, and U2 →u2,\\nwhere U1 and U2 are new variables that are added to the current set\\nV4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the three-step derivation A ⇒U1U2 ⇒u1U2 ⇒u1u2.\\n4. If u1 ∈Σ, u2 ∈Σ, and u1 = u2, then replace the rule A →u1u2 = u1u1\\nin the current set R4 by the two rules A →U1U1 and U1 →u1, where\\nU1 is a new variable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 = u1u1 by the three-step derivation A ⇒U1U1 ⇒u1U1 ⇒u1u1.\\nLet R5 be the set of rules, and let V5 be the set of variables, after Step 5\\nhas been completed. We deﬁne G5 = (V5, Σ, R5, S5), where S5 = S4. This\\ngrammar has the property that\\n• the start variable S5 does not occur on the right-hand side of any rule\\nin R5,\\n• R5 does not contain any ϵ-rule (it may contain the rule S5 →ϵ),\\n• R5 does not contain any unit-rule,\\n• R5 does not contain any rule with more than two symbols on the right-\\nhand side,\\n• R5 does not contain any rule of the form A →u1u2, where u1 and u2\\nare not both variables of V5, and\\n• L(G5) = L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nSince the grammar G5 is in Chomsky normal form, the proof is complete.\\n3.4.\\nChomsky normal form\\n109\\n3.4.1\\nAn example\\nConsider the context-free grammar G = (V, Σ, R, A), where V = {A, B},\\nΣ = {0, 1}, A is the start variable, and R consists of the rules\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nWe apply the construction given in the proof of Theorem 3.4.2 to convert\\nthis grammar to a context-free grammar in Chomsky normal form whose\\nlanguage is the same as that of G. Throughout the construction, upper case\\nletters will denote variables.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe introduce a new start variable S, and add the rule S →A. This gives\\nthe following grammar:\\nS\\n→\\nA\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nStep 2: Eliminate all ϵ-rules.\\nWe take the ϵ-rule A →ϵ, and remove it. Then we consider all rules that\\ncontain A on the right-hand side. There are two such rules:\\n• S →A; we add the rule S →ϵ;\\n• A →BAB; we add the rule A →BB.\\nThis gives the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB\\nB\\n→\\n00|ϵ\\nWe take the ϵ-rule B →ϵ, and remove it. Then we consider all rules that\\ncontain B on the right-hand side. There are three such rules:\\n• A →BAB; we add the rules A →AB, A →BA, and A →A;\\n• A →B; we do not add the rule A →ϵ, because it has already been\\nremoved;\\n110\\nChapter 3.\\nContext-Free Languages\\n• A →BB; we add the rule A →B, but not the rule A →ϵ (because it\\nhas already been removed).\\nAt this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA|A\\nB\\n→\\n00\\nSince all ϵ-rules have been eliminated, this completes Step 2. (Observe that\\nthe rule S →ϵ is allowed, because S is the start variable.)\\nStep 3: Eliminate all unit-rules.\\nWe take the unit-rule A →A. We can remove this rule, without adding\\nany new rule. At this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →A, remove it, and add the rules\\nS →BAB|B|BB|AB|BA.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BAB|B|BB|AB|BA\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →B, remove it, and add the rule S →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule A →B, remove it, and add the rule A →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|BB|AB|BA|00\\nB\\n→\\n00\\n3.5.\\nPushdown automata\\n111\\nSince all unit-rules have been eliminated, this concludes Step 3.\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside. There are two such rules:\\n• We take the rule S →BAB, remove it, and add the rules S →BA1\\nand A1 →AB.\\n• We take the rule A →BAB, remove it, and add the rules A →BA2\\nand A2 →AB.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BB|AB|BA|00|BA1\\nA\\n→\\nBB|AB|BA|00|BA2\\nB\\n→\\n00\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nStep 4 is now completed.\\nStep 5: Eliminate all rules, whose right-hand side contains exactly two\\nsymbols, which are not both variables. There are three such rules:\\n• We replace the rule S →00 by the rules S →A3A3 and A3 →0.\\n• We replace the rule A →00 by the rules A →A4A4 and A4 →0.\\n• We replace the rule B →00 by the rules B →A5A5 and A5 →0.\\nThis gives the following grammar, which is in Chomsky normal form:\\nS\\n→\\nϵ|BB|AB|BA|BA1|A3A3\\nA\\n→\\nBB|AB|BA|BA2|A4A4\\nB\\n→\\nA5A5\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nA3\\n→\\n0\\nA4\\n→\\n0\\nA5\\n→\\n0\\n112\\nChapter 3.\\nContext-Free Languages\\n3.5\\nPushdown automata\\nIn this section, we introduce nondeterministic pushdown automata. As we\\nwill see, the class of languages that can be accepted by these automata is\\nexactly the class of context-free languages.\\nWe start with an informal description of a deterministic pushdown au-\\ntomaton. Such an automaton consists of the following, see also Figure 3.1.\\n1. There is a tape which is divided into cells. Each cell stores a symbol\\nbelonging to a ﬁnite set Σ, called the tape alphabet. There is a special\\nsymbol 2 that is not contained in Σ; this symbol is called the blank\\nsymbol. If a cell contains 2, then this means that the cell is actually\\nempty.\\n2. There is a tape head which can move along the tape, one cell to the\\nright per move. This tape head can also read the cell it currently scans.\\n3. There is a stack containing symbols from a ﬁnite set Γ, called the stack\\nalphabet. This set contains a special symbol $.\\n4. There is a stack head which can read the top symbol of the stack. This\\nhead can also pop the top symbol, and it can push symbols of Γ onto\\nthe stack.\\n5. There is a state control, which can be in any one of a ﬁnite number\\nof states. The set of states is denoted by Q. The set Q contains one\\nspecial state q, called the start state.\\nThe input for a pushdown automaton is a string in Σ∗. This input string\\nis stored on the tape of the pushdown automaton and, initially, the tape head\\nis on the leftmost symbol of the input string. Initially, the stack only contains\\nthe special symbol $, and the pushdown automaton is in the start state q.\\nIn one computation step, the pushdown automaton does the following:\\n1. Assume that the pushdown automaton is currently in state r. Let a be\\nthe symbol of Σ that is read by the tape head, and let A be the symbol\\nof Γ that is on top of the stack.\\n2. Depending on the current state r, the tape symbol a, and the stack\\nsymbol A,\\n3.5.\\nPushdown automata\\n113\\nstate control\\na a b a b b a b a b 2\\ntape\\n6\\n$\\nA\\nA\\nB\\nA\\nstack\\n-\\nFigure 3.1: A pushdown automaton.\\n(a) the pushdown automaton switches to a state r′ of Q (which may\\nbe equal to r),\\n(b) the tape head either moves one cell to the right or stays at the\\ncurrent cell, and\\n(c) the top symbol A is replaced by a string w that belongs to Γ∗. To\\nbe more precise,\\ni. if w = ϵ, then A is popped from the stack, whereas\\nii. if w = B1B2 . . . Bk, with k ≥1 and B1, B2, . . . , Bk ∈Γ, then\\nA is replaced by w, and Bk becomes the new top symbol of\\nthe stack.\\nLater, we will specify when the pushdown automaton accepts the input\\nstring.\\nWe now give a formal deﬁnition of a deterministic pushdown automaton.\\nDeﬁnition 3.5.1 A deterministic pushdown automaton is a 5-tuple M =\\n(Σ, Γ, Q, δ, q), where\\n114\\nChapter 3.\\nContext-Free Languages\\n1. Σ is a ﬁnite set, called the tape alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the stack alphabet; this alphabet contains the\\nspecial symbol $,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. δ is called the transition function, which is a function\\nδ : Q × (Σ ∪{2}) × Γ →Q × {N, R} × Γ∗.\\nThe transition function δ can be thought of as being the “program” of the\\npushdown automaton. This function tells us what the automaton can do in\\none “computation step”: Let r ∈Q, a ∈Σ ∪{2}, and A ∈Γ. Furthermore,\\nlet r′ ∈Q, σ ∈{R, N}, and w ∈Γ∗be such that\\nδ(r, a, A) = (r′, σ, w).\\n(3.1)\\nThis transition means that if\\n• the pushdown automaton is in state r,\\n• the tape head reads the symbol a, and\\n• the top symbol on the stack is A,\\nthen\\n• the pushdown automaton switches to state r′,\\n• the tape head moves according to σ: if σ = R, then it moves one cell\\nto the right; if σ = N, then it does not move, and\\n• the top symbol A on the stack is replaced by the string w.\\nWe will write the computation step (3.1) in the form of the instruction\\nraA →r′σw.\\nWe now specify the computation of the pushdown automaton M = (Σ, Γ, Q, δ, q).\\n3.6.\\nExamples of pushdown automata\\n115\\nStart conﬁguration: Initially, the pushdown automaton is in the start state\\nq, the tape head is on the leftmost symbol of the input string a1a2 . . . an, and\\nthe stack only contains the special symbol $.\\nComputation and termination: Starting in the start conﬁguration, the\\npushdown automaton performs a sequence of computation steps as described\\nabove. It terminates at the moment when the stack becomes empty. (Hence,\\nif the stack never gets empty, the pushdown automaton does not terminate.)\\nAcceptance: The pushdown automaton accepts the input string a1a2 . . . an ∈\\nΣ∗, if\\n1. the automaton terminates on this input, and\\n2. at the time of termination (i.e., at the moment when the stack gets\\nempty), the tape head is on the cell immediately to the right of the cell\\ncontaining the symbol an (this cell must contain the blank symbol 2).\\nIn all other cases, the pushdown automaton rejects the input string. Thus,\\nthe pushdown automaton rejects this string if\\n1. the automaton does not terminate on this input (i.e., the computation\\n“loops forever”) or\\n2. at the time of termination, the tape head is not on the cell immediately\\nto the right of the cell containing the symbol an.\\nWe denote by L(M) the language accepted by the pushdown automaton\\nM. Thus,\\nL(M) = {w ∈Σ∗: M accepts w}.\\nThe pushdown automaton described above is deterministic. For a non-\\ndeterministic pushdown automata, the current computation step may not\\nbe uniquely deﬁned, but the automaton can make a choice out of a ﬁnite\\nnumber of possibilities. In this case, the transition function δ is a function\\nδ : Q × (Σ ∪{2}) × Γ →Pf(Q × {N, R} × Γ∗),\\nwhere Pf(K) is the set of all ﬁnite subsets of the set K.\\nWe say that a nondeterministic pushdown automaton M accepts an input\\nstring, if there exists an accepting computation, in the sense as described for\\ndeterministic pushdown automata. We say that M rejects an input string, if\\nevery computation on this string is rejecting. As before, we denote by L(M)\\nthe set of all strings in Σ∗that are accepted by M.\\n116\\nChapter 3.\\nContext-Free Languages\\n3.6\\nExamples of pushdown automata\\n3.6.1\\nProperly nested parentheses\\nWe will show how to construct a deterministic pushdown automaton, that\\naccepts the set of all strings of properly nested parentheses. Observe that a\\nstring w in {(, )}∗is properly nested if and only if\\n• in every preﬁx of w, the number of “(” is greater than or equal to the\\nnumber of “)”, and\\n• in the complete string w, the number of “(” is equal to the number of\\n“)”.\\nWe will use the tape symbol a for “(”, and the tape symbol b for “)”.\\nThe idea is as follows. Recall that initially, the stack only contains the\\nspecial symbol $. The pushdown automaton reads the input string from left\\nto right. For every a it reads, it pushes the symbol S onto the stack, and\\nfor every b it reads, it pops the top symbol from the stack. In this way, the\\nnumber of symbols S on the stack will always be equal to the number of as\\nthat have been read minus the number of bs that have been read; additionally,\\nthe bottom of the stack will contain the special symbol $. The input string\\nis properly nested if and only if (i) this diﬀerence is always non-negative and\\n(ii) this diﬀerence is zero once the entire input string has been read. Hence,\\nthe input string is accepted if and only if during this process, (i) the stack\\nalways contains at least the special symbol $ and (ii) at the end, the stack\\nonly contains the special symbol $ (which will then be popped in the ﬁnal\\nstep).\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q), where Σ = {a, b}, Γ = {$, S}, Q = {q}, and the\\ntransition function δ is speciﬁed by the following instructions:\\n3.6.\\nExamples of pushdown automata\\n117\\nqa$ →qR$S\\nbecause of the a, S is pushed onto the stack\\nqaS →qRSS\\nbecause of the a, S is pushed onto the stack\\nqbS →qRϵ\\nbecause of the b, the top element is popped\\nfrom the stack\\nqb$ →qNϵ\\nthe number of bs read is larger than the number\\nof as read; the stack is made empty (hence,\\nthe computation terminates before the entire\\nstring has been read), and the input string is rejected\\nq2$ →qNϵ\\nthe entire input string has been read; the stack is\\nmade empty, and the input string is accepted\\nq2S →qNS\\nthe entire input string has been read, it contains\\nmore as than bs; no changes are made (thus, the\\nautomaton does not terminate), and the input string\\nis rejected\\n3.6.2\\nStrings of the form 0n1n\\nWe construct a deterministic pushdown automata that accepts the language\\n{0n1n : n ≥0}.\\nThe automaton uses two states q0 and q1, where q0 is the start state.\\nInitially, the automaton is in state q0.\\n• For each 0 that it reads, the automaton pushes one symbol S onto the\\nstack and stays in state q0.\\n• When the ﬁrst 1 is read, the automaton switches to state q1. From that\\nmoment,\\n– for each 1 that is read, the automaton pops the top symbol from\\nthe stack and stays in state q1;\\n– if a 0 is read, the automaton does not make any change and,\\ntherefore, does not terminate.\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q0), where Σ = {0, 1}, Γ = {$, S}, Q = {q0, q1}, q0 is\\nthe start state, and the transition function δ is speciﬁed by the following\\ninstructions:\\n118\\nChapter 3.\\nContext-Free Languages\\nq00$ →q0R$S\\npush S onto the stack\\nq00S →q0RSS\\npush S onto the stack\\nq01$ →q0N$\\nﬁrst symbol in the input is 1; loop forever\\nq01S →q1Rϵ\\nﬁrst 1 is encountered\\nq02$ →q0Nϵ\\ninput string is empty; accept\\nq02S →q0NS\\ninput only consists of 0s; loop forever\\nq10$ →q1N$\\n0 to the right of 1; loop forever\\nq10S →q1NS\\n0 to the right of 1; loop forever\\nq11$ →q1N$\\ntoo many 1s; loop forever\\nq11S →q1Rϵ\\npop top symbol from the stack\\nq12$ →q1Nϵ\\naccept\\nq12S →q1NS\\ntoo many 0s; loop forever\\n3.6.3\\nStrings with b in the middle\\nWe will construct a nondeterministic pushdown automaton that accepts the\\nset L of all strings in {a, b}∗having an odd length and whose middle symbol\\nis b, i.e.,\\nL = {vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|}.\\nThe idea is as follows. The automaton uses two states q and q′, where q\\nis the start state. These states have the following meaning:\\n• If the automaton is in state q, then it has not reached the middle symbol\\nb of the input string.\\n• If the automaton is in state q′, then it has read the middle symbol b.\\nObserve that since the automaton can only make one single pass over the\\ninput string, it has to “guess” (i.e., use nondeterminism) when it reaches the\\nmiddle of the string.\\n• If the automaton is in state q, then, when reading the current tape\\nsymbol,\\n– it either pushes one symbol S onto the stack and stays in state q\\n– or, in case the current tape symbol is b, it “guesses” that it has\\nreached the middle of the input string, by switching to state q′.\\n• If the automaton is in state q′, then, when reading the current tape\\nsymbol, it pops the top symbol S from the stack and stays in state q′.\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n119\\nIn this way, the number of symbols S on the stack will always be equal to the\\ndiﬀerence of (i) the number of symbols in the part to the left of the middle\\nsymbol b that have been read and (ii) the number of symbols in the part\\nto the right of the middle symbol b that have been read; additionally, the\\nbottom of the stack will contain the special symbol $.\\nThe input string is accepted if and only if, at the moment when the blank\\nsymbol 2 is read, the automaton is in state q′ and the top symbol on the\\nstack is $. In this case, the stack is made empty and, thus, the computation\\nterminates.\\nWe obtain the nondeterministic pushdown automaton M = (Σ, Γ, Q, δ, q),\\nwhere Σ = {a, b}, Γ = {$, S}, Q = {q, q′}, q is the start state, and the\\ntransition function δ is speciﬁed by the following instructions:\\nqa$ →qR$S\\npush S onto the stack\\nqaS →qRSS\\npush S onto the stack\\nqb$ →q′R$\\nreached the middle\\nqb$ →qR$S\\ndid not reach the middle; push S onto the stack\\nqbS →q′RS\\nreached the middle\\nqbS →qRSS\\ndid not reach the middle; push S onto the stack\\nq2$ →qN$\\ninput string is empty; loop forever\\nq2S →qNS\\nloop forever\\nq′a$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′aS →q′Rϵ\\npop top symbol from stack\\nq′b$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′bS →q′Rϵ\\npop top symbol from stack\\nq′2$ →q′Nϵ\\naccept\\nq′2S →q′NS\\nloop forever\\nRemark 3.6.1 It can be shown that there is no deterministic pushdown\\nautomaton that accepts the language L. The reason is that a deterministic\\npushdown automaton cannot determine when it reaches the middle of the\\ninput string. Thus, unlike as for ﬁnite automata, nondeterministic pushdown\\nautomata are more powerful than their deterministic counterparts.\\n120\\nChapter 3.\\nContext-Free Languages\\n3.7\\nEquivalence of pushdown automata and\\ncontext-free grammars\\nThe main result of this section is that nondeterministic pushdown automata\\nand context-free grammars are equivalent in power:\\nTheorem 3.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then\\nA is context-free if and only if there exists a nondeterministic pushdown\\nautomaton that accepts A.\\nWe will only prove one direction of this theorem. That is, we will show\\nhow to convert an arbitrary context-free grammar to a nondeterministic push-\\ndown automaton.\\nLet G = (V, Σ, R, $) be a context-free grammar, where V is the set of\\nvariables, Σ is the set of terminals, R is the set of rules, and $ is the start\\nvariable. By Theorem 3.4.2, we may assume that G is in Chomsky normal\\nform. Hence, every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are variables, B ̸= $, and C ̸= $.\\n2. A →a, where A is a variable and a is a terminal.\\n3. $ →ϵ.\\nWe will construct a nondeterministic pushdown automaton M that ac-\\ncepts the language L(G) of this grammar G. Observe that M must have the\\nfollowing property: For every string w = a1a2 . . . an ∈Σ∗,\\nw ∈L(G) if and only if M accepts w.\\nThis can be reformulated as follows:\\n$\\n∗⇒a1a2 . . . an\\nif and only if there exists a computation of M that starts in the initial\\nconﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n121\\nand ends in the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nwhere ∅indicates that the stack is empty.\\nAssume that $\\n∗⇒a1a2 . . . an. Then there exists a derivation (using the\\nrules of R) of the string a1a2 . . . an from the start variable $. We may assume\\nthat in each step in this derivation, a rule is applied to the leftmost variable\\nin the current string. Hence, because the grammar G is in Chomsky normal\\nform, at any moment during the derivation, the current string has the form\\na1a2 . . . ai−1AkAk−1 . . . A1,\\n(3.2)\\nfor some integers i and k with 1 ≤i ≤n + 1 and k ≥0, and variables\\nA1, A2, . . . , Ak. (In particular, at the start of the derivation, we have i = 1\\nand k = 1, and the current string is Ak = $. At the end of the derivation,\\nwe have i = n + 1 and k = 0, and the current string is a1a2 . . . an.)\\nWe will deﬁne the pushdown automaton M in such a way that the current\\nstring (3.2) corresponds to the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nBased on this discussion, we obtain the nondeterministic pushdown au-\\ntomaton M = (Σ, V, {q}, δ, q), where\\n• the tape alphabet is the set Σ of terminals of G,\\n• the stack alphabet is the set V of variables of G,\\n• the set of states consists of one state q, which is the start state, and\\n• the transition function δ is obtained from the rules in R, in the following\\nway:\\n122\\nChapter 3.\\nContext-Free Languages\\n– For each rule in R that is of the form A →BC, with A, B, C ∈V ,\\nthe pushdown automaton M has the instructions\\nqaA →qNCB, for all a ∈Σ.\\n– For each rule in R that is of the form A →a, with A ∈V and\\na ∈Σ, the pushdown automaton M has the instruction\\nqaA →qRϵ.\\n– If R contains the rule $ →ϵ, then the pushdown automaton M\\nhas the instruction\\nq2$ →qNϵ.\\nThis concludes the deﬁnition of M. It remains to prove that L(M) =\\nL(G), i.e., the language of the nondeterministic pushdown automaton M is\\nequal to the language of the context-free grammar G. Hence, we have to\\nshow that for every string w ∈Σ∗,\\nw ∈L(G) if and only if w ∈L(M),\\nwhich can be rewritten as\\n$\\n∗⇒w if and only if M accepts w.\\nClaim 3.7.2 Let a1a2 . . . an be a string in Σ∗, let A1, A2, . . . , Ak be variables\\nin V , and let i and k be integers with 1 ≤i ≤n + 1 and k ≥0. Then the\\nfollowing holds:\\n$\\n∗⇒a1a2 . . . ai−1AkAk−1 . . . A1\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n123\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nProof. The claim can be proved by induction. Let\\nw = a1a2 . . . ai−1AkAk−1 . . . A1.\\nAssume that k ≥1 and assume that the claim is true for the string w. Then\\nwe have to show that the claim is still true after applying a rule in R to the\\nleftmost variable Ak in w. Since the grammar is in Chomsky normal form,\\nthe rule to be applied is either of the form Ak →BC or of the form Ak →ai.\\nIn both cases, the property mentioned in the claim is maintained.\\nWe now use Claim 3.7.2 to prove that L(M) = L(G). Let w = a1a2 . . . an\\nbe an arbitrary string in Σ∗. By applying Claim 3.7.2, with i = n + 1 and\\nk = 0, we see that w ∈L(G), i.e.,\\n$\\n∗⇒a1a2 . . . an,\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nBut this means that w ∈L(G) if and only if the automaton M accepts the\\nstring w.\\nThis concludes the proof of the fact that every context-free grammar can\\nbe converted to a nondeterministic pushdown automaton.\\nAs mentioned\\nalready, we will not give the conversion in the other direction. We ﬁnish this\\nsection with the following observation:\\n124\\nChapter 3.\\nContext-Free Languages\\nTheorem 3.7.3 Let Σ be an alphabet and let A ⊆Σ∗be a context-free lan-\\nguage. Then there exists a nondeterministic pushdown automaton that ac-\\ncepts A and has only one state.\\nProof. Since A is context-free, there exists a context-free grammar G0 such\\nthat L(G0) = A. By Theorem 3.4.2, there exists a context-free grammar G\\nthat is in Chomsky normal form and for which L(G) = L(G0). The construc-\\ntion given above converts G to a nondeterministic pushdown automaton M\\nthat has only one state and for which L(M) = L(G).\\n3.8\\nThe pumping lemma for context-free lan-\\nguages\\nIn Section 2.9, we proved the pumping lemma for regular languages and\\nused it to prove that certain languages are not regular. In this section, we\\ngeneralize the pumping lemma to context-free languages.\\nThe idea is to\\nconsider the parse tree (see Section 3.1) that describes the derivation of a\\nsuﬃciently long string in the context-free language L. Since the number of\\nvariables in the corresponding context-free grammar G is ﬁnite, there is at\\nleast one variable, say Aj, that occurs more than once on the longest root-\\nto-leaf path in the parse tree. The subtree which is sandwiched between two\\noccurrences of Aj on this path can be copied any number of times. This will\\nresult in a legal parse tree and, hence, in a “pumped” string that is in the\\nlanguage L.\\nTheorem 3.8.1 (Pumping Lemma for Context-Free Languages) Let\\nL be a context-free language. Then there exists an integer p ≥1, called the\\npumping length, such that the following holds: Every string s in L, with\\n|s| ≥p, can be written as s = uvxyz, such that\\n1. |vy| ≥1 (i.e., v and y are not both empty),\\n2. |vxy| ≤p, and\\n3. uvixyiz ∈L, for all i ≥0.\\n3.8.\\nThe pumping lemma for context-free languages\\n125\\n3.8.1\\nProof of the pumping lemma\\nThe proof of the pumping lemma will use the following result about parse\\ntrees:\\nLemma 3.8.2 Let G be a context-free grammar in Chomsky normal form,\\nlet s be a non-empty string in L(G), and let T be a parse tree for s. Let ℓbe\\nthe height of T, i.e., ℓis the number of edges on a longest root-to-leaf path\\nin T. Then\\n|s| ≤2ℓ−1.\\nProof. The claim can be proved by induction on ℓ. By looking at some\\nsmall values of ℓand using the fact that G is in Chomsky normal form, you\\nshould be able to verify the claim.\\nNow we can start with the proof of the pumping lemma. Let L be a\\ncontext-free language and let Σ be the alphabet of L. By Theorem 3.4.2, there\\nexists a context-free grammar in Chomsky normal form, G = (V, Σ, R, S),\\nsuch that L = L(G).\\nDeﬁne r to be the number of variables of G and deﬁne p = 2r. We will\\nprove that the value of p can be used as the pumping length. Consider an\\narbitrary string s in L such that |s| ≥p, and let T be a parse tree for s. Let\\nℓbe the height of T. Then, by Lemma 3.8.2, we have\\n|s| ≤2ℓ−1.\\nOn the other hand, we have\\n|s| ≥p = 2r.\\nBy combining these inequalities, we see that 2r ≤2ℓ−1, which can be rewrit-\\nten as\\nℓ≥r + 1.\\nConsider the nodes on a longest root-to-leaf path in T.\\nSince this path\\nconsists of ℓedges, it consists of ℓ+ 1 nodes. The ﬁrst ℓof these nodes store\\nvariables, which we denote by A0, A1, . . . , Aℓ−1 (where A0 = S), and the last\\nnode (which is a leaf) stores a terminal, which we denote by a.\\nSince ℓ−1 −r ≥0, the sequence\\nAℓ−1−r, Aℓ−r, . . . , Aℓ−1\\n126\\nChapter 3.\\nContext-Free Languages\\nof variables is well-deﬁned.\\nObserve that this sequence consists of r + 1\\nvariables. Since the number of variables in the grammar G is equal to r,\\nthe pigeonhole principle implies that there is a variable that occurs at least\\ntwice in this sequence. In other words, there are indices j and k, such that\\nℓ−1 −r ≤j < k ≤ℓ−1 and Aj = Ak. Refer to the ﬁgure below for an\\nillustration.\\nS\\nA j\\nAk\\nu\\nv\\nx\\ny\\nz\\ns\\nA0 = S\\nA1\\nAℓ−1−r\\nAℓ−r\\nAℓ−2\\nAℓ−1\\na\\nr +1\\nvariables\\nRecall that T is a parse tree for the string s. Therefore, the terminals\\nstored at the leaves of T, in the order from left to right, form s. As indicated\\nin the ﬁgure above, the nodes storing the variables Aj and Ak partition s\\ninto ﬁve substrings u, v, x, y, and z, such that s = uvxyz.\\nIt remains to prove that the three properties stated in the pumping lemma\\n3.8.\\nThe pumping lemma for context-free languages\\n127\\nhold. We start with the third property, i.e., we prove that\\nuvixyiz ∈L, for all i ≥0.\\nIn the grammar G, we have\\nS\\n∗⇒uAjz.\\n(3.3)\\nSince Aj\\n∗⇒vAky and Ak = Aj, we have\\nAj\\n∗⇒vAjy.\\n(3.4)\\nFinally, since Ak\\n∗⇒x and Ak = Aj, we have\\nAj\\n∗⇒x.\\n(3.5)\\nFrom (3.3) and (3.5), it follows that\\nS\\n∗⇒uAjz\\n∗⇒uxz,\\nwhich implies that the string uxz is in the language L. Similarly, it follows\\nfrom (3.3), (3.4), and (3.5) that\\nS\\n∗⇒uAjz\\n∗⇒uvAjyz\\n∗⇒uvvAjyyz\\n∗⇒uvvxyyz.\\nHence, the string uv2xy2z is in the language L. In general, for each i ≥0,\\nthe string uvixyiz is in the language L, because\\nS\\n∗⇒uAjz\\n∗⇒uviAjyiz\\n∗⇒uvixyiz.\\nThis proves that the third property in the pumping lemma holds.\\nNext we show that the second property holds. That is, we prove that\\n|vxy| ≤p.\\nConsider the subtree rooted at the node storing the variable\\nAj.\\nThe path from the node storing Aj to the leaf storing the terminal\\na is a longest path in this subtree. (Convince yourself that this is true.)\\nMoreover, this path consists of ℓ−j edges. Since Aj\\n∗⇒vxy, this subtree\\nis a parse tree for the string vxy (where Aj is used as the start variable).\\nTherefore, by Lemma 3.8.2, we can conclude that |vxy| ≤2ℓ−j−1. We know\\nthat ℓ−1 −r ≤j, which is equivalent to ℓ−j −1 ≤r. It follows that\\n|vxy| ≤2ℓ−j−1 ≤2r = p.\\n128\\nChapter 3.\\nContext-Free Languages\\nFinally, we show that the ﬁrst property in the pumping lemma holds.\\nThat is, we prove that |vy| ≥1. Recall that\\nAj\\n∗⇒vAky.\\nLet the ﬁrst rule used in this derivation be Aj →BC. (Since the variables\\nAj and Ak, even though they are equal, are stored at diﬀerent nodes of the\\nparse tree, and since the grammar G is in Chomsky normal form, this ﬁrst\\nrule exists.) Then\\nAj ⇒BC\\n∗⇒vAky.\\nObserve that the string BC has length two. Moreover, by applying rules of\\na grammar in Chomsky normal form, strings cannot become shorter. (Here,\\nwe use the fact that the start variable does not occur on the right-hand side\\nof any rule.) Therefore, we have |vAky| ≥2. But this implies that |vy| ≥1.\\nThis completes the proof of the pumping lemma.\\n3.8.2\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {anbncn : n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. Consider the string s = apbpcp.\\nObserve that s ∈A and |s| = 3p ≥p. Hence, by the pumping lemma, s can\\nbe written as s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all\\ni ≥0.\\nObserve that the pumping lemma does not tell us the location of the\\nsubstring vxy in the string s, it only gives us an upper bound on the length\\nof this substring. Therefore, we have to consider three cases, depending on\\nthe location of vxy in s.\\nCase 1: The substring vxy does not contain any c.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many as or more than p many bs. Since it contains\\nexactly p many cs, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\n3.8.\\nThe pumping lemma for context-free languages\\n129\\nCase 2: The substring vxy does not contain any a.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many bs or more than p many cs. Since it contains\\nexactly p many as, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\nCase 3: The substring vxy contains at least one a and at least one c.\\nSince s = apbpcp, this implies that |vxy| > p, which again contradicts the\\npumping lemma.\\nThus, in all of the three cases, we have obtained a contradiction. There-\\nfore, we have shown that the language A is not context-free.\\nSecond example\\nConsider the languages\\nA = {wwR : w ∈{a, b}∗},\\nwhere wR is the string obtained by writing w backwards, and\\nB = {ww : w ∈{a, b}∗}.\\nEven though these languages look similar, we will show that A is context-free\\nand B is not context-free.\\nConsider the following context-free grammar, in which S is the start vari-\\nable:\\nS →ϵ|aSa|bSb.\\nIt is easy to see that the language of this grammar is exactly the language A.\\nTherefore, A is context-free. Alternatively, we can show that A is context-\\nfree, by constructing a (nondeterministic) pushdown automaton that accepts\\nA. This automaton has two states q and q′, where q is the start state. If the\\nautomaton is in state q, then it did not yet ﬁnish reading the leftmost half of\\nthe input string; it pushes all symbols read onto the stack. If the automaton\\nis in state q′, then it is reading the rightmost half of the input string; for each\\nsymbol read, it checks whether it is equal to the symbol on top of the stack\\nand, if so, pops the top symbol from the stack. The pushdown automaton\\nuses nondeterminism to “guess” when to switch from state q to state q′ (i.e.,\\nwhen it has completed reading the leftmost half of the input string).\\n130\\nChapter 3.\\nContext-Free Languages\\nAt this point, you should convince yourself that the two approaches above,\\nwhich showed that A is context-free, do not work for B. The reason why\\nthey do not work is that the language B is not context-free, as we will prove\\nnow.\\nAssume that B is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. At this point, we must choose a\\nstring s in B, whose length is at least p, and that does not satisfy the three\\nproperties stated in the pumping lemma. Let us try the string s = apbapb.\\nThen s ∈B and |s| = 2p + 2 ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = uvxyz, where (i) |vy| ≥1, (ii) |vxy| ≤p, and (iii) uvixyiz ∈B\\nfor all i ≥0. It may happen that p ≥3, u = ap−1, v = a, x = b, y = a,\\nand z = ap−1b. If this is the case, then properties (i), (ii), and (iii) hold,\\nand, thus, we do not get a contradiction. In other words, we have chosen\\nthe “wrong” string s. This string is “wrong”, because there is only one b\\nbetween the as. Because of this, v can be in the leftmost block of as, and\\ny can be in the rightmost block of as. Observe that if there were at least p\\nmany bs between the as, then this would not happen, because |vxy| ≤p.\\nBased on the discussion above, we choose s = apbpapbp. Observe that\\ns ∈B and |s| = 4p ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈B for all i ≥0. Based\\non the location of vxy in the string s, we distinguish three cases:\\nCase 1: The substring vxy overlaps both the leftmost half and the rightmost\\nhalf of s.\\nSince |vxy| ≤p, the substring vxy is contained in the “middle” part of s,\\ni.e., vxy is contained in the block bpap. Consider the string uv0xy0z = uxz.\\nSince |vy| ≥1, we know that at least one of v and y is non-empty.\\n• If v ̸= ϵ, then v contains at least one b from the leftmost block of bs in\\ns, whereas y does not contain any b from the rightmost block of bs in s.\\nTherefore, in the string uxz, the leftmost block of bs contains fewer bs\\nthan the rightmost block of bs. Hence, the string uxz is not contained\\nin B.\\n• If y ̸= ϵ, then y contains at least one a from the rightmost block of\\nas in s, whereas v does not contain any a from the leftmost block of\\nas in s. Therefore, in the string uxz, the leftmost block of as contains\\nmore as than the rightmost block of as. Hence, the string uxz is not\\ncontained in B.\\n3.8.\\nThe pumping lemma for context-free languages\\n131\\nIn both cases, we conclude that the string uxz is not an element of the\\nlanguage B. But, by the pumping lemma, this string is contained in B.\\nCase 2: The substring vxy is in the leftmost half of s.\\nIn this case, none of the strings uxz, uv2xy2z, uv3xy3z, uv4xy4z, etc.,\\nis contained in B.\\nBut, by the pumping lemma, each of these strings is\\ncontained in B.\\nCase 3: The substring vxy is in the rightmost half of s.\\nThis case is symmetric to Case 2: None of the strings uxz, uv2xy2z,\\nuv3xy3z, uv4xy4z, etc., is contained in B. But, by the pumping lemma, each\\nof these strings is contained in B.\\nTo summarize, in each of the three cases, we have obtained a contradic-\\ntion. Therefore, the language B is not context-free.\\nThird example\\nWe have seen in Section 3.2.4 that the language\\n{ambncm+n : m ≥0, n ≥0}\\nis context-free. Using the pumping lemma for regular languages, it is easy to\\nprove that this language is not regular. In other words, context-free gram-\\nmars can verify addition, whereas ﬁnite automata are not powerful enough\\nfor this. We now consider the problem of verifying multiplication: Let A be\\nthe language deﬁned as\\nA = {ambncmn : m ≥0, n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is context-free. Let p ≥1 be the pumping length, as\\ngiven by the pumping lemma. Consider the string s = apbpcp2. Then, s ∈A\\nand |s| = 2p + p2 ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all i ≥0.\\nThere are three possible cases, depending on the locations of v and y in\\nthe string s.\\nCase 1: The substring v does not contain any a and does not contain any\\nb, and the substring y does not contain any a and does not contain any b.\\n132\\nChapter 3.\\nContext-Free Languages\\nConsider the string uv2xy2z. Since |vy| ≥1, this string consists of p\\nmany as, p many bs, but more than p2 many cs. Therefore, this string is not\\ncontained in A. But, by the pumping lemma, it is contained in A.\\nCase 2: The substring v does not contain any c and the substring y does\\nnot contain any c.\\nConsider again the string uv2xy2z. This string consists of p2 many cs.\\nSince |vy| ≥1, in this string,\\n• the number of as is at least p + 1 and the number of bs is at least p, or\\n• the number of as is at least p and the number of bs is at least p + 1.\\nTherefore, the number of as multiplied by the number of bs is at least p(p+1),\\nwhich is larger than p2. Therefore, uv2xy2z is not contained in A. But, by\\nthe pumping lemma, this string is contained in A.\\nCase 3: The substring v contains at least one b and the substring y contains\\nat least one c.\\nSince |vxy| ≤p, the substring vy does not contain any a. Thus, we can\\nwrite vy = bjck, where j ≥1 and k ≥1. Consider the string uxz. We can\\nwrite this string as uxz = apbp−jcp2−k. Since, by the pumping lemma, this\\nstring is contained in A, we have p(p−j) = p2−k, which implies that jp = k.\\nThus,\\n|vxy| ≥|vy| = j + k = j + jp ≥1 + p.\\nBut, by the pumping lemma, we have |vxy| ≤p.\\nObserve that, since |vxy| ≤p, the above three cases cover all possibilities\\nfor the locations of v and y in the string s. In each of the three cases, we\\nhave obtained a contradiction. Therefore, the language A is not context-free.\\nExercises\\n3.1 Construct context-free grammars that generate the following languages.\\nIn all cases, Σ = {0, 1}.\\n• {02n1n : n ≥0}\\n• {w : w contains at least three 1s}\\n• {w : the length of w is odd and its middle symbol is 0}\\nExercises\\n133\\n• {w : w is a palindrome}.\\nA palindrome is a string w having the property that w = wR, i.e.,\\nreading w from left to right gives the same result as reading w from\\nright to left.\\n• {w : w starts and ends with the same symbol}\\n• {w : w starts and ends with diﬀerent symbols}\\n3.2 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {0, 1}, S is the start variable, and R consists of the rules\\nS\\n→\\n0S|1A|ϵ\\nA\\n→\\n0B|1S\\nB\\n→\\n0A|1B\\nDeﬁne the following language L:\\nL = {w ∈{0, 1}∗:\\nw is the binary representation of a non-negative\\ninteger that is divisible by three } ∪{ϵ}.\\nProve that L = L(G). (Hint: The variables S, A, and B are used to\\nremember the remainder after division by three.)\\n3.3 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {a, b}, S is the start variable, and R consists of the rules\\nS\\n→\\naB|bA\\nA\\n→\\na|aS|BAA\\nB\\n→\\nb|bS|ABB\\n• Prove that ababba ∈L(G).\\n• Prove that L(G) is the set of all non-empty strings w over the alphabet\\n{a, b} such that the number of as in w is equal to the number of bs in\\nw.\\n3.4 Let A and B be context-free languages over the same alphabet Σ.\\n• Prove that the union A ∪B of A and B is also context-free.\\n• Prove that the concatenation AB of A and B is also context-free.\\n134\\nChapter 3.\\nContext-Free Languages\\n• Prove that the star A∗of A is also context-free.\\n3.5 Deﬁne the following two languages A and B:\\nA = {ambncn : m ≥0, n ≥0}\\nand\\nB = {ambmcn : m ≥0, n ≥0}.\\n• Prove that both A and B are context-free, by constructing two context-\\nfree grammars, one that generates A and one that generates B.\\n• We have seen in Section 3.8.2 that the language\\n{anbncn : n ≥0}\\nis not context-free. Explain why this implies that the intersection of\\ntwo context-free languages is not necessarily context-free.\\n• Use De Morgan’s Law to conclude that the complement of a context-\\nfree language is not necessarily context-free.\\n3.6 Let A be a context-free language and let B be a regular language.\\n• Prove that the intersection A ∩B of A and B is context-free.\\n• Prove that the set-diﬀerence\\nA \\\\ B = {w : w ∈A, w ̸∈B}\\nof A and B is context-free.\\n• Is the set-diﬀerence of two context-free languages necessarily context-\\nfree?\\n3.7 Let L be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that\\n• the number of as in w is equal to the number of bs in w,\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\nExercises\\n135\\nIn this exercise, you will prove that L is context-free.\\nLet A be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that the number of as in w is equal to the number of bs\\nin w. In Exercise 3.3, you have shown that A is context-free.\\nLet B be the language consisting of all strings w over the alphabet {a, b}\\nsuch that\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\n1. Give a regular expression that describes the complement of B.\\n2. Argue that B is a regular language.\\n3. Use Exercise 3.6 to argue that L is a context-free language.\\n3.8 Construct (deterministic or nondeterministic) pushdown automata that\\naccept the following languages.\\n1. {02n1n : n ≥0}.\\n2. {0n1m0n : n ≥1, m ≥1}.\\n3. {w ∈{0, 1}∗: w contains more 1s than 0s}.\\n4. {wwR : w ∈{0, 1}∗}.\\n(If w = w1 . . . wn, then wR = wn . . . w1.)\\n5. {w ∈{0, 1}∗: w is a palindrome}.\\n3.9 Let L be the language\\nL = {ambn : 0 ≤m ≤n ≤2m}.\\n1. Prove that L is context-free, by constructing a context-free grammar\\nwhose language is equal to L.\\n2. Prove that L is context-free, by constructing a nondeterministic push-\\ndown automaton that accepts L.\\n3.10 Prove that the following languages are not context-free.\\n136\\nChapter 3.\\nContext-Free Languages\\n• {an b a2n b a3n : n ≥0}.\\n• {anbnanbn : n ≥0}.\\n• {ambnck : m ≥0, n ≥0, k = max(m, n)}.\\n• {w#x : w is a substring of x, and w, x ∈{a, b}∗}.\\nFor example, the string aba#abbababbb is in the language, whereas the\\nstring aba#baabbaabb is not in the language. The alphabet is {a, b, #}.\\n•\\n{ w ∈{a, b, c}∗\\n:\\nw contains more b’s than a’s and\\nw contains more c’s than a’s }.\\n• {1n : n is a prime number}.\\n• {(abn)n : n ≥0}. (The parentheses are not part of the alphabet; thus,\\nthe alphabet is {a, b, }.)\\n3.11 Let L be a language consisting of ﬁnitely many strings. Show that L\\nis regular and, therefore, context-free. Let k be the maximum length of any\\nstring in L.\\n• Prove that every context-free grammar in Chomsky normal form that\\ngenerates L has more than log k variables. (The logarithm is in base\\n2.)\\n• Prove that there is a context-free grammar that generates L and that\\nhas only one variable.\\n3.12 Let L be a context-free language. Prove that there exists an integer\\np ≥1, such that the following is true: For every string s in L with |s| ≥p,\\nthere exists a string s′ in L such that |s| < |s′| ≤|s| + p.\\nChapter 4\\nTuring Machines and the\\nChurch-Turing Thesis\\nIn the previous chapters, we have seen several computational devices that\\ncan be used to accept or generate regular and context-free languages. Even\\nthough these two classes of languages are fairly large, we have seen in Sec-\\ntion 3.8.2 that these devices are not powerful enough to accept simple lan-\\nguages such as A = {ambncmn : m ≥0, n ≥0}. In this chapter, we introduce\\nthe Turing machine, which is a simple model of a real computer. Turing ma-\\nchines can be used to accept all context-free languages, but also languages\\nsuch as A. We will argue that every problem that can be solved on a real\\ncomputer can also be solved by a Turing machine (this statement is known\\nas the Church-Turing Thesis). In Chapter 5, we will consider the limitations\\nof Turing machines and, hence, of real computers.\\n4.1\\nDeﬁnition of a Turing machine\\nWe start with an informal description of a Turing machine. Such a machine\\nconsists of the following, see also Figure 4.1.\\n1. There are k tapes, for some ﬁxed k ≥1. Each tape is divided into\\ncells, and is inﬁnite both to the left and to the right. Each cell stores\\na symbol belonging to a ﬁnite set Γ, which is called the tape alphabet.\\nThe tape alphabet contains the blank symbol 2. If a cell contains 2,\\nthen this means that the cell is actually empty.\\n138\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nstate control\\n. . . 2 2 2 a a b a b b a b a b 2 2 2\\n. . .\\n?\\n. . . 2 2 2 b a a b 2 a b 2 2 2\\n. . .\\n?\\nFigure 4.1: A Turing machine with k = 2 tapes.\\n2. Each tape has a tape head which can move along the tape, one cell\\nper move. It can also read the cell it currently scans and replace the\\nsymbol in this cell by another symbol.\\n3. There is a state control, which can be in any one of a ﬁnite number of\\nstates. The ﬁnite set of states is denoted by Q. The set Q contains\\nthree special states: a start state, an accept state, and a reject state.\\nThe Turing machine performs a sequence of computation steps. In one\\nsuch step, it does the following:\\n1. Immediately before the computation step, the Turing machine is in a\\nstate r of Q, and each of the k tape heads is on a certain cell.\\n2. Depending on the current state r and the k symbols that are read by\\nthe tape heads,\\n(a) the Turing machine switches to a state r′ of Q (which may be\\nequal to r),\\n(b) each tape head writes a symbol of Γ in the cell it is currently\\nscanning (this symbol may be equal to the symbol currently stored\\nin the cell), and\\n4.1.\\nDeﬁnition of a Turing machine\\n139\\n(c) each tape head either moves one cell to the left, moves one cell to\\nthe right, or stays at the current cell.\\nWe now give a formal deﬁnition of a deterministic Turing machine.\\nDeﬁnition 4.1.1 A deterministic Turing machine is a 7-tuple\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject),\\nwhere\\n1. Σ is a ﬁnite set, called the input alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the tape alphabet; this alphabet contains the\\nblank symbol 2, and Σ ⊆Γ,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. qaccept is an element of Q; it is called the accept state,\\n6. qreject is an element of Q; it is called the reject state,\\n7. δ is called the transition function, which is a function\\nδ : Q × Γk →Q × Γk × {L, R, N}k.\\nThe transition function δ is basically the “program” of the Turing ma-\\nchine. This function tells us what the machine can do in “one computation\\nstep”: Let r ∈Q, and let a1, a2, . . . , ak ∈Γ.\\nFurthermore, let r′ ∈Q,\\na′\\n1, a′\\n2, . . . , a′\\nk ∈Γ, and σ1, σ2, . . . , σk ∈{L, R, N} be such that\\nδ(r, a1, a2, . . . , ak) = (r′, a′\\n1, a′\\n2, . . . , a′\\nk, σ1, σ2, . . . , σk).\\n(4.1)\\nThis transition means that if\\n• the Turing machine is in state r, and\\n• the head of the i-th tape reads the symbol ai, 1 ≤i ≤k,\\nthen\\n140\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• the Turing machine switches to state r′,\\n• the head of the i-th tape replaces the scanned symbol ai by the symbol\\na′\\ni, 1 ≤i ≤k, and\\n• the head of the i-th tape moves according to σi, 1 ≤i ≤k: if σi = L,\\nthen the tape head moves one cell to the left; if σi = R, then it moves\\none cell to the right; if σi = N, then the tape head does not move.\\nWe will write the computation step (4.1) in the form of the instruction\\nra1a2 . . . ak →r′a′\\n1a′\\n2 . . . a′\\nkσ1σ2 . . . σk.\\nWe now specify the computation of the Turing machine\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject).\\nStart conﬁguration: The input is a string over the input alphabet Σ.\\nInitially, this input string is stored on the ﬁrst tape, and the head of this\\ntape is on the leftmost symbol of the input string. Initially, all other k −1\\ntapes are empty, i.e., only contain blank symbols, and the Turing machine is\\nin the start state q.\\nComputation and termination: Starting in the start conﬁguration, the\\nTuring machine performs a sequence of computation steps as described above.\\nThe computation terminates at the moment when the Turing machine en-\\nters the accept state qaccept or the reject state qreject. (Hence, if the Turing\\nmachine never enters the states qaccept and qreject, the computation does not\\nterminate.)\\nAcceptance: The Turing machine M accepts the input string w ∈Σ∗, if the\\ncomputation on this input terminates in the state qaccept. If the computation\\non this input terminates in the state qreject, then M rejects the input string\\nw.\\nWe denote by L(M) the language accepted by the Turing machine M.\\nThus, L(M) is the set of all strings in Σ∗that are accepted by M.\\nObserve that a string w ∈Σ∗does not belong to L(M) if and only if on\\ninput w,\\n• the computation of M terminates in the state qreject or\\n• the computation of M does not terminate.\\n4.2.\\nExamples of Turing machines\\n141\\n4.2\\nExamples of Turing machines\\n4.2.1\\nAccepting palindromes using one tape\\nWe will show how to construct a Turing machine with one tape, that decides\\nwhether or not any input string w ∈{a, b}∗is a palindrome. Recall that the\\nstring w is called a palindrome, if reading w from left to right gives the same\\nresult as reading w from right to left. Examples of palindromes are abba,\\nbaabbbbaab, and the empty string ϵ.\\nStart of the computation: The tape contains the input string w, the tape\\nhead is on the leftmost symbol of w, and the Turing machine is in the start\\nstate q0.\\nIdea: The tape head reads the leftmost symbol of w, deletes this symbol\\nand “remembers” it by means of a state.\\nThen the tape head moves to\\nthe rightmost symbol and tests whether it is equal to the (already deleted)\\nleftmost symbol.\\n• If they are equal, then the rightmost symbol is deleted, the tape head\\nmoves to the new leftmost symbol, and the whole process is repeated.\\n• If they are not equal, the Turing machine enters the reject state, and\\nthe computation terminates.\\nThe Turing machine enters the accept state as soon as the string currently\\nstored on the tape is empty.\\nWe will use the input alphabet Σ = {a, b} and the tape alphabet Γ =\\n{a, b, 2}. The set Q of states consists of the following eight states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost symbol was a; tape head is moving to the right\\nqb :\\nleftmost symbol was b; tape head is moving to the right\\nq′\\na :\\nreached rightmost symbol; test whether it is equal to a, and delete it\\nq′\\nb :\\nreached rightmost symbol; test whether it is equal to b, and delete it\\nqL :\\ntest was positive; tape head is moving to the left\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n142\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nqba →qbaR\\nq0b →qb2R\\nqab →qabR\\nqbb →qbbR\\nq02 →qaccept\\nqa2 →q′\\na2L\\nqb2 →q′\\nb2L\\nq′\\naa →qL2L\\nq′\\nba →qreject\\nqLa →qLaL\\nq′\\nab →qreject\\nq′\\nbb →qL2L\\nqLb →qLbL\\nq′\\na2 →qaccept\\nq′\\nb2 →qaccept\\nqL2 →q02R\\nYou should go through the computation of this Turing machine for some\\nsample inputs, for example abba, b, abb and the empty string (which is a\\npalindrome).\\n4.2.2\\nAccepting palindromes using two tapes\\nWe again consider the palindrome problem, but now we use a Turing machine\\nwith two tapes.\\nStart of the computation: The ﬁrst tape contains the input string w and\\nthe head of the ﬁrst tape is on the leftmost symbol of w. The second tape is\\nempty and its tape head is at an arbitrary position. The Turing machine is\\nin the start state q0.\\nIdea: First, the input string w is copied to the second tape. Then the head\\nof the ﬁrst tape moves back to the leftmost symbol of w, while the head of\\nthe second tape stays at the rightmost symbol of w. Finally, the actual test\\nstarts: The head of the ﬁrst tape moves to the right and, at the same time,\\nthe head of the second tape moves to the left. While moving, the Turing\\nmachine tests whether the two tape heads read the same symbol in each\\nstep.\\nThe input alphabet is Σ = {a, b} and the tape alphabet is Γ = {a, b, 2}.\\nThe set Q of states consists of the following ﬁve states:\\nq0 :\\nstart state; copy w to the second tape\\nq1 :\\nw has been copied; head of ﬁrst tape moves to the left\\nq2 :\\nhead of ﬁrst tape moves to the right; head of second tape moves\\nto the left; until now, all tests were positive\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n4.2.\\nExamples of Turing machines\\n143\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a2 →q0aaRR\\nq1aa →q1aaLN\\nq0b2 →q0bbRR\\nq1ab →q1abLN\\nq022 →q122LL\\nq1ba →q1baLN\\nq1bb →q1bbLN\\nq12a →q22aRN\\nq12b →q22bRN\\nq122 →qaccept\\nq2aa →q2aaRL\\nq2ab →qreject\\nq2ba →qreject\\nq2bb →q2bbRL\\nq222 →qaccept\\nAgain, you should run this Turing machine for some sample inputs.\\n4.2.3\\nAccepting anbncn using one tape\\nWe will construct1 a Turing machine with one tape that accepts the language\\n{anbncn : n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: In the previous examples, the tape alphabet Γ was equal to the union\\nof the input alphabet Σ and {2}. In this example, we will add one symbol\\nd to the tape alphabet. As we will see, this simpliﬁes the construction of\\nthe Turing machine. Thus, the input alphabet is Σ = {a, b, c} and the tape\\nalphabet is Γ = {a, b, c, d, 2}. Recall that the input string w belongs to Σ∗.\\nThe general approach is to split the computation into two stages.\\n1Thanks to Michael Fleming for pointing out an error in a previous version of this\\nconstruction.\\n144\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nStage 1: In this stage, we check if the string w is in the language described\\nby the regular expression a∗b∗c∗. If this is the case, then we walk back to\\nthe leftmost symbol. For this stage, we use the following states, besides the\\nstates qaccept and qreject:\\nqa :\\nstart state; we are reading the block of a’s\\nqb :\\nwe are reading the block of b’s\\nqc :\\nwe are reading the block of c’s\\nqL :\\nwalk to the leftmost symbol\\nStage 2: In this stage, we repeat the following: Walk along the string from\\nleft to right, replace the leftmost a by d, replace the leftmost b by d, replace\\nthe leftmost c by d, and walk back to the leftmost symbol.\\nFor this stage, we use the following states:\\nq′\\na :\\nstart state of Stage 2; search for the leftmost a\\nq′\\nb :\\nleftmost a has been replaced by d;\\nsearch for the leftmost b\\nq′\\nc :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nsearch for the leftmost c\\nq′\\nL :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nleftmost c has been replaced by d;\\nwalk to the leftmost symbol\\nThe transition function δ is speciﬁed by the following instructions:\\nqaa →qaaR\\nqba →qreject\\nqab →qbbR\\nqbb →qbbR\\nqac →qccR\\nqbc →qccR\\nqad →cannot happen\\nqbd →cannot happen\\nqa2 →qL2L\\nqb2 →qL2L\\nqca →qreject\\nqLa →qLaL\\nqcb →qreject\\nqLb →qLbL\\nqcc →qccR\\nqLc →qLcL\\nqcd →cannot happen\\nqLd →cannot happen\\nqc2 →qL2L\\nqL2 →q′\\na2R\\n4.2.\\nExamples of Turing machines\\n145\\nq′\\naa →q′\\nbdR\\nq′\\nba →q′\\nbaR\\nq′\\nab →qreject\\nq′\\nbb →q′\\ncdR\\nq′\\nac →qreject\\nq′\\nbc →qreject\\nq′\\nad →q′\\nadR\\nq′\\nbd →q′\\nbdR\\nq′\\na2 →qaccept\\nq′\\nb2 →qreject\\nq′\\nca →qreject\\nq′\\nLa →q′\\nLaL\\nq′\\ncb →q′\\ncbR\\nq′\\nLb →q′\\nLbL\\nq′\\ncc →q′\\nLdL\\nq′\\nLc →q′\\nLcL\\nq′\\ncd →q′\\ncdR\\nq′\\nLd →q′\\nLdL\\nq′\\nc2 →qreject\\nq′\\nL2 →q′\\na2R\\nWe remark that Stage 1 is really necessary for this Turing machine: If we\\nomit this stage, and use only Stage 2, then the string aabcbc will be accepted.\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2}\\nWe consider again the language {anbncn : n ≥0}. In the previous section,\\nwe presented a Turing machine that uses an extra symbol d. The reader may\\nwonder if we can construct a Turing machine for this language that does not\\nuse any extra symbols. We will show below that this is indeed possible.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate q0.\\nIdea: Repeat the following Stages 1 and 2, until the string is empty.\\nStage 1. Walk along the string from left to right, delete the leftmost a,\\ndelete the leftmost b, and delete the rightmost c.\\nStage 2. Shift the substring of bs and cs one position to the left; then walk\\nback to the leftmost symbol.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, 2}.\\n146\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nFor Stage 1, we use the following states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost a has been deleted; have not read b\\nqb :\\nleftmost b has been deleted; have not read c\\nqc :\\nleftmost c has been read; tape head moves to the right\\nq′\\nc :\\ntape head is on the rightmost c\\nq1 :\\nrightmost c has been deleted; tape head is on the rightmost\\nsymbol or 2\\nqaccept :\\naccept state\\nqreject :\\nreject state\\nThe transitions for Stage 1 are speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nq0b →qreject\\nqab →qb2R\\nq0c →qreject\\nqac →qreject\\nq02 →qaccept\\nqa2 →qreject\\nqba →qreject\\nqca →qreject\\nqbb →qbbR\\nqcb →qreject\\nqbc →qccR\\nqcc →qccR\\nqb2 →qreject\\nqc2 →q′\\nc2L\\nq′\\ncc →q12L\\nFor Stage 2, we use the following states:\\nq1 :\\nas above; tape head is on the rightmost symbol or on 2\\nqc :\\ncopy c one cell to the left\\nqb :\\ncopy b one cell to the left\\nq2 :\\ndone with shifting; head moves to the left\\nAdditionally, we use a state q′\\n1 which has the following meaning: If the input\\nstring is of the form aibc, for some i ≥1, then after Stage 1, the tape contains\\nthe string ai−122, the tape head is on the 2 immediately to the right of the\\nas, and the Turing machine is in state q1. In this case, we move one cell to\\nthe left; if we then read 2, then i = 1, and we accept; otherwise, we read a,\\nand we reject.\\n4.2.\\nExamples of Turing machines\\n147\\nThe transitions for Stage 2 are speciﬁed by the following instructions:\\nq1a →cannot happen\\nq′\\n1a →qreject\\nq1b →qreject\\nq′\\n1b →cannot happen\\nq1c →qc2L\\nq′\\n1c →cannot happen\\nq12 →q′\\n12L\\nq′\\n12 →qaccept\\nqca →cannot happen\\nqba →cannot happen\\nqcb →qbcL\\nqbb →qbbL\\nqcc →qccL\\nqbc →cannot happen\\nqc2 →qreject\\nqb2 →q2bL\\nq2a →q2aL\\nq2b →cannot happen\\nq2c →cannot happen\\nq22 →q02R\\n4.2.5\\nAccepting ambncmn using one tape\\nWe will sketch how to construct a Turing machine with one tape that accepts\\nthe language\\n{ambncmn : m ≥0, n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, $, 2},\\nwhere the purpose of the symbol $ will become clear below.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: Observe that a string ambnck is in the language if and only if for every\\na, the string contains n many cs. Based on this, the computation consists of\\nthe following stages:\\nStage 1. Walk along the input string w from left to right and check whether\\nw is an element of the language described by the regular expression a∗b∗c∗.\\nIf this is not the case, then reject the input string. Otherwise, go to Stage 2.\\nStage 2. Walk back to the leftmost symbol of w. Go to Stage 3.\\nStage 3. In this stage, the Turing machine does the following:\\n148\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• Replace the leftmost a by the blank symbol 2.\\n• Walk to the leftmost b.\\n• Zigzag between the bs and cs; each time, replace the leftmost b by the\\nsymbol $, and replace the rightmost c by the blank symbol 2. If, for\\nsome b, there is no c left, the Turing machine rejects the input string.\\n• Continue zigzagging until there are no bs left. Then go to Stage 4.\\nObserve that in this third stage, the string ambnck is transformed to the\\nstring am−1$nck−n.\\nStage 4. In this stage, the Turing machine does the following:\\n• Replace each $ by b.\\n• Walk to the leftmost a.\\nHence, in this fourth stage, the string am−1$nck−n is transformed to the string\\nam−1bnck−n.\\nObserve that the input string ambnck is in the language if and only if the\\nstring am−1bnck−n is in the language. Therefore, the Turing machine repeats\\nStages 3 and 4, until there are no as left. At that moment, it checks whether\\nthere are any cs left; if so, it rejects the input string; otherwise, it accepts\\nthe input string.\\nWe hope that you believe that this description of the algorithm can be\\nturned into a formal description of a Turing machine.\\n4.3\\nMulti-tape Turing machines\\nIn Section 4.2, we have seen two Turing machines that accept palindromes;\\nthe ﬁrst Turing machine has one tape, whereas the second one has two tapes.\\nYou will have noticed that the two-tape Turing machine was easier to obtain\\nthan the one-tape Turing machine. This leads to the question whether multi-\\ntape Turing machines are more powerful than their one-tape counterparts.\\nThe answer is “no”:\\nTheorem 4.3.1 Let k ≥1 be an integer. Any k-tape Turing machine can\\nbe converted to an equivalent one-tape Turing machine.\\n4.3.\\nMulti-tape Turing machines\\n149\\nProof.2\\nWe will sketch the proof for the case when k = 2.\\nLet M =\\n(Σ, Γ, Q, δ, q, qaccept, qreject) be a two-tape Turing machine.\\nOur goal is to\\nconvert M to an equivalent one-tape Turing machine N. That is, N should\\nhave the property that for all strings w ∈Σ∗,\\n• M accepts w if and only if N accepts w,\\n• M rejects w if and only if N rejects w,\\n• M does not terminate on input w if and only if N does not terminate\\non input w.\\nThe tape alphabet of the one-tape Turing machine N is\\nΓ ∪{ ˙x : x ∈Γ} ∪{#}.\\nIn words, we take the tape alphabet Γ of M, and add, for each x ∈Γ, the\\nsymbol ˙x. Moreover, we add a special symbol #.\\nThe Turing machine N will be deﬁned in such a way that any conﬁgura-\\ntion of the two-tape Turing machine M, for example\\n. . . 2 1 0 0 1 2 . . .\\n6\\n. . . 2 a a b a 2 . . .\\n6\\ncorresponds to the following conﬁguration of the one-tape Turing machine\\nN:\\n. . .\\n2\\n#\\n1\\n0\\n˙0\\n1\\n#\\na\\n˙a\\nb\\na\\n#\\n2 . . .\\n6\\n2Thanks to Sergio Cabello for pointing out an error in a previous version of this proof.\\n150\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThus, the contents of the two tapes of M are encoded on the single tape of\\nN. The dotted symbols are used to indicate the positions of the two tape\\nheads of M, whereas the three occurrences of the special symbol # are used\\nto mark the boundaries of the strings on the two tapes of M.\\nThe Turing machine N simulates one computation step of M, in the\\nfollowing way:\\n• Throughout the simulation of this step, N “remembers” the current\\nstate of M.\\n• At the start of the simulation, the tape head of N is on the leftmost\\nsymbol #.\\n• N walks along the string to the right until it ﬁnds the ﬁrst dotted\\nsymbol. (This symbol indicates the location of the head on the ﬁrst tape\\nof M.) N remembers this ﬁrst dotted symbol and continues walking\\nto the right until it ﬁnds the second dotted symbol.\\n(This symbol\\nindicates the location of the head on the second tape of M.) Again, N\\nremembers this second dotted symbol.\\n• At this moment, N is still at the second dotted symbol. N updates\\nthis part of the tape, by making the change that M would make on its\\nsecond tape. (This change is given by the transition function of M; it\\ndepends on the current state of M and the two symbols that M reads\\non its two tapes.)\\n• N walks to the left until it ﬁnds the ﬁrst dotted symbol.\\nThen, it\\nupdates this part of the tape, by making the change that M would\\nmake on its ﬁrst tape.\\n• In the previous two steps, in which the tape is updated, it may be\\nnecessary to shift a part of the tape.\\n• Finally, N remembers the new state of M and walks back to the left-\\nmost symbol #.\\nIt should be clear that the Turing machine N can be constructed by\\nintroducing appropriate states.\\n4.4.\\nThe Church-Turing Thesis\\n151\\n4.4\\nThe Church-Turing Thesis\\nWe all have some intuitive notion of what an algorithm is. This notion will\\nprobably be something like “an algorithm is a procedure consisting of com-\\nputation steps that can be speciﬁed in a ﬁnite amount of text”. For example,\\nany “computational process” that can be speciﬁed by a Java program, should\\nbe considered an algorithm. Similarly, a Turing machine speciﬁes a “com-\\nputational process” and, therefore, should be considered an algorithm. This\\nleads to the question of whether it is possible to give a mathematical deﬁni-\\ntion of an “algorithm”. We just saw that every Java program represents an\\nalgorithm and that every Turing machine also represents an algorithm. Are\\nthese two notions of an algorithm equivalent? The answer is “yes”. In fact,\\nthe following theorem states that many diﬀerent notions of “computational\\nprocess” are equivalent. (We hope that you have gained suﬃcient intuition,\\nso that none of the claims in this theorem comes as a surprise to you.)\\nTheorem 4.4.1 The following computation models are equivalent, i.e., any\\none of them can be converted to any other one:\\n1. One-tape Turing machines.\\n2. k-tape Turing machines, for any k ≥1.\\n3. Non-deterministic Turing machines.\\n4. Java programs.\\n5. C++ programs.\\n6. Lisp programs.\\nIn other words, if we deﬁne the notion of an algorithm using any of the\\nmodels in this theorem, then it does not matter which model we take: All\\nthese models give the same notion of an algorithm.\\nThe problem of deﬁning the notion of an algorithm goes back to David\\nHilbert. On August 8, 1900, at the Second International Congress of Math-\\nematicians in Paris, Hilbert presented a list of problems that he considered\\ncrucial for the further development of mathematics. Hilbert’s 10th problem\\nis the following:\\n152\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nDoes there exist a ﬁnite process that decides whether or not any\\ngiven polynomial with integer coeﬃcients has integral roots?\\nOf course, in our language, Hilbert asked whether or not there exists an\\nalgorithm that decides, when given an arbitrary polynomial equation (with\\ninteger coeﬃcients) such as\\n12x3y7z5 + 7x2y4z −x4 + y2z7 −z3 + 10 = 0,\\nwhether or not this equation has a solution in integers. In 1970, Matiyasevich\\nproved that such an algorithm does not exist. Of course, in order to prove\\nthis claim, we ﬁrst have to agree on what an algorithm is. In the beginning\\nof the twentieth century, mathematicians gave several deﬁnitions, such as\\nTuring machines (1936) and the λ-calculus (1936), and they proved that all\\nthese are equivalent. Later, after programming languages were invented, it\\nwas shown that these older notions of an algorithm are equivalent to notions\\nof an algorithm that are based on C programs, Java programs, Lisp programs,\\nPascal programs, etc.\\nIn other words, all attempts to give a rigorous deﬁnition of the notion of\\nan algorithm led to the same concept. Because of this, computer scientists\\nnowadays agree on what is called the Church-Turing Thesis:\\nChurch-Turing Thesis:\\nEvery computational process that is intuitively\\nconsidered to be an algorithm can be converted to a Turing machine.\\nIn other words, this basically states that we deﬁne an algorithm to be a\\nTuring machine. At this point, you should ask yourself, whether the Church-\\nTuring Thesis can be proved. Alternatively, what has to be done in order to\\ndisprove this thesis?\\nExercises\\n4.1 Construct a Turing machine with one tape, that accepts the language\\n{02n1n : n ≥0}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\nExercises\\n153\\n4.2 Construct a Turing machine with one tape, that accepts the language\\n{w : w contains twice as many 0s as 1s}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\n4.3 Let A be the language\\nA\\n=\\n{ w ∈{a, b, c}∗\\n:\\nw contains more bs than as and\\nw contains more cs than as }.\\nGive an informal description (in plain English) of a Turing machine with one\\ntape, that accepts the language A.\\n4.4 Construct a Turing machine with one tape that receives as input a non-\\nnegative integer x and returns as output the integer x + 1.\\nIntegers are\\nrepresented as binary strings.\\nStart of the computation: The tape contains the binary representation\\nof the input x. The tape head is on the leftmost symbol and the Turing\\nmachine is in the start state q0. For example, if x = 431, the tape looks as\\nfollows:\\n. . . 2 2 2 1 1 0 1 0 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x + 1. The tape head is on the leftmost symbol and the Turing\\nmachine is in the ﬁnal state q1. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 0 1 1 0 0 0 0 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state q1. As soon as state q1 is entered,\\nthe Turing machine terminates. At termination, the contents of the tape is\\nthe output of the Turing machine.\\n154\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n4.5 Construct a Turing machine with two tapes that receives as input two\\nnon-negative integers x and y, and returns as output the integer x + y.\\nIntegers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost bit\\nof y. At the start, the Turing machine is in the start state q0.\\nEnd of the computation: The ﬁrst tape contains the binary representation\\nof x and its head is on the rightmost symbol of x. The second tape contains\\nthe binary representation of the integer x + y (thus, the integer y is “gone”).\\nThe head of the second tape is on the rightmost bit of x + y. The Turing\\nmachine is in the ﬁnal state q1.\\n4.6 Give an informal description (in plain English) of a Turing machine with\\none tape that receives as input two non-negative integers x and y, and returns\\nas output the integer x+y. Integers are represented as binary strings. If you\\nare an adventurous student, you may give a formal deﬁnition of your Turing\\nmachine.\\n4.7 Construct a Turing machine with one tape that receives as input an\\ninteger x ≥1 and returns as output the integer x−1. Integers are represented\\nin binary.\\nStart of the computation: The tape contains the binary representation of\\nthe input x. The tape head is on the rightmost symbol of x and the Turing\\nmachine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x −1. The tape head is on the rightmost bit of x −1 and the\\nTuring machine is in the ﬁnal state q1.\\n4.8 Give an informal description (in plain English) of a Turing machine with\\nthree tapes that receives as input two non-negative integers x and y, and\\nreturns as output the integer xy. Integers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost sym-\\nbol of y. The third tape is empty and its head is at an arbitrary location.\\nThe Turing machine is in the start state q0.\\nExercises\\n155\\nEnd of the computation: The ﬁrst and second tapes are empty. The third\\ntape contains the binary representation of the product xy and its head is on\\nthe rightmost bit of xy. The Turing machine is in the ﬁnal state q1.\\nHint: Use the Turing machines of Exercises 4.5 and 4.7.\\n4.9 Construct a Turing machine with one tape that receives as input a string\\nof the form 1n for some integer n ≥0; thus, the input is a string of n many\\n1s. The output of the Turing machine is the string 1n21n. Thus, this Turing\\nmachine makes a copy of its input.\\nThe input alphabet is Σ = {1} and the tape alphabet is Γ = {1, 2}.\\nStart of the computation: The tape contains a string of the form 1n, for\\nsome integer n ≥0, the tape head is on the leftmost symbol, and the Turing\\nmachine is in the start state. For example, if n = 4, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the string 1n21n, the tape\\nhead is on the 2 in the middle of this string, and the Turing machine is in\\nthe ﬁnal state. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 1 1 1 1 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this state is entered, the\\nTuring machine terminates. At termination, the contents of the tape is the\\noutput of the Turing machine.\\n156\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nChapter 5\\nDecidable and Undecidable\\nLanguages\\nWe have seen in Chapter 4 that Turing machines form a model for “everything\\nthat is intuitively computable”. In this chapter, we consider the limitations\\nof Turing machines. That is, we ask ourselves the question whether or not\\n“everything” is computable. As we will see, the answer is “no”. In fact, we\\nwill even see that “most” problems are not solvable by Turing machines and,\\ntherefore, not solvable by computers.\\n5.1\\nDecidability\\nIn Chapter 4, we have deﬁned when a Turing machine accepts an input string\\nand when it rejects an input string. Based on this, we deﬁne the following\\nclass of languages.\\nDeﬁnition 5.1.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is decidable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the reject state.\\n158\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is decidable, if there exists an algorithm\\nthat (i) terminates on every input string w, and (ii) correctly tells us whether\\nw ∈A or w ̸∈A.\\nA language A that is not decidable is called undecidable.\\nFor such a\\nlanguage, there does not exist an algorithm that satisﬁes (i) and (ii) above.\\nIn Section 4.2, we have seen several examples of languages that are de-\\ncidable.\\nIn the following subsections, we will give some examples of decidable and\\nundecidable languages. These examples involve languages A whose elements\\nare pairs of the form (C, w), where C is some computation model (for ex-\\nample, a deterministic ﬁnite automaton) and w is a string over the alphabet\\nΣ. The pair (C, w) is in the language A if and only if the string w is in the\\nlanguage of the computation model C. For diﬀerent computation models C,\\nwe will ask the question whether A is decidable, i.e., whether an algorithm\\nexists that decides, for any input (C, w), whether or not this input belongs\\nto the language A. Since the input to any algorithm is a string over some\\nalphabet, we must encode the pair (C, w) as a string. In all cases that we\\nconsider, such a pair can be described using a ﬁnite amount of text. There-\\nfore, we assume, without loss of generality, that binary strings are used for\\nthese encodings. Throughout the rest of this chapter, we will denote the\\nbinary encoding of a pair (C, w) by\\n⟨C, w⟩.\\n5.1.1\\nThe language ADFA\\nWe deﬁne the following language:\\nADFA = {⟨M, w⟩:\\nM is a deterministic ﬁnite automaton that\\naccepts the string w}.\\nKeep in mind that ⟨M, w⟩denotes the binary string that forms an en-\\ncoding of the ﬁnite automaton M and the string w that is given as input to\\nM.\\nWe claim that the language ADFA is decidable. In order to prove this,\\nwe have to construct an algorithm with the following property, for any given\\ninput string u:\\n• If u is the encoding of a deterministic ﬁnite automaton M and a string\\nw (i.e., u is in the correct format ⟨M, w⟩), and if M accepts w, then\\n5.1.\\nDecidability\\n159\\nthe algorithm terminates in its accept state.\\n• In all other cases, the algorithm terminates in its reject state.\\nAn algorithm that exactly does this, is easy to obtain: On input u, the algo-\\nrithm ﬁrst checks whether or not u encodes a deterministic ﬁnite automaton\\nM and a string w. If this is not the case, then it terminates and rejects\\nthe input string u. Otherwise, the algorithm “constructs” M and w, and\\nthen simulates the computation of M on the input string w. If M accepts\\nw, then the algorithm terminates and accepts the input string u. If M does\\nnot accept w, then the algorithm terminates and rejects the input string u.\\nThus, we have proved the following result:\\nTheorem 5.1.2 The language ADFA is decidable.\\n5.1.2\\nThe language ANFA\\nWe deﬁne the following language:\\nANFA = {⟨M, w⟩:\\nM is a nondeterministic ﬁnite automaton that\\naccepts the string w}.\\nTo prove that this language is decidable, consider the algorithm that\\ndoes the following: On input u, the algorithm ﬁrst checks whether or not\\nu encodes a nondeterministic ﬁnite automaton M and a string w. If this is\\nnot the case, then it terminates and rejects the input string u. Otherwise,\\nthe algorithm constructs M and w. Since a computation of M (on input w)\\nis not unique, the algorithm ﬁrst converts M to an equivalent deterministic\\nﬁnite automaton N. Then, it proceeds as in Section 5.1.1.\\nObserve that the construction for converting a nondeterministic ﬁnite au-\\ntomaton to a deterministic ﬁnite automaton (see Section 2.5) is algorithmic,\\nin the sense that it can be described by an algorithm. Because of this, the\\nalgorithm described above is a valid algorithm; it accepts all strings u that\\nare in ANFA, and it rejects all strings u that are not in ANFA. Thus, we have\\nproved the following result:\\nTheorem 5.1.3 The language ANFA is decidable.\\n160\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.1.3\\nThe language ACFG\\nWe deﬁne the following language:\\nACFG = {⟨G, w⟩: G is a context-free grammar such that w ∈L(G)}.\\nWe claim that this language is decidable. In order to prove this claim, con-\\nsider a string u that encodes a context-free grammar G = (V, Σ, S, R) and a\\nstring w ∈Σ∗. Deciding whether or not w ∈L(G) is equivalent to deciding\\nwhether or not S\\n∗⇒w. A ﬁrst idea to decide this is by trying all possible\\nderivations that start with the start variable S and that use rules of R. The\\nproblem is that, in case w ̸∈L(G), it is not clear how many such derivations\\nhave to be checked before we can be sure that w is not in the language of\\nG: If w ∈L(G), then it may be that w can be derived from S, only by ﬁrst\\nderiving a very long string, say v, and then use rules to shorten it so as to\\nobtain the string w. Since there is no obvious upper bound on the length of\\nthe string v, we have to be careful.\\nThe trick is to do the following. First, convert the grammar G to an\\nequivalent grammar G′ in Chomsky normal form. (The construction given\\nin Section 3.4 can be described by an algorithm.) Let n be the length of the\\nstring w. Then, if w ∈L(G) = L(G′), any derivation of w in G′, from the\\nstart variable of G′, consists of exactly 2n−1 steps (where a “step” is deﬁned\\nas applying one rule of G′). Hence, we can decide whether or not w ∈L(G),\\nby trying all possible derivations, in G′, consisting of 2n −1 steps. If one of\\nthese (ﬁnite number of) derivations leads to the string w, then w ∈L(G).\\nOtherwise, w ̸∈L(G). Thus, we have proved the following result:\\nTheorem 5.1.4 The language ACFG is decidable.\\nIn fact, the arguments above imply the following result:\\nTheorem 5.1.5 Every context-free language is decidable.\\nProof. Let Σ be an alphabet and let A ⊆Σ∗be an arbitrary context-free\\nlanguage. There exists a context-free grammar in Chomsky normal form,\\nwhose language is equal to A. Given an arbitrary string w ∈Σ∗, we have\\nseen above how we can decide whether or not w can be derived from the\\nstart variable of this grammar.\\n5.1.\\nDecidability\\n161\\n5.1.4\\nThe language ATM\\nAfter having seen the languages ADFA, ANFA, and ACFG, it is natural to\\nconsider the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nWe will prove that this language is undecidable. Before we give the proof,\\nlet us mention what this means:\\nThere is no algorithm that, when given an arbitrary algorithm M\\nand an arbitrary input string w for M, decides in a ﬁnite amount\\nof time, whether or not M accepts w.\\nThe proof of the claim that ATM is undecidable is by contradiction. Thus,\\nwe assume that ATM is decidable. Then there exists a Turing machine H\\nthat has the following property. For every input string ⟨M, w⟩for H:\\n• If ⟨M, w⟩∈ATM (i.e., M accepts w), then H terminates in its accept\\nstate.\\n• If ⟨M, w⟩̸∈ATM (i.e., M rejects w or M does not terminate on input\\nw), then H terminates in its reject state.\\n• In particular, H terminates on any input ⟨M, w⟩.\\nWe construct a new Turing machine D, that does the following: On input\\n⟨M⟩, the Turing machine D uses H as a subroutine to determine what M\\ndoes when it is given its own description as input. Once D has determined\\nthis information, it does the opposite of what H does.\\nTuring machine D: On input ⟨M⟩, where M is a Turing machine,\\nthe new Turing machine D does the following:\\nStep 1: Run the Turing machine H on the input ⟨M, ⟨M⟩⟩.\\nStep 2:\\n• If H terminates in its accept state, then D terminates in its\\nreject state.\\n• If H terminates in its reject state, then D terminates in its\\naccept state.\\n162\\nChapter 5.\\nDecidable and Undecidable Languages\\nFirst observe that this new Turing machine D terminates on any input\\nstring ⟨M⟩, because H terminates on every input. Next observe that, for any\\ninput string ⟨M⟩for D:\\n• If ⟨M, ⟨M⟩⟩∈ATM (i.e., M accepts ⟨M⟩), then D terminates in its\\nreject state.\\n• If ⟨M, ⟨M⟩⟩̸∈ATM (i.e., M rejects ⟨M⟩or M does not terminate on\\ninput ⟨M⟩), then D terminates in its accept state.\\nThis means that for any string ⟨M⟩:\\n• If M accepts ⟨M⟩, then D rejects ⟨M⟩.\\n• If M rejects ⟨M⟩or M does not terminate on input ⟨M⟩, then D\\naccepts ⟨M⟩.\\nWe now consider what happens if we give the Turing machine D the string\\n⟨D⟩as input, i.e., we take M = D:\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩or D does not terminate on input ⟨D⟩, then D accepts\\n⟨D⟩.\\nSince D terminates on every input string, this means that\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩, then D accepts ⟨D⟩.\\nThis is clearly a contradiction. Therefore, the Turing machine H that decides\\nthe language ATM cannot exist and, thus, ATM is undecidable. We have\\nproved the following result:\\nTheorem 5.1.6 The language ATM is undecidable.\\n5.1.\\nDecidability\\n163\\n5.1.5\\nThe Halting Problem\\nWe deﬁne the following language:\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}.\\nTheorem 5.1.7 The language Halt is undecidable.\\nProof. The proof is by contradiction. Thus, we assume that the language\\nHalt is decidable. Then there exists a Java program H that takes as input a\\nstring of the form ⟨P, w⟩, where P is an arbitrary Java program and w is an\\narbitrary input for P. The program H has the following property:\\n• If ⟨P, w⟩∈Halt (i.e., program P terminates on input w), then H\\noutputs true.\\n• If ⟨P, w⟩̸∈Halt (i.e., program P does not terminate on input w), then\\nH outputs false.\\n• In particular, H terminates on any input ⟨P, w⟩.\\nWe will write the output of H as H(P, w). Moreover, we will denote by P(w)\\nthe computation obtained by running the program P on the input w. Hence,\\nH(P, w) =\\n\\x1a true\\nif P(w) terminates,\\nfalse\\nif P(w) does not terminate.\\nConsider the following algorithm Q, which takes as input the encoding\\n⟨P⟩of an arbitrary Java program P:\\nAlgorithm Q(⟨P⟩):\\nwhile H(P, ⟨P⟩) = true\\ndo have a beer\\nendwhile\\nSince H is a Java program, this new algorithm Q can also be written as\\na Java program. Observe that\\nQ(⟨P⟩) terminates if and only if H(P, ⟨P⟩) = false.\\n164\\nChapter 5.\\nDecidable and Undecidable Languages\\nThis means that for every Java program P,\\nQ(⟨P⟩) terminates if and only if P(⟨P⟩) does not terminate.\\n(5.1)\\nWhat happens if we run the Java program Q on the input string ⟨Q⟩?\\nIn other words, what happens if we run Q(⟨Q⟩)? Then, in (5.1), we have to\\nreplace all occurrences of P by Q. Hence,\\nQ(⟨Q⟩) terminates if and only if Q(⟨Q⟩) does not terminate.\\nThis is obviously a contradiction, and we can conclude that the Java program\\nH does not exist. Therefore, the language Halt is undecidable.\\nRemark 5.1.8 In this proof, we run the Java program Q on the input ⟨Q⟩.\\nThis means that the input to Q is a description of itself. In other words, we\\ngive Q itself as input. This is an example of what is called self-reference. An-\\nother example of self-reference can be found in Remark 5.1.8 of the textbook\\nIntroduction to Theory of Computation by A. Maheshwari and M. Smid.\\n5.2\\nCountable sets\\nThe proofs that we gave in Sections 5.1.4 and 5.1.5 seem to be bizarre. In\\nthis section, we will convince you that these proofs in fact use a technique\\nthat you have seen in the course COMP 1805: Cantor’s Diagonalization.\\nLet A and B be two sets and let f : A →B be a function. Recall that f\\nis called a bijection, if\\n• f is one-to-one (or injective), i.e., for any two distinct elements a and\\na′ in A, we have f(a) ̸= f(a′), and\\n• f is onto (or surjective), i.e., for each element b ∈B, there exists an\\nelement a ∈A, such that f(a) = b.\\nThe set of natural numbers is denoted by N. That is, N = {1, 2, 3, . . .}.\\nDeﬁnition 5.2.1 Let A and B be two sets. We say that A and B have the\\nsame size, if there exists a bijection f : A →B.\\nDeﬁnition 5.2.2 Let A be a set. We say that A is countable, if A is ﬁnite,\\nor A and N have the same size.\\n5.2.\\nCountable sets\\n165\\nIn other words, if A is an inﬁnite and countable set, then there exists a\\nbijection f : N →A, and we can write A as\\nA = {f(1), f(2), f(3), f(4), . . .}.\\nSince f is a bijection, every element of A occurs exactly once in the set on\\nthe right-hand side. This means that we can number the elements of A using\\nthe positive integers: Every element of A receives a unique number.\\nTheorem 5.2.3 The following sets are countable:\\n1. The set Z of integers:\\nZ = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n2. The Cartesian product N × N:\\nN × N = {(m, n) : m ∈N, n ∈N}.\\n3. The set Q of rational numbers:\\nQ = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\nProof. To prove that the set Z is countable, we have to give each element of\\nZ a unique number in N. We obtain this numbering, by listing the elements\\nof Z in the following order:\\n0, 1, −1, 2, −2, 3, −3, 4, −4, . . .\\nIn this (inﬁnite) list, every element of Z occurs exactly once. The number of\\nan element of Z is given by its position in this list.\\nFormally, deﬁne the function f : N →Z by\\nf(n) =\\n\\x1a n/2\\nif n is even,\\n−(n −1)/2\\nif n is odd.\\nThis function f is a bijection and, therefore, the sets N and Z have the same\\nsize. Hence, the set Z is countable.\\nFor the proofs of the other two claims, we refer to the course COMP 1805.\\nWe now use Cantor’s Diagonalization principle to prove that the set of\\nreal numbers is not countable:\\n166\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.2.4 The set R of real numbers is not countable.\\nProof. Deﬁne\\nA = {x ∈R : 0 ≤x < 1}.\\nWe will prove that the set A is not countable. This will imply that the set\\nR is not countable, because A ⊆R.\\nThe proof that A is not countable is by contradiction. So we assume that\\nA is countable. Then there exists a bijection f : N →A. Thus, for each\\nn ∈N, f(n) is a real number between zero and one. We can write\\nA = {f(1), f(2), f(3), . . .},\\n(5.2)\\nwhere every element of A occurs exactly once in the set on the right-hand\\nside.\\nConsider the real number f(1). We can write this number in decimal\\nnotation as\\nf(1) = 0.d11d12d13 . . . ,\\nwhere each d1i is a digit in the set {0, 1, 2, . . . , 9}. In general, for every n ∈N,\\nwe can write the real number f(n) as\\nf(n) = 0.dn1dn2dn3 . . . ,\\nwhere, again, each dni is a digit in {0, 1, 2, . . . , 9}.\\nWe deﬁne the real number\\nx = 0.d1d2d3 . . . ,\\nwhere, for each integer n ≥1,\\ndn =\\n\\x1a 4\\nif dnn ̸= 4,\\n5\\nif dnn = 4.\\nObserve that x is a real number between zero and one, i.e., x ∈A. Therefore,\\nby (5.2), there is an element n ∈N, such that f(n) = x. We compare the\\nn-th digits of f(n) and x:\\n• The n-th digit of f(n) is equal to dnn.\\n• The n-th digit of x is equal to dn.\\n5.2.\\nCountable sets\\n167\\nSince f(n) and x are equal, their n-th digits must be equal, i.e., dnn = dn.\\nBut, by the deﬁnition of dn, we have dnn ̸= dn. This is a contradiction and,\\ntherefore, the set A is not countable.\\nNotice how we deﬁned the real number x: For each n ≥1, the n-th digit\\nof x is not equal to the n-th digit of f(n). Therefore, for each n ≥1, x ̸= f(n)\\nand, thus, x ̸∈A.\\nThe ﬁnal result of this section is the fact that for every set A, its power\\nset\\nP(A) = {B : B ⊆A}\\nis “strictly larger” than A. Deﬁne the function f : A →P(A) by\\nf(a) = {a},\\nfor any a in A. Since f is one-to-one, we can say that P(A) is “at least as\\nlarge as” A.\\nTheorem 5.2.5 Let A be an arbitrary set. Then A and P(A) do not have\\nthe same size.\\nProof. The proof is by contradiction. Thus, we assume that there exists a\\nbijection g : A →P(A). Deﬁne the set B as\\nB = {a ∈A : a ̸∈g(a)}.\\nSince B ∈P(A) and g is a bijection, there exists an element a in A such that\\ng(a) = B.\\nFirst assume that a ∈B. Since g(a) = B, we have a ∈g(a). But then,\\nfrom the deﬁnition of the set B, we have a ̸∈B, which is a contradiction.\\nNext assume that a ̸∈B.\\nSince g(a) = B, we have a ̸∈g(a).\\nBut\\nthen, from the deﬁnition of the set B, we have a ∈B, which is again a\\ncontradiction.\\nWe conclude that the bijection g does not exist. Therefore, A and P(A)\\ndo not have the same size.\\n168\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.2.1\\nThe Halting Problem revisited\\nNow that we know about countability, we give a diﬀerent way to look at the\\nproof in Section 5.1.5 of the fact that the language\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}\\nis undecidable.\\nYou should convince yourself that the proof given below\\nfollows the same reasoning as the one used in the proof of Theorem 5.2.4.\\nWe ﬁrst argue that the set of all Java programs is countable. Indeed,\\nevery Java program P can be described by a ﬁnite amount of text. In fact,\\nwe have been using ⟨P⟩to denote such a description by a binary string. For\\nany integer n ≥0, there are at most 2n (i.e., ﬁnitely many) Java programs\\nP whose description ⟨P⟩has length n. Therefore, to obtain a list of all Java\\nprograms, we do the following:\\n• List all Java programs P whose description ⟨P⟩has length 0. (Well,\\nthe empty string does not describe any Java program, so in this step,\\nnothing happens.)\\n• List all Java programs P whose description ⟨P⟩has length 1.\\n• List all Java programs P whose description ⟨P⟩has length 2.\\n• List all Java programs P whose description ⟨P⟩has length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every Java program occurs exactly once. Therefore, the\\nset of all Java programs is countable.\\nConsider an inﬁnite list\\nP1, P2, P3, . . .\\nin which every Java program occurs exactly once.\\nAssume that the language Halt is decidable. Then there exists a Java\\nprogram H that decides this language. We may assume that, on input ⟨P, w⟩,\\nH returns true if P terminates on input w, and false if P does not terminate\\non input w.\\nWe construct a new Java program D that does the following:\\n5.3.\\nRice’s Theorem\\n169\\nAlgorithm D:\\nOn input ⟨Pn⟩, where n is a positive integer, the\\nnew Java program D does the following:\\nStep 1: Run the Java program H on the input ⟨Pn, ⟨Pn⟩⟩.\\nStep 2:\\n• If H returns true, then D goes into an inﬁnite loop.\\n• If H returns false, then D returns true and terminates its com-\\nputation.\\nObserve that D can be written as a Java program. Therefore, there exists\\nan integer n ≥1 such that D = Pn. The next two observations follow from\\nthe pseudocode:\\n• If D terminates on input ⟨Pn⟩, then H returns false on input ⟨Pn, ⟨Pn⟩⟩,\\ni.e., Pn does not terminate on input ⟨Pn⟩.\\n• If D does not terminate on input ⟨Pn⟩, then H returns true on input\\n⟨Pn, ⟨Pn⟩⟩, i.e., Pn terminates on input ⟨Pn⟩.\\nThus,\\n• D terminates on input ⟨Pn⟩if and only if Pn does not terminate on\\ninput ⟨Pn⟩.\\nSince D = Pn, this becomes\\n• D terminates on input ⟨D⟩if and only if D does not terminate on input\\n⟨D⟩.\\nThus, we have obtained a contradiction.\\nRemark 5.2.6 We deﬁned the Java program D in such a way that, for each\\nn ≥1, the computation of D on input ⟨Pn⟩diﬀers from the computation of\\nPn on input ⟨Pn⟩. Hence, for each n ≥1, D ̸= Pn. However, since D is a\\nJava program, there must be an integer n ≥1 such that D = Pn.\\n170\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.3\\nRice’s Theorem\\nWe have seen two examples of undecidable languages: ATM and Halt. In this\\nsection, we prove that many languages involving Turing machines (or Java\\nprograms) are undecidable.\\nDeﬁne T to be the set of binary encodings of all Turing machines, i.e.,\\nT = {⟨M⟩: M is a Turing machine with input alphabet {0,1}}.\\nTheorem 5.3.1 (Rice) Let P be a subset of T such that\\n1. P ̸= ∅, i.e., there exists a Turing machine M such that ⟨M⟩∈P,\\n2. P is a proper subset of T , i.e., there exists a Turing machine N such\\nthat ⟨N⟩̸∈P, and\\n3. for any two Turing machines M1 and M2 with L(M1) = L(M2),\\n(a) either both ⟨M1⟩and ⟨M2⟩are in P or\\n(b) none of ⟨M1⟩and ⟨M2⟩is in P.\\nThen the language P is undecidable.\\nYou can think of P as the set of all Turing machines that satisfy a certain\\nproperty. The ﬁrst two conditions state that at least one Turing machine\\nsatisﬁes this property and not all Turing machines satisfy this property. The\\nthird condition states that, for any Turing machine M, whether or not M\\nsatisﬁes this property only depends on the language L(M) of M.\\nHere are some examples of languages that satisfy the conditions in Rice’s\\nTheorem:\\nP1 = {⟨M⟩: M is a Turing machine and ϵ ∈L(M)},\\nP2 = {⟨M⟩: M is a Turing machine and L(M) = {1011, 001100}},\\nP3 = {⟨M⟩: M is a Turing machine and L(M) is a regular language}.\\nYou are encouraged to verify that Rice’s Theorem indeed implies that each\\nof P1, P2, and P3 is undecidable.\\n5.3.\\nRice’s Theorem\\n171\\n5.3.1\\nProof of Rice’s Theorem\\nThe strategy of the proof is as follows: Assuming that the language P is\\ndecidable, we show that the language\\nHalt = {⟨M, w⟩:\\nM is a Turing machine that terminates on\\nthe input string w}\\nis decidable. This will contradict Theorem 5.1.7.\\nThe assumption that P is decidable implies the existence of a Turing\\nmachine H that decides P. Observe that H takes as input a binary string\\n⟨M⟩encoding a Turing machine M. In order to show that Halt is decidable,\\nwe need a Turing machine that takes as input a binary string ⟨M, w⟩encoding\\na Turing machine M and a binary string w. In the rest of this section, we\\nwill explain how this Turing machine can be obtained.\\nLet M1 be a Turing machine that, for any input string, switches in its\\nﬁrst computation step from its start state to its reject state. In other words,\\nM1 is a Turing machine with L(M1) = ∅. We assume that\\n⟨M1⟩̸∈P.\\n(At the end of the proof, we will consider the case when ⟨M1⟩∈P.) We also\\nchoose a Turing machine M2 such that\\n⟨M2⟩∈P.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nif M terminates\\nthen run M2 on input x;\\nif M2 terminates in the accept state\\nthen terminate in the accept state\\nelse if M2 terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\n172\\nChapter 5.\\nDecidable and Undecidable Languages\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that for any string x,\\nx is accepted by TMw if and only if x is accepted by M2.\\nThus, L(TMw) = L(M2).\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩̸∈Halt.\\nThen it follows from the pseudocode that for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅. In particular, L(TMw) =\\nL(M1).\\nRecall that ⟨M1⟩̸∈P, whereas ⟨M2⟩∈P. Then the following follows from\\nthe third condition in Rice’s Theorem:\\n• If ⟨M, w⟩∈Halt, then ⟨TMw⟩∈P.\\n• If ⟨M, w⟩̸∈Halt, then ⟨TMw⟩̸∈P.\\nThus, we have obtained a connection between the languages P and Halt.\\nThis suggests that we proceed as follows.\\nAssume that the language P is decidable. Let H be a Turing machine\\nthat decides P. Then, for any Turing machine M,\\n• if ⟨M⟩∈P, then H accepts the string ⟨M⟩,\\n• if ⟨M⟩̸∈P, then H rejects the string ⟨M⟩, and\\n• H terminates on any input string.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\n5.4.\\nEnumerability\\n173\\nIt follows from the pseudocode that H′ terminates on any input. We\\nobserve the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that ⟨TMw⟩∈P.\\nSince H decides the language P, it follows that H accepts the string\\n⟨TMw⟩. Therefore, from the pseudocode, H′ accepts the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then we have seen before that ⟨TMw⟩̸∈\\nP. Since H decides the language P, it follows that H rejects (and\\nterminates on) the string ⟨TMw⟩. Therefore, from the pseudocode, H′\\nrejects (and terminates on) the string ⟨M, w⟩.\\nWe have shown that the Turing machine H′ decides the language Halt.\\nThis is a contradiction and, therefore, we conclude that the language P is\\nundecidable.\\nUntil now, we assumed that ⟨M1⟩̸∈P. If ⟨M1⟩∈P, then we repeat the\\nproof with P replaced by its complement P. This revised proof then shows\\nthat P is undecidable. Since for every language L,\\nL is decidable if and only if L is decidable,\\nwe again conclude that P is undecidable.\\n5.4\\nEnumerability\\nWe now come to the last class of languages in this chapter:\\nDeﬁnition 5.4.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is enumerable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, does not terminate in the accept state. That is, either the\\ncomputation terminates in the reject state or the computation does not\\nterminate.\\n174\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is enumerable, if there exists an algorithm\\nhaving the following property. If w ∈A, then the algorithm terminates on\\nthe input string w and tells us that w ∈A. On the other hand, if w ̸∈A,\\nthen either (i) the algorithm terminates on the input string w and tells us\\nthat w ̸∈A or (ii) the algorithm does not terminate on the input string w,\\nin which case it does not tell us that w ̸∈A.\\nIn Section 5.5, we will show where the term “enumerable” comes from.\\nThe following theorem follows immediately from Deﬁnitions 5.1.1 and 5.4.1.\\nTheorem 5.4.2 Every decidable language is enumerable.\\nIn the following subsections, we will give some examples of enumerable\\nlanguages.\\n5.4.1\\nHilbert’s problem\\nWe have seen Hilbert’s problem in Section 4.4: Is there an algorithm that\\ndecides, for any given polynomial p with integer coeﬃcients, whether or not\\np has integral roots? If we formulate this problem in terms of languages,\\nthen Hilbert asked whether or not the language\\nHilbert = {⟨p⟩:\\np is a polynomial with integer coeﬃcients\\nthat has an integral root}\\nis decidable. As usual, ⟨p⟩denotes the binary string that forms an encoding\\nof the polynomial p.\\nAs we mentioned in Section 4.4, it was proven by Matiyasevich in 1970\\nthat the language Hilbert is not decidable. We claim, that this language\\nis enumerable.\\nIn order to prove this claim, we have to construct an al-\\ngorithm Hilbert with the following property: For any input polynomial p\\nwith integer coeﬃcients,\\n• if p has an integral root, then algorithm Hilbert will ﬁnd one in a\\nﬁnite amount of time,\\n• if p does not have an integral root, then either algorithm Hilbert ter-\\nminates and tells us that p does not have an integral root, or algorithm\\nHilbert does not terminate.\\n5.4.\\nEnumerability\\n175\\nRecall that Z denotes the set of integers. Algorithm Hilbert does the\\nfollowing, on any input polynomial p with integer coeﬃcients.\\nLet n de-\\nnote the number of variables in p. Algorithm Hilbert tries all elements\\n(x1, x2, . . . , xn) ∈Zn, in a systematic way, and for each such element, it\\ncomputes p(x1, x2, . . . , xn). If this value is zero, then algorithm Hilbert\\nterminates and accepts the input.\\nWe observe the following:\\n• If ⟨p⟩∈Hilbert, then algorithm Hilbert terminates and accepts p,\\nprovided we are able to visit all elements (x1, x2, . . . , xn) ∈Zn in a\\n“systematic way”.\\n• If ⟨p⟩̸∈Hilbert, then p(x1, x2, . . . , xn) ̸= 0 for all (x1, x2, . . . , xn) ∈Zn\\nand, therefore, algorithm Hilbert does not terminate.\\nThese are exactly the requirements for the language Hilbert to be enumerable.\\nIt remains to explain how we visit all elements (x1, x2, . . . , xn) ∈Zn in a\\nsystematic way. For any integer d ≥0, let Hd denote the hypercube in Zn\\nwith sides of length 2d that is centered at the origin. That is, Hd consists\\nof the set of all points (x1, x2, . . . , xn) in Zn, such that −d ≤xi ≤d for all\\n1 ≤i ≤n and there exists at least one index j for which xj = d or xj = −d.\\nWe observe that Hd contains a ﬁnite number of elements. In fact, if d ≥1,\\nthen this number is equal to (2d + 1)n −(2d −1)n. The algorithm will visit\\nall elements (x1, x2, . . . , xn) ∈Zn, in the following order: First, it visits the\\norigin, which is the only element of H0. Then, it visits all elements of H1,\\nfollowed by all elements of H2, etc., etc.\\nTo summarize, we obtain the following algorithm, proving that the lan-\\nguage Hilbert is enumerable:\\n176\\nChapter 5.\\nDecidable and Undecidable Languages\\nAlgorithm Hilbert(⟨p⟩):\\nn := the number of variables in p;\\nd := 0;\\nwhile d ≥0\\ndo for each (x1, x2, . . . , xn) ∈Hd\\ndo R := p(x1, x2, . . . , xn);\\nif R = 0\\nthen terminate and accept\\nendif\\nendfor;\\nd := d + 1\\nendwhile\\nTheorem 5.4.3 The language Hilbert is enumerable.\\n5.4.2\\nThe language ATM\\nWe have shown in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nis undecidable. In this section, we will prove that this language is enumerable.\\nThus, we have to construct an algorithm P having the following property,\\nfor any given input string u:\\n• If\\n– u encodes a Turing machine M and an input string w for M (i.e.,\\nu is in the correct format ⟨M, w⟩) and\\n– ⟨M, w⟩∈ATM (i.e., M accepts w),\\nthen algorithm P terminates in its accept state.\\n• In all other cases, either algorithm P terminates in its reject state, or\\nalgorithm P does not terminate.\\nOn input string u = ⟨M, w⟩, which is in the correct format, algorithm P does\\nthe following:\\n5.5.\\nWhere does the term “enumerable” come from?\\n177\\n1. It simulates the computation of M on input w.\\n2. If M terminates in its accept state, then P terminates in its accept\\nstate.\\n3. If M terminates in its reject state, then P terminates in its reject state.\\n4. If M does not terminate, then P does not terminate.\\nHence, if u = ⟨M, w⟩∈ATM, then M accepts w and, therefore, P accepts\\nu. On the other hand, if u = ⟨M, w⟩̸∈ATM, then M does not accept w. This\\nmeans that, on input w, M either terminates in its reject state or does not\\nterminate. But this implies that, on input u, P either terminates in its reject\\nstate or does not terminate. This proves that algorithm P has the properties\\nthat are needed in order to show that the language ATM is enumerable. We\\nhave proved the following result:\\nTheorem 5.4.4 The language ATM is enumerable.\\n5.5\\nWhere does the term “enumerable” come\\nfrom?\\nIn Deﬁnition 5.4.1, we have deﬁned what it means for a language to be\\nenumerable. In this section, we will see where this term comes from.\\nDeﬁnition 5.5.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. An\\nenumerator for A is a Turing machine E having the following properties:\\n1. Besides the standard features as in Section 4.1, E has a print tape and\\na print state. During its computation, E writes symbols of Σ on the\\nprint tape. Each time, E enters the print state, the current string on\\nthe print tape is sent to the printer and the print tape is made empty.\\n2. At the start of the computation, all tapes are empty and E is in the\\nstart state.\\n3. Every string w in A is sent to the printer at least once.\\n4. Every string w that is not in A is never sent to the printer.\\n178\\nChapter 5.\\nDecidable and Undecidable Languages\\nThus, an enumerator E for A really enumerates all strings in the language\\nA. There is no particular order in which the strings of A are sent to the\\nprinter. Moreover, a string in A may be sent to the printer multiple times.\\nIf the language A is inﬁnite, then the Turing machine E obviously does not\\nterminate; however, every string in A (and only strings in A) will be sent to\\nthe printer at some time during the computation.\\nTo give an example, let A = {0n : n ≥0}. The following Turing machine\\nis an enumerator for A.\\nTuring machine StringsOfZeros:\\nn := 0;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo write 0 on the print tape\\nendfor;\\nenter the print state;\\nn := n + 1\\nendwhile\\nIn the rest of this section, we will prove the following result.\\nTheorem 5.5.2 A language is enumerable if and only if it has an enumer-\\nator.\\nFor the ﬁrst part of the proof, assume that the language A has an enu-\\nmerator E. We construct the following Turing machine M, which takes an\\narbitrary string w as input:\\nTuring machine M(w):\\nrun E; every time E enters the print state:\\nlet v be the string on the print tape;\\nif w = v\\nthen terminate in the accept state\\nendif\\nThe Turing machine M has the following properties:\\n• If w ∈A, then w will be sent to the printer at some time during the\\n5.5.\\nWhere does the term “enumerable” come from?\\n179\\ncomputation of E. It follows from the pseudocode that, on input w,\\nM terminates in the accept state.\\n• If w ̸∈A, then E will never sent w to the printer. It follows from the\\npseudocode that, on input w, M does not terminate.\\nThus, M satisﬁes the conditions in Deﬁnition 5.4.1. We conclude that the\\nlanguage A is enumerable.\\nTo prove the converse, we now assume that A is enumerable. Let M be\\na Turing machine that satisﬁes the conditions in Deﬁnition 5.4.1.\\nWe ﬁx an inﬁnite list\\ns1, s2, s3, . . .\\nof all strings in Σ∗. For example, if Σ = {0, 1}, then we can take this list to\\nbe\\nϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .\\nWe construct the following Turing machine E, which takes the empty\\nstring as input:\\nTuring machine E:\\nn := 1;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo run M for n steps on the input string si;\\nif M accepts si within n steps\\nthen write si on the print tape;\\nenter the print state\\nendif\\nendfor;\\nn := n + 1\\nendwhile\\nWe claim that E is an enumerator for the language A. To prove this, it\\nis obvious that any string that is sent to the printer by E belongs to A.\\nIt remains to prove that every string in A will be sent to the printer by E.\\nLet w be a string in A. Then, on input w, the Turing machine M terminates\\nin the accept state. Let m be the number of steps made by M on input w.\\nLet i be the index such that w = si. Deﬁne n = max(m, i). Consider the\\n180\\nChapter 5.\\nDecidable and Undecidable Languages\\nn-th iteration of the while-loop and the i-th iteration of the for-loop. In this\\niteration, M accepts si = w in m ≤n steps and, therefore, w is sent to the\\nprinter.\\n5.6\\nMost languages are not enumerable\\nIn this section, we will prove that most languages are not enumerable. The\\nproof is based on the following two facts:\\n• The set consisting of all enumerable languages is countable; we will\\nprove this in Section 5.6.1.\\n• The set consisting of all languages is not countable; we will prove this\\nin Section 5.6.2.\\n5.6.1\\nThe set of enumerable languages is countable\\nWe deﬁne the set E as\\nE = {A : A ⊆{0, 1}∗is an enumerable language}.\\nIn words, E is the set whose elements are the enumerable languages. Every\\nelement of E is an enumerable language. Hence, every element of the set E\\nis itself a set consisting of strings.\\nLemma 5.6.1 The set E is countable.\\nProof. Let A ⊆{0, 1}∗be an enumerable language. There exists a Turing\\nmachine TA that satisﬁes the conditions in Deﬁnition 5.4.1.\\nThis Turing\\nmachine TA can be uniquely speciﬁed by a string in English. This string can\\nbe converted to a binary string sA. Hence, the binary string sA is a unique\\nencoding of the Turing machine TA.\\nConsider the set\\nS = {sA : A ⊆{0, 1}∗is an enumerable language}.\\nObserve that the function f : E →S, deﬁned by f(A) = sA for each A ∈E,\\nis a bijection. Therefore, the sets E and S have the same size. Hence, in\\norder to prove that the set E is countable, it is suﬃcient to prove that the\\nset S is countable.\\n5.6.\\nMost languages are not enumerable\\n181\\nWhy is the set S countable? For each integer n ≥0, there are exactly 2n\\nbinary strings of length n. Since there are binary strings that are not encod-\\nings of Turing machines, the set S contains at most 2n strings of length n.\\nIn particular, the number of strings in S having length n is ﬁnite. Therefore,\\nwe obtain an inﬁnite list of the elements of S in the following way:\\n• List all strings in S having length 0. (Well, the empty string is not in\\nS, so in this step, nothing happens.)\\n• List all strings in S having length 1.\\n• List all strings in S having length 2.\\n• List all strings in S having length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every element of S occurs exactly once. Therefore, S is\\ncountable.\\n5.6.2\\nThe set of all languages is not countable\\nWe deﬁne the set L as\\nL = {A : A ⊆{0, 1}∗is a language}.\\nIn words, L is the set consisting of all languages. Every element of the set L\\nis a set consisting of strings.\\nLemma 5.6.2 The set L is not countable.\\nProof. We deﬁne the set B as\\nB = {w : w is an inﬁnite binary sequence}.\\nWe claim that this set is not countable. The proof of this claim is almost\\nidentical to the proof of Theorem 5.2.4. We assume that the set B is count-\\nable. Then there exists a bijection f : N →B. Thus, for each n ∈N, f(n) is\\nan inﬁnite binary sequence. We can write\\nB = {f(1), f(2), f(3), . . .},\\n(5.3)\\n182\\nChapter 5.\\nDecidable and Undecidable Languages\\nwhere every element of B occurs exactly once in the set on the right-hand\\nside.\\nWe deﬁne the inﬁnite binary sequence w = w1w2w3 . . ., where, for each\\ninteger n ≥1,\\nwn =\\n\\x1a 1\\nif the n-th bit of f(n) is 0,\\n0\\nif the n-th bit of f(n) is 1.\\nSince w ∈B, it follows from (5.3) that there is an element n ∈N, such that\\nf(n) = w. Hence, the n-th bits of f(n) and w are equal. But, by deﬁnition,\\nthese n-th bits are not equal. This is a contradiction and, therefore, the set\\nB is not countable.\\nIn the rest of the proof, we will show that the sets L and B have the same\\nsize. Since B is not countable, this will imply that L is not countable.\\nIn order to prove that L and B have the same size, we have to show that\\nthere exists a bijection\\ng : L →B.\\nWe ﬁrst observe that the set {0, 1}∗is countable, because for each integer\\nn ≥0, there are only ﬁnitely many (to be precise, exactly 2n) binary strings\\nof length n. In fact, we can write\\n{0, 1}∗= {ϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .}.\\nFor each integer n ≥1, we denote by sn the n-th string in this list. Hence,\\n{0, 1}∗= {s1, s2, s3, . . .}.\\n(5.4)\\nNow we are ready to deﬁne the bijection g : L →B: Let A ∈L, i.e.,\\nA ⊆{0, 1}∗is a language. We deﬁne the inﬁnite binary sequence g(A) as\\nfollows: For each integer n ≥1, the n-th bit of g(A) is equal to\\n\\x1a 1\\nif sn ∈A,\\n0\\nif sn ̸∈A.\\nIn words, the inﬁnite binary sequence g(A) contains a 1 exactly in those\\npositions n for which the string sn in (5.4) is in the language A.\\nTo give an example, assume that A is the language consisting of all binary\\nstrings that start with 0. The following table gives the corresponding inﬁnite\\nbinary sequence g(A) (this sequence is obtained by reading the rightmost\\ncolumn from top to bottom):\\n5.6.\\nMost languages are not enumerable\\n183\\n{0, 1}∗\\nA\\ng(A)\\nϵ\\nnot in A\\n0\\n0\\nin A\\n1\\n1\\nnot in A\\n0\\n00\\nin A\\n1\\n01\\nin A\\n1\\n10\\nnot in A\\n0\\n11\\nnot in A\\n0\\n000\\nin A\\n1\\n001\\nin A\\n1\\n010\\nin A\\n1\\n100\\nnot in A\\n0\\n011\\nin A\\n1\\n101\\nnot in A\\n0\\n110\\nnot in A\\n0\\n111\\nnot in A\\n0\\n...\\n...\\n...\\nThe function g deﬁned above has the following properties:\\n• If A and A′ are two diﬀerent languages in L, then g(A) ̸= g(A′).\\n• For every inﬁnite binary sequence w in B, there exists a language A in\\nL, such that g(A) = w.\\nThis means that the function g is a bijection from L to B.\\n5.6.3\\nThere are languages that are not enumerable\\nWe have proved that the set\\nE = {A : A ⊆{0, 1}∗is an enumerable language}\\nis countable, whereas the set\\nL = {A : A ⊆{0, 1}∗is a language}\\nis not countable. This means that there are “more” languages in L than\\nthere are in E, proving the following result:\\n184\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.6.3 There exist languages that are not enumerable.\\nThe proof given above shows the existence of languages that are not\\nenumerable. However, the proof does not give us a speciﬁc example of a\\nlanguage that is not enumerable. In the next sections, we will see examples\\nof such languages. Before we move on to these examples, we mention the\\ndiﬀerence between being countable and being enumerable:\\n• Any language A is countable, i.e., we can number the elements of A\\nand, thus, write\\nA = {s1, s2, s3, s4, . . .}.\\n• If the language A is enumerable, then, by Theorem 5.5.2, there is an\\nalgorithm that produces this numbering.\\n• If the language A is not enumerable, then, again by Theorem 5.5.2,\\nthere does not exist an algorithm that produces this numbering.\\n5.7\\nThe relationship between decidable and\\nenumerable languages\\nWe know from Theorem 5.4.2 that every decidable language is enumerable.\\nOn the other hand, we know from Theorems 5.1.6 and 5.4.4 that the converse\\nis not true. The following result should not come as a surprise:\\nTheorem 5.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then,\\nA is decidable if and only if both A and its complement A are enumerable.\\nProof. We ﬁrst assume that A is decidable. Then, by Theorem 5.4.2, A\\nis enumerable. Since A is decidable, it is not diﬃcult to see that A is also\\ndecidable. Then, again by Theorem 5.4.2, A is enumerable.\\nTo prove the converse, we assume that both A and A are enumerable.\\nSince A is enumerable, there exists a Turing machine M1, such that for any\\nstring w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M1, on the input string w, terminates\\nin the accept state of M1.\\n5.7.\\nDecidable versus enumerable languages\\n185\\n• If w ̸∈A, then the computation of M1, on the input string w, terminates\\nin the reject state of M1 or does not terminate.\\nSimilarly, since A is enumerable, there exists a Turing machine M2, such that\\nfor any string w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M2, on the input string w, terminates\\nin the accept state of M2.\\n• If w ̸∈A, then the computation of M2, on the input string w, terminates\\nin the reject state of M2 or does not terminate.\\nWe construct a two-tape Turing machine M:\\nTwo-tape Turing machine M: For any input string w ∈Σ∗, M\\ndoes the following:\\n• M simulates the computation of M1, on input w, on the ﬁrst\\ntape, and, simultaneously, it simulates the computation of M2,\\non input w, on the second tape.\\n• If the simulation of M1 terminates in the accept state of M1,\\nthen M terminates in its accept state.\\n• If the simulation of M2 terminates in the accept state of M2,\\nthen M terminates in its reject state.\\nObserve the following:\\n• If w ∈A, then M1 terminates in its accept state and, therefore, M\\nterminates in its accept state.\\n• If w ̸∈A, then M2 terminates in its accept state and, therefore, M\\nterminates in its reject state.\\nWe conclude that the Turing machine M accepts all strings in A, and rejects\\nall strings that are not in A. This proves that the language A is decidable.\\nWe now use Theorem 5.7.1 to give examples of languages that are not\\nenumerable:\\n186\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.7.2 The language ATM is not enumerable.\\nProof. We know from Theorems 5.4.4 and 5.1.6 that the language ATM is\\nenumerable but not decidable. Combining these facts with Theorem 5.7.1\\nimplies that the language ATM is not enumerable.\\nThe following result can be proved in exactly the same way:\\nTheorem 5.7.3 The language Halt is not enumerable.\\n5.8\\nA language A such that both A and A are\\nnot enumerable\\nIn Theorem 5.7.2, we have seen that the complement of the language ATM\\nis not enumerable.\\nIn Theorem 5.4.4, however, we have shown that the\\nlanguage ATM itself is enumerable. In this section, we consider the language\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\nWe will show the following result:\\nTheorem 5.8.1 Both EQTM and its complement EQTM are not enumer-\\nable.\\n5.8.1\\nEQTM is not enumerable\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We construct\\na new Turing machine TMw that takes as input an arbitrary binary string x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nterminate in the accept state\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that every string x is accepted by TMw.\\nThus, L(TMw) = {0, 1}∗.\\n5.8.\\nBoth A and A not enumerable\\n187\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩∈Halt.\\nThen it follows from the pseudocode that, for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that rejects every input string;\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. By our choice of M1, we have L(M1) = ∅as well. Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt, i.e., ⟨M, w⟩∈Halt. Then we have seen\\nbefore that L(TMw) ̸= ∅= L(M1). Therefore, on input ⟨M1, TMw⟩, H\\neither terminates in the reject state or does not terminate. It follows\\n188\\nChapter 5.\\nDecidable and Undecidable Languages\\nfrom the pseudocode that, on input ⟨M, w⟩, H′ either terminates in the\\nreject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\n5.8.2\\nEQTM is not enumerable\\nThis proof is symmetric to the one in Section 5.8.1.\\nFor a ﬁxed Turing\\nmachine M and a ﬁxed binary string w, we will use the same Turing machine\\nTMw as in Section 5.8.1.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that accepts every input string;\\nconstruct the Turing machine TMw of Section 5.8.1;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\nExercises\\n189\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. Thus, by our choice of M1, we have L(TMw) ̸= L(M1). Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then L(TMw) = {0, 1}∗= L(M1) and, on\\ninput ⟨M1, TMw⟩, H either terminates in the reject state or does not\\nterminate. It follows from the pseudocode that, on input ⟨M, w⟩, H′\\neither terminates in the reject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\nExercises\\n5.1 Prove that the language\\n{w ∈{0, 1}∗: w is the binary representation of 2n for some n ≥0}\\nis decidable. In other words, construct a Turing machine that gets as input\\nan arbitrary number x ∈N, represented in binary as a string w, and that\\ndecides whether or not x is a power of two.\\n5.2 Let F be the set of all functions f : N →N.\\nProve that F is not\\ncountable.\\n5.3 A function f : N →N is called computable, if there exists a Turing\\nmachine, that gets as input an arbitrary positive integer n, written in binary,\\nand gives as output the value of f(n), again written in binary. This Turing\\nmachine has a ﬁnal state. As soon as the Turing machine enters this ﬁnal\\nstate, the computation terminates, and the output is the binary string that\\nis written on its tape.\\nProve that there exist functions f : N →N that are not computable.\\n5.4 Let n be a ﬁxed positive integer, and let k be the number of bits in the\\nbinary representation of n. (Hence, k = 1 + ⌊log n⌋.) Construct a Turing\\nmachine with one tape, tape alphabet {0, 1, 2}, and exactly k + 1 states\\nq0, q1, . . . , qk, that does the following:\\n190\\nChapter 5.\\nDecidable and Undecidable Languages\\nStart of the computation: The tape is empty, i.e., every cell of the tape\\ncontains 2, and the Turing machine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer n, the tape head is on the rightmost bit of the binary represen-\\ntation of n, and the Turing machine is in the ﬁnal state qk.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state qk. As soon as state qk is entered,\\nthe Turing machine terminates.\\n5.5 Give an informal description (in plain English) of a Turing machine\\nwith three tapes, that gets as input the binary representation of an arbitrary\\ninteger m ≥1, and returns as output the unary representation of m.\\nStart of the computation: The ﬁrst tape contains the binary representa-\\ntion of the input m. The other two tapes are empty (i.e., contain only 2s).\\nThe Turing machine is in the start state.\\nEnd of the computation: The third tape contains the unary representation\\nof m, i.e., a string consisting of m many ones. The Turing machine is in the\\nﬁnal state.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this ﬁnal state is entered,\\nthe Turing machine terminates.\\nHint: Use the second tape to maintain a string of ones, whose length is\\na power of two.\\n5.6 In this exercise, you are asked to prove that the busy beaver function\\nBB : N →N is not computable.\\nFor any integer n ≥1, we deﬁne TM n to be the set of all Turing machines\\nM, such that\\n• M has one tape,\\n• M has exactly n states,\\n• the tape alphabet of M is {0, 1, 2}, and\\n• M terminates, when given the empty string ϵ as input.\\nExercises\\n191\\nFor every Turing machine M ∈TM n, we deﬁne f(M) to be the number of\\nones on the tape, after the computation of M, on the empty input string,\\nhas terminated.\\nThe busy beaver function BB : N →N is deﬁned as\\nBB(n) := max{f(M) : M ∈TM n}, for every n ≥1.\\nIn words, BB(n) is the maximum number of ones that any Turing machine\\nwith n states can produce, when given the empty string as input, and as-\\nsuming the Turing machine terminates on this input.\\nProve that the function BB is not computable.\\nHint: Assume that BB is computable. Then there exists a Turing ma-\\nchine M that, for any given n ≥1, computes the value of BB(n). Fix a large\\ninteger n ≥1. Deﬁne (in plain English) a Turing machine that, when given\\nthe empty string as input, terminates and outputs a string consisting of more\\nthan BB(n) many ones. Use Exercises 5.4 and 5.5 to argue that there exists\\nsuch a Turing machine having O(log n) states. Then, if you assume that n\\nis large enough, the number of states is at most n.\\n5.7 Since the set\\nT = {M : M is a Turing machine}\\nis countable, there is an inﬁnite list\\nM1, M2, M3, M4, . . . ,\\nsuch that every Turing machine occurs exactly once in this list.\\nFor any positive integer n, let ⟨n⟩denote the binary representation of n;\\nobserve that ⟨n⟩is a binary string.\\nLet A be the language deﬁned as\\nA = {⟨n⟩:\\nthe Turing machine Mn terminates on the input string ⟨n⟩,\\nand it rejects this string}.\\nProve that the language A is undecidable.\\n5.8 Consider the three languages\\nEmpty = {⟨M⟩: M is a Turing machine for which L(M) = ∅},\\n192\\nChapter 5.\\nDecidable and Undecidable Languages\\nUselessState = {⟨M, q⟩:\\nM is a Turing machine, q is a state of M,\\nfor every input string w, the computation of M on\\ninput w never visits state q},\\nand\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\n• Use Rice’s Theorem to show that Empty is undecidable.\\n• Use the ﬁrst part to show that UselessState is undecidable.\\n• Use the ﬁrst part to show that EQTM is undecidable.\\n5.9 Consider the language\\nREGTM = {⟨M⟩: M is a Turing machine whose language L(M) is regular}.\\nUse Rice’s Theorem to prove that REGTM is undecidable.\\n5.10 We have seen in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts w}\\nis undecidable. Consider the language REGTM of Exercise 5.9. The questions\\nbelow will lead you through a proof of the claim that the language REGTM\\nis undecidable.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nif x = 0n1n for some n ≥0\\nthen terminate in the accept state\\nelse run M on the input string w;\\nif M terminates in the accept state\\nthen terminate in the accept state\\nelse if M terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\nExercises\\n193\\nAnswer the following two questions:\\n• Assume that M accepts the string w. What is the language L(TMw) of\\nthe new Turing machine TMw?\\n• Assume that M does not accept the string w. What is the language\\nL(TMw) of the new Turing machine TMw?\\nThe goal is to prove that the language REGTM is undecidable. We will\\nprove this by contradiction. Thus, we assume that R is a Turing machine\\nthat decides REGTM. Recall what this means:\\n• If M is a Turing machine whose language is regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the accept state.\\n• If M is a Turing machine whose language is not regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the reject state.\\nWe construct a new Turing machine R′ which takes as input an arbitrary\\nTuring machine M and an arbitrary binary string w:\\nTuring machine R′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun R on the input ⟨TMw⟩;\\nif R terminates in the accept state\\nthen terminate in the accept state\\nelse if R terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nProve that M accepts w if and only if R′ (when given ⟨M, w⟩as input),\\nterminates in the accept state.\\nNow ﬁnish the proof by arguing that the language REGTM is undecidable.\\n5.11 A Java program P is called a Hello-World-program, if the following is\\ntrue: When given the empty string ϵ as input, P outputs the string Hello\\nWorld and then terminates. (We do not care what P does when the input\\nstring is non-empty.)\\n194\\nChapter 5.\\nDecidable and Undecidable Languages\\nConsider the language\\nHW = {⟨P⟩: P is a Hello-World-program}.\\nThe questions below will lead you through a proof of the claim that the\\nlanguage HW is undecidable.\\nConsider a ﬁxed Java program P and a ﬁxed binary string w. We write\\na new Java program JPw which takes as input an arbitrary binary string x:\\nJava program JPw(x):\\nrun P on the input w;\\nprint Hello World\\n• Argue that P terminates on input w if and only if ⟨JPw⟩∈HW .\\nThe goal is to prove that the language HW is undecidable. We will prove this\\nby contradiction. Thus, we assume that H is a Java program that decides\\nHW . Recall what this means:\\n• If P is a Hello-World-program, then H, when given ⟨P⟩as input, will\\nterminate in the accept state.\\n• If P is not a Hello-World-program, then H, when given ⟨P⟩as input,\\nwill terminate in the reject state.\\nWe write a new Java program H′ which takes as input the binary encoding\\n⟨P, w⟩of an arbitrary Java program P and an arbitrary binary string w:\\nJava program H′(⟨P, w⟩):\\nconstruct the Java program JPw described above;\\nrun H on the input ⟨JPw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\nArgue that the following are true:\\nExercises\\n195\\n• For any input ⟨P, w⟩, H′ terminates.\\n• If P terminates on input w, then H′ (when given ⟨P, w⟩as input),\\nterminates in the accept state.\\n• If P does not terminate on input w, then H′ (when given ⟨P, w⟩as\\ninput), terminates in the reject state.\\nNow ﬁnish the proof by arguing that the language HW is undecidable.\\n5.12 Prove that the language Halt, see Section 5.1.5, is enumerable.\\n5.13 We deﬁne the following language:\\nL = {u\\n:\\nu = ⟨0, M, w⟩for some ⟨M, w⟩∈ATM,\\nor u = ⟨1, M, w⟩for some ⟨M, w⟩̸∈ATM } .\\nProve that neither L nor its complement L is enumerable.\\nHint: There are two ways to solve this exercise. In the ﬁrst solution, (i)\\nyou assume that L is enumerable, and then prove that ATM is decidable, and\\n(ii) you assume that L is enumerable, and then prove that ATM is decidable.\\nIn the second solution, (i) you assume that L is enumerable, and then prove\\nthat ATM is enumerable, and (ii) you assume that L is enumerable, and then\\nprove that ATM is enumerable.\\n196\\nChapter 5.\\nDecidable and Undecidable Languages\\nChapter 6\\nComplexity Theory\\nIn the previous chapters, we have considered the problem of what can be\\ncomputed by Turing machines (i.e., computers) and what cannot be com-\\nputed. We did not, however, take the eﬃciency of the computations into\\naccount. In this chapter, we introduce a classiﬁcation of decidable languages\\nA, based on the running time of the “best” algorithm that decides A. That\\nis, given a decidable language A, we are interested in the “fastest” algorithm\\nthat, for any given string w, decides whether or not w ∈A.\\n6.1\\nThe running time of algorithms\\nLet M be a Turing machine, and let w be an input string for M. We deﬁne\\nthe running time tM(w) of M on input w as\\ntM(w) := the number of computation steps made by M on input w.\\nAs usual, we denote by |w|, the number of symbols in the string w. We\\ndenote the set of non-negative integers by N0.\\nDeﬁnition 6.1.1 Let Σ be an alphabet, let T : N0 →N0 be a function, let\\nA ⊆Σ∗be a decidable language, and let F : Σ∗→Σ∗be a computable\\nfunction.\\n• We say that the Turing machine M decides the language A in time T,\\nif\\ntM(w) ≤T(|w|)\\nfor all strings w in Σ∗.\\n198\\nChapter 6.\\nComplexity Theory\\n• We say that the Turing machine M computes the function F in time\\nT, if\\ntM(w) ≤T(|w|)\\nfor all strings w ∈Σ∗.\\nIn other words, the “running time function” T is a function of the length\\nof the input, which we usually denote by n. For any n, the value of T(n) is\\nan upper bound on the running time of the Turing machine M, on any input\\nstring of length n.\\nTo give an example, consider the Turing machine of Section 4.2.1 that\\ndecides, using one tape, the language consisting of all palindromes. The tape\\nhead of this Turing machine moves from the left to the right, then back to\\nthe left, then to the right again, back to the left, etc. Each time it reaches\\nthe leftmost or rightmost symbol, it deletes this symbol. The running time\\nof this Turing machine, on any input string of length n, is\\nO(1 + 2 + 3 + . . . + n) = O(n2).\\nOn the other hand, the running time of the Turing machine of Section 4.2.2,\\nwhich also decides the palindromes, but using two tapes instead of just one,\\nis O(n).\\nIn Section 4.4, we mentioned that all computation models listed there are\\nequivalent, in the sense that if a language can be decided in one model, it\\ncan be decided in any of the other models. We just saw, however, that the\\nlanguage consisting of all palindromes allows a faster algorithm on a two-\\ntape Turing machine than on one-tape Turing machines. (Even though we\\ndid not prove this, it is true that Ω(n2) is a lower bound on the running\\ntime to decide palindromes on a one-tape Turing machine.) The following\\ntheorem can be proved.\\nTheorem 6.1.2 Let A be a language (resp. let F be a function) that can be\\ndecided (resp. computed) in time T by an algorithm of type M. Then there is\\nan algorithm of type N that decides A (resp. computes F) in time T ′, where\\nM\\nN\\nT ′\\nk-tape Turing machine\\none-tape Turing machine\\nO(T 2)\\none-tape Turing machine\\nJava program\\nO(T 2)\\nJava program\\nk-tape Turing machine\\nO(T 4)\\n6.2.\\nThe complexity class P\\n199\\n6.2\\nThe complexity class P\\nDeﬁnition 6.2.1 We say that algorithm M decides the language A (resp.\\ncomputes the function F) in polynomial time, if there exists an integer k ≥1,\\nsuch that the running time of M is O(nk), for any input string of length n.\\nIt follows from Theorem 6.1.2 that this notion of “polynomial time” does\\nnot depend on the model of computation:\\nTheorem 6.2.2 Consider the models of computation “Java program”, “k-\\ntape Turing machine”, and “one-tape Turing machine”. If a language can\\nbe decided (resp. a function can be computed) in polynomial time in one of\\nthese models, then it can be decided (resp. computed) in polynomial time in\\nall of these models.\\nBecause of this theorem, we can deﬁne the following two complexity\\nclasses:\\nP := {A : the language A is decidable in polynomial time},\\nand\\nFP := {F : the function F is computable in polynomial time}.\\n6.2.1\\nSome examples\\nPalindromes\\nLet Pal be the language\\nPal := {w ∈{a, b}∗: w is a palindrome}.\\nWe have seen that there exists a one-tape Turing machine that decides Pal\\nin O(n2) time. Therefore, Pal ∈P.\\nSome functions in FP\\nThe following functions are in the class FP:\\n• F1 : N0 →N0 deﬁned by F1(x) := x + 1,\\n• F2 : N2\\n0 →N0 deﬁned by F2(x, y) := x + y,\\n• F3 : N2\\n0 →N0 deﬁned by F3(x, y) := xy.\\n200\\nChapter 6.\\nComplexity Theory\\nr\\nb\\nb\\nb\\nr\\nr\\nb\\nG1\\nG2\\nFigure 6.1: The graph G1 is 2-colorable; r stands for red; b stands for blue.\\nThe graph G2 is not 2-colorable.\\nContext-free languages\\nWe have shown in Section 5.1.3 that every context-free language is decid-\\nable. The algorithm presented there, however, does not run in polynomial\\ntime. Using a technique called dynamic programming (which you will learn\\nin COMP 3804), the following result can be shown:\\nTheorem 6.2.3 Let Σ be an alphabet, and let A ⊆Σ∗be a context-free\\nlanguage. Then A ∈P.\\nObserve that, obviously, every language in P is decidable.\\nThe 2-coloring problem\\nLet G be a graph with vertex set V and edge set E.\\nWe say that G is\\n2-colorable, if it is possible to give each vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. only two colors are used to color all vertices.\\nSee Figure 6.1 for two examples. We deﬁne the following language:\\n2Color := {⟨G⟩: the graph G is 2-colorable},\\nwhere ⟨G⟩denotes the binary string that encodes the graph G.\\n6.2.\\nThe complexity class P\\n201\\nWe claim that 2Color ∈P. In order to show this, we have to construct an\\nalgorithm that decides in polynomial time, whether or not any given graph\\nis 2-colorable.\\nLet G be an arbitrary graph with vertex set V = {1, 2, . . . , m}. The edge\\nset of G is given by an adjacency matrix. This matrix, which we denote by\\nE, is a two-dimensional array with m rows and m columns. For all i and j\\nwith 1 ≤i ≤m and 1 ≤j ≤m, we have\\nE(i, j) =\\n\\x1a 1\\nif (i, j) is an edge of G,\\n0\\notherwise.\\nThe length of the input G, i.e., the number of bits needed to specify G, is\\nequal to m2 =: n. We will present an algorithm that decides, in O(n) time,\\nwhether or not the graph G is 2-colorable.\\nThe algorithm uses the colors red and blue. It gives the ﬁrst vertex the\\ncolor red. Then, the algorithm considers all vertices that are connected by\\nan edge to the ﬁrst vertex, and colors them blue. Now the algorithm is done\\nwith the ﬁrst vertex; it marks this ﬁrst vertex.\\nNext, the algorithm chooses a vertex i that already has a color, but that\\nhas not been marked. Then it considers all vertices j that are connected by\\nan edge to i. If j has the same color as i, then the input graph G is not\\n2-colorable. Otherwise, if vertex j does not have a color yet, the algorithm\\ngives j the color that is diﬀerent from i’s color. After having done this for\\nall neighbors j of i, the algorithm is done with vertex i, so it marks i.\\nIt may happen that there is no vertex i that already has a color but that\\nhas not been marked. (In other words, each vertex i that is not marked does\\nnot have a color yet.) In this case, the algorithm chooses an arbitrary vertex\\ni having this property, and colors it red. (This vertex i is the ﬁrst vertex in\\nits connected component that gets a color.)\\nThis procedure is repeated until all vertices of G have been marked.\\nWe now give a formal description of this algorithm. Vertex i has been\\nmarked, if\\n1. i has a color,\\n2. all vertices that are connected by an edge to i have a color, and\\n3. the algorithm has veriﬁed that each vertex that is connected by an edge\\nto i has a color diﬀerent from i’s color.\\n202\\nChapter 6.\\nComplexity Theory\\nThe algorithm uses two arrays f(1 . . . m) and a(1 . . . m), and a variable\\nM. The value of f(i) is equal to the color (red or blue) of vertex i; if i does\\nnot have a color yet, then f(i) = 0. The value of a(i) is equal to\\na(i) =\\n\\x1a 1\\nif vertex i has been marked,\\n0\\notherwise.\\nThe value of M is equal to the number of marked vertices. The algorithm\\nis presented in Figure 6.2. You are encouraged to convince yourself of the\\ncorrectness of this algorithm. That is, you should convince yourself that this\\nalgorithm returns YES if the graph G is 2-colorable, whereas it returns NO\\notherwise.\\nWhat is the running time of this algorithm? First we count the number\\nof iterations of the outer while-loop. In one iteration, either M increases by\\none, or a vertex i, for which a(i) = 0, gets the color red. In the latter case,\\nthe variable M is increased during the next iteration of the outer while-loop.\\nSince, during the entire outer while-loop, the value of M is increased from\\nzero to m, it follows that there are at most 2m iterations of the outer while-\\nloop. (In fact, the number of iterations is equal to m plus the number of\\nconnected components of G minus one.)\\nOne iteration of the outer while-loop takes O(m) time. Hence, the total\\nrunning time of the algorithm is O(m2), which is O(n). Therefore, we have\\nshown that 2Color ∈P.\\n6.3\\nThe complexity class NP\\nBefore we deﬁne the class NP, we consider some examples.\\nExample 6.3.1 Let G be a graph with vertex set V and edge set E, and\\nlet k ≥1 be an integer. We say that G is k-colorable, if it is possible to give\\neach vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. at most k diﬀerent colors are used to color all vertices.\\nWe deﬁne the following language:\\nkColor := {⟨G⟩: the graph G is k-colorable}.\\n6.3.\\nThe complexity class NP\\n203\\nAlgorithm 2Color\\nfor i := 1 to m do f(i) := 0; a(i) := 0 endfor;\\nf(1) := red; M := 0;\\nwhile M ̸= m\\ndo (∗Find the minimum index i for which vertex i has not\\nbeen marked, but has a color already ∗)\\nbool := false; i := 1;\\nwhile bool = false and i ≤m\\ndo if a(i) = 0 and f(i) ̸= 0 then bool := true else i := i + 1 endif;\\nendwhile;\\n(∗If bool = true, then i is the smallest index such that\\na(i) = 0 and f(i) ̸= 0.\\nIf bool = false, then for all i, the following holds: if a(i) = 0, then\\nf(i) = 0; because M < m, there is at least one such i. ∗)\\nif bool = true\\nthen for j := 1 to m\\ndo if E(i, j) = 1\\nthen if f(i) = f(j)\\nthen return NO and terminate\\nelse if f(j) = 0\\nthen if f(i) = red\\nthen f(j) := blue\\nelse f(j) := red\\nendif\\nendif\\nendif\\nendif\\nendfor;\\na(i) := 1; M := M + 1;\\nelse i := 1;\\nwhile a(i) ̸= 0 do i := i + 1 endwhile;\\n(∗an unvisited connected component starts at vertex i ∗)\\nf(i) := red\\nendif\\nendwhile;\\nreturn YES\\nFigure 6.2:\\nAn algorithm that decides whether or not a graph G is 2-\\ncolorable.\\nWe have seen that for k = 2, this problem is in the class P. For k ≥3, it\\nis not known whether there exists an algorithm that decides, in polynomial\\ntime, whether or not any given graph is k-colorable. In other words, for\\n204\\nChapter 6.\\nComplexity Theory\\nk ≥3, it is not known whether or not kColor is in the class P.\\nExample 6.3.2 Let G be a graph with vertex set V = {1, 2, . . . , m} and\\nedge set E. A Hamilton cycle is a cycle in G that visits each vertex exactly\\nonce. Formally, it is a sequence v1, v2, . . . , vm of vertices such that\\n1. {v1, v2, . . . , vm} = V , and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nWe deﬁne the following language:\\nHC := {⟨G⟩: the graph G contains a Hamilton cycle}.\\nIt is not known whether or not HC is in the class P.\\nExample 6.3.3 The sum of subset language is deﬁned as follows:\\nSOS := {⟨a1, a2, . . . , am, b⟩:\\nm, a1, a2, . . . , am, b ∈N0 and\\n∃I ⊆{1, 2, . . . , m}, P\\ni∈I ai = b}.\\nAlso in this case, no polynomial-time algorithm is known that decides the\\nlanguage SOS. That is, it is not known whether or not SOS is in the class\\nP.\\nExample 6.3.4 An integer x ≥2 is a prime number, if there are no a, b ∈N\\nsuch that a ̸= x, b ̸= x, and x = ab. Hence, the language of all non-primes\\nthat are greater than or equal to two, is\\nNPrim := {⟨x⟩: x ≥2 and x is not a prime number}.\\nIt is not obvious at all, whether or not NPrim is in the class P. In fact, it\\nwas shown only in 2002 that NPrim is in the class P.\\nObservation 6.3.5 The four languages above have the following in com-\\nmon: If someone gives us a “solution” for any given input, then we can\\neasily, i.e., in polynomial time, verify whether or not this “solution” is a cor-\\nrect solution. Moreover, for any input to each of these four problems, there\\nexists a “solution” whose length is polynomial in the length of the input.\\n6.3.\\nThe complexity class NP\\n205\\nLet us again consider the language kColor. Let G be a graph with vertex\\nset V = {1, 2, . . . , m} and edge set E, and let k be a positive integer. We\\nwant to decide whether or not G is k-colorable. A “solution” is a coloring of\\nthe nodes using at most k diﬀerent colors. That is, a solution is a sequence\\nf1, f2, . . . , fm. (Interpret this as: vertex i receives color fi, 1 ≤i ≤m). This\\nsequence is a correct solution if and only if\\n1. fi ∈{1, 2, . . . , k}, for all i with 1 ≤i ≤m, and\\n2. for all i with 1 ≤i ≤m, and for all j with 1 ≤j ≤m, if (i, j) ∈E,\\nthen fi ̸= fj.\\nIf someone gives us this solution (i.e., the sequence f1, f2, . . . , fm), then\\nwe can verify in polynomial time whether or not these two conditions are\\nsatisﬁed. The length of this solution is O(m log k): for each i, we need about\\nlog k bits to represent fi. Hence, the length of the solution is polynomial in\\nthe length of the input, i.e., it is polynomial in the number of bits needed to\\nrepresent the graph G and the number k.\\nFor the Hamilton cycle problem, a solution consists of a sequence v1,\\nv2, . . . , vm of vertices. This sequence is a correct solution if and only if\\n1. {v1, v2, . . . , vm} = {1, 2, . . . , m} and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nThese two conditions can be veriﬁed in polynomial time.\\nMoreover, the\\nlength of the solution is polynomial in the length of the input graph.\\nConsider the sum of subset problem. A solution is a sequence c1, c2, . . . , cm.\\nIt is a correct solution if and only if\\n1. ci ∈{0, 1}, for all i with 1 ≤i ≤m, and\\n2. Pm\\ni=1 ciai = b.\\nHence, the set I ⊆{1, 2, . . . , m} in the deﬁnition of SOS is the set of indices\\ni for which ci = 1. Again, these two conditions can be veriﬁed in polynomial\\ntime, and the length of the solution is polynomial in the length of the input.\\nFinally, let us consider the language NPrim. Let x ≥2 be an integer.\\nThe integers a and b form a “solution” for x if and only if\\n206\\nChapter 6.\\nComplexity Theory\\n1. 2 ≤a < x,\\n2. 2 ≤b < x, and\\n3. x = ab.\\nClearly, these three conditions can be veriﬁed in polynomial time. Moreover,\\nthe length of this solution, i.e., the total number of bits in the binary rep-\\nresentations of a and b, is polynomial in the number of bits in the binary\\nrepresentation of x.\\nLanguages having the property that the correctness of a proposed “solu-\\ntion” can be veriﬁed in polynomial time, form the class NP:\\nDeﬁnition 6.3.6 A language A belongs to the class NP, if there exist a\\npolynomial p and a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\nIn words, a language A is in the class NP, if for every string w, w ∈A if\\nand only if the following two conditions are satisﬁed:\\n1. There is a “solution” s, whose length |s| is polynomial in the length of\\nw (i.e., |s| ≤p(|w|), where p is a polynomial).\\n2. In polynomial time, we can verify whether or not s is a correct “solu-\\ntion” for w (i.e., ⟨w, s⟩∈B and B ∈P).\\nHence, the language B can be regarded to be the “veriﬁcation language”:\\nB = {⟨w, s⟩: s is a correct “solution” for w}.\\nWe have given already informal proofs of the fact that the languages\\nkColor, HC, SOS, and NPrim are all contained in the class NP. Below, we\\nformally prove that NPrim ∈NP. To prove this claim, we have to specify\\nthe polynomial p and the language B ∈P. First, we observe that\\nNPrim = {⟨x⟩:\\nthere exist a and b in N such that\\n2 ≤a < x, 2 ≤b < x and x = ab }.\\n(6.1)\\nWe deﬁne the polynomial p by p(n) := n + 2, and the language B as\\nB := {⟨x, a, b⟩: x ≥2, 2 ≤a < x, 2 ≤b < x and x = ab}.\\n6.3.\\nThe complexity class NP\\n207\\nIt is obvious that B ∈P: For any three positive integers x, a, and b, we\\ncan verify in polynomial time whether or not ⟨x, a, b⟩∈B. In order to do\\nthis, we only have to verify whether or not x ≥2, 2 ≤a < x, 2 ≤b < x,\\nand x = ab. If all these four conditions are satisﬁed, then ⟨x, a, b⟩∈B. If at\\nleast one of them is not satisﬁed, then ⟨x, a, b⟩̸∈B.\\nIt remains to show that for all x ∈N:\\n⟨x⟩∈NPrim ⇐⇒∃a, b : |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B.\\n(6.2)\\n(Remember that |⟨x⟩| denotes the number of bits in the binary representation\\nof x; |⟨a, b⟩| denotes the total number of bits of a and b, i.e., |⟨a, b⟩| =\\n|⟨a⟩| + |⟨b⟩|.)\\nLet x ∈NPrim. It follows from (6.1) that there exist a and b in N, such\\nthat 2 ≤a < x, 2 ≤b < x, and x = ab. Since x = ab ≥2 · 2 = 4 ≥2, it\\nfollows that ⟨x, a, b⟩∈B. Hence, it remains to show that\\n|⟨a, b⟩| ≤|⟨x⟩| + 2.\\nThe binary representation of x contains ⌊log x⌋+1 bits, i.e., |⟨x⟩| = ⌊log x⌋+1.\\nWe have\\n|⟨a, b⟩|\\n=\\n|⟨a⟩| + |⟨b⟩|\\n=\\n(⌊log a⌋+ 1) + (⌊log b⌋+ 1)\\n≤\\nlog a + log b + 2\\n=\\nlog ab + 2\\n=\\nlog x + 2\\n≤\\n⌊log x⌋+ 3\\n=\\n|⟨x⟩| + 2.\\nThis proves one direction of (6.2).\\nTo prove the other direction, we assume that there are positive integers\\na and b, such that |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B. Then it follows\\nimmediately from (6.1) and the deﬁnition of the language B, that x ∈NPrim.\\nHence, we have proved the other direction of (6.2). This completes the proof\\nof the claim that\\nNPrim ∈NP.\\n208\\nChapter 6.\\nComplexity Theory\\n6.3.1\\nP is contained in NP\\nIntuitively, it is clear that P ⊆NP, because a language is\\n• in P, if for every string w, it is possible to compute the “solution” s in\\npolynomial time,\\n• in NP, if for every string w and for any given “solution” s, it is possible\\nto verify in polynomial time whether or not s is a correct solution for\\nw (hence, we do not need to compute the solution s ourselves, we only\\nhave to verify it).\\nWe give a formal proof of this:\\nTheorem 6.3.7 P ⊆NP.\\nProof. Let A ∈P. We will prove that A ∈NP. Deﬁne the polynomial p\\nby p(n) := 0 for all n ∈N0, and deﬁne\\nB := {⟨w, ϵ⟩: w ∈A}.\\nSince A ∈P, the language B is also contained in P. It is easy to see that\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) = 0 and ⟨w, s⟩∈B.\\nThis completes the proof.\\n6.3.2\\nDeciding NP-languages in exponential time\\nLet us look again at the deﬁnition of the class NP. Let A be a language in\\nthis class. Then there exist a polynomial p and a language B ∈P, such that\\nfor all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.3)\\nHow do we decide whether or not any given string w belongs to the language\\nA? If we can ﬁnd a string s that satisﬁes the right-hand side in (6.3), then\\nwe know that w ∈A. On the other hand, if there is no such string s, then\\nw ̸∈A. How much time do we need to decide whether or not such a string s\\nexists?\\n6.3.\\nThe complexity class NP\\n209\\nAlgorithm NonPrime\\n(∗decides whether or not ⟨x⟩∈NPrim ∗)\\nif x = 0 or x = 1 or x = 2\\nthen return NO and terminate\\nelse a := 2;\\nwhile a < x\\ndo if x mod a = 0\\nthen return YES and terminate\\nelse a := a + 1\\nendif\\nendwhile;\\nreturn NO\\nendif\\nFigure 6.3: An algorithm that decides whether or not a number x is contained\\nin the language NPrim.\\nFor example, let A be the language\\nNPrim = {⟨x⟩: x ≥2 and x is not a prime number},\\nand let x ∈N. The algorithm in Figure 6.3 decides whether or not ⟨x⟩∈\\nNPrim.\\nIt is clear that this algorithm is correct. Let n be the length of the binary\\nrepresentation of x, i.e., n = ⌊log x⌋+ 1. If x > 2 and x is a prime number,\\nthen the while-loop makes x−2 iterations. Therefore, since n−1 = ⌊log x⌋≤\\nlog x, the running time of this algorithm is at least\\nx −2 ≥2n−1 −2,\\ni.e., it is at least exponential in the length of the input.\\nWe now prove that every language in NP can be decided in exponential\\ntime. Let A be an arbitrary language in NP. Let p be the polynomial, and\\nlet B ∈P be the language such that for all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.4)\\nThe following algorithm decides, for any given string w, whether or not\\nw ∈A. It does so by looking at all possible strings s for which |s| ≤p(|w|):\\n210\\nChapter 6.\\nComplexity Theory\\nfor all s with |s| ≤p(|w|)\\ndo if ⟨w, s⟩∈B\\nthen return YES and terminate\\nendif\\nendfor;\\nreturn NO\\nThe correctness of the algorithm follows from (6.4). What is the running\\ntime? We assume that w and s are represented as binary strings. Let n be\\nthe length of the input, i.e., n = |w|.\\nHow many binary strings s are there whose length is at most p(|w|)? Any\\nsuch s can be described by a sequence of length p(|w|) = p(n), consisting of\\nthe symbols “0”, “1”, and the blank symbol. Hence, there are at most 3p(n)\\nmany binary strings s with |s| ≤p(n). Therefore, the for-loop makes at most\\n3p(n) iterations.\\nSince B ∈P, there is an algorithm and a polynomial q, such that this\\nalgorithm, when given any input string z, decides in q(|z|) time, whether or\\nnot z ∈B. This input z has the form ⟨w, s⟩, and we have\\n|z| = |w| + |s| ≤|w| + p(|w|) = n + p(n).\\nIt follows that the total running time of our algorithm that decides whether\\nor not w ∈A, is bounded from above by\\n3p(n) · q(n + p(n))\\n≤\\n22p(n) · q(n + p(n))\\n≤\\n22p(n) · 2q(n+p(n))\\n=\\n2p′(n),\\nwhere p′ is the polynomial that is deﬁned by p′(n) := 2p(n) + q(n + p(n)).\\nIf we deﬁne the class EXP as\\nEXP :=\\n{A :\\nthere exists a polynomial p, such that A can be\\ndecided in time 2p(n) } ,\\nthen we have proved the following theorem.\\nTheorem 6.3.8 NP ⊆EXP.\\n6.4.\\nNon-deterministic algorithms\\n211\\n6.3.3\\nSummary\\n• P ⊆NP. It is not known whether P is a proper subclass of NP, or\\nwhether P = NP. This is one of the most important open problems in\\ncomputer science. If you can solve this problem, then you will get one\\nmillion dollars; not from us, but from the Clay Mathematics Institute,\\nsee\\nhttp://www.claymath.org/prizeproblems/index.htm\\nMost people believe that P is a proper subclass of NP.\\n• NP ⊆EXP, i.e., each language in NP can be decided in exponential\\ntime. It is not known whether NP is a proper subclass of EXP, or\\nwhether NP = EXP.\\n• It follows from P ⊆NP and NP ⊆EXP, that P ⊆EXP. It can\\nbe shown that P is a proper subset of EXP, i.e., there exist languages\\nthat can be decided in exponential time, but that cannot be decided in\\npolynomial time.\\n• P is the class of those languages that can be decided eﬃciently, i.e., in\\npolynomial time. Sets that are not in P, are not eﬃciently decidable.\\n6.4\\nNon-deterministic algorithms\\nThe abbreviation NP stands for Non-deterministic Polynomial time. The al-\\ngorithms that we have considered so far are deterministic, which means that\\nat any time during the computation, the next computation step is uniquely\\ndetermined. In a non-deterministic algorithm, there are one or more possi-\\nbilities for being the next computation step, and the algorithm chooses one\\nof them.\\nTo give an example, we consider the language SOS, see Example 6.3.3.\\nLet m, a1, a2, . . . , am, and b be elements of N0. Then\\n⟨a1, a2, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, c2, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciai = b.\\nThe following non-deterministic algorithm decides the language SOS:\\n212\\nChapter 6.\\nComplexity Theory\\nAlgorithm SOS(m, a1, a2, . . . , am, b):\\ns := 0;\\nfor i := 1 to m\\ndo s := s | s := s + ai\\nendfor;\\nif s = b\\nthen return YES\\nelse return NO\\nendif\\nThe line\\ns := s | s := s + ai\\nmeans that either the instruction “s := s” or the instruction “s := s + ai” is\\nexecuted.\\nLet us assume that ⟨a1, a2, . . . , am, b⟩∈SOS. Then there are c1, c2, . . . , cm ∈\\n{0, 1} such that Pm\\ni=1 ciai = b. Assume our algorithm does the following, for\\neach i with 1 ≤i ≤m: In the i-th iteration,\\n• if ci = 0, then it executes the instruction “s := s”,\\n• if ci = 1, then it executes the instruction “s := s + ai”.\\nThen after the for-loop, we have s = b, and the algorithm returns YES;\\nhence, the algorithm has correctly found out that ⟨a1, a2, . . . , am, b⟩∈SOS.\\nIn other words, in this case, there exists at least one accepting computation.\\nOn the other hand, if ⟨a1, a2, . . . , am, b⟩̸∈SOS, then the algorithm always\\nreturns NO, no matter which of the two instructions is executed in each\\niteration of the for-loop. In this case, there is no accepting computation.\\nDeﬁnition 6.4.1 Let M be a non-deterministic algorithm. We say that M\\naccepts a string w, if there exists at least one computation that, on input w,\\nreturns YES.\\nDeﬁnition 6.4.2 We say that a non-deterministic algorithm M decides a\\nlanguage A in time T, if for every string w, the following holds: w ∈A if\\nand only if there exists at least one computation that, on input w, returns\\nYES and that takes at most T(|w|) time.\\n6.5.\\nNP-complete languages\\n213\\nThe non-deterministic algorithm that we have seen above decides the\\nlanguage SOS in linear time: Let ⟨a1, a2, . . . , am, b⟩∈SOS, and let n be the\\nlength of this input. Then\\nn = |⟨a1⟩| + |⟨a2⟩| + . . . + |⟨am⟩| + |⟨b⟩| ≥m.\\nFor this input, there is a computation that returns YES and that takes\\nO(m) = O(n) time.\\nAs in Section 6.2, we deﬁne the notion of “polynomial time” for non-\\ndeterministic algorithms. The following theorem relates this notion to the\\nclass NP that we deﬁned in Deﬁnition 6.3.6.\\nTheorem 6.4.3 A language A is in the class NP if and only if there exists\\na non-deterministic Turing machine (or Java program) that decides A in\\npolynomial time.\\n6.5\\nNP-complete languages\\nLanguages in the class P are considered easy, i.e., they can be decided in\\npolynomial time. People believe (but cannot prove) that P is a proper sub-\\nclass of NP. If this is true, then there are languages in NP that are hard,\\ni.e., cannot be decided in polynomial time.\\nIntuition tells us that if P ̸= NP, then the hardest languages in NP are\\nnot contained in P. These languages are called NP-complete. In this section,\\nwe will give a formal deﬁnition of this concept.\\nIf we want to talk about the “hardest” languages in NP, then we have to\\nbe able to compare two languages according to their “diﬃculty”. The idea is\\nas follows: We say that a language B is “at least as hard” as a language A,\\nif the following holds: If B can be decided in polynomial time, then A can\\nalso be decided in polynomial time.\\nDeﬁnition 6.5.1 Let A ⊆{0, 1}∗and B ⊆{0, 1}∗be languages. We say\\nthat A ≤P B, if there exists a function\\nf : {0, 1}∗→{0, 1}∗\\nsuch that\\n1. f ∈FP and\\n214\\nChapter 6.\\nComplexity Theory\\n2. for all strings w in {0, 1}∗,\\nw ∈A ⇐⇒f(w) ∈B.\\nIf A ≤P B, then we also say that “B is at least as hard as A”, or “A is\\npolynomial-time reducible to B”.\\nWe ﬁrst show that this formal deﬁnition is in accordance with the intuitive\\ndeﬁnition given above.\\nTheorem 6.5.2 Let A and B be languages such that B ∈P and A ≤P B.\\nThen A ∈P.\\nProof. Let f : {0, 1}∗→{0, 1}∗be the function in FP for which\\nw ∈A ⇐⇒f(w) ∈B.\\n(6.5)\\nThe following algorithm decides whether or not any given binary string w is\\nin A:\\nu := f(w);\\nif u ∈B\\nthen return YES\\nelse return NO\\nendif\\nThe correctness of this algorithm follows immediately from (6.5). So it\\nremains to show that the running time is polynomial in the length of the\\ninput string w.\\nSince f ∈FP, there exists a polynomial p such that the function f can\\nbe computed in time p. Similarly, since B ∈P, there exists a polynomial q,\\nsuch that the language B can be decided in time q.\\nLet n be the length of the input string w, i.e., n = |w|. Then the length\\nof the string u is less than or equal to p(|w|) = p(n). (Why?) Therefore, the\\nrunning time of our algorithm is bounded from above by\\np(|w|) + q(|u|) ≤p(n) + q(p(n)).\\nSince the function p′, deﬁned by p′(n) := p(n)+q(p(n)), is a polynomial, this\\nproves that A ∈P.\\nThe following theorem states that the relation ≤P is reﬂexive and tran-\\nsitive. We leave the proof as an exercise.\\n6.5.\\nNP-complete languages\\n215\\nTheorem 6.5.3 Let A, B, and C be languages. Then\\n1. A ≤P A, and\\n2. if A ≤P B and B ≤P C, then A ≤P C.\\nWe next show that the languages in P are the easiest languages in NP:\\nTheorem 6.5.4 Let A be a language in P, and let B be an arbitrary lan-\\nguage such that B ̸= ∅and B ̸= {0, 1}∗. Then A ≤P B.\\nProof. We choose two strings u and v in {0, 1}∗, such that u ∈B and v ̸∈B.\\n(Observe that this is possible.) Deﬁne the function f : {0, 1}∗→{0, 1}∗by\\nf(w) :=\\n\\x1a u\\nif w ∈A,\\nv\\nif w ̸∈A.\\nThen it is clear that for any binary string w,\\nw ∈A ⇐⇒f(w) ∈B.\\nSince A ∈P, the function f can be computed in polynomial time, i.e.,\\nf ∈FP.\\n6.5.1\\nTwo examples of reductions\\nSum of subsets and knapsacks\\nWe start with a simple reduction. Consider the two languages\\nSOS := {⟨a1, . . . , am, b⟩:\\nm, a1, . . . , am, b ∈N0 and there exist\\nc1, . . . , cm ∈{0, 1}, such that Pm\\ni=1 ciai = b}\\nand\\nKS\\n:=\\n{⟨w1, . . . , wm, k1, . . . , km, W, K⟩:\\nm, w1, . . . , wm, k1, . . . , km, W, K ∈N0\\nand there exist c1, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciwi ≤W and Pm\\ni=1 ciki ≥K}.\\nThe notation KS stands for knapsack: We have m pieces of food. The\\ni-th piece has weight wi and contains ki calories. We want to decide whether\\nor not we can ﬁll our knapsack with a subset of the pieces of food such that\\nthe total weight is at most W, and the total amount of calories is at least K.\\n216\\nChapter 6.\\nComplexity Theory\\nTheorem 6.5.5 SOS ≤P KS.\\nProof. Let us ﬁrst see what we have to show. According to Deﬁnition 6.5.1,\\nwe need a function f ∈FP, that maps input strings for SOS to input strings\\nfor KS, in such a way that\\n⟨a1, . . . , am, b⟩∈SOS ⇐⇒f(⟨a1, . . . , am, b⟩) ∈KS.\\nIn order for f(⟨a1, . . . , am, b⟩) to be an input string for KS, this function\\nvalue has to be of the form\\nf(⟨a1, . . . , am, b⟩) = ⟨w1, . . . , wm, k1, . . . , km, W, K⟩.\\nWe deﬁne\\nf(⟨a1, . . . , am, b⟩) := ⟨a1, . . . , am, a1, . . . , am, b, b⟩.\\nIt is clear that f ∈FP. We have\\n⟨a1, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai = b\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai ≤b and Pm\\ni=1 ciai ≥b\\n⇐⇒\\n⟨a1, . . . , am, a1, . . . , am, b, b⟩∈KS\\n⇐⇒\\nf(⟨a1, . . . , am, b⟩) ∈KS.\\nCliques and Boolean formulas\\nWe will deﬁne two languages A = 3SAT and B = Clique that have, at\\nﬁrst sight, nothing to do with each other. Then we show that, nevertheless,\\nA ≤P B.\\nLet G be a graph with vertex set V and edge set E. A subset V ′ of V is\\ncalled a clique, if each pair of distinct vertices in V ′ is connected by an edge\\nin E. We deﬁne the following language:\\nClique := {⟨G, k⟩: k ∈N and G has a clique with k vertices}.\\nWe encourage you to prove the following claim:\\n6.5.\\nNP-complete languages\\n217\\nTheorem 6.5.6 Clique ∈NP.\\nNext we consider Boolean formulas ϕ, with variables x1, x2, . . . , xm, hav-\\ning the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.6)\\nwhere each Ci, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nEach ℓi\\na is either a variable or the negation of a variable. An example of such\\na formula is\\nϕ = (x1 ∨¬x1 ∨¬x2) ∧(x3 ∨x2 ∨x4) ∧(¬x1 ∨¬x3 ∨¬x4).\\nA formula ϕ of the form (6.6) is said to be satisﬁable, if there exists a truth-\\nvalue in {0, 1} for each of the variables x1, x2, . . . , xm, such that the entire\\nformula ϕ is true. Our example formula is satisﬁable: If we take x1 = 0 and\\nx2 = 1, and give x3 and x4 an arbitrary value, then\\nϕ = (0 ∨1 ∨0) ∧(x3 ∨1 ∨x4) ∧(1 ∨¬x3 ∨¬x4) = 1.\\nWe deﬁne the following language:\\n3SAT := {⟨ϕ⟩: ϕ is of the form (6.6) and is satisﬁable}.\\nAgain, we encourage you to prove the following claim:\\nTheorem 6.5.7 3SAT ∈NP.\\nObserve that the elements of Clique (which are pairs consisting of a graph\\nand a positive integer) are completely diﬀerent from the elements of 3SAT\\n(which are Boolean formulas). We will show that 3SAT ≤P Clique. Recall\\nthat this means the following: If the language Clique can be decided in\\npolynomial time, then the language 3SAT can also be decided in polynomial\\ntime. In other words, any polynomial-time algorithm that decides Clique can\\nbe converted to a polynomial-time algorithm that decides 3SAT.\\nTheorem 6.5.8 3SAT ≤P Clique.\\n218\\nChapter 6.\\nComplexity Theory\\nProof. We have to show that there exists a function f ∈FP, that maps\\ninput strings for 3SAT to input strings for Clique, such that for each Boolean\\nformula ϕ that is of the form (6.6),\\n⟨ϕ⟩∈3SAT ⇐⇒f(⟨ϕ⟩) ∈Clique.\\nThe function f maps the binary string encoding an arbitrary Boolean formula\\nϕ to a binary string encoding a pair (G, k), where G is a graph and k is a\\npositive integer. We have to deﬁne this function f in such a way that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\nLet\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an arbitrary Boolean formula in the variables x1, x2, . . . , xm, where each\\nCi, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nRemember that each ℓi\\na is either a variable or the negation of a variable.\\nThe formula ϕ is mapped to the pair (G, k), where the vertex set V and\\nthe edge set E of the graph G are deﬁned as follows:\\n• V = {v1\\n1, v1\\n2, v1\\n3, . . . , vk\\n1, vk\\n2, vk\\n3}. The idea is that each vertex vi\\na corre-\\nsponds to one term ℓi\\na.\\n• The pair (vi\\na, vj\\nb) of vertices form an edge in E if and only if\\n– i ̸= j and\\n– ℓi\\na is not the negation of ℓj\\nb.\\nTo give an example, let ϕ be the Boolean formula\\nϕ = (x1 ∨¬x2 ∨¬x3) ∧(¬x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3),\\n(6.7)\\ni.e., k = 3, C1 = x1 ∨¬x2 ∨¬x3, C2 = ¬x1 ∨x2 ∨x3, and C3 = x1 ∨x2 ∨x3.\\nThe graph G that corresponds to this formula is given in Figure 6.4.\\nIt is not diﬃcult to see that the function f can be computed in polynomial\\ntime. So it remains to prove that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\n(6.8)\\n6.5.\\nNP-complete languages\\n219\\n¬x2\\n¬x3\\nx1\\n¬x1\\nx2\\nx3\\nx1\\nx2\\nx3\\nFigure 6.4: The formula ϕ in (6.7) is mapped to this graph. The vertices on\\nthe top represent C1; the vertices on the left represent C2; the vertices on\\nthe right represent C3.\\nTo prove this, we ﬁrst assume that the formula\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nis satisﬁable. Then there exists a truth-value in {0, 1} for each of the variables\\nx1, x2, . . . , xm, such that the entire formula ϕ is true. Hence, for each i with\\n1 ≤i ≤k, there is at least one term ℓi\\na in\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3\\nthat is true (i.e., has value 1).\\nLet V ′ be the set of vertices obtained by choosing for each i, 1 ≤i ≤k,\\nexactly one vertex vi\\na such that ℓi\\na has value 1.\\nIt is clear that V ′ contains exactly k vertices. We claim that this set is\\na clique in G. To prove this claim, let vi\\na and vj\\nb be two distinct vertices in\\nV ′. It follows from the deﬁnition of V ′ that i ̸= j and ℓi\\na = ℓj\\nb = 1. Hence,\\nℓi\\na is not the negation of ℓj\\nb. But this means that the vertices vi\\na and vj\\nb are\\nconnected by an edge in G.\\nThis proves one direction of (6.8). To prove the other direction, we assume\\nthat the graph G contains a clique V ′ with k vertices.\\n220\\nChapter 6.\\nComplexity Theory\\nThe vertices of G consist of k groups, where each group contains exactly\\nthree vertices. Since vertices within the same group are not connected by\\nedges, the clique V ′ contains exactly one vertex from each group. Hence, for\\neach i with 1 ≤i ≤k, there is exactly one a, such that vi\\na ∈V ′. Consider\\nthe corresponding term ℓi\\na. We know that this term is either a variable or\\nthe negation of a variable, i.e., ℓi\\na is either of the form xj or of the form ¬xj.\\nIf ℓi\\na = xj, then we give xj the truth-value 1. Otherwise, we have ℓi\\na = ¬xj,\\nin which case we give xj the truth-value 0. Since V ′ is a clique, each variable\\ngets at most one truth-value. If a variable has no truth-value yet, then we\\ngive it an arbitrary truth-value.\\nIf we substitute these truth-values into ϕ, then the entire formula has\\nvalue 1. Hence, ϕ is satisﬁable.\\nIn order to get a better understanding of this proof, you should verify the\\nproof for the formula ϕ in (6.7) and the graph G in Figure 6.4.\\n6.5.2\\nDeﬁnition of NP-completeness\\nReductions, as deﬁned in Deﬁnition 6.5.1, allow us to compare two language\\naccording to their diﬃculty. A language B in NP is called NP-complete,\\nif B belongs to the most diﬃcult languages in NP; in other words, B is at\\nleast as hard as any other language in NP.\\nDeﬁnition 6.5.9 Let B ⊆{0, 1}∗be a language. We say that B is NP-\\ncomplete, if\\n1. B ∈NP and\\n2. A ≤P B, for every language A in NP.\\nTheorem 6.5.10 Let B be an NP-complete language. Then\\nB ∈P ⇐⇒P = NP.\\nProof. Intuitively, this theorem should be true: If the language B is in P,\\nthen B is an easy language. On the other hand, since B is NP-complete,\\nit belongs to the most diﬃcult languages in NP. Hence, the most diﬃcult\\nlanguage in NP is easy. But then all languages in NP must be easy, i.e.,\\nP = NP.\\n6.5.\\nNP-complete languages\\n221\\nWe give a formal proof. Let us ﬁrst assume that B ∈P. We already\\nknow that P ⊆NP. Hence, it remains to show that NP ⊆P. Let A be an\\narbitrary language in NP. Since B is NP-complete, we have A ≤P B. Then,\\nby Theorem 6.5.2, we have A ∈P.\\nTo prove the converse, assume that P = NP. Since B ∈NP, it follows\\nimmediately that B ∈P.\\nTheorem 6.5.11 Let B and C be languages, such that C ∈NP and B ≤P\\nC. If B is NP-complete, then C is also NP-complete.\\nProof. First, we give an intuitive explanation of the claim: By assumption,\\nB belongs to the most diﬃcult languages in NP, and C is at least as hard as\\nB. Since C ∈NP, it follows that C belongs to the most diﬃcult languages\\nin NP. Hence, C is NP-complete.\\nTo give a formal proof, we have to show that A ≤P C, for all languages A\\nin NP. Let A be an arbitrary language in NP. Since B is NP-complete, we\\nhave A ≤P B. Since B ≤P C, it follows from Theorem 6.5.3, that A ≤P C.\\nTherefore, C is NP-complete.\\nTheorem 6.5.11 can be used to prove the NP-completeness of languages:\\nLet C be a language, and assume that we want to prove that C is NP-\\ncomplete. We can do this in the following way:\\n1. We ﬁrst prove that C ∈NP.\\n2. Then we ﬁnd a language B that looks “similar” to C, and for which\\nwe already know that it is NP-complete.\\n3. Finally, we prove that B ≤P C.\\n4. Then, Theorem 6.5.11 tells us that C is NP-complete.\\nOf course, this leads to the question “How do we know that the language\\nB is NP-complete?” In order to apply Theorem 6.5.11, we need a “ﬁrst” NP-\\ncomplete language; the NP-completeness of this language must be proven\\nusing Deﬁnition 6.5.9.\\nObserve that it is not clear at all that there exist NP-complete languages!\\nFor example, consider the language 3SAT. If we want to use Deﬁnition 6.5.9\\nto show that this language is NP-complete, then we have to show that\\n222\\nChapter 6.\\nComplexity Theory\\n• 3SAT ∈NP. We know from Theorem 6.5.7 that this is true.\\n• A ≤P 3SAT, for every language A ∈NP. Hence, we have to show this\\nfor languages A such as kColor, HC, SOS, NPrim, KS, Clique, and\\nfor inﬁnitely many other languages.\\nIn 1971, Cook has exactly done this: He showed that the language 3SAT\\nis NP-complete. Since his proof is rather technical, we will prove the NP-\\ncompleteness of another language.\\n6.5.3\\nAn NP-complete domino game\\nWe are given a ﬁnite collection of tile types. For each such type, there are\\narbitrarily many tiles of this type. A tile is a square that is partitioned into\\nfour triangles. Each of these triangles contains a symbol that belongs to a\\nﬁnite alphabet Σ. Hence, a tile looks as follows:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\na\\nb\\nc\\nd\\nWe are also given a square frame, consisting of cells. Each cell has the same\\nsize as a tile, and contains a symbol of Σ.\\nThe problem is to decide whether or not this domino game has a solution.\\nThat is, can we completely ﬁll the frame with tiles such that\\n• for any two neighboring tiles s and s′, the two triangles of s and s′ that\\ntouch each other contain the same symbol, and\\n• each triangle that touches the frame contains the same symbol as the\\ncell of the frame that is touched by this triangle.\\nThere is one ﬁnal restriction: The orientation of the tiles is ﬁxed, they cannot\\nbe rotated.\\nLet us give a formal deﬁnition of this problem. We assume that the sym-\\nbols belong to the ﬁnite alphabet Σ = {0, 1}m, i.e., each symbol is encoded\\nas a bit-string of length m. Then, a tile type can be encoded as a tuple of\\nfour bit-strings, i.e., as an element of Σ4. A frame consisting of t rows and t\\ncolumns can be encoded as a string in Σ4t.\\n6.5.\\nNP-complete languages\\n223\\nWe denote the language of all solvable domino games by Domino:\\nDomino\\n:=\\n{⟨m, k, t, R, T1, . . . , Tk⟩:\\nm ≥1, k ≥1, t ≥1, R ∈Σ4t, Ti ∈Σ4, 1 ≤i ≤k,\\nframe R can be ﬁlled using tiles of types\\nT1, . . . , Tk.}\\nWe will prove the following theorem.\\nTheorem 6.5.12 The language Domino is NP-complete.\\nProof. It is clear that Domino ∈NP: A solution consists of a t × t matrix,\\nin which the (i, j)-entry indicates the type of the tile that occupies position\\n(i, j) in the frame. The number of bits needed to specify such a solution is\\npolynomial in the length of the input. Moreover, we can verify in polynomial\\ntime whether or not any given “solution” is correct.\\nIt remains to show that\\nA ≤P Domino, for every language A in NP.\\nLet A be an arbitrary language in NP. Then there exist a polynomial p and\\na non-deterministic Turing machine M, that decides the language A in time\\np. We may assume that this Turing machine has only one tape.\\nOn input w = a1a2 . . . an, the Turing machine M starts in the start state\\nz0, with its tape head on the cell containing the symbol a1. We may assume\\nthat during the entire computation, the tape head never moves to the left of\\nthis initial cell. Hence, the entire computation “takes place” in and to the\\nright of the initial cell. We know that\\nw ∈A\\n⇐⇒\\non input w, there exists an accepting computation\\nthat makes at most p(n) computation steps.\\nAt the end of such an accepting computation, the tape only contains the\\nsymbol 1, which we may assume to be in the initial cell, and M is in the ﬁnal\\nstate z1. In this case, we may assume that the accepting computation makes\\nexactly p(n) computation steps. (If this is not the case, then we extend the\\ncomputation using the instruction z11 →z11N.)\\nWe need one more technical detail: We may assume that za →z′bR and\\nza′ →z′′b′L are not both instructions of M. Hence, the state of the Turing\\nmachine uniquely determines the direction in which the tape head moves.\\n224\\nChapter 6.\\nComplexity Theory\\nWe have to deﬁne a domino game, that depends on the input string w\\nand the Turing machine M, such that\\nw ∈A ⇐⇒this domino game is solvable.\\nThe idea is to encode an accepting computation of the Turing machine M as\\na solution of the domino game. In order to do this, we use a frame in which\\neach row corresponds to one computation step. This frame consists of p(n)\\nrows. Since an accepting computation makes exactly p(n) computation steps,\\nand since the tape head never moves to the left of the initial cell, this tape\\nhead can visit only p(n) cells. Therefore, our frame will have p(n) columns.\\nThe domino game will use the following tile types:\\n1. For each symbol a in the alphabet of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\n#\\na\\nIntuition: Before and after the computation step, the tape head is not\\non this cell.\\n2. For each instruction za →z′bR of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\nz′\\nb\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the right.\\n3. For each instruction za →z′bL of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz′\\n(z, a)\\n#\\nb\\n6.5.\\nNP-complete languages\\n225\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the left.\\n4. For each instruction za →z′bN of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\n#\\n(z′, b)\\nIntuition: Before and after the computation step, the tape head is on\\nthis cell.\\n5. For each state z and for each symbol a in the alphabet of the Turing\\nmachine M, there are two tile types:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz\\na\\n#\\n(z, a)\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\nz\\n(z, a)\\nIntuition: The leftmost tile indicates that the tape head enters this cell\\nfrom the left; the righmost tile indicates that the tape head enters this\\ncell from the right.\\nThis speciﬁes all tile types. The p(n) × p(n) frame is given in Figure 6.5.\\nThe top row corresponds to the start of the computation, whereas the bottom\\nrow corresponds to the end of the computation. The left and right columns\\ncorrespond to the part of the tape in which the tape head can move.\\nThe encodings of these tile types and the frame can be computed in\\npolynomial time.\\nIt can be shown that, for any input string w, any accepting computation\\nof length p(n) of the Turing machine M can be encoded as a solution of\\nthis domino game. Conversely, any solution of this domino game can be\\n“translated” to an accepting computation of length p(n) of M, on input\\nstring w. Hence, the following holds.\\nw ∈A\\n⇐⇒\\nthere exists an accepting computation that makes\\np(n) computation steps\\n⇐⇒\\nthe domino game is solvable.\\n226\\nChapter 6.\\nComplexity Theory\\n(z0, a1)\\na2\\n. . .\\nan\\n✷\\n. . .\\n✷\\n#\\n#\\n#\\n#\\n#\\n...\\n#\\n...\\n✷\\n✷\\n✷\\n✷\\n✷\\n. . .\\n(z1, 1)\\np(n)\\np(n)\\nFigure 6.5: The p(n) × p(n) frame for the domino game.\\nTherefore, we have A ≤P Domino. Hence, the language Domino is NP-\\ncomplete.\\nAn example of a domino game\\nWe have deﬁned the domino game corresponding to a Turing machine that\\nsolves a decision problem. Of course, we can also do this for Turing machines\\nthat compute functions. In this section, we will exactly do this for a Turing\\nmachine that computes the successor function x →x + 1.\\nWe will design a Turing machine with one tape, that gets as input the\\nbinary representation of a natural number x, and that computes the binary\\nrepresentation of x + 1.\\nStart of the computation: The tape contains a 0 followed by the binary\\nrepresentation of the integer x ∈N0. The tape head is on the leftmost bit\\n(which is 0), and the Turing machine is in the start state z0. Here is an\\nexample, where x = 431:\\n6.5.\\nNP-complete languages\\n227\\n0 1 1 0 1 0 1 1 1 1 2\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe number x + 1. The tape head is on the rightmost 1, and the Turing\\nmachine is in the ﬁnal state z1. For our example, the tape looks as follows:\\n0 1 1 0 1 1 0 0 0 0 2\\n6\\nOur Turing machine will use the following states:\\nz0 :\\nstart state; tape head moves to the right\\nz1 :\\nﬁnal state\\nz2 :\\ntape head moves to the left; on its way to the left, it has not read 0\\nThe Turing machine has the following instructions:\\nz00 →z00R\\nz21 →z20L\\nz01 →z01R\\nz20 →z11N\\nz02 →z22L\\nIn Figure 6.6, you can see the sequence of states and tape contents of this\\nTuring machine on input x = 11.\\nWe now construct the domino game that corresponds to the computation\\nof this Turing machine on input x = 11. Following the general construction\\nin Section 6.5.3, we obtain the following tile types:\\n1. The three symbols of the alphabet yield three tile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n2\\n2\\n2. The ﬁve instructions of the Turing machine yield ﬁve tile types:\\n228\\nChapter 6.\\nComplexity Theory\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n0\\n(z0, 1)\\n0\\n1\\n1\\n2\\n0\\n1\\n(z0, 0)\\n1\\n1\\n2\\n0\\n1\\n0\\n(z0, 1)\\n1\\n2\\n0\\n1\\n0\\n1\\n(z0, 1)\\n2\\n0\\n1\\n0\\n1\\n1\\n(z0, 2)\\n0\\n1\\n0\\n1\\n(z2, 1)\\n2\\n0\\n1\\n0\\n(z2, 1)\\n0\\n2\\n0\\n1\\n(z2, 0)\\n0\\n0\\n2\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\nFigure 6.6: The computation of the Turing machine on input x = 11. The\\npair (state,symbol) indicates the position of the tape head.\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n3. The states z0 and z2, and the three symbols of the alphabet yield twelve\\ntile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 2)\\n2\\nThe computation of the Turing machine on input x = 11 consists of nine\\ncomputation steps. During this computation, the tape head visits exactly\\nsix cells. Therefore, the frame for the domino game has nine rows and six\\ncolumns.\\nThis frame is given in Figure 6.7.\\nIn Figure 6.8, you ﬁnd the\\nsolution of the domino game.\\nObserve that this solution is nothing but\\nan equivalent way of writing the computation of Figure 6.6.\\nHence, the\\ncomputation of the Turing machine corresponds to a solution of the domino\\ngame; in fact, the converse also holds.\\n6.5.\\nNP-complete languages\\n229\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\nFigure 6.7: The frame for the domino game for input x = 11.\\n230\\nChapter 6.\\nComplexity Theory\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\nFigure 6.8: The solution for the domino game for input x = 11.\\n6.5.\\nNP-complete languages\\n231\\n6.5.4\\nExamples of NP-complete languages\\nIn Section 6.5.3, we have shown that Domino is NP-complete. Using this\\nresult, we will apply Theorem 6.5.11 to prove the NP-completeness of some\\nother languages.\\nSatisﬁability\\nWe consider Boolean formulas ϕ, in the variables x1, x2, . . . , xm, having the\\nform\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.9)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nEach ℓi\\nj is either a variable or the negation of a variable. Such a formula ϕ\\nis said to be satisﬁable, if there exists a truth-value in {0, 1} for each of the\\nvariables x1, x2, . . . , xm, such that the entire formula ϕ is true. We deﬁne the\\nfollowing language:\\nSAT := {⟨ϕ⟩: ϕ is of the form (6.9) and is satisﬁable}.\\nWe will prove that SAT is NP-complete.\\nIt is clear that SAT ∈NP. If we can show that\\nDomino ≤P SAT,\\nthen it follows from Theorem 6.5.11 that SAT is NP-complete. (In Theo-\\nrem 6.5.11, take B := Domino and C := SAT.)\\nHence, we need a function f ∈FP, that maps input strings for Domino\\nto input strings for SAT, in such a way that for every domino game D, the\\nfollowing holds:\\ndomino game D is solvable ⇐⇒the formula encoded by the\\nstring f(⟨D⟩) is satisﬁable.\\n(6.10)\\nLet us consider an arbitrary domino game D. Let k be the number of\\ntile types, and let the frame have t rows and t columns. We denote the tile\\ntypes by T1, T2, . . . , Tk.\\n232\\nChapter 6.\\nComplexity Theory\\nWe map this domino game D to a Boolean formula ϕ, such that (6.10)\\nholds. The formula ϕ will have variables\\nxijℓ, 1 ≤i ≤t, 1 ≤j ≤t, 1 ≤ℓ≤k.\\nThese variables can be interpretated as follows:\\nxijℓ= 1 ⇐⇒there is a tile of type Tℓat position (i, j) of the frame.\\nWe deﬁne:\\n• For all i and j with 1 ≤i ≤t and 1 ≤j ≤t:\\nC1\\nij := xij1 ∨xij2 ∨. . . ∨xijk.\\nThis formula expresses the condition that there is at least one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j ≤t, and 1 ≤ℓ< ℓ′ ≤k:\\nC2\\nijℓℓ′ := ¬xijℓ∨¬xijℓ′.\\nThis formula expresses the condition that there is at most one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j < t, 1 ≤ℓ≤k and 1 ≤ℓ′ ≤k,\\nsuch that i < t and the right symbol on a tile of type Tℓis not equal\\nto the left symbol on a tile of type Tℓ′:\\nC3\\nijℓℓ′ := ¬xijℓ∨¬xi,j+1,ℓ′.\\nThis formula expresses the condition that neighboring tiles in the same\\nrow “ﬁt” together. There are symmetric formulas for neighboring tiles\\nin the same column.\\n• For all j and ℓwith 1 ≤j ≤t and 1 ≤ℓ≤k, such that the top symbol\\non a tile of type Tℓis not equal to the symbol at position j of the upper\\nboundary of the frame:\\nC4\\njℓ:= ¬x1jℓ.\\nThis formula expresses the condition that tiles that touch the upper\\nboundary of the frame “ﬁt” there. There are symmetric formulas for\\nthe lower, left, and right boundaries of the frame.\\n6.5.\\nNP-complete languages\\n233\\nThe formula ϕ is the conjunction of all these formulas C1\\nij, C2\\nijℓℓ′, C3\\nijℓℓ′, and\\nC4\\njℓ. The complete formula ϕ consists of\\nO(t2k + t2k2 + t2k2 + tk) = O(t2k2)\\nterms, i.e., its length is polynomial in the length of the domino game. This\\nimplies that ϕ can be constructed in polynomial time. Hence, the function\\nf that maps the domino game D to the Boolean formula ϕ, is in the class\\nFP. It is not diﬃcult to see that (6.10) holds for this function f. Therefore,\\nwe have proved the following result.\\nTheorem 6.5.13 The language SAT is NP-complete.\\nIn Section 6.5.1, we have deﬁned the language 3SAT.\\nTheorem 6.5.14 The language 3SAT is NP-complete.\\nProof. It is clear that 3SAT ∈NP. If we can show that\\nSAT ≤P 3SAT,\\nthen the claim follows from Theorem 6.5.11. Let\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an input for SAT, in the variables x1, x2, . . . , xm. We map ϕ, in polynomial\\ntime, to an input ϕ′ for 3SAT, such that\\nϕ is satisﬁable ⇐⇒ϕ′ is satisﬁable.\\n(6.11)\\nFor each i with 1 ≤i ≤k, we do the following. Consider\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\n• If ki = 1, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n1 ∨ℓi\\n1.\\n• If ki = 2, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n2.\\n234\\nChapter 6.\\nComplexity Theory\\n• If ki = 3, then we deﬁne\\nC′\\ni := Ci.\\n• If ki ≥4, then we deﬁne\\nC′\\ni\\n:=\\n(ℓi\\n1 ∨ℓi\\n2 ∨zi\\n1) ∧(¬zi\\n1 ∨ℓi\\n3 ∨zi\\n2) ∧(¬zi\\n2 ∨ℓi\\n4 ∨zi\\n3) ∧. . .\\n∧(¬zi\\nki−3 ∨ℓi\\nki−1 ∨ℓi\\nki),\\nwhere zi\\n1, . . . , zi\\nki−3 are new variables.\\nLet\\nϕ′ := C′\\n1 ∧C′\\n2 ∧. . . ∧C′\\nk.\\nThen ϕ′ is an input for 3SAT, and (6.11) holds.\\nTheorems 6.5.6, 6.5.8, 6.5.11, and 6.5.14 imply:\\nTheorem 6.5.15 The language Clique is NP-complete.\\nThe traveling salesperson problem\\nWe are given two positive integers k and m, a set of m cities, and an integer\\nm × m matrix M, where\\nM(i, j) = the cost of driving from city i to city j,\\nfor all i, j ∈{1, 2, . . . , m}. We want to decide whether or not there is a tour\\nthrough all cities whose total cost is less than or equal to k. This problem is\\nNP-complete.\\nBin packing\\nWe are given three positive integers m, k, and ℓ, a set of m objects having\\nvolumes a1, a2, . . . , am, and k bins. Each bin has volume ℓ. We want to\\ndecide whether or not all objects ﬁt within these bins. This problem is NP-\\ncomplete.\\nHere is another interpretation of this problem: We are given m jobs that\\nneed time a1, a2, . . . , am to complete. We are also given k processors, and an\\ninteger ℓ. We want to decide whether or not it is possible to divide the jobs\\nover the k processors, such that no processor needs more than ℓtime.\\nExercises\\n235\\nTime tables\\nWe are given a set of courses, class rooms, and professors.\\nWe want to\\ndecide whether or not there exists a time table such that all courses are\\nbeing taught, no two courses are taught at the same time in the same class\\nroom, no professor teaches two courses at the same time, and conditions such\\nas “Prof. L. Azy does not teach before 1pm” are satisﬁed. This problem is\\nNP-complete.\\nMotion planning\\nWe are given two positive integers k and ℓ, a set of k polyhedra, and two\\npoints s and t in Q3. We want to decide whether or not there exists a path\\nbetween s and t, that does not intersect any of the polyhedra, and whose\\nlength is less than or equal to ℓ. This problem is NP-complete.\\nMap labeling\\nWe are given a map with m cities, where each city is represented by a point.\\nFor each city, we are given a rectangle that is large enough to contain the\\nname of the city. We want to decide whether or not these rectangles can be\\nplaced on the map, such that\\n• no two rectangles overlap,\\n• For each i with 1 ≤i ≤m, the point that represents city i is a corner\\nof its rectangle.\\nThis problem is NP-complete.\\nThis list of NP-complete problems can be extended almost arbitrarily:\\nFor thousands of problems, it is known that they are NP-complete. For all\\nof these, it is not known, whether or not they can be solved eﬃciently (i.e.,\\nin polynomial time). Collections of NP-complete problems can be found in\\nthe book\\n• M.R. Garey and D.S. Johnson. Computers and Intractability: A Guide\\nto the Theory of NP-Completeness. W.H. Freeman, New York, 1979,\\nand on the web page\\nhttp://www.nada.kth.se/~viggo/wwwcompendium/\\n236\\nChapter 6.\\nComplexity Theory\\nExercises\\n6.1 Prove that the function F : N →N, deﬁned by F(x) := 2x, is not in FP.\\n6.2 Prove Theorem 6.5.3.\\n6.3 Prove that the language Clique is in the class NP.\\n6.4 Prove that the language 3SAT is in the class NP.\\n6.5 We deﬁne the following languages:\\n• Sum of subset:\\nSOS := {⟨a1, a2, . . . , am, b⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai = b}.\\n• Set partition:\\nSP := {⟨a1, a2, . . . , am⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai =\\nX\\ni̸∈I\\nai}.\\n• Bin packing: BP is the set of all strings ⟨s1, s2, . . . , sm, B⟩for which\\n1. 0 < si < 1, for all i,\\n2. B ∈N,\\n3. the numbers s1, s2, . . . , sm ﬁt into B bins, where each bin has size\\none, i.e., there exists a partition of {1, 2, . . . , m} into subsets Ik,\\n1 ≤k ≤B, such that P\\ni∈Ik si ≤1 for all k, 1 ≤k ≤B.\\nFor example, ⟨1/6, 1/2, 1/5, 1/9, 3/5, 1/5, 1/2, 11/18, 3⟩∈BP, because\\nthe eight fractions ﬁt into three bins:\\n1/6 + 1/9 + 11/18 ≤1, 1/2 + 1/2 = 1, and 1/5 + 3/5 + 1/5 = 1.\\n1. Prove that SOS ≤P SP.\\n2. Prove that the language SOS is NP-complete. You may use the fact\\nthat the language SP is NP-complete.\\nExercises\\n237\\n3. Prove that the language BP is NP-complete. Again, you may use the\\nfact that the language SP is NP-complete.\\n6.6 Prove that 3Color ≤P 3SAT.\\nHint: For each vertex i, and for each of the three colors k, introduce a\\nBoolean variable xik.\\n6.7 The (0, 1)-integer programming language IP is deﬁned as follows:\\nIP := {⟨A, c⟩:\\nA is an integer m × n matrix for some m, n ∈N,\\nc is an integer vector of length m, and\\n∃x ∈{0, 1}n such that Ax ≤c (componentwise) }.\\nProve that the language IP is NP-complete. You may use the fact that\\nthe language SOS is NP-complete.\\n6.8 Let ϕ be a Boolean formula in the variables x1, x2, . . . , xm.\\nWe say that ϕ is in disjunctive normal form (DNF) if it is of the form\\nϕ = C1 ∨C2 ∨. . . ∨Ck,\\n(6.12)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∧ℓi\\n2 ∧. . . ∧ℓi\\nki.\\nEach ℓi\\nj is a literal, which is either a variable or the negation of a variable.\\nWe say that ϕ is in conjunctive normal form (CNF) if it is of the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.13)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nAgain, each ℓi\\nj is a literal.\\nWe deﬁne the following two languages:\\nDNFSAT := {⟨ϕ⟩: ϕ is in DNF-form and is satisﬁable},\\nand\\nCNFSAT := {⟨ϕ⟩: ϕ is in CNF-form and is satisﬁable}.\\n238\\nChapter 6.\\nComplexity Theory\\n1. Prove that the language DNFSAT is in P.\\n2. What is wrong with the following argument: Since we can rewrite\\nany Boolean formula in DNF-form, we have CNFSAT ≤P DNFSAT.\\nHence, since CNFSAT is NP-complete and since DNFSAT ∈P, we\\nhave P = NP.\\n3. Prove directly that for every language A in P, A ≤P CNFSAT. “Di-\\nrectly” means that you should not use the fact that CNFSAT is NP-\\ncomplete.\\n6.9 1 Prove that the polynomial upper bound on the length of the string y\\nin the deﬁnition of NP is necessary, in the sense that if it is left out, then\\nany enumerable language would satisfy the condition.\\nMore precisely, we say that the language A belongs to the class E, if there\\nexists a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃y : ⟨w, y⟩∈B.\\nProve that E is equal to the class of all enumerable languages.\\n1Thanks to Antoine Vigneron for poining out an error in a previous version of this\\nexercise.\\nChapter 7\\nSummary\\nWe have seen several diﬀerent models for “processing” languages, i.e., pro-\\ncessing sets of strings over some ﬁnite alphabet. For each of these models,\\nwe have asked the question which types of languages can be processed, and\\nwhich types of languages cannot be processed. In this ﬁnal chapter, we give\\na brief summary of these results.\\nRegular languages:\\nThis class of languages was considered in Chapter 2.\\nThe following statements are equivalent:\\n1. The language A is regular, i.e., there exists a deterministic ﬁnite au-\\ntomaton that accepts A.\\n2. There exists a nondeterministic ﬁnite automaton that accepts A.\\n3. There exists a regular expression that describes A.\\nThis claim was proved by the following conversions:\\n1. Every nondeterministic ﬁnite automaton can be converted to an equiv-\\nalent deterministic ﬁnite automaton.\\n2. Every deterministic ﬁnite automaton can be converted to an equivalent\\nregular expression.\\n3. Every regular expression can be converted to an equivalent nondeter-\\nministic ﬁnite automaton.\\n240\\nChapter 7.\\nSummary\\nWe have seen that the class of regular languages is closed under the regular\\noperations: If A and B are regular languages, then\\n1. A ∪B is regular,\\n2. AB is regular,\\n3. A∗is regular,\\n4. A is regular, and\\n5. A ∩B is regular.\\nFinally, the pumping lemma for regular languages gives a property that\\nevery regular language possesses. We have used this to prove that languages\\nsuch as {anbn : n ≥0} are not regular.\\nContext-free languages:\\nThis class of languages was considered in Chap-\\nter 3. We have seen that every regular language is context-free. Moreover,\\nthere exist languages, for example {anbn : n ≥0}, that are context-free, but\\nnot regular. The following statements are equivalent:\\n1. The language A is context-free, i.e., there exists a context-free grammar\\nwhose language is A.\\n2. There exists a context-free grammar in Chomsky normal form whose\\nlanguage is A.\\n3. There exists a nondeterministic pushdown automaton that accepts A.\\nThis claim was proved by the following conversions:\\n1. Every context-free grammar can be converted to an equivalent context-\\nfree grammar in Chomsky normal form.\\n2. Every context-free grammar in Chomsky normal form can be converted\\nto an equivalent nondeterministic pushdown automaton.\\n3. Every nondeterministic pushdown automaton can be converted to an\\nequivalent context-free grammar. (This conversion was not covered in\\nthis book.)\\nChapter 7.\\nSummary\\n241\\nNondeterministic pushdown automata are more powerful than determin-\\nistic pushdown automata: There exists a nondeterministic pushdown au-\\ntomaton that accepts the language\\n{vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|},\\nbut there is no deterministic pushdown automaton that accepts this language.\\n(We did not prove this in this book.)\\nWe have seen that the class of context-free languages is closed under\\nthe union, concatenation, and star operations: If A and B are context-free\\nlanguages, then\\n1. A ∪B is context-free,\\n2. AB is context-free, and\\n3. A∗is context-free.\\nHowever,\\n1. the intersection of two context-free languages is not necessarily context-\\nfree, and\\n2. the complement of a context-free language is not necessarily context-\\nfree.\\nFinally, the pumping lemma for context-free languages gives a property\\nthat every context-free language possesses. We have used this to prove that\\nlanguages such as {anbncn : n ≥0} are not context-free.\\nThe Church-Turing Thesis:\\nIn Chapter 4, we considered “reasonable”\\ncomputational devices that model real computers. Examples of such devices\\nare Turing machines (with one or more tapes) and Java programs. It turns\\nout that all known “reasonable” devices are equivalent, i.e., can be converted\\nto each other. This led to the Church-Turing Thesis:\\n• Every computational process that is intuitively considered to be an\\nalgorithm can be converted to a Turing machine.\\n242\\nChapter 7.\\nSummary\\nDecidable and enumerable languages:\\nThese classes of languages were\\nconsidered in Chapter 5. They are deﬁned based on “reasonable” computa-\\ntional devices, such as Turing machines and Java programs. We have seen\\nthat\\n1. every context-free language is decidable, and\\n2. every decidable language is enumerable.\\nMoreover,\\n1. there exist languages, for example {anbncn : n ≥0}, that are decidable,\\nbut not context-free,\\n2. there exist languages, for example the Halting Problem, that are enu-\\nmerable, but not decidable,\\n3. there exist languages, for example the complement of the Halting Prob-\\nlem, that are not enumerable.\\nIn fact,\\n1. the class of all languages is not countable, whereas\\n2. the class of all enumerable languages is countable.\\nThe following statements are equivalent:\\n1. The language A is decidable.\\n2. Both A and its complement A are enumerable.\\nComplexity classes:\\nThese classes of languages were considered in Chap-\\nter 6.\\n1. The class P consists of all languages that can be decided in polynomial\\ntime by a deterministic Turing machine.\\n2. The class NP consists of all languages that can be decided in poly-\\nnomial time by a nondeterministic Turing machine. Equivalently, a\\nlanguage A is in the class NP, if for every string w ∈A, there exists a\\n“solution” s, such that (i) the length of s is polynomial in the length\\nof w, and (ii) the correctness of s can be veriﬁed in polynomial time.\\nChapter 7.\\nSummary\\n243\\nThe following properties hold:\\n1. Every context-free language is in P. (We did not prove this).\\n2. Every language in P is also in NP.\\n3. It is not known if there exist languages that are in NP, but not in P.\\n4. Every language in NP is decidable.\\nWe have introduced reductions to deﬁne the notion of a language B to be\\n“at least as hard” as a language A. A language B is called NP-complete, if\\n1. B belongs to the class NP, and\\n2. B is at least as hard as every language in the class NP.\\nWe have seen that NP-complete exist.\\nThe ﬁgure below summarizes the relationships among the various classes\\nof languages.\\n244\\nChapter 7.\\nSummary\\nregular\\ncontext-free\\nP\\nNP\\ndecidable\\nenumerable\\nall languages\\n', 'Introduction to Theory of Computation\\nAnil Maheshwari\\nMichiel Smid\\nSchool of Computer Science\\nCarleton University\\nOttawa\\nCanada\\n{anil,michiel}@scs.carleton.ca\\nAugust 29, 2024\\nii\\nContents\\nContents\\nPreface\\nvi\\n1\\nIntroduction\\n1\\n1.1\\nPurpose and motivation\\n. . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1.1\\nComplexity theory\\n. . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.2\\nComputability theory . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.3\\nAutomata theory . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.1.4\\nThis course\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.2\\nMathematical preliminaries\\n. . . . . . . . . . . . . . . . . . .\\n4\\n1.3\\nProof techniques\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.3.1\\nDirect proofs\\n. . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.3.2\\nConstructive proofs . . . . . . . . . . . . . . . . . . . .\\n9\\n1.3.3\\nNonconstructive proofs . . . . . . . . . . . . . . . . . .\\n10\\n1.3.4\\nProofs by contradiction . . . . . . . . . . . . . . . . . .\\n11\\n1.3.5\\nThe pigeon hole principle . . . . . . . . . . . . . . . . .\\n12\\n1.3.6\\nProofs by induction . . . . . . . . . . . . . . . . . . . .\\n13\\n1.3.7\\nMore examples of proofs . . . . . . . . . . . . . . . . .\\n15\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2\\nFinite Automata and Regular Languages\\n21\\n2.1\\nAn example: Controling a toll gate . . . . . . . . . . . . . . .\\n21\\n2.2\\nDeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . . . .\\n23\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton . . . . . . . . . .\\n26\\n2.2.2\\nA second example of a ﬁnite automaton\\n. . . . . . . .\\n28\\n2.2.3\\nA third example of a ﬁnite automaton\\n. . . . . . . . .\\n29\\n2.3\\nRegular operations . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n2.4\\nNondeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . .\\n35\\n2.4.1\\nA ﬁrst example . . . . . . . . . . . . . . . . . . . . . .\\n35\\niv\\nContents\\n2.4.2\\nA second example . . . . . . . . . . . . . . . . . . . . .\\n37\\n2.4.3\\nA third example . . . . . . . . . . . . . . . . . . . . . .\\n38\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\n. . . .\\n39\\n2.5\\nEquivalence of DFAs and NFAs . . . . . . . . . . . . . . . . .\\n41\\n2.5.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n2.6\\nClosure under the regular operations\\n. . . . . . . . . . . . . .\\n48\\n2.7\\nRegular expressions . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n2.8\\nEquivalence of regular expressions and regular languages . . .\\n57\\n2.8.1\\nEvery regular expression describes a regular language .\\n58\\n2.8.2\\nConverting a DFA to a regular expression\\n. . . . . . .\\n61\\n2.9\\nThe pumping lemma and nonregular languages . . . . . . . . .\\n68\\n2.9.1\\nApplications of the pumping lemma . . . . . . . . . . .\\n70\\n2.10 Higman’s Theorem . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.1 Dickson’s Theorem . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.2 Proof of Higman’s Theorem . . . . . . . . . . . . . . .\\n78\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n3\\nContext-Free Languages\\n91\\n3.1\\nContext-free grammars . . . . . . . . . . . . . . . . . . . . . .\\n91\\n3.2\\nExamples of context-free grammars . . . . . . . . . . . . . . .\\n94\\n3.2.1\\nProperly nested parentheses . . . . . . . . . . . . . . .\\n94\\n3.2.2\\nA context-free grammar for a nonregular language . . .\\n95\\n3.2.3\\nA context-free grammar for the complement of a non-\\nregular language\\n. . . . . . . . . . . . . . . . . . . . .\\n97\\n3.2.4\\nA context-free grammar that veriﬁes addition\\n. . . . .\\n98\\n3.3\\nRegular languages are context-free . . . . . . . . . . . . . . . . 100\\n3.3.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 102\\n3.4\\nChomsky normal form\\n. . . . . . . . . . . . . . . . . . . . . . 104\\n3.4.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 109\\n3.5\\nPushdown automata\\n. . . . . . . . . . . . . . . . . . . . . . . 112\\n3.6\\nExamples of pushdown automata\\n. . . . . . . . . . . . . . . . 116\\n3.6.1\\nProperly nested parentheses . . . . . . . . . . . . . . . 116\\n3.6.2\\nStrings of the form 0n1n\\n. . . . . . . . . . . . . . . . . 117\\n3.6.3\\nStrings with b in the middle . . . . . . . . . . . . . . . 118\\n3.7\\nEquivalence of pushdown automata and context-free grammars 120\\n3.8\\nThe pumping lemma for context-free languages\\n. . . . . . . . 124\\n3.8.1\\nProof of the pumping lemma . . . . . . . . . . . . . . . 125\\n3.8.2\\nApplications of the pumping lemma . . . . . . . . . . . 128\\nContents\\nv\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n4\\nTuring Machines and the Church-Turing Thesis\\n137\\n4.1\\nDeﬁnition of a Turing machine . . . . . . . . . . . . . . . . . . 137\\n4.2\\nExamples of Turing machines\\n. . . . . . . . . . . . . . . . . . 141\\n4.2.1\\nAccepting palindromes using one tape\\n. . . . . . . . . 141\\n4.2.2\\nAccepting palindromes using two tapes . . . . . . . . . 142\\n4.2.3\\nAccepting anbncn using one tape . . . . . . . . . . . . . 143\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2} . . . . 145\\n4.2.5\\nAccepting ambncmn using one tape . . . . . . . . . . . . 147\\n4.3\\nMulti-tape Turing machines . . . . . . . . . . . . . . . . . . . 148\\n4.4\\nThe Church-Turing Thesis . . . . . . . . . . . . . . . . . . . . 151\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n5\\nDecidable and Undecidable Languages\\n157\\n5.1\\nDecidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n5.1.1\\nThe language ADFA . . . . . . . . . . . . . . . . . . . . 158\\n5.1.2\\nThe language ANFA . . . . . . . . . . . . . . . . . . . . 159\\n5.1.3\\nThe language ACFG . . . . . . . . . . . . . . . . . . . . 160\\n5.1.4\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 161\\n5.1.5\\nThe Halting Problem . . . . . . . . . . . . . . . . . . . 163\\n5.2\\nCountable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n5.2.1\\nThe Halting Problem revisited . . . . . . . . . . . . . . 168\\n5.3\\nRice’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n5.3.1\\nProof of Rice’s Theorem . . . . . . . . . . . . . . . . . 171\\n5.4\\nEnumerability . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n5.4.1\\nHilbert’s problem . . . . . . . . . . . . . . . . . . . . . 174\\n5.4.2\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 176\\n5.5\\nWhere does the term “enumerable” come from? . . . . . . . . 177\\n5.6\\nMost languages are not enumerable . . . . . . . . . . . . . . . 180\\n5.6.1\\nThe set of enumerable languages is countable\\n. . . . . 180\\n5.6.2\\nThe set of all languages is not countable . . . . . . . . 181\\n5.6.3\\nThere are languages that are not enumerable . . . . . . 183\\n5.7\\nThe relationship between decidable and enumerable languages 184\\n5.8\\nA language A such that both A and A are not enumerable . . 186\\n5.8.1\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 186\\n5.8.2\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 188\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\nvi\\nContents\\n6\\nComplexity Theory\\n197\\n6.1\\nThe running time of algorithms . . . . . . . . . . . . . . . . . 197\\n6.2\\nThe complexity class P . . . . . . . . . . . . . . . . . . . . . . 199\\n6.2.1\\nSome examples . . . . . . . . . . . . . . . . . . . . . . 199\\n6.3\\nThe complexity class NP . . . . . . . . . . . . . . . . . . . . . 202\\n6.3.1\\nP is contained in NP . . . . . . . . . . . . . . . . . . . 208\\n6.3.2\\nDeciding NP-languages in exponential time\\n. . . . . . 208\\n6.3.3\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . 211\\n6.4\\nNon-deterministic algorithms\\n. . . . . . . . . . . . . . . . . . 211\\n6.5\\nNP-complete languages\\n. . . . . . . . . . . . . . . . . . . . . 213\\n6.5.1\\nTwo examples of reductions . . . . . . . . . . . . . . . 215\\n6.5.2\\nDeﬁnition of NP-completeness . . . . . . . . . . . . . . 220\\n6.5.3\\nAn NP-complete domino game\\n. . . . . . . . . . . . . 222\\n6.5.4\\nExamples of NP-complete languages . . . . . . . . . . 231\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\n7\\nSummary\\n239\\nPreface\\nThis is a free textbook for an undergraduate course on the Theory of Com-\\nputation, which we have been teaching at Carleton University since 2002.\\nUntil the 2011/2012 academic year, this course was oﬀered as a second-year\\ncourse (COMP 2805) and was compulsory for all Computer Science students.\\nStarting with the 2012/2013 academic year, the course has been downgraded\\nto a third-year optional course (COMP 3803).\\nWe have been developing this book since we started teaching this course.\\nCurrently, we cover most of the material from Chapters 2–5 during a 12-week\\nterm with three hours of classes per week.\\nThe material from Chapter 6, on Complexity Theory, is taught in the\\nthird-year course COMP 3804 (Design and Analysis of Algorithms). In the\\nearly years of COMP 2805, we gave a two-lecture overview of Complexity\\nTheory at the end of the term. Even though this overview has disappeared\\nfrom the course, we decided to keep Chapter 6. This chapter has not been\\nrevised/modiﬁed for a long time.\\nThe course as we teach it today has been inﬂuenced by the following two\\ntextbooks:\\n• Introduction to the Theory of Computation (second edition), by Michael\\nSipser, Thomson Course Technnology, Boston, 2006.\\n• Einf¨uhrung in die Theoretische Informatik, by Klaus Wagner, Springer-\\nVerlag, Berlin, 1994.\\nBesides reading this text, we recommend that you also take a look at\\nthese excellent textbooks, as well as one or more of the following ones:\\n• Elements of the Theory of Computation (second edition), by Harry\\nLewis and Christos Papadimitriou, Prentice-Hall, 1998.\\nviii\\n• Introduction to Languages and the Theory of Computation (third edi-\\ntion), by John Martin, McGraw-Hill, 2003.\\n• Introduction to Automata Theory, Languages, and Computation (third\\nedition), by John Hopcroft, Rajeev Motwani, Jeﬀrey Ullman, Addison\\nWesley, 2007.\\nPlease let us know if you ﬁnd errors, typos, simpler proofs, comments,\\nomissions, or if you think that some parts of the book “need improvement”.\\nChapter 1\\nIntroduction\\n1.1\\nPurpose and motivation\\nThis course is on the Theory of Computation, which tries to answer the\\nfollowing questions:\\n• What are the mathematical properties of computer hardware and soft-\\nware?\\n• What is a computation and what is an algorithm? Can we give rigorous\\nmathematical deﬁnitions of these notions?\\n• What are the limitations of computers?\\nCan “everything” be com-\\nputed? (As we will see, the answer to this question is “no”.)\\nPurpose of the Theory of Computation: Develop formal math-\\nematical models of computation that reﬂect real-world computers.\\nThis ﬁeld of research was started by mathematicians and logicians in the\\n1930’s, when they were trying to understand the meaning of a “computation”.\\nA central question asked was whether all mathematical problems can be\\nsolved in a systematic way. The research that started in those days led to\\ncomputers as we know them today.\\nNowadays, the Theory of Computation can be divided into the follow-\\ning three areas: Complexity Theory, Computability Theory, and Automata\\nTheory.\\n2\\nChapter 1.\\nIntroduction\\n1.1.1\\nComplexity theory\\nThe main question asked in this area is “What makes some problems com-\\nputationally hard and other problems easy?”\\nInformally, a problem is called “easy”, if it is eﬃciently solvable. Exam-\\nples of “easy” problems are (i) sorting a sequence of, say, 1,000,000 numbers,\\n(ii) searching for a name in a telephone directory, and (iii) computing the\\nfastest way to drive from Ottawa to Miami. On the other hand, a problem is\\ncalled “hard”, if it cannot be solved eﬃciently, or if we don’t know whether\\nit can be solved eﬃciently. Examples of “hard” problems are (i) time table\\nscheduling for all courses at Carleton, (ii) factoring a 300-digit integer into\\nits prime factors, and (iii) computing a layout for chips in VLSI.\\nCentral Question in Complexity Theory: Classify problems ac-\\ncording to their degree of “diﬃculty”. Give a rigorous proof that\\nproblems that seem to be “hard” are really “hard”.\\n1.1.2\\nComputability theory\\nIn the 1930’s, G¨odel, Turing, and Church discovered that some of the fun-\\ndamental mathematical problems cannot be solved by a “computer”. (This\\nmay sound strange, because computers were invented only in the 1940’s).\\nAn example of such a problem is “Is an arbitrary mathematical statement\\ntrue or false?” To attack such a problem, we need formal deﬁnitions of the\\nnotions of\\n• computer,\\n• algorithm, and\\n• computation.\\nThe theoretical models that were proposed in order to understand solvable\\nand unsolvable problems led to the development of real computers.\\nCentral Question in Computability Theory: Classify problems\\nas being solvable or unsolvable.\\n1.1.\\nPurpose and motivation\\n3\\n1.1.3\\nAutomata theory\\nAutomata Theory deals with deﬁnitions and properties of diﬀerent types of\\n“computation models”. Examples of such models are:\\n• Finite Automata. These are used in text processing, compilers, and\\nhardware design.\\n• Context-Free Grammars. These are used to deﬁne programming lan-\\nguages and in Artiﬁcial Intelligence.\\n• Turing Machines.\\nThese form a simple abstract model of a “real”\\ncomputer, such as your PC at home.\\nCentral Question in Automata Theory: Do these models have\\nthe same power, or can one model solve more problems than the\\nother?\\n1.1.4\\nThis course\\nIn this course, we will study the last two areas in reverse order: We will start\\nwith Automata Theory, followed by Computability Theory. The ﬁrst area,\\nComplexity Theory, will be covered in COMP 3804.\\nActually, before we start, we will review some mathematical proof tech-\\nniques. As you may guess, this is a fairly theoretical course, with lots of\\ndeﬁnitions, theorems, and proofs. You may guess this course is fun stuﬀfor\\nmath lovers, but boring and irrelevant for others. You guessed it wrong, and\\nhere are the reasons:\\n1. This course is about the fundamental capabilities and limitations of\\ncomputers. These topics form the core of computer science.\\n2. It is about mathematical properties of computer hardware and software.\\n3. This theory is very much relevant to practice, for example, in the design\\nof new programming languages, compilers, string searching, pattern\\nmatching, computer security, artiﬁcial intelligence, etc., etc.\\n4. This course helps you to learn problem solving skills. Theory teaches\\nyou how to think, prove, argue, solve problems, express, and abstract.\\n4\\nChapter 1.\\nIntroduction\\n5. This theory simpliﬁes the complex computers to an abstract and simple\\nmathematical model, and helps you to understand them better.\\n6. This course is about rigorously analyzing capabilities and limitations\\nof systems.\\nWhere does this course ﬁt in the Computer Science Curriculum at Car-\\nleton University? It is a theory course that is the third part in the series\\nCOMP 1805, COMP 2804, COMP 3803, COMP 3804, and COMP 4804.\\nThis course also widens your understanding of computers and will inﬂuence\\nother courses including Compilers, Programming Languages, and Artiﬁcial\\nIntelligence.\\n1.2\\nMathematical preliminaries\\nThroughout this course, we will assume that you know the following mathe-\\nmatical concepts:\\n1. A set is a collection of well-deﬁned objects. Examples are (i) the set of\\nall Dutch Olympic Gold Medallists, (ii) the set of all pubs in Ottawa,\\nand (iii) the set of all even natural numbers.\\n2. The set of natural numbers is N = {1, 2, 3, . . .}.\\n3. The set of integers is Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n4. The set of rational numbers is Q = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\n5. The set of real numbers is denoted by R.\\n6. If A and B are sets, then A is a subset of B, written as A ⊆B, if every\\nelement of A is also an element of B. For example, the set of even\\nnatural numbers is a subset of the set of all natural numbers. Every\\nset A is a subset of itself, i.e., A ⊆A. The empty set is a subset of\\nevery set A, i.e., ∅⊆A.\\n7. If B is a set, then the power set P(B) of B is deﬁned to be the set of\\nall subsets of B:\\nP(B) = {A : A ⊆B}.\\nObserve that ∅∈P(B) and B ∈P(B).\\n1.2.\\nMathematical preliminaries\\n5\\n8. If A and B are two sets, then\\n(a) their union is deﬁned as\\nA ∪B = {x : x ∈A or x ∈B},\\n(b) their intersection is deﬁned as\\nA ∩B = {x : x ∈A and x ∈B},\\n(c) their diﬀerence is deﬁned as\\nA \\\\ B = {x : x ∈A and x ̸∈B},\\n(d) the Cartesian product of A and B is deﬁned as\\nA × B = {(x, y) : x ∈A and y ∈B},\\n(e) the complement of A is deﬁned as\\nA = {x : x ̸∈A}.\\n9. A binary relation on two sets A and B is a subset of A × B.\\n10. A function f from A to B, denoted by f : A →B, is a binary relation\\nR, having the property that for each element a ∈A, there is exactly\\none ordered pair in R, whose ﬁrst component is a. We will also say\\nthat f(a) = b, or f maps a to b, or the image of a under f is b. The\\nset A is called the domain of f, and the set\\n{b ∈B : there is an a ∈A with f(a) = b}\\nis called the range of f.\\n11. A function f : A →B is one-to-one (or injective), if for any two distinct\\nelements a and a′ in A, we have f(a) ̸= f(a′). The function f is onto\\n(or surjective), if for each element b ∈B, there exists an element a ∈A,\\nsuch that f(a) = b; in other words, the range of f is equal to the set\\nB. A function f is a bijection, if f is both injective and surjective.\\n12. A binary relation R ⊆A × A is an equivalence relation, if it satisﬁes\\nthe following three conditions:\\n6\\nChapter 1.\\nIntroduction\\n(a) R is reﬂexive: For every element in a ∈A, we have (a, a) ∈R.\\n(b) R is symmetric: For all a and b in A, if (a, b) ∈R, then also\\n(b, a) ∈R.\\n(c) R is transitive: For all a, b, and c in A, if (a, b) ∈R and (b, c) ∈R,\\nthen also (a, c) ∈R.\\n13. A graph G = (V, E) is a pair consisting of a set V , whose elements are\\ncalled vertices, and a set E, where each element of E is a pair of distinct\\nvertices. The elements of E are called edges. The ﬁgure below shows\\nsome well-known graphs: K5 (the complete graph on ﬁve vertices), K3,3\\n(the complete bipartite graph on 2 × 3 = 6 vertices), and the Peterson\\ngraph.\\nK5\\nK3,3\\nPeterson graph\\nThe degree of a vertex v, denoted by deg(v), is deﬁned to be the number\\nof edges that are incident on v.\\nA path in a graph is a sequence of vertices that are connected by edges.\\nA path is a cycle, if it starts and ends at the same vertex. A simple\\npath is a path without any repeated vertices. A graph is connected, if\\nthere is a path between every pair of vertices.\\n14. In the context of strings, an alphabet is a ﬁnite set, whose elements\\nare called symbols. Examples of alphabets are Σ = {0, 1} and Σ =\\n{a, b, c, . . . , z}.\\n15. A string over an alphabet Σ is a ﬁnite sequence of symbols, where each\\nsymbol is an element of Σ. The length of a string w, denoted by |w|, is\\nthe number of symbols contained in w. The empty string, denoted by\\n1.3.\\nProof techniques\\n7\\nϵ, is the string having length zero. For example, if the alphabet Σ is\\nequal to {0, 1}, then 10, 1000, 0, 101, and ϵ are strings over Σ, having\\nlengths 2, 4, 1, 3, and 0, respectively.\\n16. A language is a set of strings.\\n17. The Boolean values are 1 and 0, that represent true and false, respec-\\ntively. The basic Boolean operations include\\n(a) negation (or NOT), represented by ¬,\\n(b) conjunction (or AND), represented by ∧,\\n(c) disjunction (or OR), represented by ∨,\\n(d) exclusive-or (or XOR), represented by ⊕,\\n(e) equivalence, represented by ↔or ⇔,\\n(f) implication, represented by →or ⇒.\\nThe following table explains the meanings of these operations.\\nNOT\\nAND\\nOR\\nXOR\\nequivalence\\nimplication\\n¬0 = 1\\n0 ∧0 = 0\\n0 ∨0 = 0\\n0 ⊕0 = 0\\n0 ↔0 = 1\\n0 →0 = 1\\n¬1 = 0\\n0 ∧1 = 0\\n0 ∨1 = 1\\n0 ⊕1 = 1\\n0 ↔1 = 0\\n0 →1 = 1\\n1 ∧0 = 0\\n1 ∨0 = 1\\n1 ⊕0 = 1\\n1 ↔0 = 0\\n1 →0 = 0\\n1 ∧1 = 1\\n1 ∨1 = 1\\n1 ⊕1 = 0\\n1 ↔1 = 1\\n1 →1 = 1\\n1.3\\nProof techniques\\nIn mathematics, a theorem is a statement that is true. A proof is a sequence\\nof mathematical statements that form an argument to show that a theorem is\\ntrue. The statements in the proof of a theorem include axioms (assumptions\\nabout the underlying mathematical structures), hypotheses of the theorem\\nto be proved, and previously proved theorems. The main question is “How\\ndo we go about proving theorems?” This question is similar to the question\\nof how to solve a given problem. Of course, the answer is that ﬁnding proofs,\\nor solving problems, is not easy; otherwise life would be dull! There is no\\nspeciﬁed way of coming up with a proof, but there are some generic strategies\\nthat could be of help. In this section, we review some of these strategies,\\nthat will be suﬃcient for this course. The best way to get a feeling of how\\nto come up with a proof is by solving a large number of problems. Here are\\n8\\nChapter 1.\\nIntroduction\\nsome useful tips. (You may take a look at the book How to Solve It, by G.\\nP´olya).\\n1. Read and completely understand the statement of the theorem to be\\nproved. Most often this is the hardest part.\\n2. Sometimes, theorems contain theorems inside them.\\nFor example,\\n“Property A if and only if property B”, requires showing two state-\\nments:\\n(a) If property A is true, then property B is true (A ⇒B).\\n(b) If property B is true, then property A is true (B ⇒A).\\nAnother example is the theorem “Set A equals set B.” To prove this,\\nwe need to prove that A ⊆B and B ⊆A. That is, we need to show\\nthat each element of set A is in set B, and that each element of set B\\nis in set A.\\n3. Try to work out a few simple cases of the theorem just to get a grip on\\nit (i.e., crack a few simple cases ﬁrst).\\n4. Try to write down the proof once you have it. This is to ensure the\\ncorrectness of your proof. Often, mistakes are found at the time of\\nwriting.\\n5. Finding proofs takes time, we do not come prewired to produce proofs.\\nBe patient, think, express and write clearly and try to be precise as\\nmuch as possible.\\nIn the next sections, we will go through some of the proof strategies.\\n1.3.1\\nDirect proofs\\nAs the name suggests, in a direct proof of a theorem, we just approach the\\ntheorem directly.\\nTheorem 1.3.1 If n is an odd positive integer, then n2 is odd as well.\\n1.3.\\nProof techniques\\n9\\nProof. An odd positive integer n can be written as n = 2k + 1, for some\\ninteger k ≥0. Then\\nn2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1.\\nSince 2(2k2 + 2k) is even, and “even plus one is odd”, we can conclude that\\nn2 is odd.\\nTheorem 1.3.2 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is an even integer, i.e.,\\nX\\nv∈V\\ndeg(v)\\nis even.\\nProof. If you do not see the meaning of this statement, then ﬁrst try it out\\nfor a few graphs. The reason why the statement holds is very simple: Each\\nedge contributes 2 to the summation (because an edge is incident on exactly\\ntwo distinct vertices).\\nActually, the proof above proves the following theorem.\\nTheorem 1.3.3 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2|E|.\\n1.3.2\\nConstructive proofs\\nThis technique not only shows the existence of a certain object, it actually\\ngives a method of creating it. Here is how a constructive proof looks like:\\nTheorem 1.3.4 There exists an object with property P.\\nProof. Here is the object: [. . .]\\nAnd here is the proof that the object satisﬁes property P: [. . .]\\nHere is an example of a constructive proof. A graph is called 3-regular, if\\neach vertex has degree three.\\n10\\nChapter 1.\\nIntroduction\\nTheorem 1.3.5 For every even integer n ≥4, there exists a 3-regular graph\\nwith n vertices.\\nProof. Deﬁne\\nV = {0, 1, 2, . . . , n −1},\\nand\\nE = {{i, i+1} : 0 ≤i ≤n−2}∪{{n−1, 0}}∪{{i, i+n/2} : 0 ≤i ≤n/2−1}.\\nThen the graph G = (V, E) is 3-regular.\\nConvince yourself that this graph is indeed 3-regular. It may help to draw\\nthe graph for, say, n = 8.\\n1.3.3\\nNonconstructive proofs\\nIn a nonconstructive proof, we show that a certain object exists, without\\nactually creating it. Here is an example of such a proof:\\nTheorem 1.3.6 There exist irrational numbers x and y such that xy is ra-\\ntional.\\nProof. There are two possible cases.\\nCase 1:\\n√\\n2\\n√\\n2 ∈Q.\\nIn this case, we take x = y =\\n√\\n2. In Theorem 1.3.9 below, we will prove\\nthat\\n√\\n2 is irrational.\\nCase 2:\\n√\\n2\\n√\\n2 ̸∈Q.\\nIn this case, we take x =\\n√\\n2\\n√\\n2 and y =\\n√\\n2. Since\\nxy =\\n\\x12√\\n2\\n√\\n2\\x13√\\n2\\n=\\n√\\n2\\n2 = 2,\\nthe claim in the theorem follows.\\nObserve that this proof indeed proves the theorem, but it does not give\\nan example of a pair of irrational numbers x and y such that xy is rational.\\n1.3.\\nProof techniques\\n11\\n1.3.4\\nProofs by contradiction\\nThis is how a proof by contradiction looks like:\\nTheorem 1.3.7 Statement S is true.\\nProof. Assume that statement S is false. Then, derive a contradiction (such\\nas 1 + 1 = 3).\\nIn other words, show that the statement “¬S ⇒false” is true. This is\\nsuﬃcient, because the contrapositive of the statement “¬S ⇒false” is the\\nstatement “true ⇒S”. The latter logical formula is equivalent to S, and\\nthat is what we wanted to show.\\nBelow, we give two examples of proofs by contradiction.\\nTheorem 1.3.8 Let n be a positive integer. If n2 is even, then n is even.\\nProof. We will prove the theorem by contradiction. So we assume that n2\\nis even, but n is odd. Since n is odd, we know from Theorem 1.3.1 that n2\\nis odd. This is a contradiction, because we assumed that n2 is even.\\nTheorem 1.3.9\\n√\\n2 is irrational, i.e.,\\n√\\n2 cannot be written as a fraction of\\ntwo integers m and n.\\nProof. We will prove the theorem by contradiction. So we assume that\\n√\\n2\\nis rational. Then\\n√\\n2 can be written as a fraction of two integers,\\n√\\n2 = m/n,\\nwhere m ≥1 and n ≥1. We may assume that m and n do not share any\\ncommon factors, i.e., the greatest common divisor of m and n is equal to\\none; if this is not the case, then we can get rid of the common factors. By\\nsquaring\\n√\\n2 = m/n, we get 2n2 = m2. This implies that m2 is even. Then,\\nby Theorem 1.3.8, m is even, which means that we can write m as m = 2k,\\nfor some positive integer k. It follows that 2n2 = m2 = 4k2, which implies\\nthat n2 = 2k2. Hence, n2 is even. Again by Theorem 1.3.8, it follows that n\\nis even.\\nWe have shown that m and n are both even. But we know that m and\\nn are not both even. Hence, we have a contradiction. Our assumption that\\n√\\n2 is rational is wrong. Thus, we can conclude that\\n√\\n2 is irrational.\\nThere is a nice discussion of this proof in the book My Brain is Open:\\nThe Mathematical Journeys of Paul Erd˝os by B. Schechter.\\n12\\nChapter 1.\\nIntroduction\\n1.3.5\\nThe pigeon hole principle\\nThis is a simple principle with surprising consequences.\\nPigeon Hole Principle: If n + 1 or more objects are placed into n\\nboxes, then there is at least one box containing two or more objects.\\nIn other words, if A and B are two sets such that |A| > |B|, then\\nthere is no one-to-one function from A to B.\\nTheorem 1.3.10 Let n be a positive integer. Every sequence of n2 + 1 dis-\\ntinct real numbers contains a subsequence of length n + 1 that is either in-\\ncreasing or decreasing.\\nProof. For example consider the sequence (20, 10, 9, 7, 11, 2, 21, 1, 20, 31) of\\n10 = 32 + 1 numbers. This sequence contains an increasing subsequence of\\nlength 4 = 3 + 1, namely (10, 11, 21, 31).\\nThe proof of this theorem is by contradiction, and uses the pigeon hole\\nprinciple.\\nLet (a1, a2, . . . , an2+1) be an arbitrary sequence of n2 + 1 distinct real\\nnumbers. For each i with 1 ≤i ≤n2 + 1, let inci denote the length of\\nthe longest increasing subsequence that starts at ai, and let deci denote the\\nlength of the longest decreasing subsequence that starts at ai.\\nUsing this notation, the claim in the theorem can be formulated as follows:\\nThere is an index i such that inci ≥n + 1 or deci ≥n + 1.\\nWe will prove the claim by contradiction. So we assume that inci ≤n\\nand deci ≤n for all i with 1 ≤i ≤n2 + 1.\\nConsider the set\\nB = {(b, c) : 1 ≤b ≤n, 1 ≤c ≤n},\\nand think of the elements of B as being boxes. For each i with 1 ≤i ≤n2+1,\\nthe pair (inci, deci) is an element of B. So we have n2+1 elements (inci, deci),\\nwhich are placed in the n2 boxes of B. By the pigeon hole principle, there\\nmust be a box that contains two (or more) elements. In other words, there\\nexist two integers i and j such that i < j and\\n(inci, deci) = (incj, decj).\\nRecall that the elements in the sequence are distinct. Hence, ai ̸= aj. We\\nconsider two cases.\\n1.3.\\nProof techniques\\n13\\nFirst assume that ai < aj. Then the length of the longest increasing\\nsubsequence starting at ai must be at least 1+incj, because we can append ai\\nto the longest increasing subsequence starting at aj. Therefore, inci ̸= incj,\\nwhich is a contradiction.\\nThe second case is when ai > aj. Then the length of the longest decreasing\\nsubsequence starting at ai must be at least 1+decj, because we can append ai\\nto the longest decreasing subsequence starting at aj. Therefore, deci ̸= decj,\\nwhich is again a contradiction.\\n1.3.6\\nProofs by induction\\nThis is a very powerful and important technique for proving theorems.\\nFor each positive integer n, let P(n) be a mathematical statement that\\ndepends on n. Assume we wish to prove that P(n) is true for all positive\\nintegers n. A proof by induction of such a statement is carried out as follows:\\nBasis: Prove that P(1) is true.\\nInduction step: Prove that for all n ≥1, the following holds: If P(n) is\\ntrue, then P(n + 1) is also true.\\nIn the induction step, we choose an arbitrary integer n ≥1 and assume\\nthat P(n) is true; this is called the induction hypothesis. Then we prove that\\nP(n + 1) is also true.\\nTheorem 1.3.11 For all positive integers n, we have\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\nProof. We start with the basis of the induction. If n = 1, then the left-hand\\nside is equal to 1, and so is the right-hand side. So the theorem is true for\\nn = 1.\\nFor the induction step, let n ≥1 and assume that the theorem is true for\\nn, i.e., assume that\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\n14\\nChapter 1.\\nIntroduction\\nWe have to prove that the theorem is true for n + 1, i.e., we have to prove\\nthat\\n1 + 2 + 3 + . . . + (n + 1) = (n + 1)(n + 2)\\n2\\n.\\nHere is the proof:\\n1 + 2 + 3 + . . . + (n + 1)\\n=\\n1 + 2 + 3 + . . . + n\\n|\\n{z\\n}\\n= n(n+1)\\n2\\n+(n + 1)\\n=\\nn(n + 1)\\n2\\n+ (n + 1)\\n=\\n(n + 1)(n + 2)\\n2\\n.\\nBy the way, here is an alternative proof of the theorem above: Let S =\\n1 + 2 + 3 + . . . + n. Then,\\nS\\n=\\n1\\n+\\n2\\n+\\n3\\n+\\n. . .\\n+\\n(n −2)\\n+\\n(n −1)\\n+\\nn\\nS\\n=\\nn\\n+\\n(n −1)\\n+\\n(n −2)\\n+\\n. . .\\n+\\n3\\n+\\n2\\n+\\n1\\n2S\\n=\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n. . .\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\nSince there are n terms on the right-hand side, we have 2S = n(n + 1). This\\nimplies that S = n(n + 1)/2.\\nTheorem 1.3.12 For every positive integer n, a −b is a factor of an −bn.\\nProof. A direct proof can be given by providing a factorization of an −bn:\\nan −bn = (a −b)(an−1 + an−2b + an−3b2 + . . . + abn−2 + bn−1).\\nWe now prove the theorem by induction. For the basis, let n = 1. The claim\\nin the theorem is “a −b is a factor of a −b”, which is obviously true.\\nLet n ≥1 and assume that a −b is a factor of an −bn. We have to prove\\nthat a −b is a factor of an+1 −bn+1. We have\\nan+1 −bn+1 = an+1 −anb + anb −bn+1 = an(a −b) + (an −bn)b.\\nThe ﬁrst term on the right-hand side is divisible by a −b. By the induction\\nhypothesis, the second term on the right-hand side is divisible by a −b as\\nwell. Therefore, the entire right-hand side is divisible by a −b. Since the\\nright-hand side is equal to an+1 −bn+1, it follows that a −b is a factor of\\nan+1 −bn+1.\\nWe now give an alternative proof of Theorem 1.3.3:\\n1.3.\\nProof techniques\\n15\\nTheorem 1.3.13 Let G = (V, E) be a graph with m edges. Then the sum\\nof the degrees of all vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2m.\\nProof. The proof is by induction on the number m of edges. For the basis of\\nthe induction, assume that m = 0. Then the graph G does not contain any\\nedges and, therefore, P\\nv∈V deg(v) = 0. Thus, the theorem is true if m = 0.\\nLet m ≥0 and assume that the theorem is true for every graph with m\\nedges. Let G be an arbitrary graph with m+1 edges. We have to prove that\\nP\\nv∈V deg(v) = 2(m + 1).\\nLet {a, b} be an arbitrary edge in G, and let G′ be the graph obtained\\nfrom G by removing the edge {a, b}. Since G′ has m edges, we know from\\nthe induction hypothesis that the sum of the degrees of all vertices in G′ is\\nequal to 2m. Using this, we obtain\\nX\\nv∈G\\ndeg(v) =\\nX\\nv∈G′\\ndeg(v) + 2 = 2m + 2 = 2(m + 1).\\n1.3.7\\nMore examples of proofs\\nRecall Theorem 1.3.5, which states that for every even integer n ≥4, there\\nexists a 3-regular graph with n vertices. The following theorem explains why\\nwe stated this theorem for even values of n.\\nTheorem 1.3.14 Let n ≥5 be an odd integer. There is no 3-regular graph\\nwith n vertices.\\nProof. The proof is by contradiction. So we assume that there exists a\\ngraph G = (V, E) with n vertices that is 3-regular. Let m be the number of\\nedges in G. Since deg(v) = 3 for every vertex, we have\\nX\\nv∈V\\ndeg(v) = 3n.\\nOn the other hand, by Theorem 1.3.3, we have\\nX\\nv∈V\\ndeg(v) = 2m.\\n16\\nChapter 1.\\nIntroduction\\nIt follows that 3n = 2m, which can be rewritten as m = 3n/2. Since m is an\\ninteger, and since gcd(2, 3) = 1, n/2 must be an integer. Hence, n is even,\\nwhich is a contradiction.\\nLet Kn be the complete graph on n vertices. This graph has a vertex set\\nof size n, and every pair of distinct vertices is joined by an edge.\\nIf G = (V, E) is a graph with n vertices, then the complement G of G is\\nthe graph with vertex set V that consists of those edges of Kn that are not\\npresent in G.\\nTheorem 1.3.15 Let n ≥2 and let G be a graph on n vertices. Then G is\\nconnected or G is connected.\\nProof. We prove the theorem by induction on the number n of vertices. For\\nthe basis, assume that n = 2. There are two possibilities for the graph G:\\n1. G contains one edge. In this case, G is connected.\\n2. G does not contain an edge. In this case, the complement G contains\\none edge and, therefore, G is connected.\\nSo for n = 2, the theorem is true.\\nLet n ≥2 and assume that the theorem is true for every graph with n\\nvertices. Let G be graph with n + 1 vertices. We have to prove that G is\\nconnected or G is connected. We consider three cases.\\nCase 1: There is a vertex v whose degree in G is equal to n.\\nSince G has n+1 vertices, v is connected by an edge to every other vertex\\nof G. Therefore, G is connected.\\nCase 2: There is a vertex v whose degree in G is equal to 0.\\nIn this case, the degree of v in the graph G is equal to n. Since G has n+1\\nvertices, v is connected by an edge to every other vertex of G. Therefore, G\\nis connected.\\nCase 3: For every vertex v, the degree of v in G is in {1, 2, . . . , n −1}.\\nLet v be an arbitrary vertex of G.\\nLet G′ be the graph obtained by\\ndeleting from G the vertex v, together with all edges that are incident on v.\\nSince G′ has n vertices, we know from the induction hypothesis that G′ is\\nconnected or G′ is connected.\\n1.3.\\nProof techniques\\n17\\nLet us ﬁrst assume that G′ is connected. Then the graph G is connected\\nas well, because there is at least one edge in G between v and some vertex\\nof G′.\\nIf G′ is not connected, then G′ must be connected. Since we are in Case 3,\\nwe know that the degree of v in G is in the set {1, 2, . . . , n −1}. It follows\\nthat the degree of v in the graph G is in this set as well. Hence, there is at\\nleast one edge in G between v and some vertex in G′. This implies that G is\\nconnected.\\nThe previous theorem can be rephrased as follows:\\nTheorem 1.3.16 Let n ≥2 and consider the complete graph Kn on n ver-\\ntices. Color each edge of this graph as either red or blue. Let R be the graph\\nconsisting of all the red edges, and let B be the graph consisting of all the\\nblue edges. Then R is connected or B is connected.\\nA graph is said to be planar, if it can be drawn (a better term is “embed-\\nded”) in the plane in such a way that no two edges intersect, except possibly\\nat their endpoints.\\nAn embedding of a planar graph consists of vertices,\\nedges, and faces. In the example below, there are 11 vertices, 18 edges, and\\n9 faces (including the unbounded face).\\nThe following theorem is known as Euler’s theorem for planar graphs.\\nApparently, this theorem was discovered by Euler around 1750. Legendre\\ngave the ﬁrst proof in 1794, see\\nhttp://www.ics.uci.edu/~eppstein/junkyard/euler/\\nTheorem 1.3.17 (Euler) Consider an embedding of a planar graph G. Let\\nv, e, and f be the number of vertices, edges, and faces (including the single\\n18\\nChapter 1.\\nIntroduction\\nunbounded face) of this embedding, respectively. Moreover, let c be the number\\nof connected components of G. Then\\nv −e + f = c + 1.\\nProof. The proof is by induction on the number of edges of G. To be more\\nprecise, we start with a graph having no edges, and prove that the theorem\\nholds for this case. Then, we add the edges one by one, and show that the\\nrelation v −e + f = c + 1 is maintained.\\nSo we ﬁrst assume that G has no edges, i.e., e = 0. Then the embedding\\nconsists of a collection of v points. In this case, we have f = 1 and c = v.\\nHence, the relation v −e + f = c + 1 holds.\\nLet e > 0 and assume that Euler’s formula holds for a subgraph of G\\nhaving e −1 edges. Let {u, v} be an edge of G that is not in the subgraph,\\nand add this edge to the subgraph. There are two cases depending on whether\\nthis new edge joins two connected components or joins two vertices in the\\nsame connected component.\\nCase 1: The new edge {u, v} joins two connected components.\\nIn this case, the number of vertices and the number of faces do not change,\\nthe number of connected components goes down by 1, and the number of\\nedges increases by 1. It follows that the relation in the theorem is still valid.\\nCase 2: The new edge {u, v} joins two vertices in the same connected com-\\nponent.\\nIn this case, the number of vertices and the number of connected com-\\nponents do not change, the number of edges increases by 1, and the number\\nof faces increases by 1 (because the new edge splits one face into two faces).\\nTherefore, the relation in the theorem is still valid.\\nEuler’s theorem is usually stated as follows:\\nTheorem 1.3.18 (Euler) Consider an embedding of a connected planar\\ngraph G. Let v, e, and f be the number of vertices, edges, and faces (in-\\ncluding the single unbounded face) of this embedding, respectively. Then\\nv −e + f = 2.\\nIf you like surprising proofs of various mathematical results, you should\\nread the book Proofs from THE BOOK by Aigner and Ziegler.\\nExercises\\n19\\nExercises\\n1.1 Use induction to prove that every integer n ≥2 can be written as a\\nproduct of prime numbers.\\n1.2 For every prime number p, prove that √p is irrational.\\n1.3 Let n be a positive integer that is not a perfect square. Prove that √n\\nis irrational.\\n1.4 Prove by induction that n4 −4n2 is divisible by 3, for all integers n ≥1.\\n1.5 Prove that\\nn\\nX\\ni=1\\n1\\ni2 < 2 −1/n,\\nfor every integer n ≥2.\\n1.6 Prove that 9 divides n3 + (n + 1)3 + (n + 2)3, for every integer n ≥0.\\n1.7 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers that are consecutive.\\n1.8 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers such that one divides the other.\\n20\\nChapter 1.\\nIntroduction\\nChapter 2\\nFinite Automata and Regular\\nLanguages\\nIn this chapter, we introduce and analyze the class of languages that are\\nknown as regular languages. Informally, these languages can be “processed”\\nby computers having a very small amount of memory.\\n2.1\\nAn example: Controling a toll gate\\nBefore we give a formal deﬁnition of a ﬁnite automaton, we consider an\\nexample in which such an automaton shows up in a natural way. We consider\\nthe problem of designing a “computer” that controls a toll gate.\\nWhen a car arrives at the toll gate, the gate is closed. The gate opens as\\nsoon as the driver has payed 25 cents. We assume that we have only three\\ncoin denominations: 5, 10, and 25 cents. We also assume that no excess\\nchange is returned.\\nAfter having arrived at the toll gate, the driver inserts a sequence of coins\\ninto the machine. At any moment, the machine has to decide whether or not\\nto open the gate, i.e., whether or not the driver has paid 25 cents (or more).\\nIn order to decide this, the machine is in one of the following six states, at\\nany moment during the process:\\n• The machine is in state q0, if it has not collected any money yet.\\n• The machine is in state q1, if it has collected exactly 5 cents.\\n• The machine is in state q2, if it has collected exactly 10 cents.\\n22\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The machine is in state q3, if it has collected exactly 15 cents.\\n• The machine is in state q4, if it has collected exactly 20 cents.\\n• The machine is in state q5, if it has collected 25 cents or more.\\nInitially (when a car arrives at the toll gate), the machine is in state q0.\\nAssume, for example, that the driver presents the sequence (10,5,5,10) of\\ncoins.\\n• After receiving the ﬁrst 10 cents coin, the machine switches from state\\nq0 to state q2.\\n• After receiving the ﬁrst 5 cents coin, the machine switches from state\\nq2 to state q3.\\n• After receiving the second 5 cents coin, the machine switches from state\\nq3 to state q4.\\n• After receiving the second 10 cents coin, the machine switches from\\nstate q4 to state q5. At this moment, the gate opens. (Remember that\\nno change is given.)\\nThe ﬁgure below represents the behavior of the machine for all possible\\nsequences of coins. State q5 is represented by two circles, because it is a\\nspecial state: As soon as the machine reaches this state, the gate opens.\\nq0\\nq1\\nq2\\nq3\\nq4\\nq5\\n5\\n5\\n5\\n5\\n10\\n10\\n10\\n25\\n25\\n25\\n10, 25\\n5, 10, 25\\n5, 10\\n25\\nstart\\nObserve that the machine (or computer) only has to remember which\\nstate it is in at any given time. Thus, it needs only a very small amount\\nof memory: It has to be able to distinguish between any one of six possible\\ncases and, therefore, it only needs a memory of ⌈log 6⌉= 3 bits.\\n2.2.\\nDeterministic ﬁnite automata\\n23\\n2.2\\nDeterministic ﬁnite automata\\nLet us look at another example. Consider the following state diagram:\\nq1\\nq2\\nq3\\n0\\n0\\n1\\n1\\n0,1\\nWe say that q1 is the start state and q2 is an accept state. Consider the\\ninput string 1101. This string is processed in the following way:\\n• Initially, the machine is in the start state q1.\\n• After having read the ﬁrst 1, the machine switches from state q1 to\\nstate q2.\\n• After having read the second 1, the machine switches from state q2 to\\nstate q2. (So actually, it does not switch.)\\n• After having read the ﬁrst 0, the machine switches from state q2 to\\nstate q3.\\n• After having read the third 1, the machine switches from state q3 to\\nstate q2.\\nAfter the entire string 1101 has been processed, the machine is in state q2,\\nwhich is an accept state. We say that the string 1101 is accepted by the\\nmachine.\\nConsider now the input string 0101010. After having read this string\\nfrom left to right (starting in the start state q1), the machine is in state q3.\\nSince q3 is not an accept state, we say that the machine rejects the string\\n0101010.\\nWe hope you are able to see that this machine accepts every binary string\\nthat ends with a 1. In fact, the machine accepts more strings:\\n• Every binary string having the property that there are an even number\\nof 0s following the rightmost 1, is accepted by this machine.\\n24\\nChapter 2.\\nFinite Automata and Regular Languages\\n• Every other binary string is rejected by the machine. Observe that each\\nsuch string is either empty, consists of 0s only, or has an odd number\\nof 0s following the rightmost 1.\\nWe now come to the formal deﬁnition of a ﬁnite automaton:\\nDeﬁnition 2.2.1 A ﬁnite automaton is a 5-tuple M = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σ →Q is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\nYou can think of the transition function δ as being the “program” of the\\nﬁnite automaton M = (Q, Σ, δ, q, F). This function tells us what M can do\\nin “one step”:\\n• Let r be a state of Q and let a be a symbol of the alphabet Σ. If\\nthe ﬁnite automaton M is in state r and reads the symbol a, then it\\nswitches from state r to state δ(r, a). (In fact, δ(r, a) may be equal to\\nr.)\\nThe “computer” that we designed in the toll gate example in Section 2.1\\nis a ﬁnite automaton. For this example, we have Q = {q0, q1, q2, q3, q4, q5},\\nΣ = {5, 10, 25}, the start state is q0, F = {q5}, and δ is given by the following\\ntable:\\n5\\n10\\n25\\nq0\\nq1\\nq2\\nq5\\nq1\\nq2\\nq3\\nq5\\nq2\\nq3\\nq4\\nq5\\nq3\\nq4\\nq5\\nq5\\nq4\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nThe example given in the beginning of this section is also a ﬁnite automa-\\nton. For this example, we have Q = {q1, q2, q3}, Σ = {0, 1}, the start state\\nis q1, F = {q2}, and δ is given by the following table:\\n2.2.\\nDeterministic ﬁnite automata\\n25\\n0\\n1\\nq1\\nq1\\nq2\\nq2\\nq3\\nq2\\nq3\\nq2\\nq2\\nLet us denote this ﬁnite automaton by M. The language of M, denoted\\nby L(M), is the set of all binary strings that are accepted by M. As we have\\nseen before, we have\\nL(M) = {w : w contains at least one 1 and ends with an even number of 0s}.\\nWe now give a formal deﬁnition of the language of a ﬁnite automaton:\\nDeﬁnition 2.2.2 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton and let w =\\nw1w2 . . . wn be a string over Σ. Deﬁne the sequence r0, r1, . . . , rn of states, in\\nthe following way:\\n• r0 = q,\\n• ri+1 = δ(ri, wi+1), for i = 0, 1, . . . , n −1.\\n1. If rn ∈F, then we say that M accepts w.\\n2. If rn ̸∈F, then we say that M rejects w.\\nIn this deﬁnition, w may be the empty string, which we denote by ϵ, and\\nwhose length is zero; thus in the deﬁnition above, n = 0. In this case, the\\nsequence r0, r1, . . . , rn of states has length one; it consists of just the state\\nr0 = q. The empty string is accepted by M if and only if the start state q\\nbelongs to F.\\nDeﬁnition 2.2.3 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton. The lan-\\nguage L(M) accepted by M is deﬁned to be the set of all strings that are\\naccepted by M:\\nL(M) = {w : w is a string over Σ and M accepts w }.\\nDeﬁnition 2.2.4 A language A is called regular, if there exists a ﬁnite au-\\ntomaton M such that A = L(M).\\n26\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe ﬁnish this section by presenting an equivalent way of deﬁning the\\nlanguage accepted by a ﬁnite automaton. Let M = (Q, Σ, δ, q, F) be a ﬁnite\\nautomaton. The transition function δ : Q × Σ →Q tells us that, when M\\nis in state r ∈Q and reads symbol a ∈Σ, it switches from state r to state\\nδ(r, a). Let Σ∗denote the set of all strings over the alphabet Σ. (Σ∗includes\\nthe empty string ϵ.) We extend the function δ to a function\\nδ : Q × Σ∗→Q,\\nthat is deﬁned as follows. For any state r ∈Q and for any string w over the\\nalphabet Σ,\\nδ(r, w) =\\n\\x1a r\\nif w = ϵ,\\nδ(δ(r, v), a)\\nif w = va, where v is a string and a ∈Σ.\\nWhat is the meaning of this function δ? Let r be a state of Q and let w be\\na string over the alphabet Σ. Then\\n• δ(r, w) is the state that M reaches, when it starts in state r, reads the\\nstring w from left to right, and uses δ to switch from state to state.\\nUsing this notation, we have\\nL(M) = {w : w is a string over Σ and δ(q, w) ∈F}.\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton\\nLet\\nA = {w : w is a binary string containing an odd number of 1s}.\\nWe claim that this language A is regular. In order to prove this, we have to\\nconstruct a ﬁnite automaton M such that A = L(M).\\nHow to construct M? Here is a ﬁrst idea: The ﬁnite automaton reads the\\ninput string w from left to right and keeps track of the number of 1s it has\\nseen. After having read the entire string w, it checks whether this number\\nis odd (in which case w is accepted) or even (in which case w is rejected).\\nUsing this approach, the ﬁnite automaton needs a state for every integer\\ni ≥0, indicating that the number of 1s read so far is equal to i. Hence,\\nto design a ﬁnite automaton that follows this approach, we need an inﬁnite\\n2.2.\\nDeterministic ﬁnite automata\\n27\\nnumber of states. But, the deﬁnition of ﬁnite automaton requires the number\\nof states to be ﬁnite.\\nA better, and correct approach, is to keep track of whether the number\\nof 1s read so far is even or odd. This leads to the following ﬁnite automaton:\\n• The set of states is Q = {qe, qo}. If the ﬁnite automaton is in state qe,\\nthen it has read an even number of 1s; if it is in state qo, then it has\\nread an odd number of 1s.\\n• The alphabet is Σ = {0, 1}.\\n• The start state is qe, because at the start, the number of 1s read by the\\nautomaton is equal to 0, and 0 is even.\\n• The set F of accept states is F = {qo}.\\n• The transition function δ is given by the following table:\\n0\\n1\\nqe\\nqe\\nqo\\nqo\\nqo\\nqe\\nThis ﬁnite automaton M = (Q, Σ, δ, qe, F) can also be described by its state\\ndiagram, which is given in the ﬁgure below. The arrow that comes “out of\\nthe blue” and enters the state qe, indicates that qe is the start state. The\\nstate depicted with double circles indicates the accept state.\\nqe\\nqo\\n0\\n0\\n1\\n1\\nWe have constructed a ﬁnite automaton M that accepts the language A.\\nTherefore, A is a regular language.\\n28\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.2.2\\nA second example of a ﬁnite automaton\\nDeﬁne the language A as\\nA = {w : w is a binary string containing 101 as a substring}.\\nAgain, we claim that A is a regular language. In other words, we claim that\\nthere exists a ﬁnite automaton M that accepts A, i.e., A = L(M).\\nThe ﬁnite automaton M will do the following, when reading an input\\nstring from left to right:\\n• It skips over all 0s, and stays in the start state.\\n• At the ﬁrst 1, it switches to the state “maybe the next two symbols are\\n01”.\\n– If the next symbol is 1, then it stays in the state “maybe the next\\ntwo symbols are 01”.\\n– On the other hand, if the next symbol is 0, then it switches to the\\nstate “maybe the next symbol is 1”.\\n∗If the next symbol is indeed 1, then it switches to the accept\\nstate (but keeps on reading until the end of the string).\\n∗On the other hand, if the next symbol is 0, then it switches\\nto the start state, and skips 0s until it reads 1 again.\\nBy deﬁning the following four states, this process will become clear:\\n• q1: M is in this state if the last symbol read was 1, but the substring\\n101 has not been read.\\n• q10: M is in this state if the last two symbols read were 10, but the\\nsubstring 101 has not been read.\\n• q101: M is in this state if the substring 101 has been read in the input\\nstring.\\n• q: In all other cases, M is in this state.\\nHere is the formal description of the ﬁnite automaton that accepts the\\nlanguage A:\\n• Q = {q, q1, q10, q101},\\n2.2.\\nDeterministic ﬁnite automata\\n29\\n• Σ = {0, 1},\\n• the start state is q,\\n• the set F of accept states is equal to F = {q101}, and\\n• the transition function δ is given by the following table:\\n0\\n1\\nq\\nq\\nq1\\nq1\\nq10\\nq1\\nq10\\nq\\nq101\\nq101\\nq101\\nq101\\nThe ﬁgure below gives the state diagram of the ﬁnite automaton M =\\n(Q, Σ, δ, q, F).\\nq\\nq1\\nq10\\nq101\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nThis ﬁnite automaton accepts the language A consisting of all binary\\nstrings that contain the substring 101. As an exercise, how would you obtain\\na ﬁnite automaton that accepts the complement of A, i.e., the language\\nconsisting of all binary strings that do not contain the substring 101?\\n2.2.3\\nA third example of a ﬁnite automaton\\nThe ﬁnite automata we have seen so far have exactly one accept state. In\\nthis section, we will see an example of a ﬁnite automaton having more accept\\nstates.\\n30\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right},\\nwhere {0, 1}∗is the set of all binary strings, including the empty string ϵ. We\\nclaim that A is a regular language. To prove this, we have to construct a ﬁnite\\nautomaton M such that A = L(M). At ﬁrst sight, it seems diﬃcult (or even\\nimpossible?) to construct such a ﬁnite automaton: How does the automaton\\n“know” that it has reached the third symbol from the right? It is, however,\\npossible to construct such an automaton. The main idea is to remember the\\nlast three symbols that have been read. Thus, the ﬁnite automaton has eight\\nstates qijk, where i, j, and k range over the two elements of {0, 1}. If the\\nautomaton is in state qijk, then the following hold:\\n• If M has read at least three symbols, then the three most recently read\\nsymbols are ijk.\\n• If M has read only two symbols, then these two symbols are jk; more-\\nover, i = 0.\\n• If M has read only one symbol, then this symbol is k; moreover, i =\\nj = 0.\\n• If M has not read any symbol, then i = j = k = 0.\\nThe start state is q000 and the set of accept states is {q100, q110, q101, q111}.\\nThe transition function of M is given by the following state diagram.\\nq000\\nq100\\nq010\\nq110\\nq001\\nq101\\nq011\\nq111\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n2.3.\\nRegular operations\\n31\\n2.3\\nRegular operations\\nIn this section, we deﬁne three operations on languages. Later, we will answer\\nthe question whether the set of all regular languages is closed under these\\noperations. Let A and B be two languages over the same alphabet.\\n1. The union of A and B is deﬁned as\\nA ∪B = {w : w ∈A or w ∈B}.\\n2. The concatenation of A and B is deﬁned as\\nAB = {ww′ : w ∈A and w′ ∈B}.\\nIn words, AB is the set of all strings obtained by taking an arbitrary\\nstring w in A and an arbitrary string w′ in B, and gluing them together\\n(such that w is to the left of w′).\\n3. The star of A is deﬁned as\\nA∗= {u1u2 . . . uk : k ≥0 and ui ∈A for all i = 1, 2, . . . , k}.\\nIn words, A∗is obtained by taking any ﬁnite number of strings in A, and\\ngluing them together. Observe that k = 0 is allowed; this corresponds\\nto the empty string ϵ. Thus, ϵ ∈A∗.\\nTo give an example, let A = {0, 01} and B = {1, 10}. Then\\nA ∪B = {0, 01, 1, 10},\\nAB = {01, 010, 011, 0110},\\nand\\nA∗= {ϵ, 0, 01, 00, 001, 010, 0101, 000, 0001, 00101, . . .}.\\nAs another example, if Σ = {0, 1}, then Σ∗is the set of all binary strings\\n(including the empty string). Observe that a string always has a ﬁnite length.\\nBefore we proceed, we give an alternative (and equivalent) deﬁnition of\\nthe star of the language A: Deﬁne\\nA0 = {ϵ}\\n32\\nChapter 2.\\nFinite Automata and Regular Languages\\nand, for k ≥1,\\nAk = AAk−1,\\ni.e., Ak is the concatenation of the two languages A and Ak−1. Then we have\\nA∗=\\n∞\\n[\\nk=0\\nAk.\\nTheorem 2.3.1 The set of regular languages is closed under the union op-\\neration, i.e., if A and B are regular languages over the same alphabet Σ, then\\nA ∪B is also a regular language.\\nProof.\\nSince A and B are regular languages, there are ﬁnite automata\\nM1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2) that accept A and B,\\nrespectively. In order to prove that A ∪B is regular, we have to construct a\\nﬁnite automaton M that accepts A ∪B. In other words, M must have the\\nproperty that for every string w ∈Σ∗,\\nM accepts w ⇔M1 accepts w or M2 accepts w.\\nAs a ﬁrst idea, we may think that M could do the following:\\n• Starting in the start state q1 of M1, M “runs” M1 on w.\\n• If, after having read w, M1 is in a state of F1, then w ∈A, thus\\nw ∈A ∪B and, therefore, M accepts w.\\n• On the other hand, if, after having read w, M1 is in a state that is not\\nin F1, then w ̸∈A and M “runs” M2 on w, starting in the start state\\nq2 of M2. If, after having read w, M2 is in a state of F2, then we know\\nthat w ∈B, thus w ∈A ∪B and, therefore, M accepts w. Otherwise,\\nwe know that w ̸∈A ∪B, and M rejects w.\\nThis idea does not work, because the ﬁnite automaton M can read the input\\nstring w only once. The correct approach is to run M1 and M2 simulta-\\nneously. We deﬁne the set Q of states of M to be the Cartesian product\\nQ1 × Q2. If M is in state (r1, r2), this means that\\n• if M1 would have read the input string up to this point, then it would\\nbe in state r1, and\\n2.3.\\nRegular operations\\n33\\n• if M2 would have read the input string up to this point, then it would\\nbe in state r2.\\nThis leads to the ﬁnite automaton M = (Q, Σ, δ, q, F), where\\n• Q = Q1 × Q2 = {(r1, r2) : r1 ∈Q1 and r2 ∈Q2}.\\nObserve that\\n|Q| = |Q1| × |Q2|, which is ﬁnite.\\n• Σ is the alphabet of A and B (recall that we assume that A and B are\\nlanguages over the same alphabet).\\n• The start state q of M is equal to q = (q1, q2).\\n• The set F of accept states of M is given by\\nF = {(r1, r2) : r1 ∈F1 or r2 ∈F2} = (F1 × Q2) ∪(Q1 × F2).\\n• The transition function δ : Q × Σ →Q is given by\\nδ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)),\\nfor all r1 ∈Q1, r2 ∈Q2, and a ∈Σ.\\nTo ﬁnish the proof, we have to show that this ﬁnite automaton M indeed\\naccepts the language A∪B. Intuitively, this should be clear from the discus-\\nsion above. The easiest way to give a formal proof is by using the extended\\ntransition functions δ1 and δ2. (The extended transition function has been\\ndeﬁned after Deﬁnition 2.2.4.) Here we go: Recall that we have to prove that\\nM accepts w ⇔M1 accepts w or M2 accepts w,\\ni.e,\\nM accepts w ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\nIn terms of the extended transition function δ of the transition function δ of\\nM, this becomes\\nδ((q1, q2), w) ∈F ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\n(2.1)\\nBy applying the deﬁnition of the extended transition function, as given after\\nDeﬁnition 2.2.4, to δ, it can be seen that\\nδ((q1, q2), w) = (δ1(q1, w), δ2(q2, w)).\\n34\\nChapter 2.\\nFinite Automata and Regular Languages\\nThe latter equality implies that (2.1) is true and, therefore, M indeed accepts\\nthe language A ∪B.\\nWhat about the closure of the regular languages under the concatenation\\nand star operations? It turns out that the regular languages are closed under\\nthese operations. But how do we prove this?\\nLet A and B be two regular languages, and let M1 and M2 be ﬁnite\\nautomata that accept A and B, respectively. How do we construct a ﬁnite\\nautomaton M that accepts the concatenation AB? Given an input string\\nu, M has to decide whether or not u can be broken into two strings w and\\nw′ (i.e., write u as u = ww′), such that w ∈A and w′ ∈B. In words, M\\nhas to decide whether or not u can be broken into two substrings, such that\\nthe ﬁrst substring is accepted by M1 and the second substring is accepted by\\nM2. The diﬃculty is caused by the fact that M has to make this decision by\\nscanning the string u only once. If u ∈AB, then M has to decide, during\\nthis single scan, where to break u into two substrings. Similarly, if u ̸∈AB,\\nthen M has to decide, during this single scan, that u cannot be broken into\\ntwo substrings such that the ﬁrst substring is in A and the second substring\\nis in B.\\nIt seems to be even more diﬃcult to prove that A∗is a regular language,\\nif A itself is regular. In order to prove this, we need a ﬁnite automaton that,\\nwhen given an arbitrary input string u, decides whether or not u can be\\nbroken into substrings such that each substring is in A. The problem is that,\\nif u ∈A∗, the ﬁnite automaton has to determine into how many substrings,\\nand where, the string u has to be broken; it has to do this during one single\\nscan of the string u.\\nAs we mentioned already, if A and B are regular languages, then both\\nAB and A∗are also regular. In order to prove these claims, we will introduce\\na more general type of ﬁnite automaton.\\nThe ﬁnite automata that we have seen so far are deterministic.\\nThis\\nmeans the following:\\n• If the ﬁnite automaton M is in state r and if it reads the symbol a,\\nthen M switches from state r to the uniquely deﬁned state δ(r, a).\\nFrom now on, we will call such a ﬁnite automaton a deterministic ﬁnite\\nautomaton (DFA). In the next section, we will deﬁne the notion of a nonde-\\nterministic ﬁnite automaton (NFA). For such an automaton, there are zero\\nor more possible states to switch to. At ﬁrst sight, nondeterministic ﬁnite\\n2.4.\\nNondeterministic ﬁnite automata\\n35\\nautomata seem to be more powerful than their deterministic counterparts.\\nWe will prove, however, that DFAs have the same power as NFAs. As we will\\nsee, using this fact, it will be easy to prove that the class of regular languages\\nis closed under the concatenation and star operations.\\n2.4\\nNondeterministic ﬁnite automata\\nWe start by giving three examples of nondeterministic ﬁnite automata. These\\nexamples will show the diﬀerence between this type of automata and the\\ndeterministic versions that we have considered in the previous sections. After\\nthese examples, we will give a formal deﬁnition of a nondeterministic ﬁnite\\nautomaton.\\n2.4.1\\nA ﬁrst example\\nConsider the following state diagram:\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,ε\\n1\\n0,1\\nYou will notice three diﬀerences with the ﬁnite automata that we have\\nseen until now. First, if the automaton is in state q1 and reads the symbol 1,\\nthen it has two options: Either it stays in state q1, or it switches to state q2.\\nSecond, if the automaton is in state q2, then it can switch to state q3 without\\nreading a symbol; this is indicated by the edge having the empty string ϵ as\\nlabel. Third, if the automaton is in state q3 and reads the symbol 0, then it\\ncannot continue.\\nLet us see what this automaton can do when it gets the string 010110 as\\ninput. Initially, the automaton is in the start state q1.\\n• Since the ﬁrst symbol in the input string is 0, the automaton stays in\\nstate q1 after having read this symbol.\\n• The second symbol is 1, and the automaton can either stay in state q1\\nor switch to state q2.\\n36\\nChapter 2.\\nFinite Automata and Regular Languages\\n– If the automaton stays in state q1, then it is still in this state after\\nhaving read the third symbol.\\n– If the automaton switches to state q2, then it again has two op-\\ntions:\\n∗Either read the third symbol in the input string, which is 0,\\nand switch to state q3,\\n∗or switch to state q3, without reading the third symbol.\\nIf we continue in this way, then we see that, for the input string 010110,\\nthere are seven possible computations. All these computations are given in\\nthe ﬁgure below.\\nq1\\nq1\\n0\\n1\\nq1\\nq1\\n0\\n1\\n1\\nq1\\nq2\\n1\\n1\\nq1\\nq2\\nq1\\n0\\n0\\nε\\nq3\\nq3\\nhang\\nhang\\nε\\nq3\\nq4\\n1\\n0\\nq4\\n1\\nq2\\n0\\nε\\nq3\\nq3\\nhang\\n1\\nq4\\n1\\nq4\\nq4\\n0\\nConsider the lowest path in the ﬁgure above:\\n• When reading the ﬁrst symbol, the automaton stays in state q1.\\n• When reading the second symbol, the automaton switches to state q2.\\n• The automaton does not read the third symbol (equivalently, it “reads”\\nthe empty string ϵ), and switches to state q3. At this moment, the\\n2.4.\\nNondeterministic ﬁnite automata\\n37\\nautomaton cannot continue: The third symbol is 0, but there is no\\nedge leaving q3 that is labeled 0, and there is no edge leaving q3 that\\nis labeled ϵ. Therefore, the computation hangs at this point.\\nFrom the ﬁgure, you can see that, out of the seven possible computations,\\nexactly two end in the accept state q4 (after the entire input string 010110 has\\nbeen read). We say that the automaton accepts the string 010110, because\\nthere is at least one computation that ends in the accept state.\\nNow consider the input string 010. In this case, there are three possible\\ncomputations:\\n1. q1\\n0→q1\\n1→q1\\n0→q1\\n2. q1\\n0→q1\\n1→q2\\n0→q3\\n3. q1\\n0→q1\\n1→q2\\nϵ→q3 →hang\\nNone of these computations ends in the accept state (after the entire input\\nstring 010 has been read). Therefore, we say that the automaton rejects the\\ninput string 010.\\nThe state diagram given above is an example of a nondeterministic ﬁnite\\nautomaton (NFA). Informally, an NFA accepts a string, if there exists at least\\none path in the state diagram that (i) starts in the start state, (ii) does not\\nhang before the entire string has been read, and (iii) ends in an accept state.\\nA string for which (i), (ii), and (iii) does not hold is rejected by the NFA.\\nThe NFA given above accepts all binary strings that contain 101 or 11 as\\na substring. All other binary strings are rejected.\\n2.4.2\\nA second example\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right}.\\nThe following state diagram deﬁnes an NFA that accepts all strings that are\\nin A, and rejects all strings that are not in A.\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,1\\n0,1\\n38\\nChapter 2.\\nFinite Automata and Regular Languages\\nThis NFA does the following. If it is in the start state q1 and reads the\\nsymbol 1, then it either stays in state q1 or it “guesses” that this symbol\\nis the third symbol from the right in the input string. In the latter case,\\nthe NFA switches to state q2, and then it “veriﬁes” that there are indeed\\nexactly two remaining symbols in the input string. If there are more than\\ntwo remaining symbols, then the NFA hangs (in state q4) after having read\\nthe next two symbols.\\nObserve how this guessing mechanism is used: The automaton can only\\nread the input string once, from left to right. Hence, it does not know when\\nit reaches the third symbol from the right. When the NFA reads a 1, it can\\nguess that this is the third symbol from the right; after having made this\\nguess, it veriﬁes whether or not the guess was correct.\\nIn Section 2.2.3, we have seen a DFA for the same language A. Observe\\nthat the NFA has a much simpler structure than the DFA.\\n2.4.3\\nA third example\\nConsider the following state diagram, which deﬁnes an NFA whose alphabet\\nis {0}.\\nε\\nε\\n0\\n0\\n0\\n0\\n0\\nThis NFA accepts the language\\nA = {0k : k ≡0 mod 2 or k ≡0 mod 3},\\nwhere 0k is the string consisting of k many 0s. (If k = 0, then 0k = ϵ.)\\nObserve that A is the union of the two languages\\nA1 = {0k : k ≡0 mod 2}\\n2.4.\\nNondeterministic ﬁnite automata\\n39\\nand\\nA2 = {0k : k ≡0 mod 3}.\\nThe NFA basically consists of two DFAs: one of these accepts A1, whereas the\\nother accepts A2. Given an input string w, the NFA has to decide whether\\nor not w ∈A, which is equivalent to deciding whether or not w ∈A1 or\\nw ∈A2. The NFA makes this decision in the following way: At the start, it\\n“guesses” whether (i) it is going to check whether or not w ∈A1 (i.e., the\\nlength of w is even), or (ii) it is going to check whether or not w ∈A2 (i.e.,\\nthe length of w is a multiple of 3). After having made the guess, it veriﬁes\\nwhether or not the guess was correct. If w ∈A, then there exists a way of\\nmaking the correct guess and verifying that w is indeed an element of A (by\\nending in an accept state). If w ̸∈A, then no matter which guess is made,\\nthe NFA will never end in an accept state.\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\nThe previous examples give you an idea what nondeterministic ﬁnite au-\\ntomata are and how they work. In this section, we give a formal deﬁnition\\nof these automata.\\nFor any alphabet Σ, we deﬁne Σϵ to be the set\\nΣϵ = Σ ∪{ϵ}.\\nRecall the notion of a power set: For any set Q, the power set of Q, denoted\\nby P(Q), is the set of all subsets of Q, i.e.,\\nP(Q) = {R : R ⊆Q}.\\nDeﬁnition 2.4.1 A nondeterministic ﬁnite automaton (NFA) is a 5-tuple\\nM = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σϵ →P(Q) is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\n40\\nChapter 2.\\nFinite Automata and Regular Languages\\nAs for DFAs, the transition function δ can be thought of as the “program”\\nof the ﬁnite automaton M = (Q, Σ, δ, q, F):\\n• Let r ∈Q, and let a ∈Σϵ. Then δ(r, a) is a (possibly empty) subset of\\nQ. If the NFA M is in state r, and if it reads a (where a may be the\\nempty string ϵ), then M can switch from state r to any state in δ(r, a).\\nIf δ(r, a) = ∅, then M cannot continue and the computation hangs.\\nThe example given in Section 2.4.1 is an NFA, where Q = {q1, q2, q3, q4},\\nΣ = {0, 1}, the start state is q1, the set of accept states is F = {q4}, and the\\ntransition function δ is given by the following table:\\n0\\n1\\nϵ\\nq1\\n{q1}\\n{q1, q2}\\n∅\\nq2\\n{q3}\\n∅\\n{q3}\\nq3\\n∅\\n{q4}\\n∅\\nq4\\n{q4}\\n{q4}\\n∅\\nDeﬁnition 2.4.2 Let M = (Q, Σ, δ, q, F) be an NFA, and let w ∈Σ∗. We\\nsay that M accepts w, if1\\n• w = ϵ and the start state q is an accept state, or\\n• there exists an integer m ≥1, such that w can be written as w =\\ny1y2 . . . ym, where yi ∈Σϵ for all i with 1 ≤i ≤m, and there exists a\\nsequence r0, r1, . . . , rm of states in Q, such that\\n– r0 = q,\\n– ri+1 ∈δ(ri, yi+1), for i = 0, 1, . . . , m −1, and\\n– rm ∈F.\\nOtherwise, we say that M rejects the string w.\\nThe NFA in the example in Section 2.4.1 accepts the string 01100. This\\ncan be seen by taking\\n• m = 6,\\n1Thanks to Antoine Vigneron for pointing out an error in a previous version of this\\ndeﬁnition.\\n2.5.\\nEquivalence of DFAs and NFAs\\n41\\n• w = 01ϵ100 = y1y2y3y4y5y6, and\\n• r0 = q1, r1 = q1, r2 = q2, r3 = q3, r4 = q4, r5 = q4, and r6 = q4.\\nDeﬁnition 2.4.3 Let M = (Q, Σ, δ, q, F) be an NFA. The language L(M)\\naccepted by M is deﬁned as\\nL(M) = {w ∈Σ∗: M accepts w }.\\n2.5\\nEquivalence of DFAs and NFAs\\nYou may have the impression that nondeterministic ﬁnite automata are more\\npowerful than deterministic ﬁnite automata. In this section, we will show\\nthat this is not the case.\\nThat is, we will prove that a language can be\\naccepted by a DFA if and only if it can be accepted by an NFA. In order\\nto prove this, we will show how to convert an arbitrary NFA to a DFA that\\naccepts the same language.\\nWhat about converting a DFA to an NFA? Well, there is (almost) nothing\\nto do, because a DFA is also an NFA. This is not quite true, because\\n• the transition function of a DFA maps a state and a symbol to a state,\\nwhereas\\n• the transition function of an NFA maps a state and a symbol to a set\\nof zero or more states.\\nThe formal conversion of a DFA to an NFA is done as follows: Let M =\\n(Q, Σ, δ, q, F) be a DFA. Recall that δ is a function δ : Q × Σ →Q. We\\ndeﬁne the function δ′ : Q × Σϵ →P(Q) as follows. For any r ∈Q and for\\nany a ∈Σϵ,\\nδ′(r, a) =\\n\\x1a {δ(r, a)}\\nif a ̸= ϵ,\\n∅\\nif a = ϵ.\\nThen N = (Q, Σ, δ′, q, F) is an NFA, whose behavior is exactly the same as\\nthat of the DFA M; the easiest way to see this is by observing that the state\\ndiagrams of M and N are equal. Therefore, we have L(M) = L(N).\\nIn the rest of this section, we will show how to convert an NFA to a DFA:\\nTheorem 2.5.1 Let N = (Q, Σ, δ, q, F) be a nondeterministic ﬁnite automa-\\nton. There exists a deterministic ﬁnite automaton M, such that L(M) =\\nL(N).\\n42\\nChapter 2.\\nFinite Automata and Regular Languages\\nProof.\\nRecall that the NFA N can (in general) perform more than one\\ncomputation on a given input string. The idea of the proof is to construct a\\nDFA M that runs all these diﬀerent computations simultaneously. (We have\\nseen this idea already in the proof of Theorem 2.3.1.) To be more precise,\\nthe DFA M will have the following property:\\n• the state that M is in after having read an initial part of the input\\nstring corresponds exactly to the set of all states that N can reach\\nafter having read the same part of the input string.\\nWe start by presenting the conversion for the case when N does not\\ncontain ϵ-transitions. In other words, the state diagram of N does not contain\\nany edge that has ϵ as a label. (Later, we will extend the conversion to the\\ngeneral case.) Let the DFA M be deﬁned as M = (Q′, Σ, δ′, q′, F ′), where\\n• the set Q′ of states is equal to Q′ = P(Q); observe that |Q′| = 2|Q|,\\n• the start state q′ is equal to q′ = {q}; so M has the “same” start state\\nas N,\\n• the set F ′ of accept states is equal to the set of all elements R of Q′\\nhaving the property that R contains at least one accept state of N, i.e.,\\nF ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• the transition function δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each\\nR ∈Q′ and for each a ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nδ(r, a).\\nLet us see what the transition function δ′ of M does. First observe that,\\nsince N is an NFA, δ(r, a) is a subset of Q. This implies that δ′(R, a) is the\\nunion of subsets of Q and, therefore, also a subset of Q. Hence, δ′(R, a) is\\nan element of Q′.\\nThe set δ(r, a) is equal to the set of all states of the NFA N that can be\\nreached from state r by reading the symbol a. We take the union of these\\nsets δ(r, a), where r ranges over all elements of R, to obtain the new set\\nδ′(R, a). This new set is the state that the DFA M reaches from state R, by\\nreading the symbol a.\\n2.5.\\nEquivalence of DFAs and NFAs\\n43\\nIn this way, we obtain the correspondence that was given in the beginning\\nof this proof.\\nAfter this warming-up, we can consider the general case. In other words,\\nfrom now on, we allow ϵ-transitions in the NFA N. The DFA M is deﬁned as\\nabove, except that the start state q′ and the transition function δ′ have to be\\nmodiﬁed. Recall that a computation of the NFA N consists of the following:\\n1. Start in the start state q and make zero or more ϵ-transitions.\\n2. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n3. Make zero or more ϵ-transitions.\\n4. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n5. Make zero or more ϵ-transitions.\\n6. Etc.\\nThe DFA M will simulate this computation in the following way:\\n• Simulate 1. in one single step. As we will see below, this simulation is\\nimplicitly encoded in the deﬁnition of the start state q′ of M.\\n• Simulate 2. and 3. in one single step.\\n• Simulate 4. and 5. in one single step.\\n• Etc.\\nThus, in one step, the DFA M simulates the reading of one “real” symbol of\\nΣ, followed by making zero or more ϵ-transitions.\\nTo formalize this, we need the notion of ϵ-closure. For any state r of the\\nNFA N, the ϵ-closure of r, denoted by Cϵ(r), is deﬁned to be the set of all\\nstates of N that can be reached from r, by making zero or more ϵ-transitions.\\nFor any state R of the DFA M (hence, R ⊆Q), we deﬁne\\nCϵ(R) =\\n[\\nr∈R\\nCϵ(r).\\n44\\nChapter 2.\\nFinite Automata and Regular Languages\\nHow do we deﬁne the start state q′ of the DFA M? Before the NFA N\\nreads its ﬁrst “real” symbol of Σ, it makes zero or more ϵ-transitions. In\\nother words, at the moment when N reads the ﬁrst symbol of Σ, it can be\\nin any state of Cϵ(q). Therefore, we deﬁne q′ to be\\nq′ = Cϵ(q) = Cϵ({q}).\\nHow do we deﬁne the transition function δ′ of the DFA M? Assume that\\nM is in state R, and reads the symbol a. At this moment, the NFA N would\\nhave been in any state r of R. By reading the symbol a, N can switch to\\nany state in δ(r, a), and then make zero or more ϵ-transitions. Hence, the\\nNFA can switch to any state in the set Cϵ(δ(r, a)). Based on this, we deﬁne\\nδ′(R, a) to be\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nTo summarize, the NFA N = (Q, Σ, δ, q, F) is converted to the DFA\\nM = (Q′, Σ, δ′, q′, F ′), where\\n• Q′ = P(Q),\\n• q′ = Cϵ({q}),\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nThe results proved until now can be summarized in the following theorem.\\nTheorem 2.5.2 Let A be a language. Then A is regular if and only if there\\nexists a nondeterministic ﬁnite automaton that accepts A.\\n2.5.1\\nAn example\\nConsider the NFA N = (Q, Σ, δ, q, F), where Q = {1, 2, 3}, Σ = {a, b}, q = 1,\\nF = {2}, and δ is given by the following table:\\n2.5.\\nEquivalence of DFAs and NFAs\\n45\\na\\nb\\nϵ\\n1\\n{3}\\n∅\\n{2}\\n2\\n{1}\\n∅\\n∅\\n3\\n{2}\\n{2, 3}\\n∅\\nThe state diagram of N is as follows:\\n1\\n2\\n3\\na\\na\\nǫ\\nb\\na, b\\nWe will show how to convert this NFA N to a DFA M that accepts the\\nsame language. Following the proof of Theorem 2.5.1, the DFA M is speciﬁed\\nby M = (Q′, Σ, δ′, q′, F ′), where each of the components is deﬁned below.\\n• Q′ = P(Q). Hence,\\nQ′ = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}.\\n• q′ = Cϵ({q}). Hence, the start state q′ of M is the set of all states of\\nN that can be reached from N’s start state q = 1, by making zero or\\nmore ϵ-transitions. We obtain\\nq′ = Cϵ({q}) = Cϵ({1}) = {1, 2}.\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅}. Hence, the accept states of M are those\\nstates that contain the accept state 2 of N. We obtain\\nF ′ = {{2}, {1, 2}, {2, 3}, {1, 2, 3}}.\\n46\\nChapter 2.\\nFinite Automata and Regular Languages\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nIn this example δ′ is given by\\nδ′(∅, a) = ∅\\nδ′(∅, b) = ∅\\nδ′({1}, a) = {3}\\nδ′({1}, b) = ∅\\nδ′({2}, a) = {1, 2}\\nδ′({2}, b) = ∅\\nδ′({3}, a) = {2}\\nδ′({3}, b) = {2, 3}\\nδ′({1, 2}, a) = {1, 2, 3}\\nδ′({1, 2}, b) = ∅\\nδ′({1, 3}, a) = {2, 3}\\nδ′({1, 3}, b) = {2, 3}\\nδ′({2, 3}, a) = {1, 2}\\nδ′({2, 3}, b) = {2, 3}\\nδ′({1, 2, 3}, a) = {1, 2, 3}\\nδ′({1, 2, 3}, b) = {2, 3}\\nThe state diagram of the DFA M is as follows:\\n2.5.\\nEquivalence of DFAs and NFAs\\n47\\n/0\\n{1}\\n{2}\\n{3}\\n{1,2}\\n{2,3}\\n{1,3}\\n{1,2,3}\\na,b\\nb\\na\\nb\\na\\na\\nb\\na,b\\na\\nb\\nb\\na\\nb\\na\\nWe make the following observations:\\n• The states {1} and {1, 3} do not have incoming edges. Therefore, these\\ntwo states cannot be reached from the start state {1, 2}.\\n• The state {3} has only one incoming edge; it comes from the state\\n{1}. Since {1} cannot be reached from the start state, {3} cannot be\\nreached from the start state.\\n• The state {2} has only one incoming edge; it comes from the state\\n{3}. Since {3} cannot be reached from the start state, {2} cannot be\\nreached from the start state.\\nHence, we can remove the four states {1}, {2}, {3}, and {1, 3}. The\\nresulting DFA accepts the same language as the DFA above.\\nThis leads\\nto the following state diagram, which depicts a DFA that accepts the same\\nlanguage as the NFA N:\\n48\\nChapter 2.\\nFinite Automata and Regular Languages\\n/0\\n{1,2}\\n{2,3}\\n{1,2,3}\\na,b\\na\\nb\\nb\\na\\nb\\na\\n2.6\\nClosure under the regular operations\\nIn Section 2.3, we have deﬁned the regular operations union, concatenation,\\nand star. We proved in Theorem 2.3.1 that the union of two regular lan-\\nguages is a regular language. We also explained why it is not clear that the\\nconcatenation of two regular languages is regular, and that the star of a reg-\\nular language is regular. In this section, we will see that the concept of NFA,\\ntogether with Theorem 2.5.2, can be used to give a simple proof of the fact\\nthat the regular languages are indeed closed under the regular operations.\\nWe start by giving an alternative proof of Theorem 2.3.1:\\nTheorem 2.6.1 The set of regular languages is closed under the union op-\\neration, i.e., if A1 and A2 are regular languages over the same alphabet Σ,\\nthen A1 ∪A2 is also a regular language.\\n2.6.\\nClosure under the regular operations\\n49\\nq1\\nM1\\nM2\\nq2\\nq0\\nq1\\nq2\\nε\\nε\\nM\\nFigure 2.1: The NFA M accepts L(M1) ∪L(M2).\\nProof.\\nSince A1 is regular, there is, by Theorem 2.5.2, an NFA M1 =\\n(Q1, Σ, δ1, q1, F1), such that A1 = L(M1). Similarly, there is an NFA M2 =\\n(Q2, Σ, δ2, q2, F2), such that A2 = L(M2). We may assume that Q1 ∩Q2 = ∅,\\nbecause otherwise, we can give new “names” to the states of Q1 and Q2.\\nFrom these two NFAs, we will construct an NFA M = (Q, Σ, δ, q0, F), such\\nthat L(M) = A1 ∪A2. The construction is illustrated in Figure 2.1. The\\nNFA M is deﬁned as follows:\\n1. Q = {q0} ∪Q1 ∪Q2, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = F1 ∪F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\n50\\nChapter 2.\\nFinite Automata and Regular Languages\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1,\\nδ2(r, a)\\nif r ∈Q2,\\n{q1, q2}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nTheorem 2.6.2 The set of regular languages is closed under the concatena-\\ntion operation, i.e., if A1 and A2 are regular languages over the same alphabet\\nΣ, then A1A2 is also a regular language.\\nProof.\\nLet M1 = (Q1, Σ, δ1, q1, F1) be an NFA, such that A1 = L(M1).\\nSimilarly, let M2 = (Q2, Σ, δ2, q2, F2) be an NFA, such that A2 = L(M2).\\nAs in the proof of Theorem 2.6.1, we may assume that Q1 ∩Q2 = ∅. We\\nwill construct an NFA M = (Q, Σ, δ, q0, F), such that L(M) = A1A2. The\\nconstruction is illustrated in Figure 2.2. The NFA M is deﬁned as follows:\\n1. Q = Q1 ∪Q2.\\n2. q0 = q1.\\n3. F = F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q2}\\nif r ∈F1 and a = ϵ,\\nδ2(r, a)\\nif r ∈Q2.\\nTheorem 2.6.3 The set of regular languages is closed under the star oper-\\nation, i.e., if A is a regular language, then A∗is also a regular language.\\n2.6.\\nClosure under the regular operations\\n51\\nq1\\nM1\\nM2\\nq2\\nq2\\nε\\nε\\nε\\nq0\\nM\\nFigure 2.2: The NFA M accepts L(M1)L(M2).\\nq1\\nN\\nq1\\nq0\\nε\\nε\\nε\\nε\\nM\\nFigure 2.3: The NFA M accepts (L(N))∗.\\nProof. Let Σ be the alphabet of A and let N = (Q1, Σ, δ1, q1, F1) be an\\nNFA, such that A = L(N). We will construct an NFA M = (Q, Σ, δ, q0, F),\\nsuch that L(M) = A∗. The construction is illustrated in Figure 2.3. The\\nNFA M is deﬁned as follows:\\n52\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. Q = {q0} ∪Q1, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = {q0} ∪F1. (Since ϵ ∈A∗, q0 has to be an accept state.)\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ,\\n{q1}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nIn the ﬁnal theorem of this section, we mention (without proof) two more\\nclosure properties of the regular languages:\\nTheorem 2.6.4 The set of regular languages is closed under the complement\\nand intersection operations:\\n1. If A is a regular language over the alphabet Σ, then the complement\\nA = {w ∈Σ∗: w ̸∈A}\\nis also a regular language.\\n2. If A1 and A2 are regular languages over the same alphabet Σ, then the\\nintersection\\nA1 ∩A2 = {w ∈Σ∗: w ∈A1 and w ∈A2}\\nis also a regular language.\\n2.7\\nRegular expressions\\nIn this section, we present regular expressions, which are a means to describe\\nlanguages. As we will see, the class of languages that can be described by\\nregular expressions coincides with the class of regular languages.\\n2.7.\\nRegular expressions\\n53\\nBefore formally deﬁning the notion of a regular expression, we give some\\nexamples. Consider the expression\\n(0 ∪1)01∗.\\nThe language described by this expression is the set of all binary strings\\n1. that start with either 0 or 1 (this is indicated by (0 ∪1)),\\n2. for which the second symbol is 0 (this is indicated by 0), and\\n3. that end with zero or more 1s (this is indicated by 1∗).\\nThat is, the language described by this expression is\\n{00, 001, 0011, 00111, . . . , 10, 101, 1011, 10111, . . .}.\\nHere are some more examples (in all cases, the alphabet is {0, 1}):\\n• The language {w : w contains exactly two 0s} is described by the ex-\\npression\\n1∗01∗01∗.\\n• The language {w : w contains at least two 0s} is described by the ex-\\npression\\n(0 ∪1)∗0(0 ∪1)∗0(0 ∪1)∗.\\n• The language {w : 1011 is a substring of w} is described by the ex-\\npression\\n(0 ∪1)∗1011(0 ∪1)∗.\\n• The language {w : the length of w is even} is described by the expres-\\nsion\\n((0 ∪1)(0 ∪1))∗.\\n• The language {w : the length of w is odd} is described by the expres-\\nsion\\n(0 ∪1) ((0 ∪1)(0 ∪1))∗.\\n• The language {1011, 0} is described by the expression\\n1011 ∪0.\\n54\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The language {w :\\nthe ﬁrst and last symbols of w are equal} is de-\\nscribed by the expression\\n0(0 ∪1)∗0 ∪1(0 ∪1)∗1 ∪0 ∪1.\\nAfter these examples, we give a formal (and inductive) deﬁnition of regular\\nexpressions:\\nDeﬁnition 2.7.1 Let Σ be a non-empty alphabet.\\n1. ϵ is a regular expression.\\n2. ∅is a regular expression.\\n3. For each a ∈Σ, a is a regular expression.\\n4. If R1 and R2 are regular expressions, then R1 ∪R2 is a regular expres-\\nsion.\\n5. If R1 and R2 are regular expressions, then R1R2 is a regular expression.\\n6. If R is a regular expression, then R∗is a regular expression.\\nYou can regard 1., 2., and 3. as being the “building blocks” of regular\\nexpressions.\\nItems 4., 5., and 6. give rules that can be used to combine\\nregular expressions into new (and “larger”) regular expressions. To give an\\nexample, we claim that\\n(0 ∪1)∗101(0 ∪1)∗\\nis a regular expression (where the alphabet Σ is equal to {0, 1}). In order\\nto prove this, we have to show that this expression can be “built” using the\\n“rules” given in Deﬁnition 2.7.1. Here we go:\\n• By 3., 0 is a regular expression.\\n• By 3., 1 is a regular expression.\\n• Since 0 and 1 are regular expressions, by 4., 0∪1 is a regular expression.\\n• Since 0∪1 is a regular expression, by 6., (0∪1)∗is a regular expression.\\n• Since 1 and 0 are regular expressions, by 5., 10 is a regular expression.\\n2.7.\\nRegular expressions\\n55\\n• Since 10 and 1 are regular expressions, by 5., 101 is a regular expression.\\n• Since (0 ∪1)∗and 101 are regular expressions, by 5., (0 ∪1)∗101 is a\\nregular expression.\\n• Since (0 ∪1)∗101 and (0 ∪1)∗are regular expressions, by 5., (0 ∪\\n1)∗101(0 ∪1)∗is a regular expression.\\nNext we deﬁne the language that is described by a regular expression:\\nDeﬁnition 2.7.2 Let Σ be a non-empty alphabet.\\n1. The regular expression ϵ describes the language {ϵ}.\\n2. The regular expression ∅describes the language ∅.\\n3. For each a ∈Σ, the regular expression a describes the language {a}.\\n4. Let R1 and R2 be regular expressions and let L1 and L2 be the lan-\\nguages described by them, respectively. The regular expression R1∪R2\\ndescribes the language L1 ∪L2.\\n5. Let R1 and R2 be regular expressions and let L1 and L2 be the languages\\ndescribed by them, respectively. The regular expression R1R2 describes\\nthe language L1L2.\\n6. Let R be a regular expression and let L be the language described by\\nit. The regular expression R∗describes the language L∗.\\nWe consider some examples:\\n• The regular expression (0∪ϵ)(1∪ϵ) describes the language {01, 0, 1, ϵ}.\\n• The regular expression 0 ∪ϵ describes the language {0, ϵ}, whereas the\\nregular expression 1∗describes the language {ϵ, 1, 11, 111, . . .}. There-\\nfore, the regular expression (0 ∪ϵ)1∗describes the language\\n{0, 01, 011, 0111, . . . , ϵ, 1, 11, 111, . . .}.\\nObserve that this language is also described by the regular expression\\n01∗∪1∗.\\n56\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The regular expression 1∗∅describes the empty language, i.e., the lan-\\nguage ∅. (You should convince yourself that this is correct.)\\n• The regular expression ∅∗describes the language {ϵ}.\\nDeﬁnition 2.7.3 Let R1 and R2 be regular expressions and let L1 and L2\\nbe the languages described by them, respectively. If L1 = L2 (i.e., R1 and\\nR2 describe the same language), then we will write R1 = R2.\\nHence, even though (0∪ϵ)1∗and 01∗∪1∗are diﬀerent regular expressions,\\nwe write\\n(0 ∪ϵ)1∗= 01∗∪1∗,\\nbecause they describe the same language.\\nIn Section 2.8.2, we will show that every regular language can be described\\nby a regular expression. The proof of this fact is purely algebraic and uses\\nthe following algebraic identities involving regular expressions.\\nTheorem 2.7.4 Let R1, R2, and R3 be regular expressions. The following\\nidentities hold:\\n1. R1∅= ∅R1 = ∅.\\n2. R1ϵ = ϵR1 = R1.\\n3. R1 ∪∅= ∅∪R1 = R1.\\n4. R1 ∪R1 = R1.\\n5. R1 ∪R2 = R2 ∪R1.\\n6. R1(R2 ∪R3) = R1R2 ∪R1R3.\\n7. (R1 ∪R2)R3 = R1R3 ∪R2R3.\\n8. R1(R2R3) = (R1R2)R3.\\n9. ∅∗= ϵ.\\n10. ϵ∗= ϵ.\\n11. (ϵ ∪R1)∗= R∗\\n1.\\n2.8.\\nEquivalence of regular expressions and regular languages 57\\n12. (ϵ ∪R1)(ϵ ∪R1)∗= R∗\\n1.\\n13. R∗\\n1(ϵ ∪R1) = (ϵ ∪R1)R∗\\n1 = R∗\\n1.\\n14. R∗\\n1R2 ∪R2 = R∗\\n1R2.\\n15. R1(R2R1)∗= (R1R2)∗R1.\\n16. (R1 ∪R2)∗= (R∗\\n1R2)∗R∗\\n1 = (R∗\\n2R1)∗R∗\\n2.\\nWe will not present the (boring) proofs of these identities, but urge you\\nto convince yourself informally that they make perfect sense. To give an\\nexample, we mentioned above that\\n(0 ∪ϵ)1∗= 01∗∪1∗.\\nWe can verify this identity in the following way:\\n(0 ∪ϵ)1∗\\n=\\n01∗∪ϵ1∗\\n(by identity 7)\\n=\\n01∗∪1∗\\n(by identity 2)\\n2.8\\nEquivalence of regular expressions and reg-\\nular languages\\nIn the beginning of Section 2.7, we mentioned the following result:\\nTheorem 2.8.1 Let L be a language. Then L is regular if and only if there\\nexists a regular expression that describes L.\\nThe proof of this theorem consists of two parts:\\n• In Section 2.8.1, we will prove that every regular expression describes\\na regular language.\\n• In Section 2.8.2, we will prove that every DFA M can be converted to\\na regular expression that describes the language L(M).\\nThese two results will prove Theorem 2.8.1.\\n58\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.8.1\\nEvery regular expression describes a regular lan-\\nguage\\nLet R be an arbitrary regular expression over the alphabet Σ. We will prove\\nthat the language described by R is a regular language. The proof is by\\ninduction on the structure of R (i.e., by induction on the way R is “built”\\nusing the “rules” given in Deﬁnition 2.7.1).\\nThe ﬁrst base case: Assume that R = ϵ.\\nThen R describes the lan-\\nguage {ϵ}. In order to prove that this language is regular, it suﬃces, by\\nTheorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q, F) that accepts this\\nlanguage. This NFA is obtained by deﬁning Q = {q}, q is the start state,\\nF = {q}, and δ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state\\ndiagram of M:\\nq\\nThe second base case: Assume that R = ∅. Then R describes the language\\n∅. In order to prove that this language is regular, it suﬃces, by Theorem 2.5.2,\\nto construct an NFA M = (Q, Σ, δ, q, F) that accepts this language. This\\nNFA is obtained by deﬁning Q = {q}, q is the start state, F = ∅, and\\nδ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state diagram of M:\\nq\\nThe third base case: Let a ∈Σ and assume that R = a. Then R describes\\nthe language {a}. In order to prove that this language is regular, it suﬃces,\\nby Theorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q1, F) that accepts\\nthis language. This NFA is obtained by deﬁning Q = {q1, q2}, q1 is the start\\nstate, F = {q2}, and\\nδ(q1, a)\\n=\\n{q2},\\nδ(q1, b)\\n=\\n∅for all b ∈Σϵ \\\\ {a},\\nδ(q2, b)\\n=\\n∅for all b ∈Σϵ.\\nThe ﬁgure below gives the state diagram of M:\\n2.8.\\nEquivalence of regular expressions and regular languages 59\\nq1\\nq2\\na\\nThe ﬁrst case of the induction step: Assume that R = R1 ∪R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1 ∪L2, which, by Theorem 2.6.1, is regular.\\nThe second case of the induction step: Assume that R = R1R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1L2, which, by Theorem 2.6.2, is regular.\\nThe third case of the induction step: Assume that R = (R1)∗, where\\nR1 is a regular expression.\\nLet L1 be the language described by R1 and\\nassume that L1 is regular. Then R describes the language (L1)∗, which, by\\nTheorem 2.6.3, is regular.\\nThis concludes the proof of the claim that every regular expression de-\\nscribes a regular language.\\nTo give an example, consider the regular expression\\n(ab ∪a)∗,\\nwhere the alphabet is {a, b}. We will prove that this regular expression de-\\nscribes a regular language, by constructing an NFA that accepts the language\\ndescribed by this regular expression. Observe how the regular expression is\\n“built”:\\n• Take the regular expressions a and b, and combine them into the regular\\nexpression ab.\\n• Take the regular expressions ab and a, and combine them into the\\nregular expression ab ∪a.\\n• Take the regular expression ab ∪a, and transform it into the regular\\nexpression (ab ∪a)∗.\\nFirst, we construct an NFA M1 that accepts the language described by\\nthe regular expression a:\\n60\\nChapter 2.\\nFinite Automata and Regular Languages\\na\\nM1\\nNext, we construct an NFA M2 that accepts the language described by\\nthe regular expression b:\\nM2\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.2 to\\nM1 and M2. This gives an NFA M3 that accepts the language described by\\nthe regular expression ab:\\nM3\\na\\nε\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.1 to\\nM3 and M1. This gives an NFA M4 that accepts the language described by\\nthe regular expression ab ∪a:\\na\\nε\\nb\\na\\nε\\nε\\nM4\\nFinally, we apply the construction given in the proof of Theorem 2.6.3\\nto M4. This gives an NFA M5 that accepts the language described by the\\nregular expression (ab ∪a)∗:\\n2.8.\\nEquivalence of regular expressions and regular languages 61\\na\\nε\\nb\\na\\nε\\nε\\nε\\nε\\nε\\nM5\\n2.8.2\\nConverting a DFA to a regular expression\\nIn this section, we will prove that every DFA M can be converted to a regular\\nexpression that describes the language L(M). In order to prove this result,\\nwe need to solve recurrence relations involving languages.\\nSolving recurrence relations\\nLet Σ be an alphabet, let B and C be “known” languages in Σ∗such that\\nϵ ̸∈B, and let L be an “unknown” language such that\\nL = BL ∪C.\\nCan we “solve” this equation for L? That is, can we express L in terms of\\nB and C?\\nConsider an arbitrary string u in L. We are going to determine how u\\nlooks like. Since u ∈L and L = BL ∪C, we know that u is a string in\\nBL ∪C. Hence, there are two possibilities for u.\\n1. u is an element of C.\\n2. u is an element of BL. In this case, there are strings b ∈B and v ∈L\\nsuch that u = bv. Since ϵ ̸∈B, we have b ̸= ϵ and, therefore, |v| < |u|.\\n(Recall that |v| denotes the length, i.e., the number of symbols, of the\\nstring v.) Since v is a string in L, which is equal to BL ∪C, v is a\\nstring in BL ∪C. Hence, there are two possibilities for v.\\n62\\nChapter 2.\\nFinite Automata and Regular Languages\\n(a) v is an element of C. In this case,\\nu = bv, where b ∈B and v ∈C; thus, u ∈BC.\\n(b) v is an element of BL. In this case, there are strings b′ ∈B and\\nw ∈L such that v = b′w. Since ϵ ̸∈B, we have b′ ̸= ϵ and,\\ntherefore, |w| < |v|. Since w is a string in L, which is equal to\\nBL∪C, w is a string in BL∪C. Hence, there are two possibilities\\nfor w.\\ni. w is an element of C. In this case,\\nu = bb′w, where b, b′ ∈B and w ∈C; thus, u ∈BBC.\\nii. w is an element of BL. In this case, there are strings b′′ ∈B\\nand x ∈L such that w = b′′x. Since ϵ ̸∈B, we have b′′ ̸= ϵ\\nand, therefore, |x| < |w|. Since x is a string in L, which is\\nequal to BL ∪C, x is a string in BL ∪C. Hence, there are\\ntwo possibilities for x.\\nA. x is an element of C. In this case,\\nu = bb′b′′x, where b, b′, b′′ ∈B and x ∈C; thus, u ∈BBBC.\\nB. x is an element of BL. Etc., etc.\\nThis process hopefully convinces you that any string u in L can be written\\nas the concatenation of zero or more strings in B, followed by one string in\\nC. In fact, L consists of exactly those strings having this property:\\nLemma 2.8.2 Let Σ be an alphabet, and let B, C, and L be languages in\\nΣ∗such that ϵ ̸∈B and\\nL = BL ∪C.\\nThen\\nL = B∗C.\\nProof. First, we show that B∗C ⊆L. Let u be an arbitrary string in B∗C.\\nThen u is the concatenation of k strings of B, for some k ≥0, followed by\\none string of C. We proceed by induction on k.\\nThe base case is when k = 0. In this case, u is a string in C. Hence, u is\\na string in BL ∪C. Since BL ∪C = L, it follows that u is a string in L.\\n2.8.\\nEquivalence of regular expressions and regular languages 63\\nNow let k ≥1. Then we can write u = vwc, where v is a string in B,\\nw is the concatenation of k −1 strings of B, and c is a string of C. Deﬁne\\ny = wc. Observe that y is the concatenation of k −1 strings of B followed\\nby one string of C. Therefore, by induction, the string y is an element of L.\\nHence, u = vy, where v is a string in B and y is a string in L. This shows\\nthat u is a string in BL. Hence, u is a string in BL ∪C. Since BL ∪C = L,\\nit follows that u is a string in L. This completes the proof that B∗C ⊆L.\\nIt remains to show that L ⊆B∗C. Let u be an arbitrary string in L,\\nand let ℓbe its length (i.e., ℓis the number of symbols in u). We prove by\\ninduction on ℓthat u is a string in B∗C.\\nThe base case is when ℓ= 0. Then u = ϵ. Since u ∈L and L = BL ∪C,\\nu is a string in BL ∪C. Since ϵ ̸∈B, u cannot be a string in BL. Hence, u\\nmust be a string in C. Since C ⊆B∗C, it follows that u is a string in B∗C.\\nLet ℓ≥1. If u is a string in C, then u is a string in B∗C and we are done.\\nSo assume that u is not a string in C. Since u ∈L and L = BL ∪C, u is a\\nstring in BL. Hence, there are strings b ∈B and v ∈L such that u = bv.\\nSince ϵ ̸∈B, the length of b is at least one; hence, the length of v is less than\\nthe length of u. By induction, v is a string in B∗C. Hence, u = bv, where\\nb ∈B and v ∈B∗C. This shows that u ∈B(B∗C). Since B(B∗C) ⊆B∗C,\\nit follows that u ∈B∗C.\\nNote that Lemma 2.8.2 holds for any language B that does not contain\\nthe empty string ϵ. As an example, assume that B = ∅. Then the language\\nL satisﬁes the equation\\nL = BL ∪C = ∅L ∪C.\\nUsing Theorem 2.7.4, this equation becomes\\nL = ∅∪C = C.\\nWe now show that Lemma 2.8.2 also implies that L = C: Since ϵ ̸∈B,\\nLemma 2.8.2 implies that L = B∗C, which, using Theorem 2.7.4, becomes\\nL = B∗C = ∅∗C = ϵC = C.\\nThe conversion\\nWe will now use Lemma 2.8.2 to prove that every DFA can be converted to\\na regular expression.\\n64\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet M = (Q, Σ, δ, q, F) be an arbitrary deterministic ﬁnite automaton.\\nWe will show that there exists a regular expression that describes the lan-\\nguage L(M).\\nFor each state r ∈Q, we deﬁne\\nLr = {w ∈Σ∗:\\nthe path in the state diagram of M that starts\\nin state r and that corresponds to w ends in a\\nstate of F }.\\nIn words, Lr is the language accepted by M, if r were the start state.\\nWe will show that each such language Lr can be described by a regular\\nexpression. Since L(M) = Lq, this will prove that L(M) can be described by\\na regular expression.\\nThe basic idea is to set up equations for the languages Lr, which we then\\nsolve using Lemma 2.8.2. We claim that\\nLr =\\n[\\na∈Σ\\na · Lδ(r,a)\\nif r ̸∈F.\\n(2.2)\\nWhy is this true? Let w be a string in Lr. Then the path P in the state\\ndiagram of M that starts in state r and that corresponds to w ends in a\\nstate of F. Since r ̸∈F, this path contains at least one edge. Let r′ be the\\nstate that follows the ﬁrst state (i.e., r) of P. Then r′ = δ(r, b) for some\\nsymbol b ∈Σ. Hence, b is the ﬁrst symbol of w. Write w = bv, where v is\\nthe remaining part of w. Then the path P ′ = P \\\\ {r} in the state diagram\\nof M that starts in state r′ and that corresponds to v ends in a state of F.\\nTherefore, v ∈Lr′ = Lδ(r,b). Hence,\\nw ∈b · Lδ(r,b) ⊆\\n[\\na∈Σ\\na · Lδ(r,a).\\nConversely, let w be a string in S\\na∈Σ a · Lδ(r,a). Then there is a symbol b ∈Σ\\nand a string v ∈Lδ(r,b) such that w = bv. Let P ′ be the path in the state\\ndiagram of M that starts in state δ(r, b) and that corresponds to v. Since\\nv ∈Lδ(r,b), this path ends in a state of F. Let P be the path in the state\\ndiagram of M that starts in r, follows the edge to δ(r, b), and then follows P ′.\\nThis path P corresponds to w and ends in a state of F. Therefore, w ∈Lr.\\nThis proves the correctness of (2.2).\\n2.8.\\nEquivalence of regular expressions and regular languages 65\\nSimilarly, we can prove that\\nLr = ϵ ∪\\n [\\na∈Σ\\na · Lδ(r,a)\\n!\\nif r ∈F.\\n(2.3)\\nSo we now have a set of equations in the “unknowns” Lr, for r ∈Q. The\\nnumber of equations is equal to the size of Q. In other words, the number\\nof equations is equal to the number of unknowns. The regular expression for\\nL(M) = Lq is obtained by solving these equations using Lemma 2.8.2.\\nOf course, we have to convince ourselves that these equations have a so-\\nlution for any given DFA. Before we deal with this issue, we give an example.\\nAn example\\nConsider the deterministic ﬁnite automaton M = (Q, Σ, δ, q0, F), where Q =\\n{q0, q1, q2}, Σ = {a, b}, q0 is the start state, F = {q2}, and δ is given in the\\nstate diagram below. We show how to obtain the regular expression that\\ndescribes the language accepted by M.\\nq0\\nq1\\nq2\\na\\na\\na\\nb\\nb\\nb\\nFor this case, (2.2) and (2.3) give the following equations:\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nLq0\\n=\\na · Lq0 ∪b · Lq2\\nLq1\\n=\\na · Lq0 ∪b · Lq1\\nLq2\\n=\\nϵ ∪a · Lq1 ∪b · Lq0\\n66\\nChapter 2.\\nFinite Automata and Regular Languages\\nIn the third equation, Lq2 is expressed in terms of Lq0 and Lq1. Hence, if we\\nsubstitute the third equation into the ﬁrst one, and use Theorem 2.7.4, then\\nwe get\\nLq0\\n=\\na · Lq0 ∪b · (ϵ ∪a · Lq1 ∪b · Lq0)\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b.\\nWe obtain the following set of equations.\\n\\x1a Lq0\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b\\nLq1\\n=\\nb · Lq1 ∪a · Lq0\\nLet L = Lq1, B = b, and C = a · Lq0. Then ϵ ̸∈B and the second equation\\nreads L = BL ∪C. Hence, by Lemma 2.8.2,\\nLq1 = L = B∗C = b∗a · Lq0.\\nIf we substitute Lq1 into the ﬁrst equation, then we get (again using Theo-\\nrem 2.7.4)\\nLq0\\n=\\n(a ∪bb) · Lq0 ∪ba · b∗a · Lq0 ∪b\\n=\\n(a ∪bb ∪bab∗a)Lq0 ∪b.\\nAgain applying Lemma 2.8.2, this time with L = Lq0, B = a∪bb∪bab∗a and\\nC = b, gives\\nLq0 = (a ∪bb ∪bab∗a)∗b.\\nThus, the regular expression that describes the language accepted by M is\\n(a ∪bb ∪bab∗a)∗b.\\nCompleting the correctness of the conversion\\nIt remains to prove that, for any DFA, the system of equations (2.2) and (2.3)\\ncan be solved. This will follow from the following (more general) lemma.\\n(You should verify that the equations (2.2) and (2.3) are in the form as\\nspeciﬁed in this lemma.)\\n2.8.\\nEquivalence of regular expressions and regular languages 67\\nLemma 2.8.3 Let n ≥1 be an integer and, for 1 ≤i ≤n and 1 ≤j ≤n,\\nlet Bij and Ci be regular expressions such that ϵ ̸∈Bij. Let L1, L2, . . . , Ln be\\nlanguages that satisfy\\nLi =\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci for 1 ≤i ≤n.\\nThen L1 can be expressed as a regular expression only involving the regular\\nexpressions Bij and Ci.\\nProof. The proof is by induction on n. The base case is when n = 1. In\\nthis case, we have\\nL1 = B11L1 ∪C1.\\nSince ϵ ̸∈B11, it follows from Lemma 2.8.2 that L1 = B∗\\n11C1. This proves\\nthe base case.\\nLet n ≥2 and assume the lemma is true for n −1. We have\\nLn\\n=\\n n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n=\\nBnnLn ∪\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn.\\nSince ϵ ̸∈Bnn, it follows from Lemma 2.8.2 that\\nLn\\n=\\nB∗\\nnn\\n  n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n!\\n=\\nB∗\\nnn\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪B∗\\nnnCn\\n=\\n n−1\\n[\\nj=1\\nB∗\\nnnBnjLj\\n!\\n∪B∗\\nnnCn\\nBy substituting this equation for Ln into the equations for Li, 1 ≤i ≤n −1,\\n68\\nChapter 2.\\nFinite Automata and Regular Languages\\nwe obtain\\nLi\\n=\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\nBinLn ∪\\n n−1\\n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\n n−1\\n[\\nj=1\\n(BinB∗\\nnnBnj ∪Bij) Lj\\n!\\n∪BinB∗\\nnnCn ∪Ci.\\nThus, we have obtained n −1 equations in L1, L2, . . . , Ln−1.\\nSince ϵ ̸∈\\nBinB∗\\nnnBnj ∪Bij, it follows from the induction hypothesis that L1 can be\\nexpressed as a regular expression only involving the regular expressions Bij\\nand Ci.\\n2.9\\nThe pumping lemma and nonregular lan-\\nguages\\nIn the previous sections, we have seen that the class of regular languages is\\nclosed under various operations, and that these languages can be described by\\n(deterministic or nondeterministic) ﬁnite automata and regular expressions.\\nThese properties helped in developing techniques for showing that a language\\nis regular. In this section, we will present a tool that can be used to prove\\nthat certain languages are not regular. Observe that for a regular language,\\n1. the amount of memory that is needed to determine whether or not a\\ngiven string is in the language is ﬁnite and independent of the length\\nof the string, and\\n2. if the language consists of an inﬁnite number of strings, then this lan-\\nguage should contain inﬁnite subsets having a fairly repetitive struc-\\nture.\\nIntuitively, languages that do not follow 1. or 2. should be nonregular. For\\nexample, consider the language\\n{0n1n : n ≥0}.\\n2.9.\\nThe pumping lemma and nonregular languages\\n69\\nThis language should be nonregular, because it seems unlikely that a DFA can\\nremember how many 0s it has seen when it has reached the border between\\nthe 0s and the 1s. Similarly the language\\n{0n : n is a prime number}\\nshould be nonregular, because the prime numbers do not seem to have any\\nrepetitive structure that can be used by a DFA. To be more rigorous about\\nthis, we will establish a property that all regular languages must possess.\\nThis property is called the pumping lemma. If a language does not have this\\nproperty, then it must be nonregular.\\nThe pumping lemma states that any suﬃciently long string in a regular\\nlanguage can be pumped, i.e., there is a section in that string that can be\\nrepeated any number of times, so that the resulting strings are all in the\\nlanguage.\\nTheorem 2.9.1 (Pumping Lemma for Regular Languages) Let A be\\na regular language. Then there exists an integer p ≥1, called the pumping\\nlength, such that the following holds: Every string s in A, with |s| ≥p, can\\nbe written as s = xyz, such that\\n1. y ̸= ϵ (i.e., |y| ≥1),\\n2. |xy| ≤p, and\\n3. for all i ≥0, xyiz ∈A.\\nIn words, the pumping lemma states that by replacing the portion y in s\\nby zero or more copies of it, the resulting string is still in the language A.\\nProof. Let Σ be the alphabet of A. Since A is a regular language, there\\nexists a DFA M = (Q, Σ, δ, q, F) that accepts A. We deﬁne p to be the\\nnumber of states in Q.\\nLet s = s1s2 . . . sn be an arbitrary string in A such that n ≥p. Deﬁne\\nr1 = q, r2 = δ(r1, s1), r3 = δ(r2, s2), . . ., rn+1 = δ(rn, sn). Thus, when the\\nDFA M reads the string s from left to right, it visits the states r1, r2, . . . , rn+1.\\nSince s is a string in A, we know that rn+1 belongs to F.\\nConsider the ﬁrst p + 1 states r1, r2, . . . , rp+1 in this sequence. Since the\\nnumber of states of M is equal to p, the pigeonhole principle implies that\\nthere must be a state that occurs twice in this sequence. That is, there are\\nindices j and ℓsuch that 1 ≤j < ℓ≤p + 1 and rj = rℓ.\\n70\\nChapter 2.\\nFinite Automata and Regular Languages\\nq = r1\\nrn+1\\nr j = rℓ\\nread x\\nread y\\nread z\\nWe deﬁne x = s1s2 . . . sj−1, y = sj . . . sℓ−1, and z = sℓ. . . sn. Since j < ℓ,\\nwe have y ̸= ϵ, proving the ﬁrst claim in the theorem. Since ℓ≤p + 1, we\\nhave |xy| = ℓ−1 ≤p, proving the second claim in the theorem. To see that\\nthe third claim also holds, recall that the string s = xyz is accepted by M.\\nWhile reading x, M moves from the start state q to state rj. While reading\\ny, it moves from state rj to state rℓ= rj, i.e., after having read y, M is again\\nin state rj. While reading z, M moves from state rj to the accept state rn+1.\\nTherefore, the substring y can be repeated any number i ≥0 of times, and\\nthe corresponding string xyiz will still be accepted by M. It follows that\\nxyiz ∈A for all i ≥0.\\n2.9.1\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {0n1n : n ≥0}.\\nWe will prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. It is clear\\nthat s ∈A and |s| = 2p ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that, since |xy| ≤p, the string y contains only 0s. Moreover,\\nsince y ̸= ϵ, y contains at least one 0. But now we are in trouble: None of\\nthe strings xy0z = xz, xy2z = xyyz, xy3z = xyyyz, . . . , is contained in A.\\nHowever, by the pumping lemma, all these strings must be in A. Hence, we\\nhave a contradiction and we conclude that A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n71\\nSecond example\\nConsider the language\\nA = {w ∈{0, 1}∗: the number of 0s in w equals the number of 1s in w}.\\nAgain, we prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. Then s ∈A\\nand |s| = 2p ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz contains more 0s than 1s,\\nwhich implies that this string is not contained in A. But, by the pumping\\nlemma, this string is contained in A. This is a contradiction and, therefore,\\nA is not a regular language.\\nThird example\\nConsider the language\\nA = {ww : w ∈{0, 1}∗}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p10p1. Then s ∈A\\nand |s| = 2p + 2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz is not contained in A. But,\\nby the pumping lemma, this string is contained in A. This is a contradiction\\nand, therefore, A is not a regular language.\\nYou should convince yourself that by choosing s = 02p (which is a string\\nin A whose length is at least p), we do not obtain a contradiction. The reason\\nis that the string y may have an even length. Thus, 02p is the “wrong” string\\nfor showing that A is not regular. By choosing s = 0p10p1, we do obtain\\na contradiction; thus, this is the “correct” string for showing that A is not\\nregular.\\n72\\nChapter 2.\\nFinite Automata and Regular Languages\\nFourth example\\nConsider the language\\nA = {0m1n : m > n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p+11p. Then s ∈A\\nand |s| = 2p + 1 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Consider the string xy0z = xz. The number of 1s in this string\\nis equal to p, whereas the number of 0s is at most equal to p. Therefore, the\\nstring xy0z is not contained in A. But, by the pumping lemma, this string\\nis contained in A. This is a contradiction and, therefore, A is not a regular\\nlanguage.\\nFifth example\\nConsider the language\\nA = {1n2 : n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 1p2. Then s ∈A\\nand |s| = p2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that\\n|s| = |xyz| = p2\\nand\\n|xy2z| = |xyyz| = |xyz| + |y| = p2 + |y|.\\nSince |xy| ≤p, we have |y| ≤p. Since y ̸= ϵ, we have |y| ≥1. It follows that\\np2 < |xy2z| ≤p2 + p < (p + 1)2.\\nHence, the length of the string xy2z is strictly between two consecutive\\nsquares.\\nIt follows that this length is not a square and, therefore, xy2z\\nis not contained in A. But, by the pumping lemma, this string is contained\\nin A. This is a contradiction and, therefore, A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n73\\nSixth example\\nConsider the language\\nA = {1n : n is a prime number}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Let n ≥p be a prime number, and consider\\nthe string s = 1n. Then s ∈A and |s| = n ≥p. By the pumping lemma, s\\ncan be written as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nLet k be the integer such that y = 1k. Since y ̸= ϵ, we have k ≥1. For\\neach i ≥0, n + (i −1)k is a prime number, because xyiz = 1n+(i−1)k ∈A.\\nFor i = n + 1, however, we have\\nn + (i −1)k = n + nk = n(1 + k),\\nwhich is not a prime number, because n ≥2 and 1 + k ≥2.\\nThis is a\\ncontradiction and, therefore, A is not a regular language.\\nSeventh example\\nConsider the language\\nA = {w ∈{0, 1}∗:\\nthe number of occurrences of 01 in w is equal to\\nthe number of occurrences of 10 in w }.\\nSince this language has the same ﬂavor as the one in the second example,\\nwe may suspect that A is not a regular language. This is, however, not true:\\nAs we will show, A is a regular language.\\nThe key property is the following one: Let w be an arbitrary string in\\n{0, 1}∗. Then\\nthe absolute value of the number of occurrences of 01 in w minus\\nthe number of occurrences of 10 in w is at most one.\\nThis property holds, because between any two consecutive occurrences of\\n01, there must be exactly one occurrence of 10. Similarly, between any two\\nconsecutive occurrences of 10, there must be exactly one occurrence of 01.\\nWe will construct a DFA that accepts A. This DFA uses the following\\nﬁve states:\\n74\\nChapter 2.\\nFinite Automata and Regular Languages\\n• q: start state; no symbol has been read.\\n• q01: the last symbol read was 1; in the part of the string read so far, the\\nnumber of occurrences of 01 is one more than the number of occurrences\\nof 10.\\n• q10: the last symbol read was 0; in the part of the string read so far, the\\nnumber of occurrences of 10 is one more than the number of occurrences\\nof 01.\\n• q0\\nequal: the last symbol read was 0; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\n• q1\\nequal: the last symbol read was 1; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\nThe set of accept states is equal to {q, q0\\nequal, q1\\nequal}. The state diagram of\\nthe DFA is given below.\\nq0\\nequal\\nq1\\nequal\\nq01\\nq10\\nq\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\nIn fact, the key property mentioned above implies that the language A\\nconsists of the empty string ϵ and all non-empty binary strings that start\\n2.9.\\nThe pumping lemma and nonregular languages\\n75\\nand end with the same symbol. As a result, A is the language described by\\nthe regular expression\\nϵ ∪0 ∪1 ∪0(0 ∪1)∗0 ∪1(0 ∪1)∗1.\\nThis gives an alternative proof for the fact that A is a regular language.\\nEighth example\\nConsider the language\\nL = {w ∈{0, 1}∗: w is the binary representation of a prime number}.\\nWe assume that for any positive integer, the leftmost bit in its binary repre-\\nsentation is 1. In other words, we assume that there are no 0’s added to the\\nleft of such a binary representation. Thus,\\nL = {10, 11, 101, 111, 1011, 1101, 10001, . . .}.\\nWe will prove that L is not a regular language.\\nAssume that L is a regular language. Let p ≥1 be the pumping length.\\nLet N > 2p be a prime number and let s ∈{0, 1}∗be the binary representa-\\ntion of N. Observe that |s| ≥p + 1. Also, the leftmost and rightmost bits of\\ns are 1.\\nSince s ∈L and |s| ≥p + 1 ≥p, the Pumping Lemma implies that we\\ncan write s = xyz, such that\\n1. |y| ≥1,\\n2. |xy| ≤p (and, thus, |z| ≥1), and\\n3. for all i ≥0, xyiz ∈L, i.e., xyiz is the binary representation of a prime\\nnumber.\\nDeﬁne A, B, and C to be the integers whose binary representations are\\nx, y, and z, respectively. Note that both y and z may have leading 0’s. In\\nfact, y may be a string consisting of 0’s only, in which case B = 0. However,\\nsince the rightmost bit of z is 1, we have C ≥1. Observe that\\nN = C + B · 2|z| + A · 2|z|+|y|.\\n(2.4)\\n76\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet i = N, consider the bitstring xyiz = xyNz, and let M be the prime\\nnumber whose binary representation is given by this bitstring. Then,\\nM\\n=\\nC +\\nN−1\\nX\\nk=0\\nB · 2|z|+k|y| + A · 2|z|+N|y|\\n=\\nC + B · 2|z|\\nN−1\\nX\\nk=0\\n2k|y| + A · 2|z|+N|y|.\\nLet\\nT =\\nN−1\\nX\\nk=0\\n2k|y|.\\nThen\\n\\x002|y| −1\\n\\x01\\nT = 2N|y| −1.\\n(2.5)\\nBy Fermat’s Little Theorem, we have\\n2N ≡2\\n(mod N),\\nimplying that\\n2N|y| −1 =\\n\\x002N\\x01|y| −1 ≡2|y| −1\\n(mod N).\\nThus, (2.5) implies that\\n\\x002|y| −1\\n\\x01\\nT ≡2|y| −1\\n(mod N).\\n(2.6)\\nObserve that 2|y| ≤2p < N, because |y| ≤|xy| ≤p. Also, 2|y| ≥2, because\\ny ̸= ϵ. It follows that\\n1 ≤2|y| −1 < N,\\nimplying that\\n2|y| −1 ̸≡0\\n(mod N).\\nThis, together with (2.6), implies that\\nT ≡1\\n(mod N).\\nSince\\nM = C + B · 2|z| · T + A · 2|z|+N|y|,\\n2.10.\\nHigman’s Theorem\\n77\\nit follows that\\nM ≡C + B · 2|z| + A · 2|z|+|y|\\n(mod N).\\nThis, together with (2.4), implies that\\nM ≡0\\n(mod N),\\ni.e., N divides M. Since M > N, we conclude that M is not a prime number,\\nwhich is a contradiction. Thus, the language L is not regular.\\n2.10\\nHigman’s Theorem\\nLet Σ be a ﬁnite alphabet. For any two strings x and y in Σ∗, we say that x\\nis a subsequence of y, if x can be obtained by deleting zero or more symbols\\nfrom y. For example, 10110 is a subsequence of 0010010101010001. For any\\nlanguage L ⊆Σ∗, we deﬁne\\nSUBSEQ(L) := {x : there exists a y ∈L such that x is a subsequence of y}.\\nThat is, SUBSEQ(L) is the language consisting of the subsequences of all\\nstrings in L. In 1952, Higman proved the following result:\\nTheorem 2.10.1 (Higman) For any ﬁnite alphabet Σ and for any lan-\\nguage L ⊆Σ∗, the language SUBSEQ(L) is regular.\\n2.10.1\\nDickson’s Theorem\\nOur proof of Higman’s Theorem will use a theorem that was proved in 1913\\nby Dickson.\\nRecall that N denotes the set of positive integers. Let n ∈N. For any\\ntwo points p = (p1, p2, . . . , pn) and q = (q1, q2, . . . , qn) in Nn, we say that p is\\ndominated by q, if pi ≤qi for all i with 1 ≤i ≤n.\\nTheorem 2.10.2 (Dickson) Let S ⊆Nn, and let M be the set consisting of\\nall elements of S that are minimal in the relation “is dominated by”. Thus,\\nM = {q ∈S : there is no p in S \\\\ {q} such that p is dominated by q}.\\nThen, the set M is ﬁnite.\\n78\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe will prove this theorem by induction on the dimension n. If n = 1,\\nthen either M = ∅(if S = ∅) or M consists of exactly one element (if S ̸= ∅).\\nTherefore, the theorem holds if n = 1. Let n ≥2 and assume the theorem\\nholds for all subsets of Nn−1. Let S be a subset of Nn and consider the set\\nM of minimal elements in S. If S = ∅, then M = ∅and, thus, M is ﬁnite.\\nAssume that S ̸= ∅. We ﬁx an arbitrary element q in M. If p ∈M \\\\ {q},\\nthen q is not dominated by p. Therefore, there exists an index i such that\\npi ≤qi −1. It follows that\\nM \\\\ {q} ⊆\\nn[\\ni=1\\n\\x00Ni−1 × [1, qi −1] × Nn−i\\x01\\n.\\nFor all i and k with 1 ≤i ≤n and 1 ≤k ≤qi −1, we deﬁne\\nSik = {p ∈S : pi = k}\\nand\\nMik = {p ∈M : pi = k}.\\nThen,\\nM \\\\ {q} =\\nn[\\ni=1\\nqi−1\\n[\\nk=1\\nMik.\\n(2.7)\\nLemma 2.10.3 Mik is a subset of the set of all elements of Sik that are\\nminimal in the relation “is dominated by”.\\nProof. Let p be an element of Mik, and assume that p is not minimal in\\nSik. Then there is an element r in Sik, such that r ̸= p and r is dominated\\nby p. Since p and r are both elements of S, it follows that p ̸∈M. This is a\\ncontradiction.\\nSince the set Sik is basically a subset of Nn−1, it follows from the induction\\nhypothesis that Sik contains ﬁnitely many minimal elements. This, combined\\nwith Lemma 2.10.3, implies that Mik is a ﬁnite set. Thus, by (2.7), M \\\\ {q}\\nis the union of ﬁnitely many ﬁnite sets. Therefore, the set M is ﬁnite.\\n2.10.2\\nProof of Higman’s Theorem\\nWe give the proof of Theorem 2.10.1 for the case when Σ = {0, 1}. If L = ∅\\nor SUBSEQ(L) = {0, 1}∗, then SUBSEQ(L) is obviously a regular language.\\n2.10.\\nHigman’s Theorem\\n79\\nHence, we may assume that L is non-empty and SUBSEQ(L) is a proper\\nsubset of {0, 1}∗.\\nWe ﬁx a string z of length at least two in the complement SUBSEQ(L) of\\nthe language SUBSEQ(L). Observe that this is possible, because SUBSEQ(L)\\nis an inﬁnite language. We insert 0s and 1s into z, such that, in the result-\\ning string z′, 0s and 1s alternate. For example, if z = 0011101011, then\\nz′ = 01010101010101. Let n = |z′| −1, where |z′| denotes the length of z′.\\nThen, n ≥|z| −1 ≥1.\\nA (0, 1)-alternation in a binary string x is any occurrence of 01 or 10 in x.\\nFor example, the string 1101001 contains four (0, 1)-alternations. We deﬁne\\nA = {x ∈{0, 1}∗: x has at most n many (0, 1)-alternations}.\\nLemma 2.10.4 SUBSEQ(L) ⊆A.\\nProof. Let x ∈SUBSEQ(L) and assume that x ̸∈A. Then, x has at least\\nn + 1 = |z′| many (0, 1)-alternations and, therefore, z′ is a subsequence of x.\\nIn particular, z is a subsequence of x. Since x ∈SUBSEQ(L), it follows that\\nz ∈SUBSEQ(L), which is a contradiction.\\nLemma 2.10.5 SUBSEQ(L) =\\n\\x10\\nA ∩SUBSEQ(L)\\n\\x11\\n∪A.\\nProof. Follows from Lemma 2.10.4.\\nLemma 2.10.6 The language A is regular.\\nProof.\\nThe complement A of A is the language consisting of all binary\\nstrings with at least n + 1 many (0, 1)-alternations. If, for example, n = 3,\\nthen A is described by the regular expression\\n(00∗11∗00∗11∗0(0 ∪1)∗) ∪(11∗00∗11∗00∗1(0 ∪1)∗) .\\nThis should convince you that the claim is true for any value of n.\\nFor any b ∈{0, 1} and for any k ≥0, we deﬁne Abk to be the language\\nconsisting of all binary strings that start with a b and have exactly k many\\n(0, 1)-alternations. Then, we have\\nA = {ϵ} ∪\\n 1[\\nb=0\\nn[\\nk=0\\nAbk\\n!\\n.\\n80\\nChapter 2.\\nFinite Automata and Regular Languages\\nThus, if we deﬁne\\nFbk = Abk ∩SUBSEQ(L),\\nand use the fact that ϵ ∈SUBSEQ(L) (which is true because L ̸= ∅), then\\nA ∩SUBSEQ(L) =\\n1[\\nb=0\\nn[\\nk=0\\nFbk.\\n(2.8)\\nFor any b ∈{0, 1} and for any k ≥0, consider the relation “is a subse-\\nquence of” on the language Fbk. We deﬁne Mbk to be the language consisting\\nof all strings in Fbk that are minimal in this relation. Thus,\\nMbk = {x ∈Fbk : there is no x′ in Fbk \\\\ {x} such that x′ is a subsequence of x}.\\nIt is clear that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Fbk : x is a subsequence of y}.\\nIf x ∈Mbk, y ∈Abk, and x is a subsequence of y, then y must be in\\nSUBSEQ(L) and, therefore, y must be in Fbk. To prove this, assume that\\ny ∈SUBSEQ(L).\\nThen, x ∈SUBSEQ(L), contradicting the fact that\\nx ∈Mbk ⊆Fbk ⊆SUBSEQ(L). It follows that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Abk : x is a subsequence of y}.\\n(2.9)\\nLemma 2.10.7 Let b ∈{0, 1} and 0 ≤k ≤n, and let x be an element of\\nMbk. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis regular.\\nProof. We will prove the claim by means of an example. Assume that b = 1,\\nk = 3, and x = 11110001000. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis described by the regular expression\\n11111∗0000∗11∗0000∗.\\nThis should convince you that the claim is true in general.\\nExercises\\n81\\nLemma 2.10.8 For each b ∈{0, 1} and each 0 ≤k ≤n, the set Mbk is\\nﬁnite.\\nProof. Again, we will prove the claim by means of an example. Assume\\nthat b = 1 and k = 3. Any string in Fbk can be written as 1a0b1c0d, for some\\nintegers a, b, c, d ≥1. Consider the function ϕ : Fbk →N4 that is deﬁned by\\nϕ(1a0b1c0d) = (a, b, c, d). Then, ϕ is an injective function, and the following\\nis true, for any two strings x and x′ in Fbk:\\nx is a subsequence of x′ if and only if ϕ(x) is dominated by ϕ(x′).\\nIt follows that the elements of Mbk are in one-to-one correspondence with\\nthose elements of ϕ(Fbk) that are minimal in the relation “is dominated by”.\\nThe lemma thus follows from Dickson’s Theorem.\\nNow we can complete the proof of Higman’s Theorem:\\n• It follows from (2.9) and Lemmas 2.10.7 and 2.10.8, that Fbk is the\\nunion of ﬁnitely many regular languages. Therefore, by Theorem 2.3.1,\\nFbk is a regular language.\\n• It follows from (2.8) that A∩SUBSEQ(L) is the union of ﬁnitely many\\nregular languages. Therefore, again by Theorem 2.3.1, A∩SUBSEQ(L)\\nis a regular language.\\n• Since A ∩SUBSEQ(L) is regular and, by Lemma 2.10.6, A is regular,\\nit follows from Lemma 2.10.5 that SUBSEQ(L) is the union of two reg-\\nular languages. Therefore, by Theorem 2.3.1, SUBSEQ(L) is a regular\\nlanguage.\\n• Since SUBSEQ(L) is regular, it follows from Theorem 2.6.4 that the\\nlanguage SUBSEQ(L) is regular as well.\\nExercises\\n2.1 For each of the following languages, construct a DFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : the length of w is divisible by three}\\n82\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. {w : 110 is not a substring of w}\\n3. {w : w contains at least ﬁve 1s}\\n4. {w : w contains the substring 1011}\\n5. {w : w contains at least two 1s and at most two 0s}\\n6. {w : w contains an odd number of 1s or exactly two 0s}\\n7. {w : w begins with 1 and ends with 0}\\n8. {w : every odd position in w is 1}\\n9. {w : w has length at least 3 and its third symbol is 0}\\n10. {ϵ, 0}\\n2.2 For each of the following languages, construct an NFA, with the speciﬁed\\nnumber of states, that accepts the language. In all cases, the alphabet is\\n{0, 1}.\\n1. The language {w : w ends with 10} with three states.\\n2. The language {w : w contains the substring 1011} with ﬁve states.\\n3. The language {w : w contains an odd number of 1s or exactly two 0s}\\nwith six states.\\n2.3 For each of the following languages, construct an NFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : w contains the substring 11001}\\n2. {w : w has length at least 2 and does not end with 10}\\n3. {w : w begins with 1 or ends with 0}\\n2.4 Convert the following NFA to an equivalent DFA.\\nExercises\\n83\\n1\\n2\\na\\nb\\na, b\\n2.5 Convert the following NFA to an equivalent DFA.\\n1\\n3\\n2\\na\\na\\nb\\na\\nε,b\\n2.6 Convert the following NFA to an equivalent DFA.\\n0\\n1\\n2\\n3\\na, ǫ\\nb\\na\\nǫ\\nb\\n2.7 In the proof of Theorem 2.6.3, we introduced a new start state q0, which\\nis also an accept state. Explain why the following is not a valid proof of\\nTheorem 2.6.3:\\nLet N = (Q1, Σ, δ1, q1, F1) be an NFA, such that A = L(N). Deﬁne the\\nNFA M = (Q1, Σ, δ, q1, F), where\\n84\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. F = {q1} ∪F1.\\n2. δ : Q1 × Σϵ →P(Q1) is deﬁned as follows: For any r ∈Q1 and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ.\\nThen L(M) = A∗.\\n2.8 Prove Theorem 2.6.4.\\n2.9 Let A be a language over the alphabet Σ = {0, 1} and let A be the\\ncomplement of A. Thus, A is the language consisting of all binary strings\\nthat are not in A.\\nAssume that A is a regular language. Let M = (Q, Σ, δ, q, F) be a non-\\ndeterministic ﬁnite automaton (NFA) that accepts A.\\nConsider the NFA N = (Q, Σ, δ, q, F), where F = Q\\\\F is the complement\\nof F. Thus, N is obtained from M by turning all accept states into nonaccept\\nstates, and turning all nonaccept states into accept states.\\n1. Is it true that the language accepted by N is equal to A?\\n2. Assume now that M is a deterministic ﬁnite automaton (DFA) that\\naccepts A. Deﬁne N as above; thus, turn all accept states into nonac-\\ncept states, and turn all nonaccept states into accept states. Is it true\\nthat the language accepted by N is equal to A?\\n2.10 Recall the alternative deﬁnition for the star of a language A that we\\ngave just before Theorem 2.3.1.\\nIn Theorems 2.3.1 and 2.6.2, we have shown that the class of regular\\nlanguages is closed under the union and concatenation operations.\\nSince\\nA∗= S∞\\nk=0 Ak, why doesn’t this imply that the class of regular languages is\\nclosed under the star operation?\\n2.11 Let A and B be two regular languages over the same alphabet Σ. Prove\\nthat the diﬀerence of A and B, i.e., the language\\nA \\\\ B = {w : w ∈A and w ̸∈B}\\nis a regular language.\\nExercises\\n85\\n2.12 For each of the following regular expressions, give two strings that are\\nmembers and two strings that are not members of the language described by\\nthe expression. The alphabet is Σ = {a, b}.\\n1. a(ba)∗b.\\n2. (a ∪b)∗a(a ∪b)∗b(a ∪b)∗a(a ∪b)∗.\\n3. (a ∪ba ∪bb)(a ∪b)∗.\\n2.13 Give regular expressions describing the following languages.\\nIn all\\ncases, the alphabet is {0, 1}.\\n1. {w : w contains at least three 1s}.\\n2. {w : w contains at least two 1s and at most one 0},\\n3. {w : w contains an even number of 0s and exactly two 1s}.\\n4. {w : w contains exactly two 0s and at least two 1s}.\\n5. {w : w contains an even number of 0s and each 0 is followed by at least one 1}.\\n6. {w : every odd position in w is 1}.\\n2.14 Convert each of the following regular expressions to an NFA.\\n1. (0 ∪1)∗000(0 ∪1)∗\\n2. (((10)∗(00)) ∪10)∗\\n3. ((0 ∪1)(11)∗∪0)∗\\n2.15 Convert the following DFA to a regular expression.\\n86\\nChapter 2.\\nFinite Automata and Regular Languages\\n1\\n2\\n3\\na\\na\\nb\\nb\\na\\nb\\n2.16 Convert the following DFA to a regular expression.\\n1\\n2\\n3\\na, b\\na\\na\\nb\\nb\\n2.17 Convert the following DFA to a regular expression.\\na, b\\n2.18\\n1. Let A be a non-empty regular language. Prove that there exists\\nan NFA that accepts A and that has exactly one accept state.\\nExercises\\n87\\n2. For any string w = w1w2 . . . wn, we denote by wR the string obtained\\nby reading w backwards, i.e., wR = wnwn−1 . . . w2w1. For any language\\nA, we deﬁne AR to be the language obtained by reading all strings in\\nA backwards, i.e.,\\nAR = {wR : w ∈A}.\\nLet A be a non-empty regular language. Prove that the language AR\\nis also regular.\\n2.19 If n ≥1 is an integer and w = a1a2 . . . an is a string, then for any i\\nwith 0 ≤i < n, the string a1a2 . . . ai is called a proper preﬁx of w. (If i = 0,\\nthen a1a2 . . . ai = ϵ.)\\nFor any language L, we deﬁne MIN(L) to be the language\\nMIN(L) = {w ∈L : no proper preﬁx of w belongs to L}.\\nProve the following claim: If L is a regular language, then MIN(L) is regular\\nas well.\\n2.20 Use the pumping lemma to prove that the following languages are not\\nregular.\\n1. {anbmcn+m : n ≥0, m ≥0}.\\n2. {anbnc2n : n ≥0}.\\n3. {anbman : n ≥0, m ≥0}.\\n4. {a2n : n ≥0}. (Remark: a2n is the string consisting of 2n many a’s.)\\n5. {anbmck : n ≥0, m ≥0, k ≥0, n2 + m2 = k2}.\\n6. {uvu : u ∈{a, b}∗, u ̸= ϵ, v ∈{a, b}∗}.\\n2.21 Prove that the language\\n{ambn : m ≥0, n ≥0, m ̸= n}\\nis not regular. (Using the pumping lemma for this one is a bit tricky. You\\ncan avoid using the pumping lemma by combining results about the closure\\nunder regular operations.)\\n88\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.22\\n1. Give an example of a regular language A and a non-regular lan-\\nguage B for which A ⊆B.\\n2. Give an example of a non-regular language A and a regular language\\nB for which A ⊆B.\\n2.23 Let A be a language consisting of ﬁnitely many strings.\\n1. Prove that A is a regular language.\\n2. Let n be the maximum length of any string in A. Prove that every\\ndeterministic ﬁnite automaton (DFA) that accepts A has at least n+1\\nstates. (Hint: How is the pumping length chosen in the proof of the\\npumping lemma?)\\n2.24 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L ̸= ∅if and only\\nif L contains a string of length less than p.\\n2.25 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L is an inﬁnite\\nlanguage if and only if L contains a string w with p ≤|w| ≤2p −1.\\n2.26 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. We deﬁne a binary relation RL on Σ∗× Σ∗, in the following way:\\nFor any two strings u and u′ in Σ∗,\\nuRLu′ if and only if (∀v ∈Σ∗: uv ∈L ⇔u′v ∈L) .\\nProve that RL is an equivalence relation.\\n2.27 Let Σ = {0, 1}, let\\nL = {w ∈Σ∗: |w| is odd},\\nand consider the relation RL deﬁned in Exercise 2.26.\\n1. Prove that for any two strings u and u′ in Σ∗,\\nuRLu′ ⇔|u| −|u′| is even.\\nExercises\\n89\\n2. Determine all equivalence classes of the relation RL.\\n2.28 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. Recall the equivalence relation RL that was deﬁned in Exercise 2.26.\\n1. Assume that L is a regular language, and let M = (Q, Σ, δ, q0, F) be\\na DFA that accepts L. Let u and u′ be strings in Σ∗. Let q be the\\nstate reached, when following the path in the state diagram of M, that\\nstarts in q0 and that is obtained by reading the string u. Similarly, let\\nq′ be the state reached, when following the path in the state diagram\\nof M, that starts in q0 and that is obtained by reading the string u′.\\nProve the following: If q = q′, then uRLu′.\\n2. Prove the following claim: If L is a regular language, then the equiva-\\nlence relation RL has a ﬁnite number of equivalence classes.\\n2.29 Let L be the language deﬁned by\\nL = {uuR : u ∈{0, 1}∗}.\\nIn words, a string is in L if and only if its length is even, and the second half\\nis the reverse of the ﬁrst half. Consider the equivalence relation RL that was\\ndeﬁned in Exercise 2.26.\\n1. Let m and n be two distinct positive integers and consider the two\\nstrings u = 0m1 and u′ = 0n1. Prove that ¬(uRLu′).\\n2. Prove that L is not a regular language, without using the pumping\\nlemma.\\n3. Use the pumping lemma to prove that L is not a regular language.\\n2.30 In this exercise, we will show that the converse of the pumping lemma\\ndoes, in general, not hold. Consider the language\\nA = {ambncn : m ≥1, n ≥0} ∪{bnck : n ≥0, k ≥0}.\\n1. Show that A satisﬁes the conclusion of the pumping lemma for p = 1.\\nThus, show that every string s in A whose length is at least p can be\\nwritten as s = xyz, such that y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all\\ni ≥0.\\n90\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. Consider the equivalence relation RA that was deﬁned in Exercise 2.26.\\nLet n and n′ be two distinct non-negative integers and consider the two\\nstrings u = abn and u′ = abn′. Prove that ¬(uRAu′).\\n3. Prove that A is not a regular language.\\nChapter 3\\nContext-Free Languages\\nIn this chapter, we introduce the class of context-free languages.\\nAs we\\nwill see, this class contains all regular languages, as well as some nonregular\\nlanguages such as {0n1n : n ≥0}.\\nThe class of context-free languages consists of languages that have some\\nsort of recursive structure. We will see two equivalent methods to obtain this\\nclass. We start with context-free grammars, which are used for deﬁning the\\nsyntax of programming languages and their compilation. Then we introduce\\nthe notion of (nondeterministic) pushdown automata, and show that these\\nautomata have the same power as context-free grammars.\\n3.1\\nContext-free grammars\\nWe start with an example. Consider the following ﬁve (substitution) rules:\\nS\\n→\\nAB\\nA\\n→\\na\\nA\\n→\\naA\\nB\\n→\\nb\\nB\\n→\\nbB\\nHere, S, A, and B are variables, S is the start variable, and a and b are\\nterminals. We use these rules to derive strings consisting of terminals (i.e.,\\nelements of {a, b}∗), in the following manner:\\n1. Initialize the current string to be the string consisting of the start\\nvariable S.\\n92\\nChapter 3.\\nContext-Free Languages\\n2. Take any variable in the current string and take any rule that has this\\nvariable on the left-hand side. Then, in the current string, replace this\\nvariable by the right-hand side of the rule.\\n3. Repeat 2. until the current string only contains terminals.\\nFor example, the string aaaabb can be derived in the following way:\\nS\\n⇒\\nAB\\n⇒\\naAB\\n⇒\\naAbB\\n⇒\\naaAbB\\n⇒\\naaaAbB\\n⇒\\naaaabB\\n⇒\\naaaabb\\nThis derivation can also be represented using a parse tree, as in the ﬁgure\\nbelow:\\nS\\nA\\nA\\nA\\nA\\na\\na\\na\\na\\nb\\nb\\nB\\nB\\nThe ﬁve rules in this example constitute a context-free grammar. The\\nlanguage of this grammar is the set of all strings that\\n3.1.\\nContext-free grammars\\n93\\n• can be derived from the start variable and\\n• only contain terminals.\\nFor this example, the language is\\n{ambn : m ≥1, n ≥1},\\nbecause every string of the form ambn, for some m ≥1 and n ≥1, can be\\nderived from the start variable, whereas no other string over the alphabet\\n{a, b} can be derived from the start variable.\\nDeﬁnition 3.1.1 A context-free grammar is a 4-tuple G = (V, Σ, R, S),\\nwhere\\n1. V is a ﬁnite set, whose elements are called variables,\\n2. Σ is a ﬁnite set, whose elements are called terminals,\\n3. V ∩Σ = ∅,\\n4. S is an element of V ; it is called the start variable,\\n5. R is a ﬁnite set, whose elements are called rules. Each rule has the\\nform A →w, where A ∈V and w ∈(V ∪Σ)∗.\\nIn our example, we have V = {S, A, B}, Σ = {a, b}, and\\nR = {S →AB, A →a, A →aA, B →b, B →bB}.\\nDeﬁnition 3.1.2 Let G = (V, Σ, R, S) be a context-free grammar. Let A be\\nan element in V and let u, v, and w be strings in (V ∪Σ)∗such that A →w\\nis a rule in R. We say that the string uwv can be derived in one step from\\nthe string uAv, and write this as\\nuAv ⇒uwv.\\nIn other words, by applying the rule A →w to the string uAv, we obtain\\nthe string uwv. In our example, we see that aaAbb ⇒aaaAbb.\\nDeﬁnition 3.1.3 Let G = (V, Σ, R, S) be a context-free grammar. Let u\\nand v be strings in (V ∪Σ)∗. We say that v can be derived from u, and write\\nthis as u\\n∗⇒v, if one of the following two conditions holds:\\n94\\nChapter 3.\\nContext-Free Languages\\n1. u = v or\\n2. there exist an integer k ≥2 and a sequence u1, u2, . . . , uk of strings in\\n(V ∪Σ)∗, such that\\n(a) u = u1,\\n(b) v = uk, and\\n(c) u1 ⇒u2 ⇒. . . ⇒uk.\\nIn other words, by starting with the string u and applying rules zero or\\nmore times, we obtain the string v. In our example, we see that aaAbB\\n∗⇒\\naaaabbbB.\\nDeﬁnition 3.1.4 Let G = (V, Σ, R, S) be a context-free grammar.\\nThe\\nlanguage of G is deﬁned to be the set of all strings in Σ∗that can be derived\\nfrom the start variable S:\\nL(G) = {w ∈Σ∗: S\\n∗⇒w}.\\nDeﬁnition 3.1.5 A language L is called context-free, if there exists a context-\\nfree grammar G such that L(G) = L.\\n3.2\\nExamples of context-free grammars\\n3.2.1\\nProperly nested parentheses\\nConsider the context-free grammar G = (V, Σ, R, S), where V = {S}, Σ =\\n{a, b}, and\\nR = {S →ϵ, S →aSb, S →SS}.\\nWe write the three rules in R as\\nS →ϵ|aSb|SS,\\nwhere you can think of “|” as being a short-hand for “or”.\\n3.2.\\nExamples of context-free grammars\\n95\\nBy applying the rules in R, starting with the start variable S, we obtain,\\nfor example,\\nS\\n⇒\\nSS\\n⇒\\naSbS\\n⇒\\naSbSS\\n⇒\\naSSbSS\\n⇒\\naaSbSbSS\\n⇒\\naabSbSS\\n⇒\\naabbSS\\n⇒\\naabbaSbS\\n⇒\\naabbabS\\n⇒\\naabbabaSb\\n⇒\\naabbabab\\nWhat is the language L(G) of this context-free grammar G? If we think\\nof a as being a left-parenthesis “(”, and of b as being a right-parenthesis “)”,\\nthen L(G) is the language consisting of all strings of properly nested paren-\\ntheses. Here is the explanation: Any string of properly nested parentheses is\\neither\\n• empty (which we derive from S by the rule S →ϵ),\\n• consists of a left-parenthesis, followed by an arbitrary string of properly\\nnested parentheses, followed by a right-parenthesis (these are derived\\nfrom S by ﬁrst applying the rule S →aSb), or\\n• consists of an arbitrary string of properly nested parentheses, followed\\nby an arbitrary string of properly nested parentheses (these are derived\\nfrom S by ﬁrst applying the rule S →SS).\\n3.2.2\\nA context-free grammar for a nonregular lan-\\nguage\\nConsider the language L1 = {0n1n : n ≥0}. We have seen in Section 2.9.1\\nthat L1 is not a regular language. We claim that L1 is a context-free language.\\n96\\nChapter 3.\\nContext-Free Languages\\nIn order to prove this claim, we have to construct a context-free grammar\\nG1 such that L(G1) = L1.\\nObserve that any string in L1 is either\\n• empty or\\n• consists of a 0, followed by an arbitrary string in L1, followed by a 1.\\nThis leads to the context-free grammar G1 = (V1, Σ, R1, S1), where V1 =\\n{S1}, Σ = {0, 1}, and R1 consists of the rules\\nS1 →ϵ|0S11.\\nHence, R1 = {S1 →ϵ, S1 →0S11}.\\nTo derive the string 0n1n from the start variable S1, we do the following:\\n• Starting with S1, apply the rule S1 →0S11 exactly n times. This gives\\nthe string 0nS11n.\\n• Apply the rule S1 →ϵ. This gives the string 0n1n.\\nIt is not diﬃcult to see that these are the only strings that can be derived\\nfrom the start variable S1. Thus, L(G1) = L1.\\nIn a symmetric way, we see that the context-free grammar G2 = (V2, Σ, R2, S2),\\nwhere V2 = {S2}, Σ = {0, 1}, and R2 consists of the rules\\nS2 →ϵ|1S20,\\nhas the property that L(G2) = L2, where L2 = {1n0n : n ≥0}. Thus, L2 is\\na context-free language.\\nDeﬁne L = L1 ∪L2, i.e.,\\nL = {0n1n : n ≥0} ∪{1n0n : n ≥0}.\\nThe context-free grammar G = (V, Σ, R, S), where V = {S, S1, S2}, Σ =\\n{0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2\\nS1\\n→\\nϵ|0S11\\nS2\\n→\\nϵ|1S20,\\nhas the property that L(G) = L. Hence, L is a context-free language.\\n3.2.\\nExamples of context-free grammars\\n97\\n3.2.3\\nA context-free grammar for the complement of\\na nonregular language\\nLet L be the (nonregular) language L = {0n1n : n ≥0}. We want to prove\\nthat the complement L of L is a context-free language. Hence, we want to\\nconstruct a context-free grammar G whose language is equal to L. Observe\\nthat a binary string w is in L if and only if\\n1. w = 0m1n, for some integers m and n with 0 ≤m < n, or\\n2. w = 0m1n, for some integers m and n with 0 ≤n < m, or\\n3. w contains 10 as a substring.\\nThus, we can write L as the union of the languages of all strings of type 1.,\\ntype 2., and type 3.\\nAny string of type 1. is either\\n• the string 1,\\n• consists of a string of type 1., followed by one 1, or\\n• consists of one 0, followed by an arbitrary string of type 1., followed by\\none 1.\\nThus, using the rules\\nS1 →1|S11|0S11,\\nwe can derive, from S1, all strings of type 1.\\nSimilarly, using the rules\\nS2 →0|0S2|0S21,\\nwe can derive, from S2, all strings of type 2.\\nAny string of type 3.\\n• consists of an arbitrary binary string, followed by the string 10, followed\\nby an arbitrary binary string.\\nUsing the rules\\nX →ϵ|0X|1X,\\n98\\nChapter 3.\\nContext-Free Languages\\nwe can derive, from X, all binary strings. Thus, by combining these with\\nthe rule\\nS3 →X10X,\\nwe can derive, from S3, all strings of type 3.\\nWe arrive at the context-free grammar G = (V, Σ, R, S), where V =\\n{S, S1, S2, S3, X}, Σ = {0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2|S3\\nS1\\n→\\n1|S11|0S11\\nS2\\n→\\n0|0S2|0S21\\nS3\\n→\\nX10X\\nX\\n→\\nϵ|0X|1X\\nTo summarize, we have\\nS1\\n∗⇒0m1n, for all integers m and n with 0 ≤m < n,\\nS2\\n∗⇒0m1n, for all integers m and n with 0 ≤n < m,\\nX\\n∗⇒u, for each string u in {0, 1}∗,\\nand\\nS3\\n∗⇒w, for every binary string w that contains 10 as a substring.\\nFrom these observations, it follows that that L(G) = L.\\n3.2.4\\nA context-free grammar that veriﬁes addition\\nConsider the language\\nL = {anbmcn+m : n ≥0, m ≥0}.\\nUsing the pumping lemma for regular languages (Theorem 2.9.1), it can\\nbe shown that L is not a regular language. We will construct a context-\\nfree grammar G whose language is equal to L, thereby proving that L is a\\ncontext-free language.\\nFirst observe that ϵ ∈L. Therefore, we will take S →ϵ to be one of the\\nrules in the grammar.\\nLet us see how we can derive all strings in L from the start variable S:\\n3.3.\\nRegular languages are context-free\\n99\\n1. Every time we add an a, we also add a c. In this way, we obtain all\\nstrings of the form ancn, where n ≥0.\\n2. Given a string of the form ancn, we start adding bs. Every time we add\\na b, we also add a c. Observe that every b has to be added between\\nthe as and the cs. Therefore, we use a variable B as a “pointer” to\\nthe position in the current string where a b can be added: Instead of\\nderiving ancn from S, we derive the string anBcn. Then, from B, we\\nderive all strings of the form bmcm, where m ≥0.\\nWe obtain the context-free grammar G = (V, Σ, R, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R consists of the rules\\nS\\n→\\nϵ|A\\nA\\n→\\nϵ|aAc|B\\nB\\n→\\nϵ|bBc\\nThe facts that\\n• A\\n∗⇒anBcn, for every n ≥0,\\n• B\\n∗⇒bmcm, for every m ≥0,\\nimply that the following strings can be derived from the start variable S:\\n• S\\n∗⇒anBcn\\n∗⇒anbmcmcn = anbmcn+m, for all n ≥0 and m ≥0.\\nIn fact, no other strings in {a, b, c}∗can be derived from S. Therefore, we\\nhave L(G) = L. Since\\nS ⇒A ⇒B ⇒ϵ,\\nwe can simplify this grammar G, by eliminating the rules S →ϵ and A →ϵ.\\nThis gives the context-free grammar G′ = (V, Σ, R′, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R′ consists of the rules\\nS\\n→\\nA\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\nFinally, observe that we do not need S; instead, we can use A as start\\nvariable. This gives our ﬁnal context-free grammar G′′ = (V, Σ, R′′, A), where\\nV = {A, B}, Σ = {a, b, c}, and R′′ consists of the rules\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\n100\\nChapter 3.\\nContext-Free Languages\\n3.3\\nRegular languages are context-free\\nWe mentioned already that the class of context-free languages includes the\\nclass of regular languages. In this section, we will prove this claim.\\nTheorem 3.3.1 Let Σ be an alphabet and let L ⊆Σ∗be a regular language.\\nThen L is a context-free language.\\nProof.\\nSince L is a regular language, there exists a deterministic ﬁnite\\nautomaton M = (Q, Σ, δ, q, F) that accepts L.\\nTo prove that L is context-free, we have to deﬁne a context-free grammar\\nG = (V, Σ, R, S), such that L = L(M) = L(G). Thus, G must have the\\nfollowing property: For every string w ∈Σ∗,\\nw ∈L(M) if and only if w ∈L(G),\\nwhich can be reformulated as\\nM accepts w if and only if S\\n∗⇒w.\\nWe will deﬁne the context-free grammar G in such a way that the following\\ncorrespondence holds for any string w = w1w2 . . . wn:\\n• Assume that M is in state A just after it has read the substring\\nw1w2 . . . wi.\\n• Then in the context-free grammar G, we have S\\n∗⇒w1w2 . . . wiA.\\nIn the next step, M reads the symbol wi+1 and switches from state A to,\\nsay, state B; thus, δ(A, wi+1) = B. In order to guarantee that the above\\ncorrespondence still holds, we have to add the rule A →wi+1B to G.\\nConsider the moment when M has read the entire string w. Let A be the\\nstate M is in at that moment. By the above correspondence, we have\\nS\\n∗⇒w1w2 . . . wnA = wA.\\nRecall that G must have the property that\\nM accepts w if and only if S\\n∗⇒w,\\nwhich is equivalent to\\nA ∈F if and only if S\\n∗⇒w.\\n3.3.\\nRegular languages are context-free\\n101\\nWe guarantee this property by adding to G the rule A →ϵ for every accept\\nstate A of M.\\nWe are now ready to give the formal deﬁnition of the context-free gram-\\nmar G = (V, Σ, R, S):\\n• V = Q, i.e., the variables of G are the states of M.\\n• S = q, i.e., the start variable of G is the start state of M.\\n• R consists of the rules\\nA →aB, where A ∈Q, a ∈Σ, B ∈Q, and δ(A, a) = B,\\nand\\nA →ϵ, where A ∈F.\\nIn words,\\n• every transition δ(A, a) = B of M (i.e., when M is in the state A and\\nreads the symbol a, it switches to the state B) corresponds to a rule\\nA →aB in the grammar G,\\n• every accept state A of M corresponds to a rule A →ϵ in the grammar\\nG.\\nWe claim that L(G) = L. In order to prove this, we have to show that\\nL(G) ⊆L and L ⊆L(G).\\nWe prove that L ⊆L(G). Let w = w1w2 . . . wn be an arbitrary string\\nin L. When the ﬁnite automaton M reads the string w, it visits the states\\nr0, r1, . . . , rn, where\\n• r0 = q, and\\n• ri+1 = δ(ri, wi+1) for i = 0, 1, . . . , n −1.\\nSince w ∈L = L(M), we know that rn ∈F.\\nIt follows from the way we deﬁned the grammar G that\\n• for each i = 0, 1, . . . , n −1, ri →wi+1ri+1 is a rule in R, and\\n• rn →ϵ is a rule in R.\\n102\\nChapter 3.\\nContext-Free Languages\\nTherefore, we have\\nS = q = r0 ⇒w1r1 ⇒w1w2r2 ⇒. . . ⇒w1w2 . . . wnrn ⇒w1w2 . . . wn = w.\\nThis proves that w ∈L(G).\\nThe proof of the claim that L(G) ⊆L is left as an exercise.\\nIn Sections 2.9.1 and 3.2.2, we have seen that the language {0n1n : n ≥\\n0} is not regular, but context-free. Therefore, the class of all context-free\\nlanguages properly contains the class of regular languages.\\n3.3.1\\nAn example\\nLet L be the language deﬁned as\\nL = {w ∈{0, 1}∗: 101 is a substring of w}.\\nIn Section 2.2.2, we have seen that L is a regular language. In that section,\\nwe constructed the following deterministic ﬁnite automaton M that accepts\\nL (we have renamed the states):\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nS\\nA\\nB\\nC\\nWe apply the construction given in the proof of Theorem 3.3.1 to convert\\nM to a context-free grammar G whose language is equal to L. According\\nto this construction, we have G = (V, Σ, R, S), where V = {S, A, B, C},\\nΣ = {0, 1}, the start variable S is the start state of M, and R consists of the\\nrules\\nS\\n→\\n0S|1A\\nA\\n→\\n0B|1A\\nB\\n→\\n0S|1C\\nC\\n→\\n0C|1C|ϵ\\n3.4.\\nChomsky normal form\\n103\\nConsider the string 010011011, which is an element of L. When the ﬁnite\\nautomaton M reads this string, it visits the states\\nS, S, A, B, S, A, A, B, C, C.\\nIn the grammar G, this corresponds to the derivation\\nS\\n⇒\\n0S\\n⇒\\n01A\\n⇒\\n010B\\n⇒\\n0100S\\n⇒\\n01001A\\n⇒\\n010011A\\n⇒\\n0100110B\\n⇒\\n01001101C\\n⇒\\n010011011C\\n⇒\\n010011011.\\nHence,\\nS\\n∗⇒010011011,\\nimplying that the string 010011011 is in the language L(G) of the context-free\\ngrammar G.\\nThe string 10011 is not in the language L. When the ﬁnite automaton\\nM reads this string, it visits the states\\nS, A, B, S, A, A,\\ni.e., after the string has been read, M is in the non-accept state A. In the\\ngrammar G, reading the string 10011 corresponds to the derivation\\nS\\n⇒\\n1A\\n⇒\\n10B\\n⇒\\n100S\\n⇒\\n1001A\\n⇒\\n10011A.\\nSince A is not an accept state in M, the grammar G does not contain the\\nrule A →ϵ. This implies that the string 10011 cannot be derived from the\\nstart variable S. Thus, 10011 is not in the language L(G) of G.\\n104\\nChapter 3.\\nContext-Free Languages\\n3.4\\nChomsky normal form\\nThe rules in a context-free grammar G = (V, Σ, R, S) are of the form\\nA →w,\\nwhere A is a variable and w is a string over the alphabet V ∪Σ. In this\\nsection, we show that every context-free grammar G can be converted to a\\ncontext-free grammar G′, such that L(G) = L(G′), and the rules of G′ are of\\na restricted form, as speciﬁed in the following deﬁnition:\\nDeﬁnition 3.4.1 A context-free grammar G = (V, Σ, R, S) is said to be in\\nChomsky normal form, if every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are elements of V , B ̸= S, and C ̸= S.\\n2. A →a, where A is an element of V and a is an element of Σ.\\n3. S →ϵ, where S is the start variable.\\nYou should convince yourself that, for such a grammar, R contains the\\nrule S →ϵ if and only if ϵ ∈L(G).\\nTheorem 3.4.2 Let Σ be an alphabet and let L ⊆Σ∗be a context-free lan-\\nguage. There exists a context-free grammar in Chomsky normal form, whose\\nlanguage is L.\\nProof. Since L is a context-free language, there exists a context-free gram-\\nmar G = (V, Σ, R, S), such that L(G) = L. We will transform G into a\\ngrammar that is in Chomsky normal form and whose language is equal to\\nL(G). The transformation consists of ﬁve steps.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe deﬁne G1 = (V1, Σ, R1, S1), where S1 is the start variable (which is a\\nnew variable), V1 = V ∪{S1}, and R1 = R ∪{S1 →S}. This grammar has\\nthe property that\\n• the start variable S1 does not occur on the right-hand side of any rule\\nin R1, and\\n• L(G1) = L(G).\\n3.4.\\nChomsky normal form\\n105\\nStep 2: An ϵ-rule is a rule that is of the form A →ϵ, where A is a variable\\nthat is not equal to the start variable. In the second step, we eliminate all\\nϵ-rules from G1.\\nWe consider all ϵ-rules, one after another. Let A →ϵ be one such rule,\\nwhere A ∈V1 and A ̸= S1. We modify G1 as follows:\\n1. Remove the rule A →ϵ from the current set R1.\\n2. For each rule in the current set R1 that is of the form\\n(a) B →A, add the rule B →ϵ to R1, unless this rule has already\\nbeen deleted from R1; observe that in this way, we replace the two-\\nstep derivation B ⇒A ⇒ϵ by the one-step derivation B ⇒ϵ;\\n(b) B →uAv (where u and v are strings that are not both empty),\\nadd the rule B →uv to R1; observe that in this way, we replace\\nthe two-step derivation B ⇒uAv ⇒uv by the one-step derivation\\nB ⇒uv;\\n(c) B →uAvAw (where u, v, and w are strings), add the rules B →\\nuvw, B →uAvw, and B →uvAw to R1; if u = v = w = ϵ and\\nthe rule B →ϵ has already been deleted from R1, then we do not\\nadd the rule B →ϵ;\\n(d) treat rules in which A occurs more than twice on the right-hand\\nside in a similar fashion.\\nWe repeat this process until all ϵ-rules have been eliminated.\\nLet R2\\nbe the set of rules, after all ϵ-rules have been eliminated. We deﬁne G2 =\\n(V2, Σ, R2, S2), where V2 = V1 and S2 = S1. This grammar has the property\\nthat\\n• the start variable S2 does not occur on the right-hand side of any rule\\nin R2,\\n• R2 does not contain any ϵ-rule (it may contain the rule S2 →ϵ), and\\n• L(G2) = L(G1) = L(G).\\nStep 3: A unit-rule is a rule that is of the form A →B, where A and B are\\nvariables. In the third step, we eliminate all unit-rules from G2.\\n106\\nChapter 3.\\nContext-Free Languages\\nWe consider all unit-rules, one after another. Let A →B be one such\\nrule, where A and B are elements of V2. We know that B ̸= S2. We modify\\nG2 as follows:\\n1. Remove the rule A →B from the current set R2.\\n2. For each rule in the current set R2 that is of the form B →u, where\\nu ∈(V2 ∪Σ)∗, add the rule A →u to the current set R2, unless this is\\na unit-rule that has already been eliminated.\\nObserve that in this way, we replace the two-step derivation A ⇒B ⇒\\nu by the one-step derivation A ⇒u.\\nWe repeat this process until all unit-rules have been eliminated.\\nLet\\nR3 be the set of rules, after all unit-rules have been eliminated. We deﬁne\\nG3 = (V3, Σ, R3, S3), where V3 = V2 and S3 = S2. This grammar has the\\nproperty that\\n• the start variable S3 does not occur on the right-hand side of any rule\\nin R3,\\n• R3 does not contain any ϵ-rule (it may contain the rule S3 →ϵ),\\n• R3 does not contain any unit-rule, and\\n• L(G3) = L(G2) = L(G1) = L(G).\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside.\\nFor each rule in the current set R3 that is of the form A →u1u2 . . . uk,\\nwhere k ≥3 and each ui is an element of V3 ∪Σ, we modify G3 as follows:\\n1. Remove the rule A →u1u2 . . . uk from the current set R3.\\n2. Add the following rules to the current set R3:\\nA\\n→\\nu1A1\\nA1\\n→\\nu2A2\\nA2\\n→\\nu3A3\\n...\\nAk−3\\n→\\nuk−2Ak−2\\nAk−2\\n→\\nuk−1uk\\n3.4.\\nChomsky normal form\\n107\\nwhere A1, A2, . . . , Ak−2 are new variables that are added to the current\\nset V3.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 . . . uk by the (k −1)-step derivation\\nA ⇒u1A1 ⇒u1u2A2 ⇒. . . ⇒u1u2 . . . uk−2Ak−2 ⇒u1u2 . . . uk.\\nLet R4 be the set of rules, and let V4 be the set of variables, after all rules\\nwith more than two symbols on the right-hand side have been eliminated. We\\ndeﬁne G4 = (V4, Σ, R4, S4), where S4 = S3. This grammar has the property\\nthat\\n• the start variable S4 does not occur on the right-hand side of any rule\\nin R4,\\n• R4 does not contain any ϵ-rule (it may contain the rule S4 →ϵ),\\n• R4 does not contain any unit-rule,\\n• R4 does not contain any rule with more than two symbols on the right-\\nhand side, and\\n• L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nStep 5: Eliminate all rules of the form A →u1u2, where u1 and u2 are not\\nboth variables.\\nFor each rule in the current set R4 that is of the form A →u1u2, where\\nu1 and u2 are elements of V4 ∪Σ, but u1 and u2 are not both contained in\\nV4, we modify G3 as follows:\\n1. If u1 ∈Σ and u2 ∈V4, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →U1u2 and U1 →u1, where U1 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒U1u2 ⇒u1u2.\\n2. If u1 ∈V4 and u2 ∈Σ, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →u1U2 and U2 →u2, where U2 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒u1U2 ⇒u1u2.\\n108\\nChapter 3.\\nContext-Free Languages\\n3. If u1 ∈Σ, u2 ∈Σ, and u1 ̸= u2, then replace the rule A →u1u2 in the\\ncurrent set R4 by the three rules A →U1U2, U1 →u1, and U2 →u2,\\nwhere U1 and U2 are new variables that are added to the current set\\nV4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the three-step derivation A ⇒U1U2 ⇒u1U2 ⇒u1u2.\\n4. If u1 ∈Σ, u2 ∈Σ, and u1 = u2, then replace the rule A →u1u2 = u1u1\\nin the current set R4 by the two rules A →U1U1 and U1 →u1, where\\nU1 is a new variable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 = u1u1 by the three-step derivation A ⇒U1U1 ⇒u1U1 ⇒u1u1.\\nLet R5 be the set of rules, and let V5 be the set of variables, after Step 5\\nhas been completed. We deﬁne G5 = (V5, Σ, R5, S5), where S5 = S4. This\\ngrammar has the property that\\n• the start variable S5 does not occur on the right-hand side of any rule\\nin R5,\\n• R5 does not contain any ϵ-rule (it may contain the rule S5 →ϵ),\\n• R5 does not contain any unit-rule,\\n• R5 does not contain any rule with more than two symbols on the right-\\nhand side,\\n• R5 does not contain any rule of the form A →u1u2, where u1 and u2\\nare not both variables of V5, and\\n• L(G5) = L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nSince the grammar G5 is in Chomsky normal form, the proof is complete.\\n3.4.\\nChomsky normal form\\n109\\n3.4.1\\nAn example\\nConsider the context-free grammar G = (V, Σ, R, A), where V = {A, B},\\nΣ = {0, 1}, A is the start variable, and R consists of the rules\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nWe apply the construction given in the proof of Theorem 3.4.2 to convert\\nthis grammar to a context-free grammar in Chomsky normal form whose\\nlanguage is the same as that of G. Throughout the construction, upper case\\nletters will denote variables.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe introduce a new start variable S, and add the rule S →A. This gives\\nthe following grammar:\\nS\\n→\\nA\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nStep 2: Eliminate all ϵ-rules.\\nWe take the ϵ-rule A →ϵ, and remove it. Then we consider all rules that\\ncontain A on the right-hand side. There are two such rules:\\n• S →A; we add the rule S →ϵ;\\n• A →BAB; we add the rule A →BB.\\nThis gives the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB\\nB\\n→\\n00|ϵ\\nWe take the ϵ-rule B →ϵ, and remove it. Then we consider all rules that\\ncontain B on the right-hand side. There are three such rules:\\n• A →BAB; we add the rules A →AB, A →BA, and A →A;\\n• A →B; we do not add the rule A →ϵ, because it has already been\\nremoved;\\n110\\nChapter 3.\\nContext-Free Languages\\n• A →BB; we add the rule A →B, but not the rule A →ϵ (because it\\nhas already been removed).\\nAt this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA|A\\nB\\n→\\n00\\nSince all ϵ-rules have been eliminated, this completes Step 2. (Observe that\\nthe rule S →ϵ is allowed, because S is the start variable.)\\nStep 3: Eliminate all unit-rules.\\nWe take the unit-rule A →A. We can remove this rule, without adding\\nany new rule. At this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →A, remove it, and add the rules\\nS →BAB|B|BB|AB|BA.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BAB|B|BB|AB|BA\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →B, remove it, and add the rule S →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule A →B, remove it, and add the rule A →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|BB|AB|BA|00\\nB\\n→\\n00\\n3.5.\\nPushdown automata\\n111\\nSince all unit-rules have been eliminated, this concludes Step 3.\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside. There are two such rules:\\n• We take the rule S →BAB, remove it, and add the rules S →BA1\\nand A1 →AB.\\n• We take the rule A →BAB, remove it, and add the rules A →BA2\\nand A2 →AB.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BB|AB|BA|00|BA1\\nA\\n→\\nBB|AB|BA|00|BA2\\nB\\n→\\n00\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nStep 4 is now completed.\\nStep 5: Eliminate all rules, whose right-hand side contains exactly two\\nsymbols, which are not both variables. There are three such rules:\\n• We replace the rule S →00 by the rules S →A3A3 and A3 →0.\\n• We replace the rule A →00 by the rules A →A4A4 and A4 →0.\\n• We replace the rule B →00 by the rules B →A5A5 and A5 →0.\\nThis gives the following grammar, which is in Chomsky normal form:\\nS\\n→\\nϵ|BB|AB|BA|BA1|A3A3\\nA\\n→\\nBB|AB|BA|BA2|A4A4\\nB\\n→\\nA5A5\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nA3\\n→\\n0\\nA4\\n→\\n0\\nA5\\n→\\n0\\n112\\nChapter 3.\\nContext-Free Languages\\n3.5\\nPushdown automata\\nIn this section, we introduce nondeterministic pushdown automata. As we\\nwill see, the class of languages that can be accepted by these automata is\\nexactly the class of context-free languages.\\nWe start with an informal description of a deterministic pushdown au-\\ntomaton. Such an automaton consists of the following, see also Figure 3.1.\\n1. There is a tape which is divided into cells. Each cell stores a symbol\\nbelonging to a ﬁnite set Σ, called the tape alphabet. There is a special\\nsymbol 2 that is not contained in Σ; this symbol is called the blank\\nsymbol. If a cell contains 2, then this means that the cell is actually\\nempty.\\n2. There is a tape head which can move along the tape, one cell to the\\nright per move. This tape head can also read the cell it currently scans.\\n3. There is a stack containing symbols from a ﬁnite set Γ, called the stack\\nalphabet. This set contains a special symbol $.\\n4. There is a stack head which can read the top symbol of the stack. This\\nhead can also pop the top symbol, and it can push symbols of Γ onto\\nthe stack.\\n5. There is a state control, which can be in any one of a ﬁnite number\\nof states. The set of states is denoted by Q. The set Q contains one\\nspecial state q, called the start state.\\nThe input for a pushdown automaton is a string in Σ∗. This input string\\nis stored on the tape of the pushdown automaton and, initially, the tape head\\nis on the leftmost symbol of the input string. Initially, the stack only contains\\nthe special symbol $, and the pushdown automaton is in the start state q.\\nIn one computation step, the pushdown automaton does the following:\\n1. Assume that the pushdown automaton is currently in state r. Let a be\\nthe symbol of Σ that is read by the tape head, and let A be the symbol\\nof Γ that is on top of the stack.\\n2. Depending on the current state r, the tape symbol a, and the stack\\nsymbol A,\\n3.5.\\nPushdown automata\\n113\\nstate control\\na a b a b b a b a b 2\\ntape\\n6\\n$\\nA\\nA\\nB\\nA\\nstack\\n-\\nFigure 3.1: A pushdown automaton.\\n(a) the pushdown automaton switches to a state r′ of Q (which may\\nbe equal to r),\\n(b) the tape head either moves one cell to the right or stays at the\\ncurrent cell, and\\n(c) the top symbol A is replaced by a string w that belongs to Γ∗. To\\nbe more precise,\\ni. if w = ϵ, then A is popped from the stack, whereas\\nii. if w = B1B2 . . . Bk, with k ≥1 and B1, B2, . . . , Bk ∈Γ, then\\nA is replaced by w, and Bk becomes the new top symbol of\\nthe stack.\\nLater, we will specify when the pushdown automaton accepts the input\\nstring.\\nWe now give a formal deﬁnition of a deterministic pushdown automaton.\\nDeﬁnition 3.5.1 A deterministic pushdown automaton is a 5-tuple M =\\n(Σ, Γ, Q, δ, q), where\\n114\\nChapter 3.\\nContext-Free Languages\\n1. Σ is a ﬁnite set, called the tape alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the stack alphabet; this alphabet contains the\\nspecial symbol $,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. δ is called the transition function, which is a function\\nδ : Q × (Σ ∪{2}) × Γ →Q × {N, R} × Γ∗.\\nThe transition function δ can be thought of as being the “program” of the\\npushdown automaton. This function tells us what the automaton can do in\\none “computation step”: Let r ∈Q, a ∈Σ ∪{2}, and A ∈Γ. Furthermore,\\nlet r′ ∈Q, σ ∈{R, N}, and w ∈Γ∗be such that\\nδ(r, a, A) = (r′, σ, w).\\n(3.1)\\nThis transition means that if\\n• the pushdown automaton is in state r,\\n• the tape head reads the symbol a, and\\n• the top symbol on the stack is A,\\nthen\\n• the pushdown automaton switches to state r′,\\n• the tape head moves according to σ: if σ = R, then it moves one cell\\nto the right; if σ = N, then it does not move, and\\n• the top symbol A on the stack is replaced by the string w.\\nWe will write the computation step (3.1) in the form of the instruction\\nraA →r′σw.\\nWe now specify the computation of the pushdown automaton M = (Σ, Γ, Q, δ, q).\\n3.6.\\nExamples of pushdown automata\\n115\\nStart conﬁguration: Initially, the pushdown automaton is in the start state\\nq, the tape head is on the leftmost symbol of the input string a1a2 . . . an, and\\nthe stack only contains the special symbol $.\\nComputation and termination: Starting in the start conﬁguration, the\\npushdown automaton performs a sequence of computation steps as described\\nabove. It terminates at the moment when the stack becomes empty. (Hence,\\nif the stack never gets empty, the pushdown automaton does not terminate.)\\nAcceptance: The pushdown automaton accepts the input string a1a2 . . . an ∈\\nΣ∗, if\\n1. the automaton terminates on this input, and\\n2. at the time of termination (i.e., at the moment when the stack gets\\nempty), the tape head is on the cell immediately to the right of the cell\\ncontaining the symbol an (this cell must contain the blank symbol 2).\\nIn all other cases, the pushdown automaton rejects the input string. Thus,\\nthe pushdown automaton rejects this string if\\n1. the automaton does not terminate on this input (i.e., the computation\\n“loops forever”) or\\n2. at the time of termination, the tape head is not on the cell immediately\\nto the right of the cell containing the symbol an.\\nWe denote by L(M) the language accepted by the pushdown automaton\\nM. Thus,\\nL(M) = {w ∈Σ∗: M accepts w}.\\nThe pushdown automaton described above is deterministic. For a non-\\ndeterministic pushdown automata, the current computation step may not\\nbe uniquely deﬁned, but the automaton can make a choice out of a ﬁnite\\nnumber of possibilities. In this case, the transition function δ is a function\\nδ : Q × (Σ ∪{2}) × Γ →Pf(Q × {N, R} × Γ∗),\\nwhere Pf(K) is the set of all ﬁnite subsets of the set K.\\nWe say that a nondeterministic pushdown automaton M accepts an input\\nstring, if there exists an accepting computation, in the sense as described for\\ndeterministic pushdown automata. We say that M rejects an input string, if\\nevery computation on this string is rejecting. As before, we denote by L(M)\\nthe set of all strings in Σ∗that are accepted by M.\\n116\\nChapter 3.\\nContext-Free Languages\\n3.6\\nExamples of pushdown automata\\n3.6.1\\nProperly nested parentheses\\nWe will show how to construct a deterministic pushdown automaton, that\\naccepts the set of all strings of properly nested parentheses. Observe that a\\nstring w in {(, )}∗is properly nested if and only if\\n• in every preﬁx of w, the number of “(” is greater than or equal to the\\nnumber of “)”, and\\n• in the complete string w, the number of “(” is equal to the number of\\n“)”.\\nWe will use the tape symbol a for “(”, and the tape symbol b for “)”.\\nThe idea is as follows. Recall that initially, the stack only contains the\\nspecial symbol $. The pushdown automaton reads the input string from left\\nto right. For every a it reads, it pushes the symbol S onto the stack, and\\nfor every b it reads, it pops the top symbol from the stack. In this way, the\\nnumber of symbols S on the stack will always be equal to the number of as\\nthat have been read minus the number of bs that have been read; additionally,\\nthe bottom of the stack will contain the special symbol $. The input string\\nis properly nested if and only if (i) this diﬀerence is always non-negative and\\n(ii) this diﬀerence is zero once the entire input string has been read. Hence,\\nthe input string is accepted if and only if during this process, (i) the stack\\nalways contains at least the special symbol $ and (ii) at the end, the stack\\nonly contains the special symbol $ (which will then be popped in the ﬁnal\\nstep).\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q), where Σ = {a, b}, Γ = {$, S}, Q = {q}, and the\\ntransition function δ is speciﬁed by the following instructions:\\n3.6.\\nExamples of pushdown automata\\n117\\nqa$ →qR$S\\nbecause of the a, S is pushed onto the stack\\nqaS →qRSS\\nbecause of the a, S is pushed onto the stack\\nqbS →qRϵ\\nbecause of the b, the top element is popped\\nfrom the stack\\nqb$ →qNϵ\\nthe number of bs read is larger than the number\\nof as read; the stack is made empty (hence,\\nthe computation terminates before the entire\\nstring has been read), and the input string is rejected\\nq2$ →qNϵ\\nthe entire input string has been read; the stack is\\nmade empty, and the input string is accepted\\nq2S →qNS\\nthe entire input string has been read, it contains\\nmore as than bs; no changes are made (thus, the\\nautomaton does not terminate), and the input string\\nis rejected\\n3.6.2\\nStrings of the form 0n1n\\nWe construct a deterministic pushdown automata that accepts the language\\n{0n1n : n ≥0}.\\nThe automaton uses two states q0 and q1, where q0 is the start state.\\nInitially, the automaton is in state q0.\\n• For each 0 that it reads, the automaton pushes one symbol S onto the\\nstack and stays in state q0.\\n• When the ﬁrst 1 is read, the automaton switches to state q1. From that\\nmoment,\\n– for each 1 that is read, the automaton pops the top symbol from\\nthe stack and stays in state q1;\\n– if a 0 is read, the automaton does not make any change and,\\ntherefore, does not terminate.\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q0), where Σ = {0, 1}, Γ = {$, S}, Q = {q0, q1}, q0 is\\nthe start state, and the transition function δ is speciﬁed by the following\\ninstructions:\\n118\\nChapter 3.\\nContext-Free Languages\\nq00$ →q0R$S\\npush S onto the stack\\nq00S →q0RSS\\npush S onto the stack\\nq01$ →q0N$\\nﬁrst symbol in the input is 1; loop forever\\nq01S →q1Rϵ\\nﬁrst 1 is encountered\\nq02$ →q0Nϵ\\ninput string is empty; accept\\nq02S →q0NS\\ninput only consists of 0s; loop forever\\nq10$ →q1N$\\n0 to the right of 1; loop forever\\nq10S →q1NS\\n0 to the right of 1; loop forever\\nq11$ →q1N$\\ntoo many 1s; loop forever\\nq11S →q1Rϵ\\npop top symbol from the stack\\nq12$ →q1Nϵ\\naccept\\nq12S →q1NS\\ntoo many 0s; loop forever\\n3.6.3\\nStrings with b in the middle\\nWe will construct a nondeterministic pushdown automaton that accepts the\\nset L of all strings in {a, b}∗having an odd length and whose middle symbol\\nis b, i.e.,\\nL = {vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|}.\\nThe idea is as follows. The automaton uses two states q and q′, where q\\nis the start state. These states have the following meaning:\\n• If the automaton is in state q, then it has not reached the middle symbol\\nb of the input string.\\n• If the automaton is in state q′, then it has read the middle symbol b.\\nObserve that since the automaton can only make one single pass over the\\ninput string, it has to “guess” (i.e., use nondeterminism) when it reaches the\\nmiddle of the string.\\n• If the automaton is in state q, then, when reading the current tape\\nsymbol,\\n– it either pushes one symbol S onto the stack and stays in state q\\n– or, in case the current tape symbol is b, it “guesses” that it has\\nreached the middle of the input string, by switching to state q′.\\n• If the automaton is in state q′, then, when reading the current tape\\nsymbol, it pops the top symbol S from the stack and stays in state q′.\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n119\\nIn this way, the number of symbols S on the stack will always be equal to the\\ndiﬀerence of (i) the number of symbols in the part to the left of the middle\\nsymbol b that have been read and (ii) the number of symbols in the part\\nto the right of the middle symbol b that have been read; additionally, the\\nbottom of the stack will contain the special symbol $.\\nThe input string is accepted if and only if, at the moment when the blank\\nsymbol 2 is read, the automaton is in state q′ and the top symbol on the\\nstack is $. In this case, the stack is made empty and, thus, the computation\\nterminates.\\nWe obtain the nondeterministic pushdown automaton M = (Σ, Γ, Q, δ, q),\\nwhere Σ = {a, b}, Γ = {$, S}, Q = {q, q′}, q is the start state, and the\\ntransition function δ is speciﬁed by the following instructions:\\nqa$ →qR$S\\npush S onto the stack\\nqaS →qRSS\\npush S onto the stack\\nqb$ →q′R$\\nreached the middle\\nqb$ →qR$S\\ndid not reach the middle; push S onto the stack\\nqbS →q′RS\\nreached the middle\\nqbS →qRSS\\ndid not reach the middle; push S onto the stack\\nq2$ →qN$\\ninput string is empty; loop forever\\nq2S →qNS\\nloop forever\\nq′a$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′aS →q′Rϵ\\npop top symbol from stack\\nq′b$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′bS →q′Rϵ\\npop top symbol from stack\\nq′2$ →q′Nϵ\\naccept\\nq′2S →q′NS\\nloop forever\\nRemark 3.6.1 It can be shown that there is no deterministic pushdown\\nautomaton that accepts the language L. The reason is that a deterministic\\npushdown automaton cannot determine when it reaches the middle of the\\ninput string. Thus, unlike as for ﬁnite automata, nondeterministic pushdown\\nautomata are more powerful than their deterministic counterparts.\\n120\\nChapter 3.\\nContext-Free Languages\\n3.7\\nEquivalence of pushdown automata and\\ncontext-free grammars\\nThe main result of this section is that nondeterministic pushdown automata\\nand context-free grammars are equivalent in power:\\nTheorem 3.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then\\nA is context-free if and only if there exists a nondeterministic pushdown\\nautomaton that accepts A.\\nWe will only prove one direction of this theorem. That is, we will show\\nhow to convert an arbitrary context-free grammar to a nondeterministic push-\\ndown automaton.\\nLet G = (V, Σ, R, $) be a context-free grammar, where V is the set of\\nvariables, Σ is the set of terminals, R is the set of rules, and $ is the start\\nvariable. By Theorem 3.4.2, we may assume that G is in Chomsky normal\\nform. Hence, every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are variables, B ̸= $, and C ̸= $.\\n2. A →a, where A is a variable and a is a terminal.\\n3. $ →ϵ.\\nWe will construct a nondeterministic pushdown automaton M that ac-\\ncepts the language L(G) of this grammar G. Observe that M must have the\\nfollowing property: For every string w = a1a2 . . . an ∈Σ∗,\\nw ∈L(G) if and only if M accepts w.\\nThis can be reformulated as follows:\\n$\\n∗⇒a1a2 . . . an\\nif and only if there exists a computation of M that starts in the initial\\nconﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n121\\nand ends in the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nwhere ∅indicates that the stack is empty.\\nAssume that $\\n∗⇒a1a2 . . . an. Then there exists a derivation (using the\\nrules of R) of the string a1a2 . . . an from the start variable $. We may assume\\nthat in each step in this derivation, a rule is applied to the leftmost variable\\nin the current string. Hence, because the grammar G is in Chomsky normal\\nform, at any moment during the derivation, the current string has the form\\na1a2 . . . ai−1AkAk−1 . . . A1,\\n(3.2)\\nfor some integers i and k with 1 ≤i ≤n + 1 and k ≥0, and variables\\nA1, A2, . . . , Ak. (In particular, at the start of the derivation, we have i = 1\\nand k = 1, and the current string is Ak = $. At the end of the derivation,\\nwe have i = n + 1 and k = 0, and the current string is a1a2 . . . an.)\\nWe will deﬁne the pushdown automaton M in such a way that the current\\nstring (3.2) corresponds to the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nBased on this discussion, we obtain the nondeterministic pushdown au-\\ntomaton M = (Σ, V, {q}, δ, q), where\\n• the tape alphabet is the set Σ of terminals of G,\\n• the stack alphabet is the set V of variables of G,\\n• the set of states consists of one state q, which is the start state, and\\n• the transition function δ is obtained from the rules in R, in the following\\nway:\\n122\\nChapter 3.\\nContext-Free Languages\\n– For each rule in R that is of the form A →BC, with A, B, C ∈V ,\\nthe pushdown automaton M has the instructions\\nqaA →qNCB, for all a ∈Σ.\\n– For each rule in R that is of the form A →a, with A ∈V and\\na ∈Σ, the pushdown automaton M has the instruction\\nqaA →qRϵ.\\n– If R contains the rule $ →ϵ, then the pushdown automaton M\\nhas the instruction\\nq2$ →qNϵ.\\nThis concludes the deﬁnition of M. It remains to prove that L(M) =\\nL(G), i.e., the language of the nondeterministic pushdown automaton M is\\nequal to the language of the context-free grammar G. Hence, we have to\\nshow that for every string w ∈Σ∗,\\nw ∈L(G) if and only if w ∈L(M),\\nwhich can be rewritten as\\n$\\n∗⇒w if and only if M accepts w.\\nClaim 3.7.2 Let a1a2 . . . an be a string in Σ∗, let A1, A2, . . . , Ak be variables\\nin V , and let i and k be integers with 1 ≤i ≤n + 1 and k ≥0. Then the\\nfollowing holds:\\n$\\n∗⇒a1a2 . . . ai−1AkAk−1 . . . A1\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n123\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nProof. The claim can be proved by induction. Let\\nw = a1a2 . . . ai−1AkAk−1 . . . A1.\\nAssume that k ≥1 and assume that the claim is true for the string w. Then\\nwe have to show that the claim is still true after applying a rule in R to the\\nleftmost variable Ak in w. Since the grammar is in Chomsky normal form,\\nthe rule to be applied is either of the form Ak →BC or of the form Ak →ai.\\nIn both cases, the property mentioned in the claim is maintained.\\nWe now use Claim 3.7.2 to prove that L(M) = L(G). Let w = a1a2 . . . an\\nbe an arbitrary string in Σ∗. By applying Claim 3.7.2, with i = n + 1 and\\nk = 0, we see that w ∈L(G), i.e.,\\n$\\n∗⇒a1a2 . . . an,\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nBut this means that w ∈L(G) if and only if the automaton M accepts the\\nstring w.\\nThis concludes the proof of the fact that every context-free grammar can\\nbe converted to a nondeterministic pushdown automaton.\\nAs mentioned\\nalready, we will not give the conversion in the other direction. We ﬁnish this\\nsection with the following observation:\\n124\\nChapter 3.\\nContext-Free Languages\\nTheorem 3.7.3 Let Σ be an alphabet and let A ⊆Σ∗be a context-free lan-\\nguage. Then there exists a nondeterministic pushdown automaton that ac-\\ncepts A and has only one state.\\nProof. Since A is context-free, there exists a context-free grammar G0 such\\nthat L(G0) = A. By Theorem 3.4.2, there exists a context-free grammar G\\nthat is in Chomsky normal form and for which L(G) = L(G0). The construc-\\ntion given above converts G to a nondeterministic pushdown automaton M\\nthat has only one state and for which L(M) = L(G).\\n3.8\\nThe pumping lemma for context-free lan-\\nguages\\nIn Section 2.9, we proved the pumping lemma for regular languages and\\nused it to prove that certain languages are not regular. In this section, we\\ngeneralize the pumping lemma to context-free languages.\\nThe idea is to\\nconsider the parse tree (see Section 3.1) that describes the derivation of a\\nsuﬃciently long string in the context-free language L. Since the number of\\nvariables in the corresponding context-free grammar G is ﬁnite, there is at\\nleast one variable, say Aj, that occurs more than once on the longest root-\\nto-leaf path in the parse tree. The subtree which is sandwiched between two\\noccurrences of Aj on this path can be copied any number of times. This will\\nresult in a legal parse tree and, hence, in a “pumped” string that is in the\\nlanguage L.\\nTheorem 3.8.1 (Pumping Lemma for Context-Free Languages) Let\\nL be a context-free language. Then there exists an integer p ≥1, called the\\npumping length, such that the following holds: Every string s in L, with\\n|s| ≥p, can be written as s = uvxyz, such that\\n1. |vy| ≥1 (i.e., v and y are not both empty),\\n2. |vxy| ≤p, and\\n3. uvixyiz ∈L, for all i ≥0.\\n3.8.\\nThe pumping lemma for context-free languages\\n125\\n3.8.1\\nProof of the pumping lemma\\nThe proof of the pumping lemma will use the following result about parse\\ntrees:\\nLemma 3.8.2 Let G be a context-free grammar in Chomsky normal form,\\nlet s be a non-empty string in L(G), and let T be a parse tree for s. Let ℓbe\\nthe height of T, i.e., ℓis the number of edges on a longest root-to-leaf path\\nin T. Then\\n|s| ≤2ℓ−1.\\nProof. The claim can be proved by induction on ℓ. By looking at some\\nsmall values of ℓand using the fact that G is in Chomsky normal form, you\\nshould be able to verify the claim.\\nNow we can start with the proof of the pumping lemma. Let L be a\\ncontext-free language and let Σ be the alphabet of L. By Theorem 3.4.2, there\\nexists a context-free grammar in Chomsky normal form, G = (V, Σ, R, S),\\nsuch that L = L(G).\\nDeﬁne r to be the number of variables of G and deﬁne p = 2r. We will\\nprove that the value of p can be used as the pumping length. Consider an\\narbitrary string s in L such that |s| ≥p, and let T be a parse tree for s. Let\\nℓbe the height of T. Then, by Lemma 3.8.2, we have\\n|s| ≤2ℓ−1.\\nOn the other hand, we have\\n|s| ≥p = 2r.\\nBy combining these inequalities, we see that 2r ≤2ℓ−1, which can be rewrit-\\nten as\\nℓ≥r + 1.\\nConsider the nodes on a longest root-to-leaf path in T.\\nSince this path\\nconsists of ℓedges, it consists of ℓ+ 1 nodes. The ﬁrst ℓof these nodes store\\nvariables, which we denote by A0, A1, . . . , Aℓ−1 (where A0 = S), and the last\\nnode (which is a leaf) stores a terminal, which we denote by a.\\nSince ℓ−1 −r ≥0, the sequence\\nAℓ−1−r, Aℓ−r, . . . , Aℓ−1\\n126\\nChapter 3.\\nContext-Free Languages\\nof variables is well-deﬁned.\\nObserve that this sequence consists of r + 1\\nvariables. Since the number of variables in the grammar G is equal to r,\\nthe pigeonhole principle implies that there is a variable that occurs at least\\ntwice in this sequence. In other words, there are indices j and k, such that\\nℓ−1 −r ≤j < k ≤ℓ−1 and Aj = Ak. Refer to the ﬁgure below for an\\nillustration.\\nS\\nA j\\nAk\\nu\\nv\\nx\\ny\\nz\\ns\\nA0 = S\\nA1\\nAℓ−1−r\\nAℓ−r\\nAℓ−2\\nAℓ−1\\na\\nr +1\\nvariables\\nRecall that T is a parse tree for the string s. Therefore, the terminals\\nstored at the leaves of T, in the order from left to right, form s. As indicated\\nin the ﬁgure above, the nodes storing the variables Aj and Ak partition s\\ninto ﬁve substrings u, v, x, y, and z, such that s = uvxyz.\\nIt remains to prove that the three properties stated in the pumping lemma\\n3.8.\\nThe pumping lemma for context-free languages\\n127\\nhold. We start with the third property, i.e., we prove that\\nuvixyiz ∈L, for all i ≥0.\\nIn the grammar G, we have\\nS\\n∗⇒uAjz.\\n(3.3)\\nSince Aj\\n∗⇒vAky and Ak = Aj, we have\\nAj\\n∗⇒vAjy.\\n(3.4)\\nFinally, since Ak\\n∗⇒x and Ak = Aj, we have\\nAj\\n∗⇒x.\\n(3.5)\\nFrom (3.3) and (3.5), it follows that\\nS\\n∗⇒uAjz\\n∗⇒uxz,\\nwhich implies that the string uxz is in the language L. Similarly, it follows\\nfrom (3.3), (3.4), and (3.5) that\\nS\\n∗⇒uAjz\\n∗⇒uvAjyz\\n∗⇒uvvAjyyz\\n∗⇒uvvxyyz.\\nHence, the string uv2xy2z is in the language L. In general, for each i ≥0,\\nthe string uvixyiz is in the language L, because\\nS\\n∗⇒uAjz\\n∗⇒uviAjyiz\\n∗⇒uvixyiz.\\nThis proves that the third property in the pumping lemma holds.\\nNext we show that the second property holds. That is, we prove that\\n|vxy| ≤p.\\nConsider the subtree rooted at the node storing the variable\\nAj.\\nThe path from the node storing Aj to the leaf storing the terminal\\na is a longest path in this subtree. (Convince yourself that this is true.)\\nMoreover, this path consists of ℓ−j edges. Since Aj\\n∗⇒vxy, this subtree\\nis a parse tree for the string vxy (where Aj is used as the start variable).\\nTherefore, by Lemma 3.8.2, we can conclude that |vxy| ≤2ℓ−j−1. We know\\nthat ℓ−1 −r ≤j, which is equivalent to ℓ−j −1 ≤r. It follows that\\n|vxy| ≤2ℓ−j−1 ≤2r = p.\\n128\\nChapter 3.\\nContext-Free Languages\\nFinally, we show that the ﬁrst property in the pumping lemma holds.\\nThat is, we prove that |vy| ≥1. Recall that\\nAj\\n∗⇒vAky.\\nLet the ﬁrst rule used in this derivation be Aj →BC. (Since the variables\\nAj and Ak, even though they are equal, are stored at diﬀerent nodes of the\\nparse tree, and since the grammar G is in Chomsky normal form, this ﬁrst\\nrule exists.) Then\\nAj ⇒BC\\n∗⇒vAky.\\nObserve that the string BC has length two. Moreover, by applying rules of\\na grammar in Chomsky normal form, strings cannot become shorter. (Here,\\nwe use the fact that the start variable does not occur on the right-hand side\\nof any rule.) Therefore, we have |vAky| ≥2. But this implies that |vy| ≥1.\\nThis completes the proof of the pumping lemma.\\n3.8.2\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {anbncn : n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. Consider the string s = apbpcp.\\nObserve that s ∈A and |s| = 3p ≥p. Hence, by the pumping lemma, s can\\nbe written as s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all\\ni ≥0.\\nObserve that the pumping lemma does not tell us the location of the\\nsubstring vxy in the string s, it only gives us an upper bound on the length\\nof this substring. Therefore, we have to consider three cases, depending on\\nthe location of vxy in s.\\nCase 1: The substring vxy does not contain any c.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many as or more than p many bs. Since it contains\\nexactly p many cs, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\n3.8.\\nThe pumping lemma for context-free languages\\n129\\nCase 2: The substring vxy does not contain any a.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many bs or more than p many cs. Since it contains\\nexactly p many as, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\nCase 3: The substring vxy contains at least one a and at least one c.\\nSince s = apbpcp, this implies that |vxy| > p, which again contradicts the\\npumping lemma.\\nThus, in all of the three cases, we have obtained a contradiction. There-\\nfore, we have shown that the language A is not context-free.\\nSecond example\\nConsider the languages\\nA = {wwR : w ∈{a, b}∗},\\nwhere wR is the string obtained by writing w backwards, and\\nB = {ww : w ∈{a, b}∗}.\\nEven though these languages look similar, we will show that A is context-free\\nand B is not context-free.\\nConsider the following context-free grammar, in which S is the start vari-\\nable:\\nS →ϵ|aSa|bSb.\\nIt is easy to see that the language of this grammar is exactly the language A.\\nTherefore, A is context-free. Alternatively, we can show that A is context-\\nfree, by constructing a (nondeterministic) pushdown automaton that accepts\\nA. This automaton has two states q and q′, where q is the start state. If the\\nautomaton is in state q, then it did not yet ﬁnish reading the leftmost half of\\nthe input string; it pushes all symbols read onto the stack. If the automaton\\nis in state q′, then it is reading the rightmost half of the input string; for each\\nsymbol read, it checks whether it is equal to the symbol on top of the stack\\nand, if so, pops the top symbol from the stack. The pushdown automaton\\nuses nondeterminism to “guess” when to switch from state q to state q′ (i.e.,\\nwhen it has completed reading the leftmost half of the input string).\\n130\\nChapter 3.\\nContext-Free Languages\\nAt this point, you should convince yourself that the two approaches above,\\nwhich showed that A is context-free, do not work for B. The reason why\\nthey do not work is that the language B is not context-free, as we will prove\\nnow.\\nAssume that B is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. At this point, we must choose a\\nstring s in B, whose length is at least p, and that does not satisfy the three\\nproperties stated in the pumping lemma. Let us try the string s = apbapb.\\nThen s ∈B and |s| = 2p + 2 ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = uvxyz, where (i) |vy| ≥1, (ii) |vxy| ≤p, and (iii) uvixyiz ∈B\\nfor all i ≥0. It may happen that p ≥3, u = ap−1, v = a, x = b, y = a,\\nand z = ap−1b. If this is the case, then properties (i), (ii), and (iii) hold,\\nand, thus, we do not get a contradiction. In other words, we have chosen\\nthe “wrong” string s. This string is “wrong”, because there is only one b\\nbetween the as. Because of this, v can be in the leftmost block of as, and\\ny can be in the rightmost block of as. Observe that if there were at least p\\nmany bs between the as, then this would not happen, because |vxy| ≤p.\\nBased on the discussion above, we choose s = apbpapbp. Observe that\\ns ∈B and |s| = 4p ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈B for all i ≥0. Based\\non the location of vxy in the string s, we distinguish three cases:\\nCase 1: The substring vxy overlaps both the leftmost half and the rightmost\\nhalf of s.\\nSince |vxy| ≤p, the substring vxy is contained in the “middle” part of s,\\ni.e., vxy is contained in the block bpap. Consider the string uv0xy0z = uxz.\\nSince |vy| ≥1, we know that at least one of v and y is non-empty.\\n• If v ̸= ϵ, then v contains at least one b from the leftmost block of bs in\\ns, whereas y does not contain any b from the rightmost block of bs in s.\\nTherefore, in the string uxz, the leftmost block of bs contains fewer bs\\nthan the rightmost block of bs. Hence, the string uxz is not contained\\nin B.\\n• If y ̸= ϵ, then y contains at least one a from the rightmost block of\\nas in s, whereas v does not contain any a from the leftmost block of\\nas in s. Therefore, in the string uxz, the leftmost block of as contains\\nmore as than the rightmost block of as. Hence, the string uxz is not\\ncontained in B.\\n3.8.\\nThe pumping lemma for context-free languages\\n131\\nIn both cases, we conclude that the string uxz is not an element of the\\nlanguage B. But, by the pumping lemma, this string is contained in B.\\nCase 2: The substring vxy is in the leftmost half of s.\\nIn this case, none of the strings uxz, uv2xy2z, uv3xy3z, uv4xy4z, etc.,\\nis contained in B.\\nBut, by the pumping lemma, each of these strings is\\ncontained in B.\\nCase 3: The substring vxy is in the rightmost half of s.\\nThis case is symmetric to Case 2: None of the strings uxz, uv2xy2z,\\nuv3xy3z, uv4xy4z, etc., is contained in B. But, by the pumping lemma, each\\nof these strings is contained in B.\\nTo summarize, in each of the three cases, we have obtained a contradic-\\ntion. Therefore, the language B is not context-free.\\nThird example\\nWe have seen in Section 3.2.4 that the language\\n{ambncm+n : m ≥0, n ≥0}\\nis context-free. Using the pumping lemma for regular languages, it is easy to\\nprove that this language is not regular. In other words, context-free gram-\\nmars can verify addition, whereas ﬁnite automata are not powerful enough\\nfor this. We now consider the problem of verifying multiplication: Let A be\\nthe language deﬁned as\\nA = {ambncmn : m ≥0, n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is context-free. Let p ≥1 be the pumping length, as\\ngiven by the pumping lemma. Consider the string s = apbpcp2. Then, s ∈A\\nand |s| = 2p + p2 ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all i ≥0.\\nThere are three possible cases, depending on the locations of v and y in\\nthe string s.\\nCase 1: The substring v does not contain any a and does not contain any\\nb, and the substring y does not contain any a and does not contain any b.\\n132\\nChapter 3.\\nContext-Free Languages\\nConsider the string uv2xy2z. Since |vy| ≥1, this string consists of p\\nmany as, p many bs, but more than p2 many cs. Therefore, this string is not\\ncontained in A. But, by the pumping lemma, it is contained in A.\\nCase 2: The substring v does not contain any c and the substring y does\\nnot contain any c.\\nConsider again the string uv2xy2z. This string consists of p2 many cs.\\nSince |vy| ≥1, in this string,\\n• the number of as is at least p + 1 and the number of bs is at least p, or\\n• the number of as is at least p and the number of bs is at least p + 1.\\nTherefore, the number of as multiplied by the number of bs is at least p(p+1),\\nwhich is larger than p2. Therefore, uv2xy2z is not contained in A. But, by\\nthe pumping lemma, this string is contained in A.\\nCase 3: The substring v contains at least one b and the substring y contains\\nat least one c.\\nSince |vxy| ≤p, the substring vy does not contain any a. Thus, we can\\nwrite vy = bjck, where j ≥1 and k ≥1. Consider the string uxz. We can\\nwrite this string as uxz = apbp−jcp2−k. Since, by the pumping lemma, this\\nstring is contained in A, we have p(p−j) = p2−k, which implies that jp = k.\\nThus,\\n|vxy| ≥|vy| = j + k = j + jp ≥1 + p.\\nBut, by the pumping lemma, we have |vxy| ≤p.\\nObserve that, since |vxy| ≤p, the above three cases cover all possibilities\\nfor the locations of v and y in the string s. In each of the three cases, we\\nhave obtained a contradiction. Therefore, the language A is not context-free.\\nExercises\\n3.1 Construct context-free grammars that generate the following languages.\\nIn all cases, Σ = {0, 1}.\\n• {02n1n : n ≥0}\\n• {w : w contains at least three 1s}\\n• {w : the length of w is odd and its middle symbol is 0}\\nExercises\\n133\\n• {w : w is a palindrome}.\\nA palindrome is a string w having the property that w = wR, i.e.,\\nreading w from left to right gives the same result as reading w from\\nright to left.\\n• {w : w starts and ends with the same symbol}\\n• {w : w starts and ends with diﬀerent symbols}\\n3.2 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {0, 1}, S is the start variable, and R consists of the rules\\nS\\n→\\n0S|1A|ϵ\\nA\\n→\\n0B|1S\\nB\\n→\\n0A|1B\\nDeﬁne the following language L:\\nL = {w ∈{0, 1}∗:\\nw is the binary representation of a non-negative\\ninteger that is divisible by three } ∪{ϵ}.\\nProve that L = L(G). (Hint: The variables S, A, and B are used to\\nremember the remainder after division by three.)\\n3.3 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {a, b}, S is the start variable, and R consists of the rules\\nS\\n→\\naB|bA\\nA\\n→\\na|aS|BAA\\nB\\n→\\nb|bS|ABB\\n• Prove that ababba ∈L(G).\\n• Prove that L(G) is the set of all non-empty strings w over the alphabet\\n{a, b} such that the number of as in w is equal to the number of bs in\\nw.\\n3.4 Let A and B be context-free languages over the same alphabet Σ.\\n• Prove that the union A ∪B of A and B is also context-free.\\n• Prove that the concatenation AB of A and B is also context-free.\\n134\\nChapter 3.\\nContext-Free Languages\\n• Prove that the star A∗of A is also context-free.\\n3.5 Deﬁne the following two languages A and B:\\nA = {ambncn : m ≥0, n ≥0}\\nand\\nB = {ambmcn : m ≥0, n ≥0}.\\n• Prove that both A and B are context-free, by constructing two context-\\nfree grammars, one that generates A and one that generates B.\\n• We have seen in Section 3.8.2 that the language\\n{anbncn : n ≥0}\\nis not context-free. Explain why this implies that the intersection of\\ntwo context-free languages is not necessarily context-free.\\n• Use De Morgan’s Law to conclude that the complement of a context-\\nfree language is not necessarily context-free.\\n3.6 Let A be a context-free language and let B be a regular language.\\n• Prove that the intersection A ∩B of A and B is context-free.\\n• Prove that the set-diﬀerence\\nA \\\\ B = {w : w ∈A, w ̸∈B}\\nof A and B is context-free.\\n• Is the set-diﬀerence of two context-free languages necessarily context-\\nfree?\\n3.7 Let L be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that\\n• the number of as in w is equal to the number of bs in w,\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\nExercises\\n135\\nIn this exercise, you will prove that L is context-free.\\nLet A be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that the number of as in w is equal to the number of bs\\nin w. In Exercise 3.3, you have shown that A is context-free.\\nLet B be the language consisting of all strings w over the alphabet {a, b}\\nsuch that\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\n1. Give a regular expression that describes the complement of B.\\n2. Argue that B is a regular language.\\n3. Use Exercise 3.6 to argue that L is a context-free language.\\n3.8 Construct (deterministic or nondeterministic) pushdown automata that\\naccept the following languages.\\n1. {02n1n : n ≥0}.\\n2. {0n1m0n : n ≥1, m ≥1}.\\n3. {w ∈{0, 1}∗: w contains more 1s than 0s}.\\n4. {wwR : w ∈{0, 1}∗}.\\n(If w = w1 . . . wn, then wR = wn . . . w1.)\\n5. {w ∈{0, 1}∗: w is a palindrome}.\\n3.9 Let L be the language\\nL = {ambn : 0 ≤m ≤n ≤2m}.\\n1. Prove that L is context-free, by constructing a context-free grammar\\nwhose language is equal to L.\\n2. Prove that L is context-free, by constructing a nondeterministic push-\\ndown automaton that accepts L.\\n3.10 Prove that the following languages are not context-free.\\n136\\nChapter 3.\\nContext-Free Languages\\n• {an b a2n b a3n : n ≥0}.\\n• {anbnanbn : n ≥0}.\\n• {ambnck : m ≥0, n ≥0, k = max(m, n)}.\\n• {w#x : w is a substring of x, and w, x ∈{a, b}∗}.\\nFor example, the string aba#abbababbb is in the language, whereas the\\nstring aba#baabbaabb is not in the language. The alphabet is {a, b, #}.\\n•\\n{ w ∈{a, b, c}∗\\n:\\nw contains more b’s than a’s and\\nw contains more c’s than a’s }.\\n• {1n : n is a prime number}.\\n• {(abn)n : n ≥0}. (The parentheses are not part of the alphabet; thus,\\nthe alphabet is {a, b, }.)\\n3.11 Let L be a language consisting of ﬁnitely many strings. Show that L\\nis regular and, therefore, context-free. Let k be the maximum length of any\\nstring in L.\\n• Prove that every context-free grammar in Chomsky normal form that\\ngenerates L has more than log k variables. (The logarithm is in base\\n2.)\\n• Prove that there is a context-free grammar that generates L and that\\nhas only one variable.\\n3.12 Let L be a context-free language. Prove that there exists an integer\\np ≥1, such that the following is true: For every string s in L with |s| ≥p,\\nthere exists a string s′ in L such that |s| < |s′| ≤|s| + p.\\nChapter 4\\nTuring Machines and the\\nChurch-Turing Thesis\\nIn the previous chapters, we have seen several computational devices that\\ncan be used to accept or generate regular and context-free languages. Even\\nthough these two classes of languages are fairly large, we have seen in Sec-\\ntion 3.8.2 that these devices are not powerful enough to accept simple lan-\\nguages such as A = {ambncmn : m ≥0, n ≥0}. In this chapter, we introduce\\nthe Turing machine, which is a simple model of a real computer. Turing ma-\\nchines can be used to accept all context-free languages, but also languages\\nsuch as A. We will argue that every problem that can be solved on a real\\ncomputer can also be solved by a Turing machine (this statement is known\\nas the Church-Turing Thesis). In Chapter 5, we will consider the limitations\\nof Turing machines and, hence, of real computers.\\n4.1\\nDeﬁnition of a Turing machine\\nWe start with an informal description of a Turing machine. Such a machine\\nconsists of the following, see also Figure 4.1.\\n1. There are k tapes, for some ﬁxed k ≥1. Each tape is divided into\\ncells, and is inﬁnite both to the left and to the right. Each cell stores\\na symbol belonging to a ﬁnite set Γ, which is called the tape alphabet.\\nThe tape alphabet contains the blank symbol 2. If a cell contains 2,\\nthen this means that the cell is actually empty.\\n138\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nstate control\\n. . . 2 2 2 a a b a b b a b a b 2 2 2\\n. . .\\n?\\n. . . 2 2 2 b a a b 2 a b 2 2 2\\n. . .\\n?\\nFigure 4.1: A Turing machine with k = 2 tapes.\\n2. Each tape has a tape head which can move along the tape, one cell\\nper move. It can also read the cell it currently scans and replace the\\nsymbol in this cell by another symbol.\\n3. There is a state control, which can be in any one of a ﬁnite number of\\nstates. The ﬁnite set of states is denoted by Q. The set Q contains\\nthree special states: a start state, an accept state, and a reject state.\\nThe Turing machine performs a sequence of computation steps. In one\\nsuch step, it does the following:\\n1. Immediately before the computation step, the Turing machine is in a\\nstate r of Q, and each of the k tape heads is on a certain cell.\\n2. Depending on the current state r and the k symbols that are read by\\nthe tape heads,\\n(a) the Turing machine switches to a state r′ of Q (which may be\\nequal to r),\\n(b) each tape head writes a symbol of Γ in the cell it is currently\\nscanning (this symbol may be equal to the symbol currently stored\\nin the cell), and\\n4.1.\\nDeﬁnition of a Turing machine\\n139\\n(c) each tape head either moves one cell to the left, moves one cell to\\nthe right, or stays at the current cell.\\nWe now give a formal deﬁnition of a deterministic Turing machine.\\nDeﬁnition 4.1.1 A deterministic Turing machine is a 7-tuple\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject),\\nwhere\\n1. Σ is a ﬁnite set, called the input alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the tape alphabet; this alphabet contains the\\nblank symbol 2, and Σ ⊆Γ,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. qaccept is an element of Q; it is called the accept state,\\n6. qreject is an element of Q; it is called the reject state,\\n7. δ is called the transition function, which is a function\\nδ : Q × Γk →Q × Γk × {L, R, N}k.\\nThe transition function δ is basically the “program” of the Turing ma-\\nchine. This function tells us what the machine can do in “one computation\\nstep”: Let r ∈Q, and let a1, a2, . . . , ak ∈Γ.\\nFurthermore, let r′ ∈Q,\\na′\\n1, a′\\n2, . . . , a′\\nk ∈Γ, and σ1, σ2, . . . , σk ∈{L, R, N} be such that\\nδ(r, a1, a2, . . . , ak) = (r′, a′\\n1, a′\\n2, . . . , a′\\nk, σ1, σ2, . . . , σk).\\n(4.1)\\nThis transition means that if\\n• the Turing machine is in state r, and\\n• the head of the i-th tape reads the symbol ai, 1 ≤i ≤k,\\nthen\\n140\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• the Turing machine switches to state r′,\\n• the head of the i-th tape replaces the scanned symbol ai by the symbol\\na′\\ni, 1 ≤i ≤k, and\\n• the head of the i-th tape moves according to σi, 1 ≤i ≤k: if σi = L,\\nthen the tape head moves one cell to the left; if σi = R, then it moves\\none cell to the right; if σi = N, then the tape head does not move.\\nWe will write the computation step (4.1) in the form of the instruction\\nra1a2 . . . ak →r′a′\\n1a′\\n2 . . . a′\\nkσ1σ2 . . . σk.\\nWe now specify the computation of the Turing machine\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject).\\nStart conﬁguration: The input is a string over the input alphabet Σ.\\nInitially, this input string is stored on the ﬁrst tape, and the head of this\\ntape is on the leftmost symbol of the input string. Initially, all other k −1\\ntapes are empty, i.e., only contain blank symbols, and the Turing machine is\\nin the start state q.\\nComputation and termination: Starting in the start conﬁguration, the\\nTuring machine performs a sequence of computation steps as described above.\\nThe computation terminates at the moment when the Turing machine en-\\nters the accept state qaccept or the reject state qreject. (Hence, if the Turing\\nmachine never enters the states qaccept and qreject, the computation does not\\nterminate.)\\nAcceptance: The Turing machine M accepts the input string w ∈Σ∗, if the\\ncomputation on this input terminates in the state qaccept. If the computation\\non this input terminates in the state qreject, then M rejects the input string\\nw.\\nWe denote by L(M) the language accepted by the Turing machine M.\\nThus, L(M) is the set of all strings in Σ∗that are accepted by M.\\nObserve that a string w ∈Σ∗does not belong to L(M) if and only if on\\ninput w,\\n• the computation of M terminates in the state qreject or\\n• the computation of M does not terminate.\\n4.2.\\nExamples of Turing machines\\n141\\n4.2\\nExamples of Turing machines\\n4.2.1\\nAccepting palindromes using one tape\\nWe will show how to construct a Turing machine with one tape, that decides\\nwhether or not any input string w ∈{a, b}∗is a palindrome. Recall that the\\nstring w is called a palindrome, if reading w from left to right gives the same\\nresult as reading w from right to left. Examples of palindromes are abba,\\nbaabbbbaab, and the empty string ϵ.\\nStart of the computation: The tape contains the input string w, the tape\\nhead is on the leftmost symbol of w, and the Turing machine is in the start\\nstate q0.\\nIdea: The tape head reads the leftmost symbol of w, deletes this symbol\\nand “remembers” it by means of a state.\\nThen the tape head moves to\\nthe rightmost symbol and tests whether it is equal to the (already deleted)\\nleftmost symbol.\\n• If they are equal, then the rightmost symbol is deleted, the tape head\\nmoves to the new leftmost symbol, and the whole process is repeated.\\n• If they are not equal, the Turing machine enters the reject state, and\\nthe computation terminates.\\nThe Turing machine enters the accept state as soon as the string currently\\nstored on the tape is empty.\\nWe will use the input alphabet Σ = {a, b} and the tape alphabet Γ =\\n{a, b, 2}. The set Q of states consists of the following eight states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost symbol was a; tape head is moving to the right\\nqb :\\nleftmost symbol was b; tape head is moving to the right\\nq′\\na :\\nreached rightmost symbol; test whether it is equal to a, and delete it\\nq′\\nb :\\nreached rightmost symbol; test whether it is equal to b, and delete it\\nqL :\\ntest was positive; tape head is moving to the left\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n142\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nqba →qbaR\\nq0b →qb2R\\nqab →qabR\\nqbb →qbbR\\nq02 →qaccept\\nqa2 →q′\\na2L\\nqb2 →q′\\nb2L\\nq′\\naa →qL2L\\nq′\\nba →qreject\\nqLa →qLaL\\nq′\\nab →qreject\\nq′\\nbb →qL2L\\nqLb →qLbL\\nq′\\na2 →qaccept\\nq′\\nb2 →qaccept\\nqL2 →q02R\\nYou should go through the computation of this Turing machine for some\\nsample inputs, for example abba, b, abb and the empty string (which is a\\npalindrome).\\n4.2.2\\nAccepting palindromes using two tapes\\nWe again consider the palindrome problem, but now we use a Turing machine\\nwith two tapes.\\nStart of the computation: The ﬁrst tape contains the input string w and\\nthe head of the ﬁrst tape is on the leftmost symbol of w. The second tape is\\nempty and its tape head is at an arbitrary position. The Turing machine is\\nin the start state q0.\\nIdea: First, the input string w is copied to the second tape. Then the head\\nof the ﬁrst tape moves back to the leftmost symbol of w, while the head of\\nthe second tape stays at the rightmost symbol of w. Finally, the actual test\\nstarts: The head of the ﬁrst tape moves to the right and, at the same time,\\nthe head of the second tape moves to the left. While moving, the Turing\\nmachine tests whether the two tape heads read the same symbol in each\\nstep.\\nThe input alphabet is Σ = {a, b} and the tape alphabet is Γ = {a, b, 2}.\\nThe set Q of states consists of the following ﬁve states:\\nq0 :\\nstart state; copy w to the second tape\\nq1 :\\nw has been copied; head of ﬁrst tape moves to the left\\nq2 :\\nhead of ﬁrst tape moves to the right; head of second tape moves\\nto the left; until now, all tests were positive\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n4.2.\\nExamples of Turing machines\\n143\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a2 →q0aaRR\\nq1aa →q1aaLN\\nq0b2 →q0bbRR\\nq1ab →q1abLN\\nq022 →q122LL\\nq1ba →q1baLN\\nq1bb →q1bbLN\\nq12a →q22aRN\\nq12b →q22bRN\\nq122 →qaccept\\nq2aa →q2aaRL\\nq2ab →qreject\\nq2ba →qreject\\nq2bb →q2bbRL\\nq222 →qaccept\\nAgain, you should run this Turing machine for some sample inputs.\\n4.2.3\\nAccepting anbncn using one tape\\nWe will construct1 a Turing machine with one tape that accepts the language\\n{anbncn : n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: In the previous examples, the tape alphabet Γ was equal to the union\\nof the input alphabet Σ and {2}. In this example, we will add one symbol\\nd to the tape alphabet. As we will see, this simpliﬁes the construction of\\nthe Turing machine. Thus, the input alphabet is Σ = {a, b, c} and the tape\\nalphabet is Γ = {a, b, c, d, 2}. Recall that the input string w belongs to Σ∗.\\nThe general approach is to split the computation into two stages.\\n1Thanks to Michael Fleming for pointing out an error in a previous version of this\\nconstruction.\\n144\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nStage 1: In this stage, we check if the string w is in the language described\\nby the regular expression a∗b∗c∗. If this is the case, then we walk back to\\nthe leftmost symbol. For this stage, we use the following states, besides the\\nstates qaccept and qreject:\\nqa :\\nstart state; we are reading the block of a’s\\nqb :\\nwe are reading the block of b’s\\nqc :\\nwe are reading the block of c’s\\nqL :\\nwalk to the leftmost symbol\\nStage 2: In this stage, we repeat the following: Walk along the string from\\nleft to right, replace the leftmost a by d, replace the leftmost b by d, replace\\nthe leftmost c by d, and walk back to the leftmost symbol.\\nFor this stage, we use the following states:\\nq′\\na :\\nstart state of Stage 2; search for the leftmost a\\nq′\\nb :\\nleftmost a has been replaced by d;\\nsearch for the leftmost b\\nq′\\nc :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nsearch for the leftmost c\\nq′\\nL :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nleftmost c has been replaced by d;\\nwalk to the leftmost symbol\\nThe transition function δ is speciﬁed by the following instructions:\\nqaa →qaaR\\nqba →qreject\\nqab →qbbR\\nqbb →qbbR\\nqac →qccR\\nqbc →qccR\\nqad →cannot happen\\nqbd →cannot happen\\nqa2 →qL2L\\nqb2 →qL2L\\nqca →qreject\\nqLa →qLaL\\nqcb →qreject\\nqLb →qLbL\\nqcc →qccR\\nqLc →qLcL\\nqcd →cannot happen\\nqLd →cannot happen\\nqc2 →qL2L\\nqL2 →q′\\na2R\\n4.2.\\nExamples of Turing machines\\n145\\nq′\\naa →q′\\nbdR\\nq′\\nba →q′\\nbaR\\nq′\\nab →qreject\\nq′\\nbb →q′\\ncdR\\nq′\\nac →qreject\\nq′\\nbc →qreject\\nq′\\nad →q′\\nadR\\nq′\\nbd →q′\\nbdR\\nq′\\na2 →qaccept\\nq′\\nb2 →qreject\\nq′\\nca →qreject\\nq′\\nLa →q′\\nLaL\\nq′\\ncb →q′\\ncbR\\nq′\\nLb →q′\\nLbL\\nq′\\ncc →q′\\nLdL\\nq′\\nLc →q′\\nLcL\\nq′\\ncd →q′\\ncdR\\nq′\\nLd →q′\\nLdL\\nq′\\nc2 →qreject\\nq′\\nL2 →q′\\na2R\\nWe remark that Stage 1 is really necessary for this Turing machine: If we\\nomit this stage, and use only Stage 2, then the string aabcbc will be accepted.\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2}\\nWe consider again the language {anbncn : n ≥0}. In the previous section,\\nwe presented a Turing machine that uses an extra symbol d. The reader may\\nwonder if we can construct a Turing machine for this language that does not\\nuse any extra symbols. We will show below that this is indeed possible.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate q0.\\nIdea: Repeat the following Stages 1 and 2, until the string is empty.\\nStage 1. Walk along the string from left to right, delete the leftmost a,\\ndelete the leftmost b, and delete the rightmost c.\\nStage 2. Shift the substring of bs and cs one position to the left; then walk\\nback to the leftmost symbol.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, 2}.\\n146\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nFor Stage 1, we use the following states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost a has been deleted; have not read b\\nqb :\\nleftmost b has been deleted; have not read c\\nqc :\\nleftmost c has been read; tape head moves to the right\\nq′\\nc :\\ntape head is on the rightmost c\\nq1 :\\nrightmost c has been deleted; tape head is on the rightmost\\nsymbol or 2\\nqaccept :\\naccept state\\nqreject :\\nreject state\\nThe transitions for Stage 1 are speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nq0b →qreject\\nqab →qb2R\\nq0c →qreject\\nqac →qreject\\nq02 →qaccept\\nqa2 →qreject\\nqba →qreject\\nqca →qreject\\nqbb →qbbR\\nqcb →qreject\\nqbc →qccR\\nqcc →qccR\\nqb2 →qreject\\nqc2 →q′\\nc2L\\nq′\\ncc →q12L\\nFor Stage 2, we use the following states:\\nq1 :\\nas above; tape head is on the rightmost symbol or on 2\\nqc :\\ncopy c one cell to the left\\nqb :\\ncopy b one cell to the left\\nq2 :\\ndone with shifting; head moves to the left\\nAdditionally, we use a state q′\\n1 which has the following meaning: If the input\\nstring is of the form aibc, for some i ≥1, then after Stage 1, the tape contains\\nthe string ai−122, the tape head is on the 2 immediately to the right of the\\nas, and the Turing machine is in state q1. In this case, we move one cell to\\nthe left; if we then read 2, then i = 1, and we accept; otherwise, we read a,\\nand we reject.\\n4.2.\\nExamples of Turing machines\\n147\\nThe transitions for Stage 2 are speciﬁed by the following instructions:\\nq1a →cannot happen\\nq′\\n1a →qreject\\nq1b →qreject\\nq′\\n1b →cannot happen\\nq1c →qc2L\\nq′\\n1c →cannot happen\\nq12 →q′\\n12L\\nq′\\n12 →qaccept\\nqca →cannot happen\\nqba →cannot happen\\nqcb →qbcL\\nqbb →qbbL\\nqcc →qccL\\nqbc →cannot happen\\nqc2 →qreject\\nqb2 →q2bL\\nq2a →q2aL\\nq2b →cannot happen\\nq2c →cannot happen\\nq22 →q02R\\n4.2.5\\nAccepting ambncmn using one tape\\nWe will sketch how to construct a Turing machine with one tape that accepts\\nthe language\\n{ambncmn : m ≥0, n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, $, 2},\\nwhere the purpose of the symbol $ will become clear below.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: Observe that a string ambnck is in the language if and only if for every\\na, the string contains n many cs. Based on this, the computation consists of\\nthe following stages:\\nStage 1. Walk along the input string w from left to right and check whether\\nw is an element of the language described by the regular expression a∗b∗c∗.\\nIf this is not the case, then reject the input string. Otherwise, go to Stage 2.\\nStage 2. Walk back to the leftmost symbol of w. Go to Stage 3.\\nStage 3. In this stage, the Turing machine does the following:\\n148\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• Replace the leftmost a by the blank symbol 2.\\n• Walk to the leftmost b.\\n• Zigzag between the bs and cs; each time, replace the leftmost b by the\\nsymbol $, and replace the rightmost c by the blank symbol 2. If, for\\nsome b, there is no c left, the Turing machine rejects the input string.\\n• Continue zigzagging until there are no bs left. Then go to Stage 4.\\nObserve that in this third stage, the string ambnck is transformed to the\\nstring am−1$nck−n.\\nStage 4. In this stage, the Turing machine does the following:\\n• Replace each $ by b.\\n• Walk to the leftmost a.\\nHence, in this fourth stage, the string am−1$nck−n is transformed to the string\\nam−1bnck−n.\\nObserve that the input string ambnck is in the language if and only if the\\nstring am−1bnck−n is in the language. Therefore, the Turing machine repeats\\nStages 3 and 4, until there are no as left. At that moment, it checks whether\\nthere are any cs left; if so, it rejects the input string; otherwise, it accepts\\nthe input string.\\nWe hope that you believe that this description of the algorithm can be\\nturned into a formal description of a Turing machine.\\n4.3\\nMulti-tape Turing machines\\nIn Section 4.2, we have seen two Turing machines that accept palindromes;\\nthe ﬁrst Turing machine has one tape, whereas the second one has two tapes.\\nYou will have noticed that the two-tape Turing machine was easier to obtain\\nthan the one-tape Turing machine. This leads to the question whether multi-\\ntape Turing machines are more powerful than their one-tape counterparts.\\nThe answer is “no”:\\nTheorem 4.3.1 Let k ≥1 be an integer. Any k-tape Turing machine can\\nbe converted to an equivalent one-tape Turing machine.\\n4.3.\\nMulti-tape Turing machines\\n149\\nProof.2\\nWe will sketch the proof for the case when k = 2.\\nLet M =\\n(Σ, Γ, Q, δ, q, qaccept, qreject) be a two-tape Turing machine.\\nOur goal is to\\nconvert M to an equivalent one-tape Turing machine N. That is, N should\\nhave the property that for all strings w ∈Σ∗,\\n• M accepts w if and only if N accepts w,\\n• M rejects w if and only if N rejects w,\\n• M does not terminate on input w if and only if N does not terminate\\non input w.\\nThe tape alphabet of the one-tape Turing machine N is\\nΓ ∪{ ˙x : x ∈Γ} ∪{#}.\\nIn words, we take the tape alphabet Γ of M, and add, for each x ∈Γ, the\\nsymbol ˙x. Moreover, we add a special symbol #.\\nThe Turing machine N will be deﬁned in such a way that any conﬁgura-\\ntion of the two-tape Turing machine M, for example\\n. . . 2 1 0 0 1 2 . . .\\n6\\n. . . 2 a a b a 2 . . .\\n6\\ncorresponds to the following conﬁguration of the one-tape Turing machine\\nN:\\n. . .\\n2\\n#\\n1\\n0\\n˙0\\n1\\n#\\na\\n˙a\\nb\\na\\n#\\n2 . . .\\n6\\n2Thanks to Sergio Cabello for pointing out an error in a previous version of this proof.\\n150\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThus, the contents of the two tapes of M are encoded on the single tape of\\nN. The dotted symbols are used to indicate the positions of the two tape\\nheads of M, whereas the three occurrences of the special symbol # are used\\nto mark the boundaries of the strings on the two tapes of M.\\nThe Turing machine N simulates one computation step of M, in the\\nfollowing way:\\n• Throughout the simulation of this step, N “remembers” the current\\nstate of M.\\n• At the start of the simulation, the tape head of N is on the leftmost\\nsymbol #.\\n• N walks along the string to the right until it ﬁnds the ﬁrst dotted\\nsymbol. (This symbol indicates the location of the head on the ﬁrst tape\\nof M.) N remembers this ﬁrst dotted symbol and continues walking\\nto the right until it ﬁnds the second dotted symbol.\\n(This symbol\\nindicates the location of the head on the second tape of M.) Again, N\\nremembers this second dotted symbol.\\n• At this moment, N is still at the second dotted symbol. N updates\\nthis part of the tape, by making the change that M would make on its\\nsecond tape. (This change is given by the transition function of M; it\\ndepends on the current state of M and the two symbols that M reads\\non its two tapes.)\\n• N walks to the left until it ﬁnds the ﬁrst dotted symbol.\\nThen, it\\nupdates this part of the tape, by making the change that M would\\nmake on its ﬁrst tape.\\n• In the previous two steps, in which the tape is updated, it may be\\nnecessary to shift a part of the tape.\\n• Finally, N remembers the new state of M and walks back to the left-\\nmost symbol #.\\nIt should be clear that the Turing machine N can be constructed by\\nintroducing appropriate states.\\n4.4.\\nThe Church-Turing Thesis\\n151\\n4.4\\nThe Church-Turing Thesis\\nWe all have some intuitive notion of what an algorithm is. This notion will\\nprobably be something like “an algorithm is a procedure consisting of com-\\nputation steps that can be speciﬁed in a ﬁnite amount of text”. For example,\\nany “computational process” that can be speciﬁed by a Java program, should\\nbe considered an algorithm. Similarly, a Turing machine speciﬁes a “com-\\nputational process” and, therefore, should be considered an algorithm. This\\nleads to the question of whether it is possible to give a mathematical deﬁni-\\ntion of an “algorithm”. We just saw that every Java program represents an\\nalgorithm and that every Turing machine also represents an algorithm. Are\\nthese two notions of an algorithm equivalent? The answer is “yes”. In fact,\\nthe following theorem states that many diﬀerent notions of “computational\\nprocess” are equivalent. (We hope that you have gained suﬃcient intuition,\\nso that none of the claims in this theorem comes as a surprise to you.)\\nTheorem 4.4.1 The following computation models are equivalent, i.e., any\\none of them can be converted to any other one:\\n1. One-tape Turing machines.\\n2. k-tape Turing machines, for any k ≥1.\\n3. Non-deterministic Turing machines.\\n4. Java programs.\\n5. C++ programs.\\n6. Lisp programs.\\nIn other words, if we deﬁne the notion of an algorithm using any of the\\nmodels in this theorem, then it does not matter which model we take: All\\nthese models give the same notion of an algorithm.\\nThe problem of deﬁning the notion of an algorithm goes back to David\\nHilbert. On August 8, 1900, at the Second International Congress of Math-\\nematicians in Paris, Hilbert presented a list of problems that he considered\\ncrucial for the further development of mathematics. Hilbert’s 10th problem\\nis the following:\\n152\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nDoes there exist a ﬁnite process that decides whether or not any\\ngiven polynomial with integer coeﬃcients has integral roots?\\nOf course, in our language, Hilbert asked whether or not there exists an\\nalgorithm that decides, when given an arbitrary polynomial equation (with\\ninteger coeﬃcients) such as\\n12x3y7z5 + 7x2y4z −x4 + y2z7 −z3 + 10 = 0,\\nwhether or not this equation has a solution in integers. In 1970, Matiyasevich\\nproved that such an algorithm does not exist. Of course, in order to prove\\nthis claim, we ﬁrst have to agree on what an algorithm is. In the beginning\\nof the twentieth century, mathematicians gave several deﬁnitions, such as\\nTuring machines (1936) and the λ-calculus (1936), and they proved that all\\nthese are equivalent. Later, after programming languages were invented, it\\nwas shown that these older notions of an algorithm are equivalent to notions\\nof an algorithm that are based on C programs, Java programs, Lisp programs,\\nPascal programs, etc.\\nIn other words, all attempts to give a rigorous deﬁnition of the notion of\\nan algorithm led to the same concept. Because of this, computer scientists\\nnowadays agree on what is called the Church-Turing Thesis:\\nChurch-Turing Thesis:\\nEvery computational process that is intuitively\\nconsidered to be an algorithm can be converted to a Turing machine.\\nIn other words, this basically states that we deﬁne an algorithm to be a\\nTuring machine. At this point, you should ask yourself, whether the Church-\\nTuring Thesis can be proved. Alternatively, what has to be done in order to\\ndisprove this thesis?\\nExercises\\n4.1 Construct a Turing machine with one tape, that accepts the language\\n{02n1n : n ≥0}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\nExercises\\n153\\n4.2 Construct a Turing machine with one tape, that accepts the language\\n{w : w contains twice as many 0s as 1s}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\n4.3 Let A be the language\\nA\\n=\\n{ w ∈{a, b, c}∗\\n:\\nw contains more bs than as and\\nw contains more cs than as }.\\nGive an informal description (in plain English) of a Turing machine with one\\ntape, that accepts the language A.\\n4.4 Construct a Turing machine with one tape that receives as input a non-\\nnegative integer x and returns as output the integer x + 1.\\nIntegers are\\nrepresented as binary strings.\\nStart of the computation: The tape contains the binary representation\\nof the input x. The tape head is on the leftmost symbol and the Turing\\nmachine is in the start state q0. For example, if x = 431, the tape looks as\\nfollows:\\n. . . 2 2 2 1 1 0 1 0 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x + 1. The tape head is on the leftmost symbol and the Turing\\nmachine is in the ﬁnal state q1. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 0 1 1 0 0 0 0 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state q1. As soon as state q1 is entered,\\nthe Turing machine terminates. At termination, the contents of the tape is\\nthe output of the Turing machine.\\n154\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n4.5 Construct a Turing machine with two tapes that receives as input two\\nnon-negative integers x and y, and returns as output the integer x + y.\\nIntegers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost bit\\nof y. At the start, the Turing machine is in the start state q0.\\nEnd of the computation: The ﬁrst tape contains the binary representation\\nof x and its head is on the rightmost symbol of x. The second tape contains\\nthe binary representation of the integer x + y (thus, the integer y is “gone”).\\nThe head of the second tape is on the rightmost bit of x + y. The Turing\\nmachine is in the ﬁnal state q1.\\n4.6 Give an informal description (in plain English) of a Turing machine with\\none tape that receives as input two non-negative integers x and y, and returns\\nas output the integer x+y. Integers are represented as binary strings. If you\\nare an adventurous student, you may give a formal deﬁnition of your Turing\\nmachine.\\n4.7 Construct a Turing machine with one tape that receives as input an\\ninteger x ≥1 and returns as output the integer x−1. Integers are represented\\nin binary.\\nStart of the computation: The tape contains the binary representation of\\nthe input x. The tape head is on the rightmost symbol of x and the Turing\\nmachine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x −1. The tape head is on the rightmost bit of x −1 and the\\nTuring machine is in the ﬁnal state q1.\\n4.8 Give an informal description (in plain English) of a Turing machine with\\nthree tapes that receives as input two non-negative integers x and y, and\\nreturns as output the integer xy. Integers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost sym-\\nbol of y. The third tape is empty and its head is at an arbitrary location.\\nThe Turing machine is in the start state q0.\\nExercises\\n155\\nEnd of the computation: The ﬁrst and second tapes are empty. The third\\ntape contains the binary representation of the product xy and its head is on\\nthe rightmost bit of xy. The Turing machine is in the ﬁnal state q1.\\nHint: Use the Turing machines of Exercises 4.5 and 4.7.\\n4.9 Construct a Turing machine with one tape that receives as input a string\\nof the form 1n for some integer n ≥0; thus, the input is a string of n many\\n1s. The output of the Turing machine is the string 1n21n. Thus, this Turing\\nmachine makes a copy of its input.\\nThe input alphabet is Σ = {1} and the tape alphabet is Γ = {1, 2}.\\nStart of the computation: The tape contains a string of the form 1n, for\\nsome integer n ≥0, the tape head is on the leftmost symbol, and the Turing\\nmachine is in the start state. For example, if n = 4, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the string 1n21n, the tape\\nhead is on the 2 in the middle of this string, and the Turing machine is in\\nthe ﬁnal state. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 1 1 1 1 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this state is entered, the\\nTuring machine terminates. At termination, the contents of the tape is the\\noutput of the Turing machine.\\n156\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nChapter 5\\nDecidable and Undecidable\\nLanguages\\nWe have seen in Chapter 4 that Turing machines form a model for “everything\\nthat is intuitively computable”. In this chapter, we consider the limitations\\nof Turing machines. That is, we ask ourselves the question whether or not\\n“everything” is computable. As we will see, the answer is “no”. In fact, we\\nwill even see that “most” problems are not solvable by Turing machines and,\\ntherefore, not solvable by computers.\\n5.1\\nDecidability\\nIn Chapter 4, we have deﬁned when a Turing machine accepts an input string\\nand when it rejects an input string. Based on this, we deﬁne the following\\nclass of languages.\\nDeﬁnition 5.1.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is decidable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the reject state.\\n158\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is decidable, if there exists an algorithm\\nthat (i) terminates on every input string w, and (ii) correctly tells us whether\\nw ∈A or w ̸∈A.\\nA language A that is not decidable is called undecidable.\\nFor such a\\nlanguage, there does not exist an algorithm that satisﬁes (i) and (ii) above.\\nIn Section 4.2, we have seen several examples of languages that are de-\\ncidable.\\nIn the following subsections, we will give some examples of decidable and\\nundecidable languages. These examples involve languages A whose elements\\nare pairs of the form (C, w), where C is some computation model (for ex-\\nample, a deterministic ﬁnite automaton) and w is a string over the alphabet\\nΣ. The pair (C, w) is in the language A if and only if the string w is in the\\nlanguage of the computation model C. For diﬀerent computation models C,\\nwe will ask the question whether A is decidable, i.e., whether an algorithm\\nexists that decides, for any input (C, w), whether or not this input belongs\\nto the language A. Since the input to any algorithm is a string over some\\nalphabet, we must encode the pair (C, w) as a string. In all cases that we\\nconsider, such a pair can be described using a ﬁnite amount of text. There-\\nfore, we assume, without loss of generality, that binary strings are used for\\nthese encodings. Throughout the rest of this chapter, we will denote the\\nbinary encoding of a pair (C, w) by\\n⟨C, w⟩.\\n5.1.1\\nThe language ADFA\\nWe deﬁne the following language:\\nADFA = {⟨M, w⟩:\\nM is a deterministic ﬁnite automaton that\\naccepts the string w}.\\nKeep in mind that ⟨M, w⟩denotes the binary string that forms an en-\\ncoding of the ﬁnite automaton M and the string w that is given as input to\\nM.\\nWe claim that the language ADFA is decidable. In order to prove this,\\nwe have to construct an algorithm with the following property, for any given\\ninput string u:\\n• If u is the encoding of a deterministic ﬁnite automaton M and a string\\nw (i.e., u is in the correct format ⟨M, w⟩), and if M accepts w, then\\n5.1.\\nDecidability\\n159\\nthe algorithm terminates in its accept state.\\n• In all other cases, the algorithm terminates in its reject state.\\nAn algorithm that exactly does this, is easy to obtain: On input u, the algo-\\nrithm ﬁrst checks whether or not u encodes a deterministic ﬁnite automaton\\nM and a string w. If this is not the case, then it terminates and rejects\\nthe input string u. Otherwise, the algorithm “constructs” M and w, and\\nthen simulates the computation of M on the input string w. If M accepts\\nw, then the algorithm terminates and accepts the input string u. If M does\\nnot accept w, then the algorithm terminates and rejects the input string u.\\nThus, we have proved the following result:\\nTheorem 5.1.2 The language ADFA is decidable.\\n5.1.2\\nThe language ANFA\\nWe deﬁne the following language:\\nANFA = {⟨M, w⟩:\\nM is a nondeterministic ﬁnite automaton that\\naccepts the string w}.\\nTo prove that this language is decidable, consider the algorithm that\\ndoes the following: On input u, the algorithm ﬁrst checks whether or not\\nu encodes a nondeterministic ﬁnite automaton M and a string w. If this is\\nnot the case, then it terminates and rejects the input string u. Otherwise,\\nthe algorithm constructs M and w. Since a computation of M (on input w)\\nis not unique, the algorithm ﬁrst converts M to an equivalent deterministic\\nﬁnite automaton N. Then, it proceeds as in Section 5.1.1.\\nObserve that the construction for converting a nondeterministic ﬁnite au-\\ntomaton to a deterministic ﬁnite automaton (see Section 2.5) is algorithmic,\\nin the sense that it can be described by an algorithm. Because of this, the\\nalgorithm described above is a valid algorithm; it accepts all strings u that\\nare in ANFA, and it rejects all strings u that are not in ANFA. Thus, we have\\nproved the following result:\\nTheorem 5.1.3 The language ANFA is decidable.\\n160\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.1.3\\nThe language ACFG\\nWe deﬁne the following language:\\nACFG = {⟨G, w⟩: G is a context-free grammar such that w ∈L(G)}.\\nWe claim that this language is decidable. In order to prove this claim, con-\\nsider a string u that encodes a context-free grammar G = (V, Σ, S, R) and a\\nstring w ∈Σ∗. Deciding whether or not w ∈L(G) is equivalent to deciding\\nwhether or not S\\n∗⇒w. A ﬁrst idea to decide this is by trying all possible\\nderivations that start with the start variable S and that use rules of R. The\\nproblem is that, in case w ̸∈L(G), it is not clear how many such derivations\\nhave to be checked before we can be sure that w is not in the language of\\nG: If w ∈L(G), then it may be that w can be derived from S, only by ﬁrst\\nderiving a very long string, say v, and then use rules to shorten it so as to\\nobtain the string w. Since there is no obvious upper bound on the length of\\nthe string v, we have to be careful.\\nThe trick is to do the following. First, convert the grammar G to an\\nequivalent grammar G′ in Chomsky normal form. (The construction given\\nin Section 3.4 can be described by an algorithm.) Let n be the length of the\\nstring w. Then, if w ∈L(G) = L(G′), any derivation of w in G′, from the\\nstart variable of G′, consists of exactly 2n−1 steps (where a “step” is deﬁned\\nas applying one rule of G′). Hence, we can decide whether or not w ∈L(G),\\nby trying all possible derivations, in G′, consisting of 2n −1 steps. If one of\\nthese (ﬁnite number of) derivations leads to the string w, then w ∈L(G).\\nOtherwise, w ̸∈L(G). Thus, we have proved the following result:\\nTheorem 5.1.4 The language ACFG is decidable.\\nIn fact, the arguments above imply the following result:\\nTheorem 5.1.5 Every context-free language is decidable.\\nProof. Let Σ be an alphabet and let A ⊆Σ∗be an arbitrary context-free\\nlanguage. There exists a context-free grammar in Chomsky normal form,\\nwhose language is equal to A. Given an arbitrary string w ∈Σ∗, we have\\nseen above how we can decide whether or not w can be derived from the\\nstart variable of this grammar.\\n5.1.\\nDecidability\\n161\\n5.1.4\\nThe language ATM\\nAfter having seen the languages ADFA, ANFA, and ACFG, it is natural to\\nconsider the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nWe will prove that this language is undecidable. Before we give the proof,\\nlet us mention what this means:\\nThere is no algorithm that, when given an arbitrary algorithm M\\nand an arbitrary input string w for M, decides in a ﬁnite amount\\nof time, whether or not M accepts w.\\nThe proof of the claim that ATM is undecidable is by contradiction. Thus,\\nwe assume that ATM is decidable. Then there exists a Turing machine H\\nthat has the following property. For every input string ⟨M, w⟩for H:\\n• If ⟨M, w⟩∈ATM (i.e., M accepts w), then H terminates in its accept\\nstate.\\n• If ⟨M, w⟩̸∈ATM (i.e., M rejects w or M does not terminate on input\\nw), then H terminates in its reject state.\\n• In particular, H terminates on any input ⟨M, w⟩.\\nWe construct a new Turing machine D, that does the following: On input\\n⟨M⟩, the Turing machine D uses H as a subroutine to determine what M\\ndoes when it is given its own description as input. Once D has determined\\nthis information, it does the opposite of what H does.\\nTuring machine D: On input ⟨M⟩, where M is a Turing machine,\\nthe new Turing machine D does the following:\\nStep 1: Run the Turing machine H on the input ⟨M, ⟨M⟩⟩.\\nStep 2:\\n• If H terminates in its accept state, then D terminates in its\\nreject state.\\n• If H terminates in its reject state, then D terminates in its\\naccept state.\\n162\\nChapter 5.\\nDecidable and Undecidable Languages\\nFirst observe that this new Turing machine D terminates on any input\\nstring ⟨M⟩, because H terminates on every input. Next observe that, for any\\ninput string ⟨M⟩for D:\\n• If ⟨M, ⟨M⟩⟩∈ATM (i.e., M accepts ⟨M⟩), then D terminates in its\\nreject state.\\n• If ⟨M, ⟨M⟩⟩̸∈ATM (i.e., M rejects ⟨M⟩or M does not terminate on\\ninput ⟨M⟩), then D terminates in its accept state.\\nThis means that for any string ⟨M⟩:\\n• If M accepts ⟨M⟩, then D rejects ⟨M⟩.\\n• If M rejects ⟨M⟩or M does not terminate on input ⟨M⟩, then D\\naccepts ⟨M⟩.\\nWe now consider what happens if we give the Turing machine D the string\\n⟨D⟩as input, i.e., we take M = D:\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩or D does not terminate on input ⟨D⟩, then D accepts\\n⟨D⟩.\\nSince D terminates on every input string, this means that\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩, then D accepts ⟨D⟩.\\nThis is clearly a contradiction. Therefore, the Turing machine H that decides\\nthe language ATM cannot exist and, thus, ATM is undecidable. We have\\nproved the following result:\\nTheorem 5.1.6 The language ATM is undecidable.\\n5.1.\\nDecidability\\n163\\n5.1.5\\nThe Halting Problem\\nWe deﬁne the following language:\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}.\\nTheorem 5.1.7 The language Halt is undecidable.\\nProof. The proof is by contradiction. Thus, we assume that the language\\nHalt is decidable. Then there exists a Java program H that takes as input a\\nstring of the form ⟨P, w⟩, where P is an arbitrary Java program and w is an\\narbitrary input for P. The program H has the following property:\\n• If ⟨P, w⟩∈Halt (i.e., program P terminates on input w), then H\\noutputs true.\\n• If ⟨P, w⟩̸∈Halt (i.e., program P does not terminate on input w), then\\nH outputs false.\\n• In particular, H terminates on any input ⟨P, w⟩.\\nWe will write the output of H as H(P, w). Moreover, we will denote by P(w)\\nthe computation obtained by running the program P on the input w. Hence,\\nH(P, w) =\\n\\x1a true\\nif P(w) terminates,\\nfalse\\nif P(w) does not terminate.\\nConsider the following algorithm Q, which takes as input the encoding\\n⟨P⟩of an arbitrary Java program P:\\nAlgorithm Q(⟨P⟩):\\nwhile H(P, ⟨P⟩) = true\\ndo have a beer\\nendwhile\\nSince H is a Java program, this new algorithm Q can also be written as\\na Java program. Observe that\\nQ(⟨P⟩) terminates if and only if H(P, ⟨P⟩) = false.\\n164\\nChapter 5.\\nDecidable and Undecidable Languages\\nThis means that for every Java program P,\\nQ(⟨P⟩) terminates if and only if P(⟨P⟩) does not terminate.\\n(5.1)\\nWhat happens if we run the Java program Q on the input string ⟨Q⟩?\\nIn other words, what happens if we run Q(⟨Q⟩)? Then, in (5.1), we have to\\nreplace all occurrences of P by Q. Hence,\\nQ(⟨Q⟩) terminates if and only if Q(⟨Q⟩) does not terminate.\\nThis is obviously a contradiction, and we can conclude that the Java program\\nH does not exist. Therefore, the language Halt is undecidable.\\nRemark 5.1.8 In this proof, we run the Java program Q on the input ⟨Q⟩.\\nThis means that the input to Q is a description of itself. In other words, we\\ngive Q itself as input. This is an example of what is called self-reference. An-\\nother example of self-reference can be found in Remark 5.1.8 of the textbook\\nIntroduction to Theory of Computation by A. Maheshwari and M. Smid.\\n5.2\\nCountable sets\\nThe proofs that we gave in Sections 5.1.4 and 5.1.5 seem to be bizarre. In\\nthis section, we will convince you that these proofs in fact use a technique\\nthat you have seen in the course COMP 1805: Cantor’s Diagonalization.\\nLet A and B be two sets and let f : A →B be a function. Recall that f\\nis called a bijection, if\\n• f is one-to-one (or injective), i.e., for any two distinct elements a and\\na′ in A, we have f(a) ̸= f(a′), and\\n• f is onto (or surjective), i.e., for each element b ∈B, there exists an\\nelement a ∈A, such that f(a) = b.\\nThe set of natural numbers is denoted by N. That is, N = {1, 2, 3, . . .}.\\nDeﬁnition 5.2.1 Let A and B be two sets. We say that A and B have the\\nsame size, if there exists a bijection f : A →B.\\nDeﬁnition 5.2.2 Let A be a set. We say that A is countable, if A is ﬁnite,\\nor A and N have the same size.\\n5.2.\\nCountable sets\\n165\\nIn other words, if A is an inﬁnite and countable set, then there exists a\\nbijection f : N →A, and we can write A as\\nA = {f(1), f(2), f(3), f(4), . . .}.\\nSince f is a bijection, every element of A occurs exactly once in the set on\\nthe right-hand side. This means that we can number the elements of A using\\nthe positive integers: Every element of A receives a unique number.\\nTheorem 5.2.3 The following sets are countable:\\n1. The set Z of integers:\\nZ = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n2. The Cartesian product N × N:\\nN × N = {(m, n) : m ∈N, n ∈N}.\\n3. The set Q of rational numbers:\\nQ = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\nProof. To prove that the set Z is countable, we have to give each element of\\nZ a unique number in N. We obtain this numbering, by listing the elements\\nof Z in the following order:\\n0, 1, −1, 2, −2, 3, −3, 4, −4, . . .\\nIn this (inﬁnite) list, every element of Z occurs exactly once. The number of\\nan element of Z is given by its position in this list.\\nFormally, deﬁne the function f : N →Z by\\nf(n) =\\n\\x1a n/2\\nif n is even,\\n−(n −1)/2\\nif n is odd.\\nThis function f is a bijection and, therefore, the sets N and Z have the same\\nsize. Hence, the set Z is countable.\\nFor the proofs of the other two claims, we refer to the course COMP 1805.\\nWe now use Cantor’s Diagonalization principle to prove that the set of\\nreal numbers is not countable:\\n166\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.2.4 The set R of real numbers is not countable.\\nProof. Deﬁne\\nA = {x ∈R : 0 ≤x < 1}.\\nWe will prove that the set A is not countable. This will imply that the set\\nR is not countable, because A ⊆R.\\nThe proof that A is not countable is by contradiction. So we assume that\\nA is countable. Then there exists a bijection f : N →A. Thus, for each\\nn ∈N, f(n) is a real number between zero and one. We can write\\nA = {f(1), f(2), f(3), . . .},\\n(5.2)\\nwhere every element of A occurs exactly once in the set on the right-hand\\nside.\\nConsider the real number f(1). We can write this number in decimal\\nnotation as\\nf(1) = 0.d11d12d13 . . . ,\\nwhere each d1i is a digit in the set {0, 1, 2, . . . , 9}. In general, for every n ∈N,\\nwe can write the real number f(n) as\\nf(n) = 0.dn1dn2dn3 . . . ,\\nwhere, again, each dni is a digit in {0, 1, 2, . . . , 9}.\\nWe deﬁne the real number\\nx = 0.d1d2d3 . . . ,\\nwhere, for each integer n ≥1,\\ndn =\\n\\x1a 4\\nif dnn ̸= 4,\\n5\\nif dnn = 4.\\nObserve that x is a real number between zero and one, i.e., x ∈A. Therefore,\\nby (5.2), there is an element n ∈N, such that f(n) = x. We compare the\\nn-th digits of f(n) and x:\\n• The n-th digit of f(n) is equal to dnn.\\n• The n-th digit of x is equal to dn.\\n5.2.\\nCountable sets\\n167\\nSince f(n) and x are equal, their n-th digits must be equal, i.e., dnn = dn.\\nBut, by the deﬁnition of dn, we have dnn ̸= dn. This is a contradiction and,\\ntherefore, the set A is not countable.\\nNotice how we deﬁned the real number x: For each n ≥1, the n-th digit\\nof x is not equal to the n-th digit of f(n). Therefore, for each n ≥1, x ̸= f(n)\\nand, thus, x ̸∈A.\\nThe ﬁnal result of this section is the fact that for every set A, its power\\nset\\nP(A) = {B : B ⊆A}\\nis “strictly larger” than A. Deﬁne the function f : A →P(A) by\\nf(a) = {a},\\nfor any a in A. Since f is one-to-one, we can say that P(A) is “at least as\\nlarge as” A.\\nTheorem 5.2.5 Let A be an arbitrary set. Then A and P(A) do not have\\nthe same size.\\nProof. The proof is by contradiction. Thus, we assume that there exists a\\nbijection g : A →P(A). Deﬁne the set B as\\nB = {a ∈A : a ̸∈g(a)}.\\nSince B ∈P(A) and g is a bijection, there exists an element a in A such that\\ng(a) = B.\\nFirst assume that a ∈B. Since g(a) = B, we have a ∈g(a). But then,\\nfrom the deﬁnition of the set B, we have a ̸∈B, which is a contradiction.\\nNext assume that a ̸∈B.\\nSince g(a) = B, we have a ̸∈g(a).\\nBut\\nthen, from the deﬁnition of the set B, we have a ∈B, which is again a\\ncontradiction.\\nWe conclude that the bijection g does not exist. Therefore, A and P(A)\\ndo not have the same size.\\n168\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.2.1\\nThe Halting Problem revisited\\nNow that we know about countability, we give a diﬀerent way to look at the\\nproof in Section 5.1.5 of the fact that the language\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}\\nis undecidable.\\nYou should convince yourself that the proof given below\\nfollows the same reasoning as the one used in the proof of Theorem 5.2.4.\\nWe ﬁrst argue that the set of all Java programs is countable. Indeed,\\nevery Java program P can be described by a ﬁnite amount of text. In fact,\\nwe have been using ⟨P⟩to denote such a description by a binary string. For\\nany integer n ≥0, there are at most 2n (i.e., ﬁnitely many) Java programs\\nP whose description ⟨P⟩has length n. Therefore, to obtain a list of all Java\\nprograms, we do the following:\\n• List all Java programs P whose description ⟨P⟩has length 0. (Well,\\nthe empty string does not describe any Java program, so in this step,\\nnothing happens.)\\n• List all Java programs P whose description ⟨P⟩has length 1.\\n• List all Java programs P whose description ⟨P⟩has length 2.\\n• List all Java programs P whose description ⟨P⟩has length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every Java program occurs exactly once. Therefore, the\\nset of all Java programs is countable.\\nConsider an inﬁnite list\\nP1, P2, P3, . . .\\nin which every Java program occurs exactly once.\\nAssume that the language Halt is decidable. Then there exists a Java\\nprogram H that decides this language. We may assume that, on input ⟨P, w⟩,\\nH returns true if P terminates on input w, and false if P does not terminate\\non input w.\\nWe construct a new Java program D that does the following:\\n5.3.\\nRice’s Theorem\\n169\\nAlgorithm D:\\nOn input ⟨Pn⟩, where n is a positive integer, the\\nnew Java program D does the following:\\nStep 1: Run the Java program H on the input ⟨Pn, ⟨Pn⟩⟩.\\nStep 2:\\n• If H returns true, then D goes into an inﬁnite loop.\\n• If H returns false, then D returns true and terminates its com-\\nputation.\\nObserve that D can be written as a Java program. Therefore, there exists\\nan integer n ≥1 such that D = Pn. The next two observations follow from\\nthe pseudocode:\\n• If D terminates on input ⟨Pn⟩, then H returns false on input ⟨Pn, ⟨Pn⟩⟩,\\ni.e., Pn does not terminate on input ⟨Pn⟩.\\n• If D does not terminate on input ⟨Pn⟩, then H returns true on input\\n⟨Pn, ⟨Pn⟩⟩, i.e., Pn terminates on input ⟨Pn⟩.\\nThus,\\n• D terminates on input ⟨Pn⟩if and only if Pn does not terminate on\\ninput ⟨Pn⟩.\\nSince D = Pn, this becomes\\n• D terminates on input ⟨D⟩if and only if D does not terminate on input\\n⟨D⟩.\\nThus, we have obtained a contradiction.\\nRemark 5.2.6 We deﬁned the Java program D in such a way that, for each\\nn ≥1, the computation of D on input ⟨Pn⟩diﬀers from the computation of\\nPn on input ⟨Pn⟩. Hence, for each n ≥1, D ̸= Pn. However, since D is a\\nJava program, there must be an integer n ≥1 such that D = Pn.\\n170\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.3\\nRice’s Theorem\\nWe have seen two examples of undecidable languages: ATM and Halt. In this\\nsection, we prove that many languages involving Turing machines (or Java\\nprograms) are undecidable.\\nDeﬁne T to be the set of binary encodings of all Turing machines, i.e.,\\nT = {⟨M⟩: M is a Turing machine with input alphabet {0,1}}.\\nTheorem 5.3.1 (Rice) Let P be a subset of T such that\\n1. P ̸= ∅, i.e., there exists a Turing machine M such that ⟨M⟩∈P,\\n2. P is a proper subset of T , i.e., there exists a Turing machine N such\\nthat ⟨N⟩̸∈P, and\\n3. for any two Turing machines M1 and M2 with L(M1) = L(M2),\\n(a) either both ⟨M1⟩and ⟨M2⟩are in P or\\n(b) none of ⟨M1⟩and ⟨M2⟩is in P.\\nThen the language P is undecidable.\\nYou can think of P as the set of all Turing machines that satisfy a certain\\nproperty. The ﬁrst two conditions state that at least one Turing machine\\nsatisﬁes this property and not all Turing machines satisfy this property. The\\nthird condition states that, for any Turing machine M, whether or not M\\nsatisﬁes this property only depends on the language L(M) of M.\\nHere are some examples of languages that satisfy the conditions in Rice’s\\nTheorem:\\nP1 = {⟨M⟩: M is a Turing machine and ϵ ∈L(M)},\\nP2 = {⟨M⟩: M is a Turing machine and L(M) = {1011, 001100}},\\nP3 = {⟨M⟩: M is a Turing machine and L(M) is a regular language}.\\nYou are encouraged to verify that Rice’s Theorem indeed implies that each\\nof P1, P2, and P3 is undecidable.\\n5.3.\\nRice’s Theorem\\n171\\n5.3.1\\nProof of Rice’s Theorem\\nThe strategy of the proof is as follows: Assuming that the language P is\\ndecidable, we show that the language\\nHalt = {⟨M, w⟩:\\nM is a Turing machine that terminates on\\nthe input string w}\\nis decidable. This will contradict Theorem 5.1.7.\\nThe assumption that P is decidable implies the existence of a Turing\\nmachine H that decides P. Observe that H takes as input a binary string\\n⟨M⟩encoding a Turing machine M. In order to show that Halt is decidable,\\nwe need a Turing machine that takes as input a binary string ⟨M, w⟩encoding\\na Turing machine M and a binary string w. In the rest of this section, we\\nwill explain how this Turing machine can be obtained.\\nLet M1 be a Turing machine that, for any input string, switches in its\\nﬁrst computation step from its start state to its reject state. In other words,\\nM1 is a Turing machine with L(M1) = ∅. We assume that\\n⟨M1⟩̸∈P.\\n(At the end of the proof, we will consider the case when ⟨M1⟩∈P.) We also\\nchoose a Turing machine M2 such that\\n⟨M2⟩∈P.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nif M terminates\\nthen run M2 on input x;\\nif M2 terminates in the accept state\\nthen terminate in the accept state\\nelse if M2 terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\n172\\nChapter 5.\\nDecidable and Undecidable Languages\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that for any string x,\\nx is accepted by TMw if and only if x is accepted by M2.\\nThus, L(TMw) = L(M2).\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩̸∈Halt.\\nThen it follows from the pseudocode that for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅. In particular, L(TMw) =\\nL(M1).\\nRecall that ⟨M1⟩̸∈P, whereas ⟨M2⟩∈P. Then the following follows from\\nthe third condition in Rice’s Theorem:\\n• If ⟨M, w⟩∈Halt, then ⟨TMw⟩∈P.\\n• If ⟨M, w⟩̸∈Halt, then ⟨TMw⟩̸∈P.\\nThus, we have obtained a connection between the languages P and Halt.\\nThis suggests that we proceed as follows.\\nAssume that the language P is decidable. Let H be a Turing machine\\nthat decides P. Then, for any Turing machine M,\\n• if ⟨M⟩∈P, then H accepts the string ⟨M⟩,\\n• if ⟨M⟩̸∈P, then H rejects the string ⟨M⟩, and\\n• H terminates on any input string.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\n5.4.\\nEnumerability\\n173\\nIt follows from the pseudocode that H′ terminates on any input. We\\nobserve the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that ⟨TMw⟩∈P.\\nSince H decides the language P, it follows that H accepts the string\\n⟨TMw⟩. Therefore, from the pseudocode, H′ accepts the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then we have seen before that ⟨TMw⟩̸∈\\nP. Since H decides the language P, it follows that H rejects (and\\nterminates on) the string ⟨TMw⟩. Therefore, from the pseudocode, H′\\nrejects (and terminates on) the string ⟨M, w⟩.\\nWe have shown that the Turing machine H′ decides the language Halt.\\nThis is a contradiction and, therefore, we conclude that the language P is\\nundecidable.\\nUntil now, we assumed that ⟨M1⟩̸∈P. If ⟨M1⟩∈P, then we repeat the\\nproof with P replaced by its complement P. This revised proof then shows\\nthat P is undecidable. Since for every language L,\\nL is decidable if and only if L is decidable,\\nwe again conclude that P is undecidable.\\n5.4\\nEnumerability\\nWe now come to the last class of languages in this chapter:\\nDeﬁnition 5.4.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is enumerable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, does not terminate in the accept state. That is, either the\\ncomputation terminates in the reject state or the computation does not\\nterminate.\\n174\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is enumerable, if there exists an algorithm\\nhaving the following property. If w ∈A, then the algorithm terminates on\\nthe input string w and tells us that w ∈A. On the other hand, if w ̸∈A,\\nthen either (i) the algorithm terminates on the input string w and tells us\\nthat w ̸∈A or (ii) the algorithm does not terminate on the input string w,\\nin which case it does not tell us that w ̸∈A.\\nIn Section 5.5, we will show where the term “enumerable” comes from.\\nThe following theorem follows immediately from Deﬁnitions 5.1.1 and 5.4.1.\\nTheorem 5.4.2 Every decidable language is enumerable.\\nIn the following subsections, we will give some examples of enumerable\\nlanguages.\\n5.4.1\\nHilbert’s problem\\nWe have seen Hilbert’s problem in Section 4.4: Is there an algorithm that\\ndecides, for any given polynomial p with integer coeﬃcients, whether or not\\np has integral roots? If we formulate this problem in terms of languages,\\nthen Hilbert asked whether or not the language\\nHilbert = {⟨p⟩:\\np is a polynomial with integer coeﬃcients\\nthat has an integral root}\\nis decidable. As usual, ⟨p⟩denotes the binary string that forms an encoding\\nof the polynomial p.\\nAs we mentioned in Section 4.4, it was proven by Matiyasevich in 1970\\nthat the language Hilbert is not decidable. We claim, that this language\\nis enumerable.\\nIn order to prove this claim, we have to construct an al-\\ngorithm Hilbert with the following property: For any input polynomial p\\nwith integer coeﬃcients,\\n• if p has an integral root, then algorithm Hilbert will ﬁnd one in a\\nﬁnite amount of time,\\n• if p does not have an integral root, then either algorithm Hilbert ter-\\nminates and tells us that p does not have an integral root, or algorithm\\nHilbert does not terminate.\\n5.4.\\nEnumerability\\n175\\nRecall that Z denotes the set of integers. Algorithm Hilbert does the\\nfollowing, on any input polynomial p with integer coeﬃcients.\\nLet n de-\\nnote the number of variables in p. Algorithm Hilbert tries all elements\\n(x1, x2, . . . , xn) ∈Zn, in a systematic way, and for each such element, it\\ncomputes p(x1, x2, . . . , xn). If this value is zero, then algorithm Hilbert\\nterminates and accepts the input.\\nWe observe the following:\\n• If ⟨p⟩∈Hilbert, then algorithm Hilbert terminates and accepts p,\\nprovided we are able to visit all elements (x1, x2, . . . , xn) ∈Zn in a\\n“systematic way”.\\n• If ⟨p⟩̸∈Hilbert, then p(x1, x2, . . . , xn) ̸= 0 for all (x1, x2, . . . , xn) ∈Zn\\nand, therefore, algorithm Hilbert does not terminate.\\nThese are exactly the requirements for the language Hilbert to be enumerable.\\nIt remains to explain how we visit all elements (x1, x2, . . . , xn) ∈Zn in a\\nsystematic way. For any integer d ≥0, let Hd denote the hypercube in Zn\\nwith sides of length 2d that is centered at the origin. That is, Hd consists\\nof the set of all points (x1, x2, . . . , xn) in Zn, such that −d ≤xi ≤d for all\\n1 ≤i ≤n and there exists at least one index j for which xj = d or xj = −d.\\nWe observe that Hd contains a ﬁnite number of elements. In fact, if d ≥1,\\nthen this number is equal to (2d + 1)n −(2d −1)n. The algorithm will visit\\nall elements (x1, x2, . . . , xn) ∈Zn, in the following order: First, it visits the\\norigin, which is the only element of H0. Then, it visits all elements of H1,\\nfollowed by all elements of H2, etc., etc.\\nTo summarize, we obtain the following algorithm, proving that the lan-\\nguage Hilbert is enumerable:\\n176\\nChapter 5.\\nDecidable and Undecidable Languages\\nAlgorithm Hilbert(⟨p⟩):\\nn := the number of variables in p;\\nd := 0;\\nwhile d ≥0\\ndo for each (x1, x2, . . . , xn) ∈Hd\\ndo R := p(x1, x2, . . . , xn);\\nif R = 0\\nthen terminate and accept\\nendif\\nendfor;\\nd := d + 1\\nendwhile\\nTheorem 5.4.3 The language Hilbert is enumerable.\\n5.4.2\\nThe language ATM\\nWe have shown in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nis undecidable. In this section, we will prove that this language is enumerable.\\nThus, we have to construct an algorithm P having the following property,\\nfor any given input string u:\\n• If\\n– u encodes a Turing machine M and an input string w for M (i.e.,\\nu is in the correct format ⟨M, w⟩) and\\n– ⟨M, w⟩∈ATM (i.e., M accepts w),\\nthen algorithm P terminates in its accept state.\\n• In all other cases, either algorithm P terminates in its reject state, or\\nalgorithm P does not terminate.\\nOn input string u = ⟨M, w⟩, which is in the correct format, algorithm P does\\nthe following:\\n5.5.\\nWhere does the term “enumerable” come from?\\n177\\n1. It simulates the computation of M on input w.\\n2. If M terminates in its accept state, then P terminates in its accept\\nstate.\\n3. If M terminates in its reject state, then P terminates in its reject state.\\n4. If M does not terminate, then P does not terminate.\\nHence, if u = ⟨M, w⟩∈ATM, then M accepts w and, therefore, P accepts\\nu. On the other hand, if u = ⟨M, w⟩̸∈ATM, then M does not accept w. This\\nmeans that, on input w, M either terminates in its reject state or does not\\nterminate. But this implies that, on input u, P either terminates in its reject\\nstate or does not terminate. This proves that algorithm P has the properties\\nthat are needed in order to show that the language ATM is enumerable. We\\nhave proved the following result:\\nTheorem 5.4.4 The language ATM is enumerable.\\n5.5\\nWhere does the term “enumerable” come\\nfrom?\\nIn Deﬁnition 5.4.1, we have deﬁned what it means for a language to be\\nenumerable. In this section, we will see where this term comes from.\\nDeﬁnition 5.5.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. An\\nenumerator for A is a Turing machine E having the following properties:\\n1. Besides the standard features as in Section 4.1, E has a print tape and\\na print state. During its computation, E writes symbols of Σ on the\\nprint tape. Each time, E enters the print state, the current string on\\nthe print tape is sent to the printer and the print tape is made empty.\\n2. At the start of the computation, all tapes are empty and E is in the\\nstart state.\\n3. Every string w in A is sent to the printer at least once.\\n4. Every string w that is not in A is never sent to the printer.\\n178\\nChapter 5.\\nDecidable and Undecidable Languages\\nThus, an enumerator E for A really enumerates all strings in the language\\nA. There is no particular order in which the strings of A are sent to the\\nprinter. Moreover, a string in A may be sent to the printer multiple times.\\nIf the language A is inﬁnite, then the Turing machine E obviously does not\\nterminate; however, every string in A (and only strings in A) will be sent to\\nthe printer at some time during the computation.\\nTo give an example, let A = {0n : n ≥0}. The following Turing machine\\nis an enumerator for A.\\nTuring machine StringsOfZeros:\\nn := 0;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo write 0 on the print tape\\nendfor;\\nenter the print state;\\nn := n + 1\\nendwhile\\nIn the rest of this section, we will prove the following result.\\nTheorem 5.5.2 A language is enumerable if and only if it has an enumer-\\nator.\\nFor the ﬁrst part of the proof, assume that the language A has an enu-\\nmerator E. We construct the following Turing machine M, which takes an\\narbitrary string w as input:\\nTuring machine M(w):\\nrun E; every time E enters the print state:\\nlet v be the string on the print tape;\\nif w = v\\nthen terminate in the accept state\\nendif\\nThe Turing machine M has the following properties:\\n• If w ∈A, then w will be sent to the printer at some time during the\\n5.5.\\nWhere does the term “enumerable” come from?\\n179\\ncomputation of E. It follows from the pseudocode that, on input w,\\nM terminates in the accept state.\\n• If w ̸∈A, then E will never sent w to the printer. It follows from the\\npseudocode that, on input w, M does not terminate.\\nThus, M satisﬁes the conditions in Deﬁnition 5.4.1. We conclude that the\\nlanguage A is enumerable.\\nTo prove the converse, we now assume that A is enumerable. Let M be\\na Turing machine that satisﬁes the conditions in Deﬁnition 5.4.1.\\nWe ﬁx an inﬁnite list\\ns1, s2, s3, . . .\\nof all strings in Σ∗. For example, if Σ = {0, 1}, then we can take this list to\\nbe\\nϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .\\nWe construct the following Turing machine E, which takes the empty\\nstring as input:\\nTuring machine E:\\nn := 1;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo run M for n steps on the input string si;\\nif M accepts si within n steps\\nthen write si on the print tape;\\nenter the print state\\nendif\\nendfor;\\nn := n + 1\\nendwhile\\nWe claim that E is an enumerator for the language A. To prove this, it\\nis obvious that any string that is sent to the printer by E belongs to A.\\nIt remains to prove that every string in A will be sent to the printer by E.\\nLet w be a string in A. Then, on input w, the Turing machine M terminates\\nin the accept state. Let m be the number of steps made by M on input w.\\nLet i be the index such that w = si. Deﬁne n = max(m, i). Consider the\\n180\\nChapter 5.\\nDecidable and Undecidable Languages\\nn-th iteration of the while-loop and the i-th iteration of the for-loop. In this\\niteration, M accepts si = w in m ≤n steps and, therefore, w is sent to the\\nprinter.\\n5.6\\nMost languages are not enumerable\\nIn this section, we will prove that most languages are not enumerable. The\\nproof is based on the following two facts:\\n• The set consisting of all enumerable languages is countable; we will\\nprove this in Section 5.6.1.\\n• The set consisting of all languages is not countable; we will prove this\\nin Section 5.6.2.\\n5.6.1\\nThe set of enumerable languages is countable\\nWe deﬁne the set E as\\nE = {A : A ⊆{0, 1}∗is an enumerable language}.\\nIn words, E is the set whose elements are the enumerable languages. Every\\nelement of E is an enumerable language. Hence, every element of the set E\\nis itself a set consisting of strings.\\nLemma 5.6.1 The set E is countable.\\nProof. Let A ⊆{0, 1}∗be an enumerable language. There exists a Turing\\nmachine TA that satisﬁes the conditions in Deﬁnition 5.4.1.\\nThis Turing\\nmachine TA can be uniquely speciﬁed by a string in English. This string can\\nbe converted to a binary string sA. Hence, the binary string sA is a unique\\nencoding of the Turing machine TA.\\nConsider the set\\nS = {sA : A ⊆{0, 1}∗is an enumerable language}.\\nObserve that the function f : E →S, deﬁned by f(A) = sA for each A ∈E,\\nis a bijection. Therefore, the sets E and S have the same size. Hence, in\\norder to prove that the set E is countable, it is suﬃcient to prove that the\\nset S is countable.\\n5.6.\\nMost languages are not enumerable\\n181\\nWhy is the set S countable? For each integer n ≥0, there are exactly 2n\\nbinary strings of length n. Since there are binary strings that are not encod-\\nings of Turing machines, the set S contains at most 2n strings of length n.\\nIn particular, the number of strings in S having length n is ﬁnite. Therefore,\\nwe obtain an inﬁnite list of the elements of S in the following way:\\n• List all strings in S having length 0. (Well, the empty string is not in\\nS, so in this step, nothing happens.)\\n• List all strings in S having length 1.\\n• List all strings in S having length 2.\\n• List all strings in S having length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every element of S occurs exactly once. Therefore, S is\\ncountable.\\n5.6.2\\nThe set of all languages is not countable\\nWe deﬁne the set L as\\nL = {A : A ⊆{0, 1}∗is a language}.\\nIn words, L is the set consisting of all languages. Every element of the set L\\nis a set consisting of strings.\\nLemma 5.6.2 The set L is not countable.\\nProof. We deﬁne the set B as\\nB = {w : w is an inﬁnite binary sequence}.\\nWe claim that this set is not countable. The proof of this claim is almost\\nidentical to the proof of Theorem 5.2.4. We assume that the set B is count-\\nable. Then there exists a bijection f : N →B. Thus, for each n ∈N, f(n) is\\nan inﬁnite binary sequence. We can write\\nB = {f(1), f(2), f(3), . . .},\\n(5.3)\\n182\\nChapter 5.\\nDecidable and Undecidable Languages\\nwhere every element of B occurs exactly once in the set on the right-hand\\nside.\\nWe deﬁne the inﬁnite binary sequence w = w1w2w3 . . ., where, for each\\ninteger n ≥1,\\nwn =\\n\\x1a 1\\nif the n-th bit of f(n) is 0,\\n0\\nif the n-th bit of f(n) is 1.\\nSince w ∈B, it follows from (5.3) that there is an element n ∈N, such that\\nf(n) = w. Hence, the n-th bits of f(n) and w are equal. But, by deﬁnition,\\nthese n-th bits are not equal. This is a contradiction and, therefore, the set\\nB is not countable.\\nIn the rest of the proof, we will show that the sets L and B have the same\\nsize. Since B is not countable, this will imply that L is not countable.\\nIn order to prove that L and B have the same size, we have to show that\\nthere exists a bijection\\ng : L →B.\\nWe ﬁrst observe that the set {0, 1}∗is countable, because for each integer\\nn ≥0, there are only ﬁnitely many (to be precise, exactly 2n) binary strings\\nof length n. In fact, we can write\\n{0, 1}∗= {ϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .}.\\nFor each integer n ≥1, we denote by sn the n-th string in this list. Hence,\\n{0, 1}∗= {s1, s2, s3, . . .}.\\n(5.4)\\nNow we are ready to deﬁne the bijection g : L →B: Let A ∈L, i.e.,\\nA ⊆{0, 1}∗is a language. We deﬁne the inﬁnite binary sequence g(A) as\\nfollows: For each integer n ≥1, the n-th bit of g(A) is equal to\\n\\x1a 1\\nif sn ∈A,\\n0\\nif sn ̸∈A.\\nIn words, the inﬁnite binary sequence g(A) contains a 1 exactly in those\\npositions n for which the string sn in (5.4) is in the language A.\\nTo give an example, assume that A is the language consisting of all binary\\nstrings that start with 0. The following table gives the corresponding inﬁnite\\nbinary sequence g(A) (this sequence is obtained by reading the rightmost\\ncolumn from top to bottom):\\n5.6.\\nMost languages are not enumerable\\n183\\n{0, 1}∗\\nA\\ng(A)\\nϵ\\nnot in A\\n0\\n0\\nin A\\n1\\n1\\nnot in A\\n0\\n00\\nin A\\n1\\n01\\nin A\\n1\\n10\\nnot in A\\n0\\n11\\nnot in A\\n0\\n000\\nin A\\n1\\n001\\nin A\\n1\\n010\\nin A\\n1\\n100\\nnot in A\\n0\\n011\\nin A\\n1\\n101\\nnot in A\\n0\\n110\\nnot in A\\n0\\n111\\nnot in A\\n0\\n...\\n...\\n...\\nThe function g deﬁned above has the following properties:\\n• If A and A′ are two diﬀerent languages in L, then g(A) ̸= g(A′).\\n• For every inﬁnite binary sequence w in B, there exists a language A in\\nL, such that g(A) = w.\\nThis means that the function g is a bijection from L to B.\\n5.6.3\\nThere are languages that are not enumerable\\nWe have proved that the set\\nE = {A : A ⊆{0, 1}∗is an enumerable language}\\nis countable, whereas the set\\nL = {A : A ⊆{0, 1}∗is a language}\\nis not countable. This means that there are “more” languages in L than\\nthere are in E, proving the following result:\\n184\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.6.3 There exist languages that are not enumerable.\\nThe proof given above shows the existence of languages that are not\\nenumerable. However, the proof does not give us a speciﬁc example of a\\nlanguage that is not enumerable. In the next sections, we will see examples\\nof such languages. Before we move on to these examples, we mention the\\ndiﬀerence between being countable and being enumerable:\\n• Any language A is countable, i.e., we can number the elements of A\\nand, thus, write\\nA = {s1, s2, s3, s4, . . .}.\\n• If the language A is enumerable, then, by Theorem 5.5.2, there is an\\nalgorithm that produces this numbering.\\n• If the language A is not enumerable, then, again by Theorem 5.5.2,\\nthere does not exist an algorithm that produces this numbering.\\n5.7\\nThe relationship between decidable and\\nenumerable languages\\nWe know from Theorem 5.4.2 that every decidable language is enumerable.\\nOn the other hand, we know from Theorems 5.1.6 and 5.4.4 that the converse\\nis not true. The following result should not come as a surprise:\\nTheorem 5.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then,\\nA is decidable if and only if both A and its complement A are enumerable.\\nProof. We ﬁrst assume that A is decidable. Then, by Theorem 5.4.2, A\\nis enumerable. Since A is decidable, it is not diﬃcult to see that A is also\\ndecidable. Then, again by Theorem 5.4.2, A is enumerable.\\nTo prove the converse, we assume that both A and A are enumerable.\\nSince A is enumerable, there exists a Turing machine M1, such that for any\\nstring w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M1, on the input string w, terminates\\nin the accept state of M1.\\n5.7.\\nDecidable versus enumerable languages\\n185\\n• If w ̸∈A, then the computation of M1, on the input string w, terminates\\nin the reject state of M1 or does not terminate.\\nSimilarly, since A is enumerable, there exists a Turing machine M2, such that\\nfor any string w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M2, on the input string w, terminates\\nin the accept state of M2.\\n• If w ̸∈A, then the computation of M2, on the input string w, terminates\\nin the reject state of M2 or does not terminate.\\nWe construct a two-tape Turing machine M:\\nTwo-tape Turing machine M: For any input string w ∈Σ∗, M\\ndoes the following:\\n• M simulates the computation of M1, on input w, on the ﬁrst\\ntape, and, simultaneously, it simulates the computation of M2,\\non input w, on the second tape.\\n• If the simulation of M1 terminates in the accept state of M1,\\nthen M terminates in its accept state.\\n• If the simulation of M2 terminates in the accept state of M2,\\nthen M terminates in its reject state.\\nObserve the following:\\n• If w ∈A, then M1 terminates in its accept state and, therefore, M\\nterminates in its accept state.\\n• If w ̸∈A, then M2 terminates in its accept state and, therefore, M\\nterminates in its reject state.\\nWe conclude that the Turing machine M accepts all strings in A, and rejects\\nall strings that are not in A. This proves that the language A is decidable.\\nWe now use Theorem 5.7.1 to give examples of languages that are not\\nenumerable:\\n186\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.7.2 The language ATM is not enumerable.\\nProof. We know from Theorems 5.4.4 and 5.1.6 that the language ATM is\\nenumerable but not decidable. Combining these facts with Theorem 5.7.1\\nimplies that the language ATM is not enumerable.\\nThe following result can be proved in exactly the same way:\\nTheorem 5.7.3 The language Halt is not enumerable.\\n5.8\\nA language A such that both A and A are\\nnot enumerable\\nIn Theorem 5.7.2, we have seen that the complement of the language ATM\\nis not enumerable.\\nIn Theorem 5.4.4, however, we have shown that the\\nlanguage ATM itself is enumerable. In this section, we consider the language\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\nWe will show the following result:\\nTheorem 5.8.1 Both EQTM and its complement EQTM are not enumer-\\nable.\\n5.8.1\\nEQTM is not enumerable\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We construct\\na new Turing machine TMw that takes as input an arbitrary binary string x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nterminate in the accept state\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that every string x is accepted by TMw.\\nThus, L(TMw) = {0, 1}∗.\\n5.8.\\nBoth A and A not enumerable\\n187\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩∈Halt.\\nThen it follows from the pseudocode that, for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that rejects every input string;\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. By our choice of M1, we have L(M1) = ∅as well. Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt, i.e., ⟨M, w⟩∈Halt. Then we have seen\\nbefore that L(TMw) ̸= ∅= L(M1). Therefore, on input ⟨M1, TMw⟩, H\\neither terminates in the reject state or does not terminate. It follows\\n188\\nChapter 5.\\nDecidable and Undecidable Languages\\nfrom the pseudocode that, on input ⟨M, w⟩, H′ either terminates in the\\nreject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\n5.8.2\\nEQTM is not enumerable\\nThis proof is symmetric to the one in Section 5.8.1.\\nFor a ﬁxed Turing\\nmachine M and a ﬁxed binary string w, we will use the same Turing machine\\nTMw as in Section 5.8.1.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that accepts every input string;\\nconstruct the Turing machine TMw of Section 5.8.1;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\nExercises\\n189\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. Thus, by our choice of M1, we have L(TMw) ̸= L(M1). Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then L(TMw) = {0, 1}∗= L(M1) and, on\\ninput ⟨M1, TMw⟩, H either terminates in the reject state or does not\\nterminate. It follows from the pseudocode that, on input ⟨M, w⟩, H′\\neither terminates in the reject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\nExercises\\n5.1 Prove that the language\\n{w ∈{0, 1}∗: w is the binary representation of 2n for some n ≥0}\\nis decidable. In other words, construct a Turing machine that gets as input\\nan arbitrary number x ∈N, represented in binary as a string w, and that\\ndecides whether or not x is a power of two.\\n5.2 Let F be the set of all functions f : N →N.\\nProve that F is not\\ncountable.\\n5.3 A function f : N →N is called computable, if there exists a Turing\\nmachine, that gets as input an arbitrary positive integer n, written in binary,\\nand gives as output the value of f(n), again written in binary. This Turing\\nmachine has a ﬁnal state. As soon as the Turing machine enters this ﬁnal\\nstate, the computation terminates, and the output is the binary string that\\nis written on its tape.\\nProve that there exist functions f : N →N that are not computable.\\n5.4 Let n be a ﬁxed positive integer, and let k be the number of bits in the\\nbinary representation of n. (Hence, k = 1 + ⌊log n⌋.) Construct a Turing\\nmachine with one tape, tape alphabet {0, 1, 2}, and exactly k + 1 states\\nq0, q1, . . . , qk, that does the following:\\n190\\nChapter 5.\\nDecidable and Undecidable Languages\\nStart of the computation: The tape is empty, i.e., every cell of the tape\\ncontains 2, and the Turing machine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer n, the tape head is on the rightmost bit of the binary represen-\\ntation of n, and the Turing machine is in the ﬁnal state qk.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state qk. As soon as state qk is entered,\\nthe Turing machine terminates.\\n5.5 Give an informal description (in plain English) of a Turing machine\\nwith three tapes, that gets as input the binary representation of an arbitrary\\ninteger m ≥1, and returns as output the unary representation of m.\\nStart of the computation: The ﬁrst tape contains the binary representa-\\ntion of the input m. The other two tapes are empty (i.e., contain only 2s).\\nThe Turing machine is in the start state.\\nEnd of the computation: The third tape contains the unary representation\\nof m, i.e., a string consisting of m many ones. The Turing machine is in the\\nﬁnal state.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this ﬁnal state is entered,\\nthe Turing machine terminates.\\nHint: Use the second tape to maintain a string of ones, whose length is\\na power of two.\\n5.6 In this exercise, you are asked to prove that the busy beaver function\\nBB : N →N is not computable.\\nFor any integer n ≥1, we deﬁne TM n to be the set of all Turing machines\\nM, such that\\n• M has one tape,\\n• M has exactly n states,\\n• the tape alphabet of M is {0, 1, 2}, and\\n• M terminates, when given the empty string ϵ as input.\\nExercises\\n191\\nFor every Turing machine M ∈TM n, we deﬁne f(M) to be the number of\\nones on the tape, after the computation of M, on the empty input string,\\nhas terminated.\\nThe busy beaver function BB : N →N is deﬁned as\\nBB(n) := max{f(M) : M ∈TM n}, for every n ≥1.\\nIn words, BB(n) is the maximum number of ones that any Turing machine\\nwith n states can produce, when given the empty string as input, and as-\\nsuming the Turing machine terminates on this input.\\nProve that the function BB is not computable.\\nHint: Assume that BB is computable. Then there exists a Turing ma-\\nchine M that, for any given n ≥1, computes the value of BB(n). Fix a large\\ninteger n ≥1. Deﬁne (in plain English) a Turing machine that, when given\\nthe empty string as input, terminates and outputs a string consisting of more\\nthan BB(n) many ones. Use Exercises 5.4 and 5.5 to argue that there exists\\nsuch a Turing machine having O(log n) states. Then, if you assume that n\\nis large enough, the number of states is at most n.\\n5.7 Since the set\\nT = {M : M is a Turing machine}\\nis countable, there is an inﬁnite list\\nM1, M2, M3, M4, . . . ,\\nsuch that every Turing machine occurs exactly once in this list.\\nFor any positive integer n, let ⟨n⟩denote the binary representation of n;\\nobserve that ⟨n⟩is a binary string.\\nLet A be the language deﬁned as\\nA = {⟨n⟩:\\nthe Turing machine Mn terminates on the input string ⟨n⟩,\\nand it rejects this string}.\\nProve that the language A is undecidable.\\n5.8 Consider the three languages\\nEmpty = {⟨M⟩: M is a Turing machine for which L(M) = ∅},\\n192\\nChapter 5.\\nDecidable and Undecidable Languages\\nUselessState = {⟨M, q⟩:\\nM is a Turing machine, q is a state of M,\\nfor every input string w, the computation of M on\\ninput w never visits state q},\\nand\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\n• Use Rice’s Theorem to show that Empty is undecidable.\\n• Use the ﬁrst part to show that UselessState is undecidable.\\n• Use the ﬁrst part to show that EQTM is undecidable.\\n5.9 Consider the language\\nREGTM = {⟨M⟩: M is a Turing machine whose language L(M) is regular}.\\nUse Rice’s Theorem to prove that REGTM is undecidable.\\n5.10 We have seen in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts w}\\nis undecidable. Consider the language REGTM of Exercise 5.9. The questions\\nbelow will lead you through a proof of the claim that the language REGTM\\nis undecidable.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nif x = 0n1n for some n ≥0\\nthen terminate in the accept state\\nelse run M on the input string w;\\nif M terminates in the accept state\\nthen terminate in the accept state\\nelse if M terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\nExercises\\n193\\nAnswer the following two questions:\\n• Assume that M accepts the string w. What is the language L(TMw) of\\nthe new Turing machine TMw?\\n• Assume that M does not accept the string w. What is the language\\nL(TMw) of the new Turing machine TMw?\\nThe goal is to prove that the language REGTM is undecidable. We will\\nprove this by contradiction. Thus, we assume that R is a Turing machine\\nthat decides REGTM. Recall what this means:\\n• If M is a Turing machine whose language is regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the accept state.\\n• If M is a Turing machine whose language is not regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the reject state.\\nWe construct a new Turing machine R′ which takes as input an arbitrary\\nTuring machine M and an arbitrary binary string w:\\nTuring machine R′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun R on the input ⟨TMw⟩;\\nif R terminates in the accept state\\nthen terminate in the accept state\\nelse if R terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nProve that M accepts w if and only if R′ (when given ⟨M, w⟩as input),\\nterminates in the accept state.\\nNow ﬁnish the proof by arguing that the language REGTM is undecidable.\\n5.11 A Java program P is called a Hello-World-program, if the following is\\ntrue: When given the empty string ϵ as input, P outputs the string Hello\\nWorld and then terminates. (We do not care what P does when the input\\nstring is non-empty.)\\n194\\nChapter 5.\\nDecidable and Undecidable Languages\\nConsider the language\\nHW = {⟨P⟩: P is a Hello-World-program}.\\nThe questions below will lead you through a proof of the claim that the\\nlanguage HW is undecidable.\\nConsider a ﬁxed Java program P and a ﬁxed binary string w. We write\\na new Java program JPw which takes as input an arbitrary binary string x:\\nJava program JPw(x):\\nrun P on the input w;\\nprint Hello World\\n• Argue that P terminates on input w if and only if ⟨JPw⟩∈HW .\\nThe goal is to prove that the language HW is undecidable. We will prove this\\nby contradiction. Thus, we assume that H is a Java program that decides\\nHW . Recall what this means:\\n• If P is a Hello-World-program, then H, when given ⟨P⟩as input, will\\nterminate in the accept state.\\n• If P is not a Hello-World-program, then H, when given ⟨P⟩as input,\\nwill terminate in the reject state.\\nWe write a new Java program H′ which takes as input the binary encoding\\n⟨P, w⟩of an arbitrary Java program P and an arbitrary binary string w:\\nJava program H′(⟨P, w⟩):\\nconstruct the Java program JPw described above;\\nrun H on the input ⟨JPw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\nArgue that the following are true:\\nExercises\\n195\\n• For any input ⟨P, w⟩, H′ terminates.\\n• If P terminates on input w, then H′ (when given ⟨P, w⟩as input),\\nterminates in the accept state.\\n• If P does not terminate on input w, then H′ (when given ⟨P, w⟩as\\ninput), terminates in the reject state.\\nNow ﬁnish the proof by arguing that the language HW is undecidable.\\n5.12 Prove that the language Halt, see Section 5.1.5, is enumerable.\\n5.13 We deﬁne the following language:\\nL = {u\\n:\\nu = ⟨0, M, w⟩for some ⟨M, w⟩∈ATM,\\nor u = ⟨1, M, w⟩for some ⟨M, w⟩̸∈ATM } .\\nProve that neither L nor its complement L is enumerable.\\nHint: There are two ways to solve this exercise. In the ﬁrst solution, (i)\\nyou assume that L is enumerable, and then prove that ATM is decidable, and\\n(ii) you assume that L is enumerable, and then prove that ATM is decidable.\\nIn the second solution, (i) you assume that L is enumerable, and then prove\\nthat ATM is enumerable, and (ii) you assume that L is enumerable, and then\\nprove that ATM is enumerable.\\n196\\nChapter 5.\\nDecidable and Undecidable Languages\\nChapter 6\\nComplexity Theory\\nIn the previous chapters, we have considered the problem of what can be\\ncomputed by Turing machines (i.e., computers) and what cannot be com-\\nputed. We did not, however, take the eﬃciency of the computations into\\naccount. In this chapter, we introduce a classiﬁcation of decidable languages\\nA, based on the running time of the “best” algorithm that decides A. That\\nis, given a decidable language A, we are interested in the “fastest” algorithm\\nthat, for any given string w, decides whether or not w ∈A.\\n6.1\\nThe running time of algorithms\\nLet M be a Turing machine, and let w be an input string for M. We deﬁne\\nthe running time tM(w) of M on input w as\\ntM(w) := the number of computation steps made by M on input w.\\nAs usual, we denote by |w|, the number of symbols in the string w. We\\ndenote the set of non-negative integers by N0.\\nDeﬁnition 6.1.1 Let Σ be an alphabet, let T : N0 →N0 be a function, let\\nA ⊆Σ∗be a decidable language, and let F : Σ∗→Σ∗be a computable\\nfunction.\\n• We say that the Turing machine M decides the language A in time T,\\nif\\ntM(w) ≤T(|w|)\\nfor all strings w in Σ∗.\\n198\\nChapter 6.\\nComplexity Theory\\n• We say that the Turing machine M computes the function F in time\\nT, if\\ntM(w) ≤T(|w|)\\nfor all strings w ∈Σ∗.\\nIn other words, the “running time function” T is a function of the length\\nof the input, which we usually denote by n. For any n, the value of T(n) is\\nan upper bound on the running time of the Turing machine M, on any input\\nstring of length n.\\nTo give an example, consider the Turing machine of Section 4.2.1 that\\ndecides, using one tape, the language consisting of all palindromes. The tape\\nhead of this Turing machine moves from the left to the right, then back to\\nthe left, then to the right again, back to the left, etc. Each time it reaches\\nthe leftmost or rightmost symbol, it deletes this symbol. The running time\\nof this Turing machine, on any input string of length n, is\\nO(1 + 2 + 3 + . . . + n) = O(n2).\\nOn the other hand, the running time of the Turing machine of Section 4.2.2,\\nwhich also decides the palindromes, but using two tapes instead of just one,\\nis O(n).\\nIn Section 4.4, we mentioned that all computation models listed there are\\nequivalent, in the sense that if a language can be decided in one model, it\\ncan be decided in any of the other models. We just saw, however, that the\\nlanguage consisting of all palindromes allows a faster algorithm on a two-\\ntape Turing machine than on one-tape Turing machines. (Even though we\\ndid not prove this, it is true that Ω(n2) is a lower bound on the running\\ntime to decide palindromes on a one-tape Turing machine.) The following\\ntheorem can be proved.\\nTheorem 6.1.2 Let A be a language (resp. let F be a function) that can be\\ndecided (resp. computed) in time T by an algorithm of type M. Then there is\\nan algorithm of type N that decides A (resp. computes F) in time T ′, where\\nM\\nN\\nT ′\\nk-tape Turing machine\\none-tape Turing machine\\nO(T 2)\\none-tape Turing machine\\nJava program\\nO(T 2)\\nJava program\\nk-tape Turing machine\\nO(T 4)\\n6.2.\\nThe complexity class P\\n199\\n6.2\\nThe complexity class P\\nDeﬁnition 6.2.1 We say that algorithm M decides the language A (resp.\\ncomputes the function F) in polynomial time, if there exists an integer k ≥1,\\nsuch that the running time of M is O(nk), for any input string of length n.\\nIt follows from Theorem 6.1.2 that this notion of “polynomial time” does\\nnot depend on the model of computation:\\nTheorem 6.2.2 Consider the models of computation “Java program”, “k-\\ntape Turing machine”, and “one-tape Turing machine”. If a language can\\nbe decided (resp. a function can be computed) in polynomial time in one of\\nthese models, then it can be decided (resp. computed) in polynomial time in\\nall of these models.\\nBecause of this theorem, we can deﬁne the following two complexity\\nclasses:\\nP := {A : the language A is decidable in polynomial time},\\nand\\nFP := {F : the function F is computable in polynomial time}.\\n6.2.1\\nSome examples\\nPalindromes\\nLet Pal be the language\\nPal := {w ∈{a, b}∗: w is a palindrome}.\\nWe have seen that there exists a one-tape Turing machine that decides Pal\\nin O(n2) time. Therefore, Pal ∈P.\\nSome functions in FP\\nThe following functions are in the class FP:\\n• F1 : N0 →N0 deﬁned by F1(x) := x + 1,\\n• F2 : N2\\n0 →N0 deﬁned by F2(x, y) := x + y,\\n• F3 : N2\\n0 →N0 deﬁned by F3(x, y) := xy.\\n200\\nChapter 6.\\nComplexity Theory\\nr\\nb\\nb\\nb\\nr\\nr\\nb\\nG1\\nG2\\nFigure 6.1: The graph G1 is 2-colorable; r stands for red; b stands for blue.\\nThe graph G2 is not 2-colorable.\\nContext-free languages\\nWe have shown in Section 5.1.3 that every context-free language is decid-\\nable. The algorithm presented there, however, does not run in polynomial\\ntime. Using a technique called dynamic programming (which you will learn\\nin COMP 3804), the following result can be shown:\\nTheorem 6.2.3 Let Σ be an alphabet, and let A ⊆Σ∗be a context-free\\nlanguage. Then A ∈P.\\nObserve that, obviously, every language in P is decidable.\\nThe 2-coloring problem\\nLet G be a graph with vertex set V and edge set E.\\nWe say that G is\\n2-colorable, if it is possible to give each vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. only two colors are used to color all vertices.\\nSee Figure 6.1 for two examples. We deﬁne the following language:\\n2Color := {⟨G⟩: the graph G is 2-colorable},\\nwhere ⟨G⟩denotes the binary string that encodes the graph G.\\n6.2.\\nThe complexity class P\\n201\\nWe claim that 2Color ∈P. In order to show this, we have to construct an\\nalgorithm that decides in polynomial time, whether or not any given graph\\nis 2-colorable.\\nLet G be an arbitrary graph with vertex set V = {1, 2, . . . , m}. The edge\\nset of G is given by an adjacency matrix. This matrix, which we denote by\\nE, is a two-dimensional array with m rows and m columns. For all i and j\\nwith 1 ≤i ≤m and 1 ≤j ≤m, we have\\nE(i, j) =\\n\\x1a 1\\nif (i, j) is an edge of G,\\n0\\notherwise.\\nThe length of the input G, i.e., the number of bits needed to specify G, is\\nequal to m2 =: n. We will present an algorithm that decides, in O(n) time,\\nwhether or not the graph G is 2-colorable.\\nThe algorithm uses the colors red and blue. It gives the ﬁrst vertex the\\ncolor red. Then, the algorithm considers all vertices that are connected by\\nan edge to the ﬁrst vertex, and colors them blue. Now the algorithm is done\\nwith the ﬁrst vertex; it marks this ﬁrst vertex.\\nNext, the algorithm chooses a vertex i that already has a color, but that\\nhas not been marked. Then it considers all vertices j that are connected by\\nan edge to i. If j has the same color as i, then the input graph G is not\\n2-colorable. Otherwise, if vertex j does not have a color yet, the algorithm\\ngives j the color that is diﬀerent from i’s color. After having done this for\\nall neighbors j of i, the algorithm is done with vertex i, so it marks i.\\nIt may happen that there is no vertex i that already has a color but that\\nhas not been marked. (In other words, each vertex i that is not marked does\\nnot have a color yet.) In this case, the algorithm chooses an arbitrary vertex\\ni having this property, and colors it red. (This vertex i is the ﬁrst vertex in\\nits connected component that gets a color.)\\nThis procedure is repeated until all vertices of G have been marked.\\nWe now give a formal description of this algorithm. Vertex i has been\\nmarked, if\\n1. i has a color,\\n2. all vertices that are connected by an edge to i have a color, and\\n3. the algorithm has veriﬁed that each vertex that is connected by an edge\\nto i has a color diﬀerent from i’s color.\\n202\\nChapter 6.\\nComplexity Theory\\nThe algorithm uses two arrays f(1 . . . m) and a(1 . . . m), and a variable\\nM. The value of f(i) is equal to the color (red or blue) of vertex i; if i does\\nnot have a color yet, then f(i) = 0. The value of a(i) is equal to\\na(i) =\\n\\x1a 1\\nif vertex i has been marked,\\n0\\notherwise.\\nThe value of M is equal to the number of marked vertices. The algorithm\\nis presented in Figure 6.2. You are encouraged to convince yourself of the\\ncorrectness of this algorithm. That is, you should convince yourself that this\\nalgorithm returns YES if the graph G is 2-colorable, whereas it returns NO\\notherwise.\\nWhat is the running time of this algorithm? First we count the number\\nof iterations of the outer while-loop. In one iteration, either M increases by\\none, or a vertex i, for which a(i) = 0, gets the color red. In the latter case,\\nthe variable M is increased during the next iteration of the outer while-loop.\\nSince, during the entire outer while-loop, the value of M is increased from\\nzero to m, it follows that there are at most 2m iterations of the outer while-\\nloop. (In fact, the number of iterations is equal to m plus the number of\\nconnected components of G minus one.)\\nOne iteration of the outer while-loop takes O(m) time. Hence, the total\\nrunning time of the algorithm is O(m2), which is O(n). Therefore, we have\\nshown that 2Color ∈P.\\n6.3\\nThe complexity class NP\\nBefore we deﬁne the class NP, we consider some examples.\\nExample 6.3.1 Let G be a graph with vertex set V and edge set E, and\\nlet k ≥1 be an integer. We say that G is k-colorable, if it is possible to give\\neach vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. at most k diﬀerent colors are used to color all vertices.\\nWe deﬁne the following language:\\nkColor := {⟨G⟩: the graph G is k-colorable}.\\n6.3.\\nThe complexity class NP\\n203\\nAlgorithm 2Color\\nfor i := 1 to m do f(i) := 0; a(i) := 0 endfor;\\nf(1) := red; M := 0;\\nwhile M ̸= m\\ndo (∗Find the minimum index i for which vertex i has not\\nbeen marked, but has a color already ∗)\\nbool := false; i := 1;\\nwhile bool = false and i ≤m\\ndo if a(i) = 0 and f(i) ̸= 0 then bool := true else i := i + 1 endif;\\nendwhile;\\n(∗If bool = true, then i is the smallest index such that\\na(i) = 0 and f(i) ̸= 0.\\nIf bool = false, then for all i, the following holds: if a(i) = 0, then\\nf(i) = 0; because M < m, there is at least one such i. ∗)\\nif bool = true\\nthen for j := 1 to m\\ndo if E(i, j) = 1\\nthen if f(i) = f(j)\\nthen return NO and terminate\\nelse if f(j) = 0\\nthen if f(i) = red\\nthen f(j) := blue\\nelse f(j) := red\\nendif\\nendif\\nendif\\nendif\\nendfor;\\na(i) := 1; M := M + 1;\\nelse i := 1;\\nwhile a(i) ̸= 0 do i := i + 1 endwhile;\\n(∗an unvisited connected component starts at vertex i ∗)\\nf(i) := red\\nendif\\nendwhile;\\nreturn YES\\nFigure 6.2:\\nAn algorithm that decides whether or not a graph G is 2-\\ncolorable.\\nWe have seen that for k = 2, this problem is in the class P. For k ≥3, it\\nis not known whether there exists an algorithm that decides, in polynomial\\ntime, whether or not any given graph is k-colorable. In other words, for\\n204\\nChapter 6.\\nComplexity Theory\\nk ≥3, it is not known whether or not kColor is in the class P.\\nExample 6.3.2 Let G be a graph with vertex set V = {1, 2, . . . , m} and\\nedge set E. A Hamilton cycle is a cycle in G that visits each vertex exactly\\nonce. Formally, it is a sequence v1, v2, . . . , vm of vertices such that\\n1. {v1, v2, . . . , vm} = V , and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nWe deﬁne the following language:\\nHC := {⟨G⟩: the graph G contains a Hamilton cycle}.\\nIt is not known whether or not HC is in the class P.\\nExample 6.3.3 The sum of subset language is deﬁned as follows:\\nSOS := {⟨a1, a2, . . . , am, b⟩:\\nm, a1, a2, . . . , am, b ∈N0 and\\n∃I ⊆{1, 2, . . . , m}, P\\ni∈I ai = b}.\\nAlso in this case, no polynomial-time algorithm is known that decides the\\nlanguage SOS. That is, it is not known whether or not SOS is in the class\\nP.\\nExample 6.3.4 An integer x ≥2 is a prime number, if there are no a, b ∈N\\nsuch that a ̸= x, b ̸= x, and x = ab. Hence, the language of all non-primes\\nthat are greater than or equal to two, is\\nNPrim := {⟨x⟩: x ≥2 and x is not a prime number}.\\nIt is not obvious at all, whether or not NPrim is in the class P. In fact, it\\nwas shown only in 2002 that NPrim is in the class P.\\nObservation 6.3.5 The four languages above have the following in com-\\nmon: If someone gives us a “solution” for any given input, then we can\\neasily, i.e., in polynomial time, verify whether or not this “solution” is a cor-\\nrect solution. Moreover, for any input to each of these four problems, there\\nexists a “solution” whose length is polynomial in the length of the input.\\n6.3.\\nThe complexity class NP\\n205\\nLet us again consider the language kColor. Let G be a graph with vertex\\nset V = {1, 2, . . . , m} and edge set E, and let k be a positive integer. We\\nwant to decide whether or not G is k-colorable. A “solution” is a coloring of\\nthe nodes using at most k diﬀerent colors. That is, a solution is a sequence\\nf1, f2, . . . , fm. (Interpret this as: vertex i receives color fi, 1 ≤i ≤m). This\\nsequence is a correct solution if and only if\\n1. fi ∈{1, 2, . . . , k}, for all i with 1 ≤i ≤m, and\\n2. for all i with 1 ≤i ≤m, and for all j with 1 ≤j ≤m, if (i, j) ∈E,\\nthen fi ̸= fj.\\nIf someone gives us this solution (i.e., the sequence f1, f2, . . . , fm), then\\nwe can verify in polynomial time whether or not these two conditions are\\nsatisﬁed. The length of this solution is O(m log k): for each i, we need about\\nlog k bits to represent fi. Hence, the length of the solution is polynomial in\\nthe length of the input, i.e., it is polynomial in the number of bits needed to\\nrepresent the graph G and the number k.\\nFor the Hamilton cycle problem, a solution consists of a sequence v1,\\nv2, . . . , vm of vertices. This sequence is a correct solution if and only if\\n1. {v1, v2, . . . , vm} = {1, 2, . . . , m} and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nThese two conditions can be veriﬁed in polynomial time.\\nMoreover, the\\nlength of the solution is polynomial in the length of the input graph.\\nConsider the sum of subset problem. A solution is a sequence c1, c2, . . . , cm.\\nIt is a correct solution if and only if\\n1. ci ∈{0, 1}, for all i with 1 ≤i ≤m, and\\n2. Pm\\ni=1 ciai = b.\\nHence, the set I ⊆{1, 2, . . . , m} in the deﬁnition of SOS is the set of indices\\ni for which ci = 1. Again, these two conditions can be veriﬁed in polynomial\\ntime, and the length of the solution is polynomial in the length of the input.\\nFinally, let us consider the language NPrim. Let x ≥2 be an integer.\\nThe integers a and b form a “solution” for x if and only if\\n206\\nChapter 6.\\nComplexity Theory\\n1. 2 ≤a < x,\\n2. 2 ≤b < x, and\\n3. x = ab.\\nClearly, these three conditions can be veriﬁed in polynomial time. Moreover,\\nthe length of this solution, i.e., the total number of bits in the binary rep-\\nresentations of a and b, is polynomial in the number of bits in the binary\\nrepresentation of x.\\nLanguages having the property that the correctness of a proposed “solu-\\ntion” can be veriﬁed in polynomial time, form the class NP:\\nDeﬁnition 6.3.6 A language A belongs to the class NP, if there exist a\\npolynomial p and a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\nIn words, a language A is in the class NP, if for every string w, w ∈A if\\nand only if the following two conditions are satisﬁed:\\n1. There is a “solution” s, whose length |s| is polynomial in the length of\\nw (i.e., |s| ≤p(|w|), where p is a polynomial).\\n2. In polynomial time, we can verify whether or not s is a correct “solu-\\ntion” for w (i.e., ⟨w, s⟩∈B and B ∈P).\\nHence, the language B can be regarded to be the “veriﬁcation language”:\\nB = {⟨w, s⟩: s is a correct “solution” for w}.\\nWe have given already informal proofs of the fact that the languages\\nkColor, HC, SOS, and NPrim are all contained in the class NP. Below, we\\nformally prove that NPrim ∈NP. To prove this claim, we have to specify\\nthe polynomial p and the language B ∈P. First, we observe that\\nNPrim = {⟨x⟩:\\nthere exist a and b in N such that\\n2 ≤a < x, 2 ≤b < x and x = ab }.\\n(6.1)\\nWe deﬁne the polynomial p by p(n) := n + 2, and the language B as\\nB := {⟨x, a, b⟩: x ≥2, 2 ≤a < x, 2 ≤b < x and x = ab}.\\n6.3.\\nThe complexity class NP\\n207\\nIt is obvious that B ∈P: For any three positive integers x, a, and b, we\\ncan verify in polynomial time whether or not ⟨x, a, b⟩∈B. In order to do\\nthis, we only have to verify whether or not x ≥2, 2 ≤a < x, 2 ≤b < x,\\nand x = ab. If all these four conditions are satisﬁed, then ⟨x, a, b⟩∈B. If at\\nleast one of them is not satisﬁed, then ⟨x, a, b⟩̸∈B.\\nIt remains to show that for all x ∈N:\\n⟨x⟩∈NPrim ⇐⇒∃a, b : |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B.\\n(6.2)\\n(Remember that |⟨x⟩| denotes the number of bits in the binary representation\\nof x; |⟨a, b⟩| denotes the total number of bits of a and b, i.e., |⟨a, b⟩| =\\n|⟨a⟩| + |⟨b⟩|.)\\nLet x ∈NPrim. It follows from (6.1) that there exist a and b in N, such\\nthat 2 ≤a < x, 2 ≤b < x, and x = ab. Since x = ab ≥2 · 2 = 4 ≥2, it\\nfollows that ⟨x, a, b⟩∈B. Hence, it remains to show that\\n|⟨a, b⟩| ≤|⟨x⟩| + 2.\\nThe binary representation of x contains ⌊log x⌋+1 bits, i.e., |⟨x⟩| = ⌊log x⌋+1.\\nWe have\\n|⟨a, b⟩|\\n=\\n|⟨a⟩| + |⟨b⟩|\\n=\\n(⌊log a⌋+ 1) + (⌊log b⌋+ 1)\\n≤\\nlog a + log b + 2\\n=\\nlog ab + 2\\n=\\nlog x + 2\\n≤\\n⌊log x⌋+ 3\\n=\\n|⟨x⟩| + 2.\\nThis proves one direction of (6.2).\\nTo prove the other direction, we assume that there are positive integers\\na and b, such that |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B. Then it follows\\nimmediately from (6.1) and the deﬁnition of the language B, that x ∈NPrim.\\nHence, we have proved the other direction of (6.2). This completes the proof\\nof the claim that\\nNPrim ∈NP.\\n208\\nChapter 6.\\nComplexity Theory\\n6.3.1\\nP is contained in NP\\nIntuitively, it is clear that P ⊆NP, because a language is\\n• in P, if for every string w, it is possible to compute the “solution” s in\\npolynomial time,\\n• in NP, if for every string w and for any given “solution” s, it is possible\\nto verify in polynomial time whether or not s is a correct solution for\\nw (hence, we do not need to compute the solution s ourselves, we only\\nhave to verify it).\\nWe give a formal proof of this:\\nTheorem 6.3.7 P ⊆NP.\\nProof. Let A ∈P. We will prove that A ∈NP. Deﬁne the polynomial p\\nby p(n) := 0 for all n ∈N0, and deﬁne\\nB := {⟨w, ϵ⟩: w ∈A}.\\nSince A ∈P, the language B is also contained in P. It is easy to see that\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) = 0 and ⟨w, s⟩∈B.\\nThis completes the proof.\\n6.3.2\\nDeciding NP-languages in exponential time\\nLet us look again at the deﬁnition of the class NP. Let A be a language in\\nthis class. Then there exist a polynomial p and a language B ∈P, such that\\nfor all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.3)\\nHow do we decide whether or not any given string w belongs to the language\\nA? If we can ﬁnd a string s that satisﬁes the right-hand side in (6.3), then\\nwe know that w ∈A. On the other hand, if there is no such string s, then\\nw ̸∈A. How much time do we need to decide whether or not such a string s\\nexists?\\n6.3.\\nThe complexity class NP\\n209\\nAlgorithm NonPrime\\n(∗decides whether or not ⟨x⟩∈NPrim ∗)\\nif x = 0 or x = 1 or x = 2\\nthen return NO and terminate\\nelse a := 2;\\nwhile a < x\\ndo if x mod a = 0\\nthen return YES and terminate\\nelse a := a + 1\\nendif\\nendwhile;\\nreturn NO\\nendif\\nFigure 6.3: An algorithm that decides whether or not a number x is contained\\nin the language NPrim.\\nFor example, let A be the language\\nNPrim = {⟨x⟩: x ≥2 and x is not a prime number},\\nand let x ∈N. The algorithm in Figure 6.3 decides whether or not ⟨x⟩∈\\nNPrim.\\nIt is clear that this algorithm is correct. Let n be the length of the binary\\nrepresentation of x, i.e., n = ⌊log x⌋+ 1. If x > 2 and x is a prime number,\\nthen the while-loop makes x−2 iterations. Therefore, since n−1 = ⌊log x⌋≤\\nlog x, the running time of this algorithm is at least\\nx −2 ≥2n−1 −2,\\ni.e., it is at least exponential in the length of the input.\\nWe now prove that every language in NP can be decided in exponential\\ntime. Let A be an arbitrary language in NP. Let p be the polynomial, and\\nlet B ∈P be the language such that for all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.4)\\nThe following algorithm decides, for any given string w, whether or not\\nw ∈A. It does so by looking at all possible strings s for which |s| ≤p(|w|):\\n210\\nChapter 6.\\nComplexity Theory\\nfor all s with |s| ≤p(|w|)\\ndo if ⟨w, s⟩∈B\\nthen return YES and terminate\\nendif\\nendfor;\\nreturn NO\\nThe correctness of the algorithm follows from (6.4). What is the running\\ntime? We assume that w and s are represented as binary strings. Let n be\\nthe length of the input, i.e., n = |w|.\\nHow many binary strings s are there whose length is at most p(|w|)? Any\\nsuch s can be described by a sequence of length p(|w|) = p(n), consisting of\\nthe symbols “0”, “1”, and the blank symbol. Hence, there are at most 3p(n)\\nmany binary strings s with |s| ≤p(n). Therefore, the for-loop makes at most\\n3p(n) iterations.\\nSince B ∈P, there is an algorithm and a polynomial q, such that this\\nalgorithm, when given any input string z, decides in q(|z|) time, whether or\\nnot z ∈B. This input z has the form ⟨w, s⟩, and we have\\n|z| = |w| + |s| ≤|w| + p(|w|) = n + p(n).\\nIt follows that the total running time of our algorithm that decides whether\\nor not w ∈A, is bounded from above by\\n3p(n) · q(n + p(n))\\n≤\\n22p(n) · q(n + p(n))\\n≤\\n22p(n) · 2q(n+p(n))\\n=\\n2p′(n),\\nwhere p′ is the polynomial that is deﬁned by p′(n) := 2p(n) + q(n + p(n)).\\nIf we deﬁne the class EXP as\\nEXP :=\\n{A :\\nthere exists a polynomial p, such that A can be\\ndecided in time 2p(n) } ,\\nthen we have proved the following theorem.\\nTheorem 6.3.8 NP ⊆EXP.\\n6.4.\\nNon-deterministic algorithms\\n211\\n6.3.3\\nSummary\\n• P ⊆NP. It is not known whether P is a proper subclass of NP, or\\nwhether P = NP. This is one of the most important open problems in\\ncomputer science. If you can solve this problem, then you will get one\\nmillion dollars; not from us, but from the Clay Mathematics Institute,\\nsee\\nhttp://www.claymath.org/prizeproblems/index.htm\\nMost people believe that P is a proper subclass of NP.\\n• NP ⊆EXP, i.e., each language in NP can be decided in exponential\\ntime. It is not known whether NP is a proper subclass of EXP, or\\nwhether NP = EXP.\\n• It follows from P ⊆NP and NP ⊆EXP, that P ⊆EXP. It can\\nbe shown that P is a proper subset of EXP, i.e., there exist languages\\nthat can be decided in exponential time, but that cannot be decided in\\npolynomial time.\\n• P is the class of those languages that can be decided eﬃciently, i.e., in\\npolynomial time. Sets that are not in P, are not eﬃciently decidable.\\n6.4\\nNon-deterministic algorithms\\nThe abbreviation NP stands for Non-deterministic Polynomial time. The al-\\ngorithms that we have considered so far are deterministic, which means that\\nat any time during the computation, the next computation step is uniquely\\ndetermined. In a non-deterministic algorithm, there are one or more possi-\\nbilities for being the next computation step, and the algorithm chooses one\\nof them.\\nTo give an example, we consider the language SOS, see Example 6.3.3.\\nLet m, a1, a2, . . . , am, and b be elements of N0. Then\\n⟨a1, a2, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, c2, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciai = b.\\nThe following non-deterministic algorithm decides the language SOS:\\n212\\nChapter 6.\\nComplexity Theory\\nAlgorithm SOS(m, a1, a2, . . . , am, b):\\ns := 0;\\nfor i := 1 to m\\ndo s := s | s := s + ai\\nendfor;\\nif s = b\\nthen return YES\\nelse return NO\\nendif\\nThe line\\ns := s | s := s + ai\\nmeans that either the instruction “s := s” or the instruction “s := s + ai” is\\nexecuted.\\nLet us assume that ⟨a1, a2, . . . , am, b⟩∈SOS. Then there are c1, c2, . . . , cm ∈\\n{0, 1} such that Pm\\ni=1 ciai = b. Assume our algorithm does the following, for\\neach i with 1 ≤i ≤m: In the i-th iteration,\\n• if ci = 0, then it executes the instruction “s := s”,\\n• if ci = 1, then it executes the instruction “s := s + ai”.\\nThen after the for-loop, we have s = b, and the algorithm returns YES;\\nhence, the algorithm has correctly found out that ⟨a1, a2, . . . , am, b⟩∈SOS.\\nIn other words, in this case, there exists at least one accepting computation.\\nOn the other hand, if ⟨a1, a2, . . . , am, b⟩̸∈SOS, then the algorithm always\\nreturns NO, no matter which of the two instructions is executed in each\\niteration of the for-loop. In this case, there is no accepting computation.\\nDeﬁnition 6.4.1 Let M be a non-deterministic algorithm. We say that M\\naccepts a string w, if there exists at least one computation that, on input w,\\nreturns YES.\\nDeﬁnition 6.4.2 We say that a non-deterministic algorithm M decides a\\nlanguage A in time T, if for every string w, the following holds: w ∈A if\\nand only if there exists at least one computation that, on input w, returns\\nYES and that takes at most T(|w|) time.\\n6.5.\\nNP-complete languages\\n213\\nThe non-deterministic algorithm that we have seen above decides the\\nlanguage SOS in linear time: Let ⟨a1, a2, . . . , am, b⟩∈SOS, and let n be the\\nlength of this input. Then\\nn = |⟨a1⟩| + |⟨a2⟩| + . . . + |⟨am⟩| + |⟨b⟩| ≥m.\\nFor this input, there is a computation that returns YES and that takes\\nO(m) = O(n) time.\\nAs in Section 6.2, we deﬁne the notion of “polynomial time” for non-\\ndeterministic algorithms. The following theorem relates this notion to the\\nclass NP that we deﬁned in Deﬁnition 6.3.6.\\nTheorem 6.4.3 A language A is in the class NP if and only if there exists\\na non-deterministic Turing machine (or Java program) that decides A in\\npolynomial time.\\n6.5\\nNP-complete languages\\nLanguages in the class P are considered easy, i.e., they can be decided in\\npolynomial time. People believe (but cannot prove) that P is a proper sub-\\nclass of NP. If this is true, then there are languages in NP that are hard,\\ni.e., cannot be decided in polynomial time.\\nIntuition tells us that if P ̸= NP, then the hardest languages in NP are\\nnot contained in P. These languages are called NP-complete. In this section,\\nwe will give a formal deﬁnition of this concept.\\nIf we want to talk about the “hardest” languages in NP, then we have to\\nbe able to compare two languages according to their “diﬃculty”. The idea is\\nas follows: We say that a language B is “at least as hard” as a language A,\\nif the following holds: If B can be decided in polynomial time, then A can\\nalso be decided in polynomial time.\\nDeﬁnition 6.5.1 Let A ⊆{0, 1}∗and B ⊆{0, 1}∗be languages. We say\\nthat A ≤P B, if there exists a function\\nf : {0, 1}∗→{0, 1}∗\\nsuch that\\n1. f ∈FP and\\n214\\nChapter 6.\\nComplexity Theory\\n2. for all strings w in {0, 1}∗,\\nw ∈A ⇐⇒f(w) ∈B.\\nIf A ≤P B, then we also say that “B is at least as hard as A”, or “A is\\npolynomial-time reducible to B”.\\nWe ﬁrst show that this formal deﬁnition is in accordance with the intuitive\\ndeﬁnition given above.\\nTheorem 6.5.2 Let A and B be languages such that B ∈P and A ≤P B.\\nThen A ∈P.\\nProof. Let f : {0, 1}∗→{0, 1}∗be the function in FP for which\\nw ∈A ⇐⇒f(w) ∈B.\\n(6.5)\\nThe following algorithm decides whether or not any given binary string w is\\nin A:\\nu := f(w);\\nif u ∈B\\nthen return YES\\nelse return NO\\nendif\\nThe correctness of this algorithm follows immediately from (6.5). So it\\nremains to show that the running time is polynomial in the length of the\\ninput string w.\\nSince f ∈FP, there exists a polynomial p such that the function f can\\nbe computed in time p. Similarly, since B ∈P, there exists a polynomial q,\\nsuch that the language B can be decided in time q.\\nLet n be the length of the input string w, i.e., n = |w|. Then the length\\nof the string u is less than or equal to p(|w|) = p(n). (Why?) Therefore, the\\nrunning time of our algorithm is bounded from above by\\np(|w|) + q(|u|) ≤p(n) + q(p(n)).\\nSince the function p′, deﬁned by p′(n) := p(n)+q(p(n)), is a polynomial, this\\nproves that A ∈P.\\nThe following theorem states that the relation ≤P is reﬂexive and tran-\\nsitive. We leave the proof as an exercise.\\n6.5.\\nNP-complete languages\\n215\\nTheorem 6.5.3 Let A, B, and C be languages. Then\\n1. A ≤P A, and\\n2. if A ≤P B and B ≤P C, then A ≤P C.\\nWe next show that the languages in P are the easiest languages in NP:\\nTheorem 6.5.4 Let A be a language in P, and let B be an arbitrary lan-\\nguage such that B ̸= ∅and B ̸= {0, 1}∗. Then A ≤P B.\\nProof. We choose two strings u and v in {0, 1}∗, such that u ∈B and v ̸∈B.\\n(Observe that this is possible.) Deﬁne the function f : {0, 1}∗→{0, 1}∗by\\nf(w) :=\\n\\x1a u\\nif w ∈A,\\nv\\nif w ̸∈A.\\nThen it is clear that for any binary string w,\\nw ∈A ⇐⇒f(w) ∈B.\\nSince A ∈P, the function f can be computed in polynomial time, i.e.,\\nf ∈FP.\\n6.5.1\\nTwo examples of reductions\\nSum of subsets and knapsacks\\nWe start with a simple reduction. Consider the two languages\\nSOS := {⟨a1, . . . , am, b⟩:\\nm, a1, . . . , am, b ∈N0 and there exist\\nc1, . . . , cm ∈{0, 1}, such that Pm\\ni=1 ciai = b}\\nand\\nKS\\n:=\\n{⟨w1, . . . , wm, k1, . . . , km, W, K⟩:\\nm, w1, . . . , wm, k1, . . . , km, W, K ∈N0\\nand there exist c1, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciwi ≤W and Pm\\ni=1 ciki ≥K}.\\nThe notation KS stands for knapsack: We have m pieces of food. The\\ni-th piece has weight wi and contains ki calories. We want to decide whether\\nor not we can ﬁll our knapsack with a subset of the pieces of food such that\\nthe total weight is at most W, and the total amount of calories is at least K.\\n216\\nChapter 6.\\nComplexity Theory\\nTheorem 6.5.5 SOS ≤P KS.\\nProof. Let us ﬁrst see what we have to show. According to Deﬁnition 6.5.1,\\nwe need a function f ∈FP, that maps input strings for SOS to input strings\\nfor KS, in such a way that\\n⟨a1, . . . , am, b⟩∈SOS ⇐⇒f(⟨a1, . . . , am, b⟩) ∈KS.\\nIn order for f(⟨a1, . . . , am, b⟩) to be an input string for KS, this function\\nvalue has to be of the form\\nf(⟨a1, . . . , am, b⟩) = ⟨w1, . . . , wm, k1, . . . , km, W, K⟩.\\nWe deﬁne\\nf(⟨a1, . . . , am, b⟩) := ⟨a1, . . . , am, a1, . . . , am, b, b⟩.\\nIt is clear that f ∈FP. We have\\n⟨a1, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai = b\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai ≤b and Pm\\ni=1 ciai ≥b\\n⇐⇒\\n⟨a1, . . . , am, a1, . . . , am, b, b⟩∈KS\\n⇐⇒\\nf(⟨a1, . . . , am, b⟩) ∈KS.\\nCliques and Boolean formulas\\nWe will deﬁne two languages A = 3SAT and B = Clique that have, at\\nﬁrst sight, nothing to do with each other. Then we show that, nevertheless,\\nA ≤P B.\\nLet G be a graph with vertex set V and edge set E. A subset V ′ of V is\\ncalled a clique, if each pair of distinct vertices in V ′ is connected by an edge\\nin E. We deﬁne the following language:\\nClique := {⟨G, k⟩: k ∈N and G has a clique with k vertices}.\\nWe encourage you to prove the following claim:\\n6.5.\\nNP-complete languages\\n217\\nTheorem 6.5.6 Clique ∈NP.\\nNext we consider Boolean formulas ϕ, with variables x1, x2, . . . , xm, hav-\\ning the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.6)\\nwhere each Ci, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nEach ℓi\\na is either a variable or the negation of a variable. An example of such\\na formula is\\nϕ = (x1 ∨¬x1 ∨¬x2) ∧(x3 ∨x2 ∨x4) ∧(¬x1 ∨¬x3 ∨¬x4).\\nA formula ϕ of the form (6.6) is said to be satisﬁable, if there exists a truth-\\nvalue in {0, 1} for each of the variables x1, x2, . . . , xm, such that the entire\\nformula ϕ is true. Our example formula is satisﬁable: If we take x1 = 0 and\\nx2 = 1, and give x3 and x4 an arbitrary value, then\\nϕ = (0 ∨1 ∨0) ∧(x3 ∨1 ∨x4) ∧(1 ∨¬x3 ∨¬x4) = 1.\\nWe deﬁne the following language:\\n3SAT := {⟨ϕ⟩: ϕ is of the form (6.6) and is satisﬁable}.\\nAgain, we encourage you to prove the following claim:\\nTheorem 6.5.7 3SAT ∈NP.\\nObserve that the elements of Clique (which are pairs consisting of a graph\\nand a positive integer) are completely diﬀerent from the elements of 3SAT\\n(which are Boolean formulas). We will show that 3SAT ≤P Clique. Recall\\nthat this means the following: If the language Clique can be decided in\\npolynomial time, then the language 3SAT can also be decided in polynomial\\ntime. In other words, any polynomial-time algorithm that decides Clique can\\nbe converted to a polynomial-time algorithm that decides 3SAT.\\nTheorem 6.5.8 3SAT ≤P Clique.\\n218\\nChapter 6.\\nComplexity Theory\\nProof. We have to show that there exists a function f ∈FP, that maps\\ninput strings for 3SAT to input strings for Clique, such that for each Boolean\\nformula ϕ that is of the form (6.6),\\n⟨ϕ⟩∈3SAT ⇐⇒f(⟨ϕ⟩) ∈Clique.\\nThe function f maps the binary string encoding an arbitrary Boolean formula\\nϕ to a binary string encoding a pair (G, k), where G is a graph and k is a\\npositive integer. We have to deﬁne this function f in such a way that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\nLet\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an arbitrary Boolean formula in the variables x1, x2, . . . , xm, where each\\nCi, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nRemember that each ℓi\\na is either a variable or the negation of a variable.\\nThe formula ϕ is mapped to the pair (G, k), where the vertex set V and\\nthe edge set E of the graph G are deﬁned as follows:\\n• V = {v1\\n1, v1\\n2, v1\\n3, . . . , vk\\n1, vk\\n2, vk\\n3}. The idea is that each vertex vi\\na corre-\\nsponds to one term ℓi\\na.\\n• The pair (vi\\na, vj\\nb) of vertices form an edge in E if and only if\\n– i ̸= j and\\n– ℓi\\na is not the negation of ℓj\\nb.\\nTo give an example, let ϕ be the Boolean formula\\nϕ = (x1 ∨¬x2 ∨¬x3) ∧(¬x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3),\\n(6.7)\\ni.e., k = 3, C1 = x1 ∨¬x2 ∨¬x3, C2 = ¬x1 ∨x2 ∨x3, and C3 = x1 ∨x2 ∨x3.\\nThe graph G that corresponds to this formula is given in Figure 6.4.\\nIt is not diﬃcult to see that the function f can be computed in polynomial\\ntime. So it remains to prove that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\n(6.8)\\n6.5.\\nNP-complete languages\\n219\\n¬x2\\n¬x3\\nx1\\n¬x1\\nx2\\nx3\\nx1\\nx2\\nx3\\nFigure 6.4: The formula ϕ in (6.7) is mapped to this graph. The vertices on\\nthe top represent C1; the vertices on the left represent C2; the vertices on\\nthe right represent C3.\\nTo prove this, we ﬁrst assume that the formula\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nis satisﬁable. Then there exists a truth-value in {0, 1} for each of the variables\\nx1, x2, . . . , xm, such that the entire formula ϕ is true. Hence, for each i with\\n1 ≤i ≤k, there is at least one term ℓi\\na in\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3\\nthat is true (i.e., has value 1).\\nLet V ′ be the set of vertices obtained by choosing for each i, 1 ≤i ≤k,\\nexactly one vertex vi\\na such that ℓi\\na has value 1.\\nIt is clear that V ′ contains exactly k vertices. We claim that this set is\\na clique in G. To prove this claim, let vi\\na and vj\\nb be two distinct vertices in\\nV ′. It follows from the deﬁnition of V ′ that i ̸= j and ℓi\\na = ℓj\\nb = 1. Hence,\\nℓi\\na is not the negation of ℓj\\nb. But this means that the vertices vi\\na and vj\\nb are\\nconnected by an edge in G.\\nThis proves one direction of (6.8). To prove the other direction, we assume\\nthat the graph G contains a clique V ′ with k vertices.\\n220\\nChapter 6.\\nComplexity Theory\\nThe vertices of G consist of k groups, where each group contains exactly\\nthree vertices. Since vertices within the same group are not connected by\\nedges, the clique V ′ contains exactly one vertex from each group. Hence, for\\neach i with 1 ≤i ≤k, there is exactly one a, such that vi\\na ∈V ′. Consider\\nthe corresponding term ℓi\\na. We know that this term is either a variable or\\nthe negation of a variable, i.e., ℓi\\na is either of the form xj or of the form ¬xj.\\nIf ℓi\\na = xj, then we give xj the truth-value 1. Otherwise, we have ℓi\\na = ¬xj,\\nin which case we give xj the truth-value 0. Since V ′ is a clique, each variable\\ngets at most one truth-value. If a variable has no truth-value yet, then we\\ngive it an arbitrary truth-value.\\nIf we substitute these truth-values into ϕ, then the entire formula has\\nvalue 1. Hence, ϕ is satisﬁable.\\nIn order to get a better understanding of this proof, you should verify the\\nproof for the formula ϕ in (6.7) and the graph G in Figure 6.4.\\n6.5.2\\nDeﬁnition of NP-completeness\\nReductions, as deﬁned in Deﬁnition 6.5.1, allow us to compare two language\\naccording to their diﬃculty. A language B in NP is called NP-complete,\\nif B belongs to the most diﬃcult languages in NP; in other words, B is at\\nleast as hard as any other language in NP.\\nDeﬁnition 6.5.9 Let B ⊆{0, 1}∗be a language. We say that B is NP-\\ncomplete, if\\n1. B ∈NP and\\n2. A ≤P B, for every language A in NP.\\nTheorem 6.5.10 Let B be an NP-complete language. Then\\nB ∈P ⇐⇒P = NP.\\nProof. Intuitively, this theorem should be true: If the language B is in P,\\nthen B is an easy language. On the other hand, since B is NP-complete,\\nit belongs to the most diﬃcult languages in NP. Hence, the most diﬃcult\\nlanguage in NP is easy. But then all languages in NP must be easy, i.e.,\\nP = NP.\\n6.5.\\nNP-complete languages\\n221\\nWe give a formal proof. Let us ﬁrst assume that B ∈P. We already\\nknow that P ⊆NP. Hence, it remains to show that NP ⊆P. Let A be an\\narbitrary language in NP. Since B is NP-complete, we have A ≤P B. Then,\\nby Theorem 6.5.2, we have A ∈P.\\nTo prove the converse, assume that P = NP. Since B ∈NP, it follows\\nimmediately that B ∈P.\\nTheorem 6.5.11 Let B and C be languages, such that C ∈NP and B ≤P\\nC. If B is NP-complete, then C is also NP-complete.\\nProof. First, we give an intuitive explanation of the claim: By assumption,\\nB belongs to the most diﬃcult languages in NP, and C is at least as hard as\\nB. Since C ∈NP, it follows that C belongs to the most diﬃcult languages\\nin NP. Hence, C is NP-complete.\\nTo give a formal proof, we have to show that A ≤P C, for all languages A\\nin NP. Let A be an arbitrary language in NP. Since B is NP-complete, we\\nhave A ≤P B. Since B ≤P C, it follows from Theorem 6.5.3, that A ≤P C.\\nTherefore, C is NP-complete.\\nTheorem 6.5.11 can be used to prove the NP-completeness of languages:\\nLet C be a language, and assume that we want to prove that C is NP-\\ncomplete. We can do this in the following way:\\n1. We ﬁrst prove that C ∈NP.\\n2. Then we ﬁnd a language B that looks “similar” to C, and for which\\nwe already know that it is NP-complete.\\n3. Finally, we prove that B ≤P C.\\n4. Then, Theorem 6.5.11 tells us that C is NP-complete.\\nOf course, this leads to the question “How do we know that the language\\nB is NP-complete?” In order to apply Theorem 6.5.11, we need a “ﬁrst” NP-\\ncomplete language; the NP-completeness of this language must be proven\\nusing Deﬁnition 6.5.9.\\nObserve that it is not clear at all that there exist NP-complete languages!\\nFor example, consider the language 3SAT. If we want to use Deﬁnition 6.5.9\\nto show that this language is NP-complete, then we have to show that\\n222\\nChapter 6.\\nComplexity Theory\\n• 3SAT ∈NP. We know from Theorem 6.5.7 that this is true.\\n• A ≤P 3SAT, for every language A ∈NP. Hence, we have to show this\\nfor languages A such as kColor, HC, SOS, NPrim, KS, Clique, and\\nfor inﬁnitely many other languages.\\nIn 1971, Cook has exactly done this: He showed that the language 3SAT\\nis NP-complete. Since his proof is rather technical, we will prove the NP-\\ncompleteness of another language.\\n6.5.3\\nAn NP-complete domino game\\nWe are given a ﬁnite collection of tile types. For each such type, there are\\narbitrarily many tiles of this type. A tile is a square that is partitioned into\\nfour triangles. Each of these triangles contains a symbol that belongs to a\\nﬁnite alphabet Σ. Hence, a tile looks as follows:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\na\\nb\\nc\\nd\\nWe are also given a square frame, consisting of cells. Each cell has the same\\nsize as a tile, and contains a symbol of Σ.\\nThe problem is to decide whether or not this domino game has a solution.\\nThat is, can we completely ﬁll the frame with tiles such that\\n• for any two neighboring tiles s and s′, the two triangles of s and s′ that\\ntouch each other contain the same symbol, and\\n• each triangle that touches the frame contains the same symbol as the\\ncell of the frame that is touched by this triangle.\\nThere is one ﬁnal restriction: The orientation of the tiles is ﬁxed, they cannot\\nbe rotated.\\nLet us give a formal deﬁnition of this problem. We assume that the sym-\\nbols belong to the ﬁnite alphabet Σ = {0, 1}m, i.e., each symbol is encoded\\nas a bit-string of length m. Then, a tile type can be encoded as a tuple of\\nfour bit-strings, i.e., as an element of Σ4. A frame consisting of t rows and t\\ncolumns can be encoded as a string in Σ4t.\\n6.5.\\nNP-complete languages\\n223\\nWe denote the language of all solvable domino games by Domino:\\nDomino\\n:=\\n{⟨m, k, t, R, T1, . . . , Tk⟩:\\nm ≥1, k ≥1, t ≥1, R ∈Σ4t, Ti ∈Σ4, 1 ≤i ≤k,\\nframe R can be ﬁlled using tiles of types\\nT1, . . . , Tk.}\\nWe will prove the following theorem.\\nTheorem 6.5.12 The language Domino is NP-complete.\\nProof. It is clear that Domino ∈NP: A solution consists of a t × t matrix,\\nin which the (i, j)-entry indicates the type of the tile that occupies position\\n(i, j) in the frame. The number of bits needed to specify such a solution is\\npolynomial in the length of the input. Moreover, we can verify in polynomial\\ntime whether or not any given “solution” is correct.\\nIt remains to show that\\nA ≤P Domino, for every language A in NP.\\nLet A be an arbitrary language in NP. Then there exist a polynomial p and\\na non-deterministic Turing machine M, that decides the language A in time\\np. We may assume that this Turing machine has only one tape.\\nOn input w = a1a2 . . . an, the Turing machine M starts in the start state\\nz0, with its tape head on the cell containing the symbol a1. We may assume\\nthat during the entire computation, the tape head never moves to the left of\\nthis initial cell. Hence, the entire computation “takes place” in and to the\\nright of the initial cell. We know that\\nw ∈A\\n⇐⇒\\non input w, there exists an accepting computation\\nthat makes at most p(n) computation steps.\\nAt the end of such an accepting computation, the tape only contains the\\nsymbol 1, which we may assume to be in the initial cell, and M is in the ﬁnal\\nstate z1. In this case, we may assume that the accepting computation makes\\nexactly p(n) computation steps. (If this is not the case, then we extend the\\ncomputation using the instruction z11 →z11N.)\\nWe need one more technical detail: We may assume that za →z′bR and\\nza′ →z′′b′L are not both instructions of M. Hence, the state of the Turing\\nmachine uniquely determines the direction in which the tape head moves.\\n224\\nChapter 6.\\nComplexity Theory\\nWe have to deﬁne a domino game, that depends on the input string w\\nand the Turing machine M, such that\\nw ∈A ⇐⇒this domino game is solvable.\\nThe idea is to encode an accepting computation of the Turing machine M as\\na solution of the domino game. In order to do this, we use a frame in which\\neach row corresponds to one computation step. This frame consists of p(n)\\nrows. Since an accepting computation makes exactly p(n) computation steps,\\nand since the tape head never moves to the left of the initial cell, this tape\\nhead can visit only p(n) cells. Therefore, our frame will have p(n) columns.\\nThe domino game will use the following tile types:\\n1. For each symbol a in the alphabet of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\n#\\na\\nIntuition: Before and after the computation step, the tape head is not\\non this cell.\\n2. For each instruction za →z′bR of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\nz′\\nb\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the right.\\n3. For each instruction za →z′bL of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz′\\n(z, a)\\n#\\nb\\n6.5.\\nNP-complete languages\\n225\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the left.\\n4. For each instruction za →z′bN of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\n#\\n(z′, b)\\nIntuition: Before and after the computation step, the tape head is on\\nthis cell.\\n5. For each state z and for each symbol a in the alphabet of the Turing\\nmachine M, there are two tile types:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz\\na\\n#\\n(z, a)\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\nz\\n(z, a)\\nIntuition: The leftmost tile indicates that the tape head enters this cell\\nfrom the left; the righmost tile indicates that the tape head enters this\\ncell from the right.\\nThis speciﬁes all tile types. The p(n) × p(n) frame is given in Figure 6.5.\\nThe top row corresponds to the start of the computation, whereas the bottom\\nrow corresponds to the end of the computation. The left and right columns\\ncorrespond to the part of the tape in which the tape head can move.\\nThe encodings of these tile types and the frame can be computed in\\npolynomial time.\\nIt can be shown that, for any input string w, any accepting computation\\nof length p(n) of the Turing machine M can be encoded as a solution of\\nthis domino game. Conversely, any solution of this domino game can be\\n“translated” to an accepting computation of length p(n) of M, on input\\nstring w. Hence, the following holds.\\nw ∈A\\n⇐⇒\\nthere exists an accepting computation that makes\\np(n) computation steps\\n⇐⇒\\nthe domino game is solvable.\\n226\\nChapter 6.\\nComplexity Theory\\n(z0, a1)\\na2\\n. . .\\nan\\n✷\\n. . .\\n✷\\n#\\n#\\n#\\n#\\n#\\n...\\n#\\n...\\n✷\\n✷\\n✷\\n✷\\n✷\\n. . .\\n(z1, 1)\\np(n)\\np(n)\\nFigure 6.5: The p(n) × p(n) frame for the domino game.\\nTherefore, we have A ≤P Domino. Hence, the language Domino is NP-\\ncomplete.\\nAn example of a domino game\\nWe have deﬁned the domino game corresponding to a Turing machine that\\nsolves a decision problem. Of course, we can also do this for Turing machines\\nthat compute functions. In this section, we will exactly do this for a Turing\\nmachine that computes the successor function x →x + 1.\\nWe will design a Turing machine with one tape, that gets as input the\\nbinary representation of a natural number x, and that computes the binary\\nrepresentation of x + 1.\\nStart of the computation: The tape contains a 0 followed by the binary\\nrepresentation of the integer x ∈N0. The tape head is on the leftmost bit\\n(which is 0), and the Turing machine is in the start state z0. Here is an\\nexample, where x = 431:\\n6.5.\\nNP-complete languages\\n227\\n0 1 1 0 1 0 1 1 1 1 2\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe number x + 1. The tape head is on the rightmost 1, and the Turing\\nmachine is in the ﬁnal state z1. For our example, the tape looks as follows:\\n0 1 1 0 1 1 0 0 0 0 2\\n6\\nOur Turing machine will use the following states:\\nz0 :\\nstart state; tape head moves to the right\\nz1 :\\nﬁnal state\\nz2 :\\ntape head moves to the left; on its way to the left, it has not read 0\\nThe Turing machine has the following instructions:\\nz00 →z00R\\nz21 →z20L\\nz01 →z01R\\nz20 →z11N\\nz02 →z22L\\nIn Figure 6.6, you can see the sequence of states and tape contents of this\\nTuring machine on input x = 11.\\nWe now construct the domino game that corresponds to the computation\\nof this Turing machine on input x = 11. Following the general construction\\nin Section 6.5.3, we obtain the following tile types:\\n1. The three symbols of the alphabet yield three tile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n2\\n2\\n2. The ﬁve instructions of the Turing machine yield ﬁve tile types:\\n228\\nChapter 6.\\nComplexity Theory\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n0\\n(z0, 1)\\n0\\n1\\n1\\n2\\n0\\n1\\n(z0, 0)\\n1\\n1\\n2\\n0\\n1\\n0\\n(z0, 1)\\n1\\n2\\n0\\n1\\n0\\n1\\n(z0, 1)\\n2\\n0\\n1\\n0\\n1\\n1\\n(z0, 2)\\n0\\n1\\n0\\n1\\n(z2, 1)\\n2\\n0\\n1\\n0\\n(z2, 1)\\n0\\n2\\n0\\n1\\n(z2, 0)\\n0\\n0\\n2\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\nFigure 6.6: The computation of the Turing machine on input x = 11. The\\npair (state,symbol) indicates the position of the tape head.\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n3. The states z0 and z2, and the three symbols of the alphabet yield twelve\\ntile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 2)\\n2\\nThe computation of the Turing machine on input x = 11 consists of nine\\ncomputation steps. During this computation, the tape head visits exactly\\nsix cells. Therefore, the frame for the domino game has nine rows and six\\ncolumns.\\nThis frame is given in Figure 6.7.\\nIn Figure 6.8, you ﬁnd the\\nsolution of the domino game.\\nObserve that this solution is nothing but\\nan equivalent way of writing the computation of Figure 6.6.\\nHence, the\\ncomputation of the Turing machine corresponds to a solution of the domino\\ngame; in fact, the converse also holds.\\n6.5.\\nNP-complete languages\\n229\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\nFigure 6.7: The frame for the domino game for input x = 11.\\n230\\nChapter 6.\\nComplexity Theory\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\nFigure 6.8: The solution for the domino game for input x = 11.\\n6.5.\\nNP-complete languages\\n231\\n6.5.4\\nExamples of NP-complete languages\\nIn Section 6.5.3, we have shown that Domino is NP-complete. Using this\\nresult, we will apply Theorem 6.5.11 to prove the NP-completeness of some\\nother languages.\\nSatisﬁability\\nWe consider Boolean formulas ϕ, in the variables x1, x2, . . . , xm, having the\\nform\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.9)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nEach ℓi\\nj is either a variable or the negation of a variable. Such a formula ϕ\\nis said to be satisﬁable, if there exists a truth-value in {0, 1} for each of the\\nvariables x1, x2, . . . , xm, such that the entire formula ϕ is true. We deﬁne the\\nfollowing language:\\nSAT := {⟨ϕ⟩: ϕ is of the form (6.9) and is satisﬁable}.\\nWe will prove that SAT is NP-complete.\\nIt is clear that SAT ∈NP. If we can show that\\nDomino ≤P SAT,\\nthen it follows from Theorem 6.5.11 that SAT is NP-complete. (In Theo-\\nrem 6.5.11, take B := Domino and C := SAT.)\\nHence, we need a function f ∈FP, that maps input strings for Domino\\nto input strings for SAT, in such a way that for every domino game D, the\\nfollowing holds:\\ndomino game D is solvable ⇐⇒the formula encoded by the\\nstring f(⟨D⟩) is satisﬁable.\\n(6.10)\\nLet us consider an arbitrary domino game D. Let k be the number of\\ntile types, and let the frame have t rows and t columns. We denote the tile\\ntypes by T1, T2, . . . , Tk.\\n232\\nChapter 6.\\nComplexity Theory\\nWe map this domino game D to a Boolean formula ϕ, such that (6.10)\\nholds. The formula ϕ will have variables\\nxijℓ, 1 ≤i ≤t, 1 ≤j ≤t, 1 ≤ℓ≤k.\\nThese variables can be interpretated as follows:\\nxijℓ= 1 ⇐⇒there is a tile of type Tℓat position (i, j) of the frame.\\nWe deﬁne:\\n• For all i and j with 1 ≤i ≤t and 1 ≤j ≤t:\\nC1\\nij := xij1 ∨xij2 ∨. . . ∨xijk.\\nThis formula expresses the condition that there is at least one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j ≤t, and 1 ≤ℓ< ℓ′ ≤k:\\nC2\\nijℓℓ′ := ¬xijℓ∨¬xijℓ′.\\nThis formula expresses the condition that there is at most one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j < t, 1 ≤ℓ≤k and 1 ≤ℓ′ ≤k,\\nsuch that i < t and the right symbol on a tile of type Tℓis not equal\\nto the left symbol on a tile of type Tℓ′:\\nC3\\nijℓℓ′ := ¬xijℓ∨¬xi,j+1,ℓ′.\\nThis formula expresses the condition that neighboring tiles in the same\\nrow “ﬁt” together. There are symmetric formulas for neighboring tiles\\nin the same column.\\n• For all j and ℓwith 1 ≤j ≤t and 1 ≤ℓ≤k, such that the top symbol\\non a tile of type Tℓis not equal to the symbol at position j of the upper\\nboundary of the frame:\\nC4\\njℓ:= ¬x1jℓ.\\nThis formula expresses the condition that tiles that touch the upper\\nboundary of the frame “ﬁt” there. There are symmetric formulas for\\nthe lower, left, and right boundaries of the frame.\\n6.5.\\nNP-complete languages\\n233\\nThe formula ϕ is the conjunction of all these formulas C1\\nij, C2\\nijℓℓ′, C3\\nijℓℓ′, and\\nC4\\njℓ. The complete formula ϕ consists of\\nO(t2k + t2k2 + t2k2 + tk) = O(t2k2)\\nterms, i.e., its length is polynomial in the length of the domino game. This\\nimplies that ϕ can be constructed in polynomial time. Hence, the function\\nf that maps the domino game D to the Boolean formula ϕ, is in the class\\nFP. It is not diﬃcult to see that (6.10) holds for this function f. Therefore,\\nwe have proved the following result.\\nTheorem 6.5.13 The language SAT is NP-complete.\\nIn Section 6.5.1, we have deﬁned the language 3SAT.\\nTheorem 6.5.14 The language 3SAT is NP-complete.\\nProof. It is clear that 3SAT ∈NP. If we can show that\\nSAT ≤P 3SAT,\\nthen the claim follows from Theorem 6.5.11. Let\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an input for SAT, in the variables x1, x2, . . . , xm. We map ϕ, in polynomial\\ntime, to an input ϕ′ for 3SAT, such that\\nϕ is satisﬁable ⇐⇒ϕ′ is satisﬁable.\\n(6.11)\\nFor each i with 1 ≤i ≤k, we do the following. Consider\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\n• If ki = 1, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n1 ∨ℓi\\n1.\\n• If ki = 2, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n2.\\n234\\nChapter 6.\\nComplexity Theory\\n• If ki = 3, then we deﬁne\\nC′\\ni := Ci.\\n• If ki ≥4, then we deﬁne\\nC′\\ni\\n:=\\n(ℓi\\n1 ∨ℓi\\n2 ∨zi\\n1) ∧(¬zi\\n1 ∨ℓi\\n3 ∨zi\\n2) ∧(¬zi\\n2 ∨ℓi\\n4 ∨zi\\n3) ∧. . .\\n∧(¬zi\\nki−3 ∨ℓi\\nki−1 ∨ℓi\\nki),\\nwhere zi\\n1, . . . , zi\\nki−3 are new variables.\\nLet\\nϕ′ := C′\\n1 ∧C′\\n2 ∧. . . ∧C′\\nk.\\nThen ϕ′ is an input for 3SAT, and (6.11) holds.\\nTheorems 6.5.6, 6.5.8, 6.5.11, and 6.5.14 imply:\\nTheorem 6.5.15 The language Clique is NP-complete.\\nThe traveling salesperson problem\\nWe are given two positive integers k and m, a set of m cities, and an integer\\nm × m matrix M, where\\nM(i, j) = the cost of driving from city i to city j,\\nfor all i, j ∈{1, 2, . . . , m}. We want to decide whether or not there is a tour\\nthrough all cities whose total cost is less than or equal to k. This problem is\\nNP-complete.\\nBin packing\\nWe are given three positive integers m, k, and ℓ, a set of m objects having\\nvolumes a1, a2, . . . , am, and k bins. Each bin has volume ℓ. We want to\\ndecide whether or not all objects ﬁt within these bins. This problem is NP-\\ncomplete.\\nHere is another interpretation of this problem: We are given m jobs that\\nneed time a1, a2, . . . , am to complete. We are also given k processors, and an\\ninteger ℓ. We want to decide whether or not it is possible to divide the jobs\\nover the k processors, such that no processor needs more than ℓtime.\\nExercises\\n235\\nTime tables\\nWe are given a set of courses, class rooms, and professors.\\nWe want to\\ndecide whether or not there exists a time table such that all courses are\\nbeing taught, no two courses are taught at the same time in the same class\\nroom, no professor teaches two courses at the same time, and conditions such\\nas “Prof. L. Azy does not teach before 1pm” are satisﬁed. This problem is\\nNP-complete.\\nMotion planning\\nWe are given two positive integers k and ℓ, a set of k polyhedra, and two\\npoints s and t in Q3. We want to decide whether or not there exists a path\\nbetween s and t, that does not intersect any of the polyhedra, and whose\\nlength is less than or equal to ℓ. This problem is NP-complete.\\nMap labeling\\nWe are given a map with m cities, where each city is represented by a point.\\nFor each city, we are given a rectangle that is large enough to contain the\\nname of the city. We want to decide whether or not these rectangles can be\\nplaced on the map, such that\\n• no two rectangles overlap,\\n• For each i with 1 ≤i ≤m, the point that represents city i is a corner\\nof its rectangle.\\nThis problem is NP-complete.\\nThis list of NP-complete problems can be extended almost arbitrarily:\\nFor thousands of problems, it is known that they are NP-complete. For all\\nof these, it is not known, whether or not they can be solved eﬃciently (i.e.,\\nin polynomial time). Collections of NP-complete problems can be found in\\nthe book\\n• M.R. Garey and D.S. Johnson. Computers and Intractability: A Guide\\nto the Theory of NP-Completeness. W.H. Freeman, New York, 1979,\\nand on the web page\\nhttp://www.nada.kth.se/~viggo/wwwcompendium/\\n236\\nChapter 6.\\nComplexity Theory\\nExercises\\n6.1 Prove that the function F : N →N, deﬁned by F(x) := 2x, is not in FP.\\n6.2 Prove Theorem 6.5.3.\\n6.3 Prove that the language Clique is in the class NP.\\n6.4 Prove that the language 3SAT is in the class NP.\\n6.5 We deﬁne the following languages:\\n• Sum of subset:\\nSOS := {⟨a1, a2, . . . , am, b⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai = b}.\\n• Set partition:\\nSP := {⟨a1, a2, . . . , am⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai =\\nX\\ni̸∈I\\nai}.\\n• Bin packing: BP is the set of all strings ⟨s1, s2, . . . , sm, B⟩for which\\n1. 0 < si < 1, for all i,\\n2. B ∈N,\\n3. the numbers s1, s2, . . . , sm ﬁt into B bins, where each bin has size\\none, i.e., there exists a partition of {1, 2, . . . , m} into subsets Ik,\\n1 ≤k ≤B, such that P\\ni∈Ik si ≤1 for all k, 1 ≤k ≤B.\\nFor example, ⟨1/6, 1/2, 1/5, 1/9, 3/5, 1/5, 1/2, 11/18, 3⟩∈BP, because\\nthe eight fractions ﬁt into three bins:\\n1/6 + 1/9 + 11/18 ≤1, 1/2 + 1/2 = 1, and 1/5 + 3/5 + 1/5 = 1.\\n1. Prove that SOS ≤P SP.\\n2. Prove that the language SOS is NP-complete. You may use the fact\\nthat the language SP is NP-complete.\\nExercises\\n237\\n3. Prove that the language BP is NP-complete. Again, you may use the\\nfact that the language SP is NP-complete.\\n6.6 Prove that 3Color ≤P 3SAT.\\nHint: For each vertex i, and for each of the three colors k, introduce a\\nBoolean variable xik.\\n6.7 The (0, 1)-integer programming language IP is deﬁned as follows:\\nIP := {⟨A, c⟩:\\nA is an integer m × n matrix for some m, n ∈N,\\nc is an integer vector of length m, and\\n∃x ∈{0, 1}n such that Ax ≤c (componentwise) }.\\nProve that the language IP is NP-complete. You may use the fact that\\nthe language SOS is NP-complete.\\n6.8 Let ϕ be a Boolean formula in the variables x1, x2, . . . , xm.\\nWe say that ϕ is in disjunctive normal form (DNF) if it is of the form\\nϕ = C1 ∨C2 ∨. . . ∨Ck,\\n(6.12)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∧ℓi\\n2 ∧. . . ∧ℓi\\nki.\\nEach ℓi\\nj is a literal, which is either a variable or the negation of a variable.\\nWe say that ϕ is in conjunctive normal form (CNF) if it is of the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.13)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nAgain, each ℓi\\nj is a literal.\\nWe deﬁne the following two languages:\\nDNFSAT := {⟨ϕ⟩: ϕ is in DNF-form and is satisﬁable},\\nand\\nCNFSAT := {⟨ϕ⟩: ϕ is in CNF-form and is satisﬁable}.\\n238\\nChapter 6.\\nComplexity Theory\\n1. Prove that the language DNFSAT is in P.\\n2. What is wrong with the following argument: Since we can rewrite\\nany Boolean formula in DNF-form, we have CNFSAT ≤P DNFSAT.\\nHence, since CNFSAT is NP-complete and since DNFSAT ∈P, we\\nhave P = NP.\\n3. Prove directly that for every language A in P, A ≤P CNFSAT. “Di-\\nrectly” means that you should not use the fact that CNFSAT is NP-\\ncomplete.\\n6.9 1 Prove that the polynomial upper bound on the length of the string y\\nin the deﬁnition of NP is necessary, in the sense that if it is left out, then\\nany enumerable language would satisfy the condition.\\nMore precisely, we say that the language A belongs to the class E, if there\\nexists a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃y : ⟨w, y⟩∈B.\\nProve that E is equal to the class of all enumerable languages.\\n1Thanks to Antoine Vigneron for poining out an error in a previous version of this\\nexercise.\\nChapter 7\\nSummary\\nWe have seen several diﬀerent models for “processing” languages, i.e., pro-\\ncessing sets of strings over some ﬁnite alphabet. For each of these models,\\nwe have asked the question which types of languages can be processed, and\\nwhich types of languages cannot be processed. In this ﬁnal chapter, we give\\na brief summary of these results.\\nRegular languages:\\nThis class of languages was considered in Chapter 2.\\nThe following statements are equivalent:\\n1. The language A is regular, i.e., there exists a deterministic ﬁnite au-\\ntomaton that accepts A.\\n2. There exists a nondeterministic ﬁnite automaton that accepts A.\\n3. There exists a regular expression that describes A.\\nThis claim was proved by the following conversions:\\n1. Every nondeterministic ﬁnite automaton can be converted to an equiv-\\nalent deterministic ﬁnite automaton.\\n2. Every deterministic ﬁnite automaton can be converted to an equivalent\\nregular expression.\\n3. Every regular expression can be converted to an equivalent nondeter-\\nministic ﬁnite automaton.\\n240\\nChapter 7.\\nSummary\\nWe have seen that the class of regular languages is closed under the regular\\noperations: If A and B are regular languages, then\\n1. A ∪B is regular,\\n2. AB is regular,\\n3. A∗is regular,\\n4. A is regular, and\\n5. A ∩B is regular.\\nFinally, the pumping lemma for regular languages gives a property that\\nevery regular language possesses. We have used this to prove that languages\\nsuch as {anbn : n ≥0} are not regular.\\nContext-free languages:\\nThis class of languages was considered in Chap-\\nter 3. We have seen that every regular language is context-free. Moreover,\\nthere exist languages, for example {anbn : n ≥0}, that are context-free, but\\nnot regular. The following statements are equivalent:\\n1. The language A is context-free, i.e., there exists a context-free grammar\\nwhose language is A.\\n2. There exists a context-free grammar in Chomsky normal form whose\\nlanguage is A.\\n3. There exists a nondeterministic pushdown automaton that accepts A.\\nThis claim was proved by the following conversions:\\n1. Every context-free grammar can be converted to an equivalent context-\\nfree grammar in Chomsky normal form.\\n2. Every context-free grammar in Chomsky normal form can be converted\\nto an equivalent nondeterministic pushdown automaton.\\n3. Every nondeterministic pushdown automaton can be converted to an\\nequivalent context-free grammar. (This conversion was not covered in\\nthis book.)\\nChapter 7.\\nSummary\\n241\\nNondeterministic pushdown automata are more powerful than determin-\\nistic pushdown automata: There exists a nondeterministic pushdown au-\\ntomaton that accepts the language\\n{vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|},\\nbut there is no deterministic pushdown automaton that accepts this language.\\n(We did not prove this in this book.)\\nWe have seen that the class of context-free languages is closed under\\nthe union, concatenation, and star operations: If A and B are context-free\\nlanguages, then\\n1. A ∪B is context-free,\\n2. AB is context-free, and\\n3. A∗is context-free.\\nHowever,\\n1. the intersection of two context-free languages is not necessarily context-\\nfree, and\\n2. the complement of a context-free language is not necessarily context-\\nfree.\\nFinally, the pumping lemma for context-free languages gives a property\\nthat every context-free language possesses. We have used this to prove that\\nlanguages such as {anbncn : n ≥0} are not context-free.\\nThe Church-Turing Thesis:\\nIn Chapter 4, we considered “reasonable”\\ncomputational devices that model real computers. Examples of such devices\\nare Turing machines (with one or more tapes) and Java programs. It turns\\nout that all known “reasonable” devices are equivalent, i.e., can be converted\\nto each other. This led to the Church-Turing Thesis:\\n• Every computational process that is intuitively considered to be an\\nalgorithm can be converted to a Turing machine.\\n242\\nChapter 7.\\nSummary\\nDecidable and enumerable languages:\\nThese classes of languages were\\nconsidered in Chapter 5. They are deﬁned based on “reasonable” computa-\\ntional devices, such as Turing machines and Java programs. We have seen\\nthat\\n1. every context-free language is decidable, and\\n2. every decidable language is enumerable.\\nMoreover,\\n1. there exist languages, for example {anbncn : n ≥0}, that are decidable,\\nbut not context-free,\\n2. there exist languages, for example the Halting Problem, that are enu-\\nmerable, but not decidable,\\n3. there exist languages, for example the complement of the Halting Prob-\\nlem, that are not enumerable.\\nIn fact,\\n1. the class of all languages is not countable, whereas\\n2. the class of all enumerable languages is countable.\\nThe following statements are equivalent:\\n1. The language A is decidable.\\n2. Both A and its complement A are enumerable.\\nComplexity classes:\\nThese classes of languages were considered in Chap-\\nter 6.\\n1. The class P consists of all languages that can be decided in polynomial\\ntime by a deterministic Turing machine.\\n2. The class NP consists of all languages that can be decided in poly-\\nnomial time by a nondeterministic Turing machine. Equivalently, a\\nlanguage A is in the class NP, if for every string w ∈A, there exists a\\n“solution” s, such that (i) the length of s is polynomial in the length\\nof w, and (ii) the correctness of s can be veriﬁed in polynomial time.\\nChapter 7.\\nSummary\\n243\\nThe following properties hold:\\n1. Every context-free language is in P. (We did not prove this).\\n2. Every language in P is also in NP.\\n3. It is not known if there exist languages that are in NP, but not in P.\\n4. Every language in NP is decidable.\\nWe have introduced reductions to deﬁne the notion of a language B to be\\n“at least as hard” as a language A. A language B is called NP-complete, if\\n1. B belongs to the class NP, and\\n2. B is at least as hard as every language in the class NP.\\nWe have seen that NP-complete exist.\\nThe ﬁgure below summarizes the relationships among the various classes\\nof languages.\\n244\\nChapter 7.\\nSummary\\nregular\\ncontext-free\\nP\\nNP\\ndecidable\\nenumerable\\nall languages\\n', 'Introduction to Theory of Computation\\nAnil Maheshwari\\nMichiel Smid\\nSchool of Computer Science\\nCarleton University\\nOttawa\\nCanada\\n{anil,michiel}@scs.carleton.ca\\nAugust 29, 2024\\nii\\nContents\\nContents\\nPreface\\nvi\\n1\\nIntroduction\\n1\\n1.1\\nPurpose and motivation\\n. . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1.1\\nComplexity theory\\n. . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.2\\nComputability theory . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.3\\nAutomata theory . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.1.4\\nThis course\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.2\\nMathematical preliminaries\\n. . . . . . . . . . . . . . . . . . .\\n4\\n1.3\\nProof techniques\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.3.1\\nDirect proofs\\n. . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.3.2\\nConstructive proofs . . . . . . . . . . . . . . . . . . . .\\n9\\n1.3.3\\nNonconstructive proofs . . . . . . . . . . . . . . . . . .\\n10\\n1.3.4\\nProofs by contradiction . . . . . . . . . . . . . . . . . .\\n11\\n1.3.5\\nThe pigeon hole principle . . . . . . . . . . . . . . . . .\\n12\\n1.3.6\\nProofs by induction . . . . . . . . . . . . . . . . . . . .\\n13\\n1.3.7\\nMore examples of proofs . . . . . . . . . . . . . . . . .\\n15\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2\\nFinite Automata and Regular Languages\\n21\\n2.1\\nAn example: Controling a toll gate . . . . . . . . . . . . . . .\\n21\\n2.2\\nDeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . . . .\\n23\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton . . . . . . . . . .\\n26\\n2.2.2\\nA second example of a ﬁnite automaton\\n. . . . . . . .\\n28\\n2.2.3\\nA third example of a ﬁnite automaton\\n. . . . . . . . .\\n29\\n2.3\\nRegular operations . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n2.4\\nNondeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . .\\n35\\n2.4.1\\nA ﬁrst example . . . . . . . . . . . . . . . . . . . . . .\\n35\\niv\\nContents\\n2.4.2\\nA second example . . . . . . . . . . . . . . . . . . . . .\\n37\\n2.4.3\\nA third example . . . . . . . . . . . . . . . . . . . . . .\\n38\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\n. . . .\\n39\\n2.5\\nEquivalence of DFAs and NFAs . . . . . . . . . . . . . . . . .\\n41\\n2.5.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n2.6\\nClosure under the regular operations\\n. . . . . . . . . . . . . .\\n48\\n2.7\\nRegular expressions . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n2.8\\nEquivalence of regular expressions and regular languages . . .\\n57\\n2.8.1\\nEvery regular expression describes a regular language .\\n58\\n2.8.2\\nConverting a DFA to a regular expression\\n. . . . . . .\\n61\\n2.9\\nThe pumping lemma and nonregular languages . . . . . . . . .\\n68\\n2.9.1\\nApplications of the pumping lemma . . . . . . . . . . .\\n70\\n2.10 Higman’s Theorem . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.1 Dickson’s Theorem . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.2 Proof of Higman’s Theorem . . . . . . . . . . . . . . .\\n78\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n3\\nContext-Free Languages\\n91\\n3.1\\nContext-free grammars . . . . . . . . . . . . . . . . . . . . . .\\n91\\n3.2\\nExamples of context-free grammars . . . . . . . . . . . . . . .\\n94\\n3.2.1\\nProperly nested parentheses . . . . . . . . . . . . . . .\\n94\\n3.2.2\\nA context-free grammar for a nonregular language . . .\\n95\\n3.2.3\\nA context-free grammar for the complement of a non-\\nregular language\\n. . . . . . . . . . . . . . . . . . . . .\\n97\\n3.2.4\\nA context-free grammar that veriﬁes addition\\n. . . . .\\n98\\n3.3\\nRegular languages are context-free . . . . . . . . . . . . . . . . 100\\n3.3.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 102\\n3.4\\nChomsky normal form\\n. . . . . . . . . . . . . . . . . . . . . . 104\\n3.4.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 109\\n3.5\\nPushdown automata\\n. . . . . . . . . . . . . . . . . . . . . . . 112\\n3.6\\nExamples of pushdown automata\\n. . . . . . . . . . . . . . . . 116\\n3.6.1\\nProperly nested parentheses . . . . . . . . . . . . . . . 116\\n3.6.2\\nStrings of the form 0n1n\\n. . . . . . . . . . . . . . . . . 117\\n3.6.3\\nStrings with b in the middle . . . . . . . . . . . . . . . 118\\n3.7\\nEquivalence of pushdown automata and context-free grammars 120\\n3.8\\nThe pumping lemma for context-free languages\\n. . . . . . . . 124\\n3.8.1\\nProof of the pumping lemma . . . . . . . . . . . . . . . 125\\n3.8.2\\nApplications of the pumping lemma . . . . . . . . . . . 128\\nContents\\nv\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n4\\nTuring Machines and the Church-Turing Thesis\\n137\\n4.1\\nDeﬁnition of a Turing machine . . . . . . . . . . . . . . . . . . 137\\n4.2\\nExamples of Turing machines\\n. . . . . . . . . . . . . . . . . . 141\\n4.2.1\\nAccepting palindromes using one tape\\n. . . . . . . . . 141\\n4.2.2\\nAccepting palindromes using two tapes . . . . . . . . . 142\\n4.2.3\\nAccepting anbncn using one tape . . . . . . . . . . . . . 143\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2} . . . . 145\\n4.2.5\\nAccepting ambncmn using one tape . . . . . . . . . . . . 147\\n4.3\\nMulti-tape Turing machines . . . . . . . . . . . . . . . . . . . 148\\n4.4\\nThe Church-Turing Thesis . . . . . . . . . . . . . . . . . . . . 151\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n5\\nDecidable and Undecidable Languages\\n157\\n5.1\\nDecidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n5.1.1\\nThe language ADFA . . . . . . . . . . . . . . . . . . . . 158\\n5.1.2\\nThe language ANFA . . . . . . . . . . . . . . . . . . . . 159\\n5.1.3\\nThe language ACFG . . . . . . . . . . . . . . . . . . . . 160\\n5.1.4\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 161\\n5.1.5\\nThe Halting Problem . . . . . . . . . . . . . . . . . . . 163\\n5.2\\nCountable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n5.2.1\\nThe Halting Problem revisited . . . . . . . . . . . . . . 168\\n5.3\\nRice’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n5.3.1\\nProof of Rice’s Theorem . . . . . . . . . . . . . . . . . 171\\n5.4\\nEnumerability . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n5.4.1\\nHilbert’s problem . . . . . . . . . . . . . . . . . . . . . 174\\n5.4.2\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 176\\n5.5\\nWhere does the term “enumerable” come from? . . . . . . . . 177\\n5.6\\nMost languages are not enumerable . . . . . . . . . . . . . . . 180\\n5.6.1\\nThe set of enumerable languages is countable\\n. . . . . 180\\n5.6.2\\nThe set of all languages is not countable . . . . . . . . 181\\n5.6.3\\nThere are languages that are not enumerable . . . . . . 183\\n5.7\\nThe relationship between decidable and enumerable languages 184\\n5.8\\nA language A such that both A and A are not enumerable . . 186\\n5.8.1\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 186\\n5.8.2\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 188\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\nvi\\nContents\\n6\\nComplexity Theory\\n197\\n6.1\\nThe running time of algorithms . . . . . . . . . . . . . . . . . 197\\n6.2\\nThe complexity class P . . . . . . . . . . . . . . . . . . . . . . 199\\n6.2.1\\nSome examples . . . . . . . . . . . . . . . . . . . . . . 199\\n6.3\\nThe complexity class NP . . . . . . . . . . . . . . . . . . . . . 202\\n6.3.1\\nP is contained in NP . . . . . . . . . . . . . . . . . . . 208\\n6.3.2\\nDeciding NP-languages in exponential time\\n. . . . . . 208\\n6.3.3\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . 211\\n6.4\\nNon-deterministic algorithms\\n. . . . . . . . . . . . . . . . . . 211\\n6.5\\nNP-complete languages\\n. . . . . . . . . . . . . . . . . . . . . 213\\n6.5.1\\nTwo examples of reductions . . . . . . . . . . . . . . . 215\\n6.5.2\\nDeﬁnition of NP-completeness . . . . . . . . . . . . . . 220\\n6.5.3\\nAn NP-complete domino game\\n. . . . . . . . . . . . . 222\\n6.5.4\\nExamples of NP-complete languages . . . . . . . . . . 231\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\n7\\nSummary\\n239\\nPreface\\nThis is a free textbook for an undergraduate course on the Theory of Com-\\nputation, which we have been teaching at Carleton University since 2002.\\nUntil the 2011/2012 academic year, this course was oﬀered as a second-year\\ncourse (COMP 2805) and was compulsory for all Computer Science students.\\nStarting with the 2012/2013 academic year, the course has been downgraded\\nto a third-year optional course (COMP 3803).\\nWe have been developing this book since we started teaching this course.\\nCurrently, we cover most of the material from Chapters 2–5 during a 12-week\\nterm with three hours of classes per week.\\nThe material from Chapter 6, on Complexity Theory, is taught in the\\nthird-year course COMP 3804 (Design and Analysis of Algorithms). In the\\nearly years of COMP 2805, we gave a two-lecture overview of Complexity\\nTheory at the end of the term. Even though this overview has disappeared\\nfrom the course, we decided to keep Chapter 6. This chapter has not been\\nrevised/modiﬁed for a long time.\\nThe course as we teach it today has been inﬂuenced by the following two\\ntextbooks:\\n• Introduction to the Theory of Computation (second edition), by Michael\\nSipser, Thomson Course Technnology, Boston, 2006.\\n• Einf¨uhrung in die Theoretische Informatik, by Klaus Wagner, Springer-\\nVerlag, Berlin, 1994.\\nBesides reading this text, we recommend that you also take a look at\\nthese excellent textbooks, as well as one or more of the following ones:\\n• Elements of the Theory of Computation (second edition), by Harry\\nLewis and Christos Papadimitriou, Prentice-Hall, 1998.\\nviii\\n• Introduction to Languages and the Theory of Computation (third edi-\\ntion), by John Martin, McGraw-Hill, 2003.\\n• Introduction to Automata Theory, Languages, and Computation (third\\nedition), by John Hopcroft, Rajeev Motwani, Jeﬀrey Ullman, Addison\\nWesley, 2007.\\nPlease let us know if you ﬁnd errors, typos, simpler proofs, comments,\\nomissions, or if you think that some parts of the book “need improvement”.\\nChapter 1\\nIntroduction\\n1.1\\nPurpose and motivation\\nThis course is on the Theory of Computation, which tries to answer the\\nfollowing questions:\\n• What are the mathematical properties of computer hardware and soft-\\nware?\\n• What is a computation and what is an algorithm? Can we give rigorous\\nmathematical deﬁnitions of these notions?\\n• What are the limitations of computers?\\nCan “everything” be com-\\nputed? (As we will see, the answer to this question is “no”.)\\nPurpose of the Theory of Computation: Develop formal math-\\nematical models of computation that reﬂect real-world computers.\\nThis ﬁeld of research was started by mathematicians and logicians in the\\n1930’s, when they were trying to understand the meaning of a “computation”.\\nA central question asked was whether all mathematical problems can be\\nsolved in a systematic way. The research that started in those days led to\\ncomputers as we know them today.\\nNowadays, the Theory of Computation can be divided into the follow-\\ning three areas: Complexity Theory, Computability Theory, and Automata\\nTheory.\\n2\\nChapter 1.\\nIntroduction\\n1.1.1\\nComplexity theory\\nThe main question asked in this area is “What makes some problems com-\\nputationally hard and other problems easy?”\\nInformally, a problem is called “easy”, if it is eﬃciently solvable. Exam-\\nples of “easy” problems are (i) sorting a sequence of, say, 1,000,000 numbers,\\n(ii) searching for a name in a telephone directory, and (iii) computing the\\nfastest way to drive from Ottawa to Miami. On the other hand, a problem is\\ncalled “hard”, if it cannot be solved eﬃciently, or if we don’t know whether\\nit can be solved eﬃciently. Examples of “hard” problems are (i) time table\\nscheduling for all courses at Carleton, (ii) factoring a 300-digit integer into\\nits prime factors, and (iii) computing a layout for chips in VLSI.\\nCentral Question in Complexity Theory: Classify problems ac-\\ncording to their degree of “diﬃculty”. Give a rigorous proof that\\nproblems that seem to be “hard” are really “hard”.\\n1.1.2\\nComputability theory\\nIn the 1930’s, G¨odel, Turing, and Church discovered that some of the fun-\\ndamental mathematical problems cannot be solved by a “computer”. (This\\nmay sound strange, because computers were invented only in the 1940’s).\\nAn example of such a problem is “Is an arbitrary mathematical statement\\ntrue or false?” To attack such a problem, we need formal deﬁnitions of the\\nnotions of\\n• computer,\\n• algorithm, and\\n• computation.\\nThe theoretical models that were proposed in order to understand solvable\\nand unsolvable problems led to the development of real computers.\\nCentral Question in Computability Theory: Classify problems\\nas being solvable or unsolvable.\\n1.1.\\nPurpose and motivation\\n3\\n1.1.3\\nAutomata theory\\nAutomata Theory deals with deﬁnitions and properties of diﬀerent types of\\n“computation models”. Examples of such models are:\\n• Finite Automata. These are used in text processing, compilers, and\\nhardware design.\\n• Context-Free Grammars. These are used to deﬁne programming lan-\\nguages and in Artiﬁcial Intelligence.\\n• Turing Machines.\\nThese form a simple abstract model of a “real”\\ncomputer, such as your PC at home.\\nCentral Question in Automata Theory: Do these models have\\nthe same power, or can one model solve more problems than the\\nother?\\n1.1.4\\nThis course\\nIn this course, we will study the last two areas in reverse order: We will start\\nwith Automata Theory, followed by Computability Theory. The ﬁrst area,\\nComplexity Theory, will be covered in COMP 3804.\\nActually, before we start, we will review some mathematical proof tech-\\nniques. As you may guess, this is a fairly theoretical course, with lots of\\ndeﬁnitions, theorems, and proofs. You may guess this course is fun stuﬀfor\\nmath lovers, but boring and irrelevant for others. You guessed it wrong, and\\nhere are the reasons:\\n1. This course is about the fundamental capabilities and limitations of\\ncomputers. These topics form the core of computer science.\\n2. It is about mathematical properties of computer hardware and software.\\n3. This theory is very much relevant to practice, for example, in the design\\nof new programming languages, compilers, string searching, pattern\\nmatching, computer security, artiﬁcial intelligence, etc., etc.\\n4. This course helps you to learn problem solving skills. Theory teaches\\nyou how to think, prove, argue, solve problems, express, and abstract.\\n4\\nChapter 1.\\nIntroduction\\n5. This theory simpliﬁes the complex computers to an abstract and simple\\nmathematical model, and helps you to understand them better.\\n6. This course is about rigorously analyzing capabilities and limitations\\nof systems.\\nWhere does this course ﬁt in the Computer Science Curriculum at Car-\\nleton University? It is a theory course that is the third part in the series\\nCOMP 1805, COMP 2804, COMP 3803, COMP 3804, and COMP 4804.\\nThis course also widens your understanding of computers and will inﬂuence\\nother courses including Compilers, Programming Languages, and Artiﬁcial\\nIntelligence.\\n1.2\\nMathematical preliminaries\\nThroughout this course, we will assume that you know the following mathe-\\nmatical concepts:\\n1. A set is a collection of well-deﬁned objects. Examples are (i) the set of\\nall Dutch Olympic Gold Medallists, (ii) the set of all pubs in Ottawa,\\nand (iii) the set of all even natural numbers.\\n2. The set of natural numbers is N = {1, 2, 3, . . .}.\\n3. The set of integers is Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n4. The set of rational numbers is Q = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\n5. The set of real numbers is denoted by R.\\n6. If A and B are sets, then A is a subset of B, written as A ⊆B, if every\\nelement of A is also an element of B. For example, the set of even\\nnatural numbers is a subset of the set of all natural numbers. Every\\nset A is a subset of itself, i.e., A ⊆A. The empty set is a subset of\\nevery set A, i.e., ∅⊆A.\\n7. If B is a set, then the power set P(B) of B is deﬁned to be the set of\\nall subsets of B:\\nP(B) = {A : A ⊆B}.\\nObserve that ∅∈P(B) and B ∈P(B).\\n1.2.\\nMathematical preliminaries\\n5\\n8. If A and B are two sets, then\\n(a) their union is deﬁned as\\nA ∪B = {x : x ∈A or x ∈B},\\n(b) their intersection is deﬁned as\\nA ∩B = {x : x ∈A and x ∈B},\\n(c) their diﬀerence is deﬁned as\\nA \\\\ B = {x : x ∈A and x ̸∈B},\\n(d) the Cartesian product of A and B is deﬁned as\\nA × B = {(x, y) : x ∈A and y ∈B},\\n(e) the complement of A is deﬁned as\\nA = {x : x ̸∈A}.\\n9. A binary relation on two sets A and B is a subset of A × B.\\n10. A function f from A to B, denoted by f : A →B, is a binary relation\\nR, having the property that for each element a ∈A, there is exactly\\none ordered pair in R, whose ﬁrst component is a. We will also say\\nthat f(a) = b, or f maps a to b, or the image of a under f is b. The\\nset A is called the domain of f, and the set\\n{b ∈B : there is an a ∈A with f(a) = b}\\nis called the range of f.\\n11. A function f : A →B is one-to-one (or injective), if for any two distinct\\nelements a and a′ in A, we have f(a) ̸= f(a′). The function f is onto\\n(or surjective), if for each element b ∈B, there exists an element a ∈A,\\nsuch that f(a) = b; in other words, the range of f is equal to the set\\nB. A function f is a bijection, if f is both injective and surjective.\\n12. A binary relation R ⊆A × A is an equivalence relation, if it satisﬁes\\nthe following three conditions:\\n6\\nChapter 1.\\nIntroduction\\n(a) R is reﬂexive: For every element in a ∈A, we have (a, a) ∈R.\\n(b) R is symmetric: For all a and b in A, if (a, b) ∈R, then also\\n(b, a) ∈R.\\n(c) R is transitive: For all a, b, and c in A, if (a, b) ∈R and (b, c) ∈R,\\nthen also (a, c) ∈R.\\n13. A graph G = (V, E) is a pair consisting of a set V , whose elements are\\ncalled vertices, and a set E, where each element of E is a pair of distinct\\nvertices. The elements of E are called edges. The ﬁgure below shows\\nsome well-known graphs: K5 (the complete graph on ﬁve vertices), K3,3\\n(the complete bipartite graph on 2 × 3 = 6 vertices), and the Peterson\\ngraph.\\nK5\\nK3,3\\nPeterson graph\\nThe degree of a vertex v, denoted by deg(v), is deﬁned to be the number\\nof edges that are incident on v.\\nA path in a graph is a sequence of vertices that are connected by edges.\\nA path is a cycle, if it starts and ends at the same vertex. A simple\\npath is a path without any repeated vertices. A graph is connected, if\\nthere is a path between every pair of vertices.\\n14. In the context of strings, an alphabet is a ﬁnite set, whose elements\\nare called symbols. Examples of alphabets are Σ = {0, 1} and Σ =\\n{a, b, c, . . . , z}.\\n15. A string over an alphabet Σ is a ﬁnite sequence of symbols, where each\\nsymbol is an element of Σ. The length of a string w, denoted by |w|, is\\nthe number of symbols contained in w. The empty string, denoted by\\n1.3.\\nProof techniques\\n7\\nϵ, is the string having length zero. For example, if the alphabet Σ is\\nequal to {0, 1}, then 10, 1000, 0, 101, and ϵ are strings over Σ, having\\nlengths 2, 4, 1, 3, and 0, respectively.\\n16. A language is a set of strings.\\n17. The Boolean values are 1 and 0, that represent true and false, respec-\\ntively. The basic Boolean operations include\\n(a) negation (or NOT), represented by ¬,\\n(b) conjunction (or AND), represented by ∧,\\n(c) disjunction (or OR), represented by ∨,\\n(d) exclusive-or (or XOR), represented by ⊕,\\n(e) equivalence, represented by ↔or ⇔,\\n(f) implication, represented by →or ⇒.\\nThe following table explains the meanings of these operations.\\nNOT\\nAND\\nOR\\nXOR\\nequivalence\\nimplication\\n¬0 = 1\\n0 ∧0 = 0\\n0 ∨0 = 0\\n0 ⊕0 = 0\\n0 ↔0 = 1\\n0 →0 = 1\\n¬1 = 0\\n0 ∧1 = 0\\n0 ∨1 = 1\\n0 ⊕1 = 1\\n0 ↔1 = 0\\n0 →1 = 1\\n1 ∧0 = 0\\n1 ∨0 = 1\\n1 ⊕0 = 1\\n1 ↔0 = 0\\n1 →0 = 0\\n1 ∧1 = 1\\n1 ∨1 = 1\\n1 ⊕1 = 0\\n1 ↔1 = 1\\n1 →1 = 1\\n1.3\\nProof techniques\\nIn mathematics, a theorem is a statement that is true. A proof is a sequence\\nof mathematical statements that form an argument to show that a theorem is\\ntrue. The statements in the proof of a theorem include axioms (assumptions\\nabout the underlying mathematical structures), hypotheses of the theorem\\nto be proved, and previously proved theorems. The main question is “How\\ndo we go about proving theorems?” This question is similar to the question\\nof how to solve a given problem. Of course, the answer is that ﬁnding proofs,\\nor solving problems, is not easy; otherwise life would be dull! There is no\\nspeciﬁed way of coming up with a proof, but there are some generic strategies\\nthat could be of help. In this section, we review some of these strategies,\\nthat will be suﬃcient for this course. The best way to get a feeling of how\\nto come up with a proof is by solving a large number of problems. Here are\\n8\\nChapter 1.\\nIntroduction\\nsome useful tips. (You may take a look at the book How to Solve It, by G.\\nP´olya).\\n1. Read and completely understand the statement of the theorem to be\\nproved. Most often this is the hardest part.\\n2. Sometimes, theorems contain theorems inside them.\\nFor example,\\n“Property A if and only if property B”, requires showing two state-\\nments:\\n(a) If property A is true, then property B is true (A ⇒B).\\n(b) If property B is true, then property A is true (B ⇒A).\\nAnother example is the theorem “Set A equals set B.” To prove this,\\nwe need to prove that A ⊆B and B ⊆A. That is, we need to show\\nthat each element of set A is in set B, and that each element of set B\\nis in set A.\\n3. Try to work out a few simple cases of the theorem just to get a grip on\\nit (i.e., crack a few simple cases ﬁrst).\\n4. Try to write down the proof once you have it. This is to ensure the\\ncorrectness of your proof. Often, mistakes are found at the time of\\nwriting.\\n5. Finding proofs takes time, we do not come prewired to produce proofs.\\nBe patient, think, express and write clearly and try to be precise as\\nmuch as possible.\\nIn the next sections, we will go through some of the proof strategies.\\n1.3.1\\nDirect proofs\\nAs the name suggests, in a direct proof of a theorem, we just approach the\\ntheorem directly.\\nTheorem 1.3.1 If n is an odd positive integer, then n2 is odd as well.\\n1.3.\\nProof techniques\\n9\\nProof. An odd positive integer n can be written as n = 2k + 1, for some\\ninteger k ≥0. Then\\nn2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1.\\nSince 2(2k2 + 2k) is even, and “even plus one is odd”, we can conclude that\\nn2 is odd.\\nTheorem 1.3.2 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is an even integer, i.e.,\\nX\\nv∈V\\ndeg(v)\\nis even.\\nProof. If you do not see the meaning of this statement, then ﬁrst try it out\\nfor a few graphs. The reason why the statement holds is very simple: Each\\nedge contributes 2 to the summation (because an edge is incident on exactly\\ntwo distinct vertices).\\nActually, the proof above proves the following theorem.\\nTheorem 1.3.3 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2|E|.\\n1.3.2\\nConstructive proofs\\nThis technique not only shows the existence of a certain object, it actually\\ngives a method of creating it. Here is how a constructive proof looks like:\\nTheorem 1.3.4 There exists an object with property P.\\nProof. Here is the object: [. . .]\\nAnd here is the proof that the object satisﬁes property P: [. . .]\\nHere is an example of a constructive proof. A graph is called 3-regular, if\\neach vertex has degree three.\\n10\\nChapter 1.\\nIntroduction\\nTheorem 1.3.5 For every even integer n ≥4, there exists a 3-regular graph\\nwith n vertices.\\nProof. Deﬁne\\nV = {0, 1, 2, . . . , n −1},\\nand\\nE = {{i, i+1} : 0 ≤i ≤n−2}∪{{n−1, 0}}∪{{i, i+n/2} : 0 ≤i ≤n/2−1}.\\nThen the graph G = (V, E) is 3-regular.\\nConvince yourself that this graph is indeed 3-regular. It may help to draw\\nthe graph for, say, n = 8.\\n1.3.3\\nNonconstructive proofs\\nIn a nonconstructive proof, we show that a certain object exists, without\\nactually creating it. Here is an example of such a proof:\\nTheorem 1.3.6 There exist irrational numbers x and y such that xy is ra-\\ntional.\\nProof. There are two possible cases.\\nCase 1:\\n√\\n2\\n√\\n2 ∈Q.\\nIn this case, we take x = y =\\n√\\n2. In Theorem 1.3.9 below, we will prove\\nthat\\n√\\n2 is irrational.\\nCase 2:\\n√\\n2\\n√\\n2 ̸∈Q.\\nIn this case, we take x =\\n√\\n2\\n√\\n2 and y =\\n√\\n2. Since\\nxy =\\n\\x12√\\n2\\n√\\n2\\x13√\\n2\\n=\\n√\\n2\\n2 = 2,\\nthe claim in the theorem follows.\\nObserve that this proof indeed proves the theorem, but it does not give\\nan example of a pair of irrational numbers x and y such that xy is rational.\\n1.3.\\nProof techniques\\n11\\n1.3.4\\nProofs by contradiction\\nThis is how a proof by contradiction looks like:\\nTheorem 1.3.7 Statement S is true.\\nProof. Assume that statement S is false. Then, derive a contradiction (such\\nas 1 + 1 = 3).\\nIn other words, show that the statement “¬S ⇒false” is true. This is\\nsuﬃcient, because the contrapositive of the statement “¬S ⇒false” is the\\nstatement “true ⇒S”. The latter logical formula is equivalent to S, and\\nthat is what we wanted to show.\\nBelow, we give two examples of proofs by contradiction.\\nTheorem 1.3.8 Let n be a positive integer. If n2 is even, then n is even.\\nProof. We will prove the theorem by contradiction. So we assume that n2\\nis even, but n is odd. Since n is odd, we know from Theorem 1.3.1 that n2\\nis odd. This is a contradiction, because we assumed that n2 is even.\\nTheorem 1.3.9\\n√\\n2 is irrational, i.e.,\\n√\\n2 cannot be written as a fraction of\\ntwo integers m and n.\\nProof. We will prove the theorem by contradiction. So we assume that\\n√\\n2\\nis rational. Then\\n√\\n2 can be written as a fraction of two integers,\\n√\\n2 = m/n,\\nwhere m ≥1 and n ≥1. We may assume that m and n do not share any\\ncommon factors, i.e., the greatest common divisor of m and n is equal to\\none; if this is not the case, then we can get rid of the common factors. By\\nsquaring\\n√\\n2 = m/n, we get 2n2 = m2. This implies that m2 is even. Then,\\nby Theorem 1.3.8, m is even, which means that we can write m as m = 2k,\\nfor some positive integer k. It follows that 2n2 = m2 = 4k2, which implies\\nthat n2 = 2k2. Hence, n2 is even. Again by Theorem 1.3.8, it follows that n\\nis even.\\nWe have shown that m and n are both even. But we know that m and\\nn are not both even. Hence, we have a contradiction. Our assumption that\\n√\\n2 is rational is wrong. Thus, we can conclude that\\n√\\n2 is irrational.\\nThere is a nice discussion of this proof in the book My Brain is Open:\\nThe Mathematical Journeys of Paul Erd˝os by B. Schechter.\\n12\\nChapter 1.\\nIntroduction\\n1.3.5\\nThe pigeon hole principle\\nThis is a simple principle with surprising consequences.\\nPigeon Hole Principle: If n + 1 or more objects are placed into n\\nboxes, then there is at least one box containing two or more objects.\\nIn other words, if A and B are two sets such that |A| > |B|, then\\nthere is no one-to-one function from A to B.\\nTheorem 1.3.10 Let n be a positive integer. Every sequence of n2 + 1 dis-\\ntinct real numbers contains a subsequence of length n + 1 that is either in-\\ncreasing or decreasing.\\nProof. For example consider the sequence (20, 10, 9, 7, 11, 2, 21, 1, 20, 31) of\\n10 = 32 + 1 numbers. This sequence contains an increasing subsequence of\\nlength 4 = 3 + 1, namely (10, 11, 21, 31).\\nThe proof of this theorem is by contradiction, and uses the pigeon hole\\nprinciple.\\nLet (a1, a2, . . . , an2+1) be an arbitrary sequence of n2 + 1 distinct real\\nnumbers. For each i with 1 ≤i ≤n2 + 1, let inci denote the length of\\nthe longest increasing subsequence that starts at ai, and let deci denote the\\nlength of the longest decreasing subsequence that starts at ai.\\nUsing this notation, the claim in the theorem can be formulated as follows:\\nThere is an index i such that inci ≥n + 1 or deci ≥n + 1.\\nWe will prove the claim by contradiction. So we assume that inci ≤n\\nand deci ≤n for all i with 1 ≤i ≤n2 + 1.\\nConsider the set\\nB = {(b, c) : 1 ≤b ≤n, 1 ≤c ≤n},\\nand think of the elements of B as being boxes. For each i with 1 ≤i ≤n2+1,\\nthe pair (inci, deci) is an element of B. So we have n2+1 elements (inci, deci),\\nwhich are placed in the n2 boxes of B. By the pigeon hole principle, there\\nmust be a box that contains two (or more) elements. In other words, there\\nexist two integers i and j such that i < j and\\n(inci, deci) = (incj, decj).\\nRecall that the elements in the sequence are distinct. Hence, ai ̸= aj. We\\nconsider two cases.\\n1.3.\\nProof techniques\\n13\\nFirst assume that ai < aj. Then the length of the longest increasing\\nsubsequence starting at ai must be at least 1+incj, because we can append ai\\nto the longest increasing subsequence starting at aj. Therefore, inci ̸= incj,\\nwhich is a contradiction.\\nThe second case is when ai > aj. Then the length of the longest decreasing\\nsubsequence starting at ai must be at least 1+decj, because we can append ai\\nto the longest decreasing subsequence starting at aj. Therefore, deci ̸= decj,\\nwhich is again a contradiction.\\n1.3.6\\nProofs by induction\\nThis is a very powerful and important technique for proving theorems.\\nFor each positive integer n, let P(n) be a mathematical statement that\\ndepends on n. Assume we wish to prove that P(n) is true for all positive\\nintegers n. A proof by induction of such a statement is carried out as follows:\\nBasis: Prove that P(1) is true.\\nInduction step: Prove that for all n ≥1, the following holds: If P(n) is\\ntrue, then P(n + 1) is also true.\\nIn the induction step, we choose an arbitrary integer n ≥1 and assume\\nthat P(n) is true; this is called the induction hypothesis. Then we prove that\\nP(n + 1) is also true.\\nTheorem 1.3.11 For all positive integers n, we have\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\nProof. We start with the basis of the induction. If n = 1, then the left-hand\\nside is equal to 1, and so is the right-hand side. So the theorem is true for\\nn = 1.\\nFor the induction step, let n ≥1 and assume that the theorem is true for\\nn, i.e., assume that\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\n14\\nChapter 1.\\nIntroduction\\nWe have to prove that the theorem is true for n + 1, i.e., we have to prove\\nthat\\n1 + 2 + 3 + . . . + (n + 1) = (n + 1)(n + 2)\\n2\\n.\\nHere is the proof:\\n1 + 2 + 3 + . . . + (n + 1)\\n=\\n1 + 2 + 3 + . . . + n\\n|\\n{z\\n}\\n= n(n+1)\\n2\\n+(n + 1)\\n=\\nn(n + 1)\\n2\\n+ (n + 1)\\n=\\n(n + 1)(n + 2)\\n2\\n.\\nBy the way, here is an alternative proof of the theorem above: Let S =\\n1 + 2 + 3 + . . . + n. Then,\\nS\\n=\\n1\\n+\\n2\\n+\\n3\\n+\\n. . .\\n+\\n(n −2)\\n+\\n(n −1)\\n+\\nn\\nS\\n=\\nn\\n+\\n(n −1)\\n+\\n(n −2)\\n+\\n. . .\\n+\\n3\\n+\\n2\\n+\\n1\\n2S\\n=\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n. . .\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\nSince there are n terms on the right-hand side, we have 2S = n(n + 1). This\\nimplies that S = n(n + 1)/2.\\nTheorem 1.3.12 For every positive integer n, a −b is a factor of an −bn.\\nProof. A direct proof can be given by providing a factorization of an −bn:\\nan −bn = (a −b)(an−1 + an−2b + an−3b2 + . . . + abn−2 + bn−1).\\nWe now prove the theorem by induction. For the basis, let n = 1. The claim\\nin the theorem is “a −b is a factor of a −b”, which is obviously true.\\nLet n ≥1 and assume that a −b is a factor of an −bn. We have to prove\\nthat a −b is a factor of an+1 −bn+1. We have\\nan+1 −bn+1 = an+1 −anb + anb −bn+1 = an(a −b) + (an −bn)b.\\nThe ﬁrst term on the right-hand side is divisible by a −b. By the induction\\nhypothesis, the second term on the right-hand side is divisible by a −b as\\nwell. Therefore, the entire right-hand side is divisible by a −b. Since the\\nright-hand side is equal to an+1 −bn+1, it follows that a −b is a factor of\\nan+1 −bn+1.\\nWe now give an alternative proof of Theorem 1.3.3:\\n1.3.\\nProof techniques\\n15\\nTheorem 1.3.13 Let G = (V, E) be a graph with m edges. Then the sum\\nof the degrees of all vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2m.\\nProof. The proof is by induction on the number m of edges. For the basis of\\nthe induction, assume that m = 0. Then the graph G does not contain any\\nedges and, therefore, P\\nv∈V deg(v) = 0. Thus, the theorem is true if m = 0.\\nLet m ≥0 and assume that the theorem is true for every graph with m\\nedges. Let G be an arbitrary graph with m+1 edges. We have to prove that\\nP\\nv∈V deg(v) = 2(m + 1).\\nLet {a, b} be an arbitrary edge in G, and let G′ be the graph obtained\\nfrom G by removing the edge {a, b}. Since G′ has m edges, we know from\\nthe induction hypothesis that the sum of the degrees of all vertices in G′ is\\nequal to 2m. Using this, we obtain\\nX\\nv∈G\\ndeg(v) =\\nX\\nv∈G′\\ndeg(v) + 2 = 2m + 2 = 2(m + 1).\\n1.3.7\\nMore examples of proofs\\nRecall Theorem 1.3.5, which states that for every even integer n ≥4, there\\nexists a 3-regular graph with n vertices. The following theorem explains why\\nwe stated this theorem for even values of n.\\nTheorem 1.3.14 Let n ≥5 be an odd integer. There is no 3-regular graph\\nwith n vertices.\\nProof. The proof is by contradiction. So we assume that there exists a\\ngraph G = (V, E) with n vertices that is 3-regular. Let m be the number of\\nedges in G. Since deg(v) = 3 for every vertex, we have\\nX\\nv∈V\\ndeg(v) = 3n.\\nOn the other hand, by Theorem 1.3.3, we have\\nX\\nv∈V\\ndeg(v) = 2m.\\n16\\nChapter 1.\\nIntroduction\\nIt follows that 3n = 2m, which can be rewritten as m = 3n/2. Since m is an\\ninteger, and since gcd(2, 3) = 1, n/2 must be an integer. Hence, n is even,\\nwhich is a contradiction.\\nLet Kn be the complete graph on n vertices. This graph has a vertex set\\nof size n, and every pair of distinct vertices is joined by an edge.\\nIf G = (V, E) is a graph with n vertices, then the complement G of G is\\nthe graph with vertex set V that consists of those edges of Kn that are not\\npresent in G.\\nTheorem 1.3.15 Let n ≥2 and let G be a graph on n vertices. Then G is\\nconnected or G is connected.\\nProof. We prove the theorem by induction on the number n of vertices. For\\nthe basis, assume that n = 2. There are two possibilities for the graph G:\\n1. G contains one edge. In this case, G is connected.\\n2. G does not contain an edge. In this case, the complement G contains\\none edge and, therefore, G is connected.\\nSo for n = 2, the theorem is true.\\nLet n ≥2 and assume that the theorem is true for every graph with n\\nvertices. Let G be graph with n + 1 vertices. We have to prove that G is\\nconnected or G is connected. We consider three cases.\\nCase 1: There is a vertex v whose degree in G is equal to n.\\nSince G has n+1 vertices, v is connected by an edge to every other vertex\\nof G. Therefore, G is connected.\\nCase 2: There is a vertex v whose degree in G is equal to 0.\\nIn this case, the degree of v in the graph G is equal to n. Since G has n+1\\nvertices, v is connected by an edge to every other vertex of G. Therefore, G\\nis connected.\\nCase 3: For every vertex v, the degree of v in G is in {1, 2, . . . , n −1}.\\nLet v be an arbitrary vertex of G.\\nLet G′ be the graph obtained by\\ndeleting from G the vertex v, together with all edges that are incident on v.\\nSince G′ has n vertices, we know from the induction hypothesis that G′ is\\nconnected or G′ is connected.\\n1.3.\\nProof techniques\\n17\\nLet us ﬁrst assume that G′ is connected. Then the graph G is connected\\nas well, because there is at least one edge in G between v and some vertex\\nof G′.\\nIf G′ is not connected, then G′ must be connected. Since we are in Case 3,\\nwe know that the degree of v in G is in the set {1, 2, . . . , n −1}. It follows\\nthat the degree of v in the graph G is in this set as well. Hence, there is at\\nleast one edge in G between v and some vertex in G′. This implies that G is\\nconnected.\\nThe previous theorem can be rephrased as follows:\\nTheorem 1.3.16 Let n ≥2 and consider the complete graph Kn on n ver-\\ntices. Color each edge of this graph as either red or blue. Let R be the graph\\nconsisting of all the red edges, and let B be the graph consisting of all the\\nblue edges. Then R is connected or B is connected.\\nA graph is said to be planar, if it can be drawn (a better term is “embed-\\nded”) in the plane in such a way that no two edges intersect, except possibly\\nat their endpoints.\\nAn embedding of a planar graph consists of vertices,\\nedges, and faces. In the example below, there are 11 vertices, 18 edges, and\\n9 faces (including the unbounded face).\\nThe following theorem is known as Euler’s theorem for planar graphs.\\nApparently, this theorem was discovered by Euler around 1750. Legendre\\ngave the ﬁrst proof in 1794, see\\nhttp://www.ics.uci.edu/~eppstein/junkyard/euler/\\nTheorem 1.3.17 (Euler) Consider an embedding of a planar graph G. Let\\nv, e, and f be the number of vertices, edges, and faces (including the single\\n18\\nChapter 1.\\nIntroduction\\nunbounded face) of this embedding, respectively. Moreover, let c be the number\\nof connected components of G. Then\\nv −e + f = c + 1.\\nProof. The proof is by induction on the number of edges of G. To be more\\nprecise, we start with a graph having no edges, and prove that the theorem\\nholds for this case. Then, we add the edges one by one, and show that the\\nrelation v −e + f = c + 1 is maintained.\\nSo we ﬁrst assume that G has no edges, i.e., e = 0. Then the embedding\\nconsists of a collection of v points. In this case, we have f = 1 and c = v.\\nHence, the relation v −e + f = c + 1 holds.\\nLet e > 0 and assume that Euler’s formula holds for a subgraph of G\\nhaving e −1 edges. Let {u, v} be an edge of G that is not in the subgraph,\\nand add this edge to the subgraph. There are two cases depending on whether\\nthis new edge joins two connected components or joins two vertices in the\\nsame connected component.\\nCase 1: The new edge {u, v} joins two connected components.\\nIn this case, the number of vertices and the number of faces do not change,\\nthe number of connected components goes down by 1, and the number of\\nedges increases by 1. It follows that the relation in the theorem is still valid.\\nCase 2: The new edge {u, v} joins two vertices in the same connected com-\\nponent.\\nIn this case, the number of vertices and the number of connected com-\\nponents do not change, the number of edges increases by 1, and the number\\nof faces increases by 1 (because the new edge splits one face into two faces).\\nTherefore, the relation in the theorem is still valid.\\nEuler’s theorem is usually stated as follows:\\nTheorem 1.3.18 (Euler) Consider an embedding of a connected planar\\ngraph G. Let v, e, and f be the number of vertices, edges, and faces (in-\\ncluding the single unbounded face) of this embedding, respectively. Then\\nv −e + f = 2.\\nIf you like surprising proofs of various mathematical results, you should\\nread the book Proofs from THE BOOK by Aigner and Ziegler.\\nExercises\\n19\\nExercises\\n1.1 Use induction to prove that every integer n ≥2 can be written as a\\nproduct of prime numbers.\\n1.2 For every prime number p, prove that √p is irrational.\\n1.3 Let n be a positive integer that is not a perfect square. Prove that √n\\nis irrational.\\n1.4 Prove by induction that n4 −4n2 is divisible by 3, for all integers n ≥1.\\n1.5 Prove that\\nn\\nX\\ni=1\\n1\\ni2 < 2 −1/n,\\nfor every integer n ≥2.\\n1.6 Prove that 9 divides n3 + (n + 1)3 + (n + 2)3, for every integer n ≥0.\\n1.7 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers that are consecutive.\\n1.8 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers such that one divides the other.\\n20\\nChapter 1.\\nIntroduction\\nChapter 2\\nFinite Automata and Regular\\nLanguages\\nIn this chapter, we introduce and analyze the class of languages that are\\nknown as regular languages. Informally, these languages can be “processed”\\nby computers having a very small amount of memory.\\n2.1\\nAn example: Controling a toll gate\\nBefore we give a formal deﬁnition of a ﬁnite automaton, we consider an\\nexample in which such an automaton shows up in a natural way. We consider\\nthe problem of designing a “computer” that controls a toll gate.\\nWhen a car arrives at the toll gate, the gate is closed. The gate opens as\\nsoon as the driver has payed 25 cents. We assume that we have only three\\ncoin denominations: 5, 10, and 25 cents. We also assume that no excess\\nchange is returned.\\nAfter having arrived at the toll gate, the driver inserts a sequence of coins\\ninto the machine. At any moment, the machine has to decide whether or not\\nto open the gate, i.e., whether or not the driver has paid 25 cents (or more).\\nIn order to decide this, the machine is in one of the following six states, at\\nany moment during the process:\\n• The machine is in state q0, if it has not collected any money yet.\\n• The machine is in state q1, if it has collected exactly 5 cents.\\n• The machine is in state q2, if it has collected exactly 10 cents.\\n22\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The machine is in state q3, if it has collected exactly 15 cents.\\n• The machine is in state q4, if it has collected exactly 20 cents.\\n• The machine is in state q5, if it has collected 25 cents or more.\\nInitially (when a car arrives at the toll gate), the machine is in state q0.\\nAssume, for example, that the driver presents the sequence (10,5,5,10) of\\ncoins.\\n• After receiving the ﬁrst 10 cents coin, the machine switches from state\\nq0 to state q2.\\n• After receiving the ﬁrst 5 cents coin, the machine switches from state\\nq2 to state q3.\\n• After receiving the second 5 cents coin, the machine switches from state\\nq3 to state q4.\\n• After receiving the second 10 cents coin, the machine switches from\\nstate q4 to state q5. At this moment, the gate opens. (Remember that\\nno change is given.)\\nThe ﬁgure below represents the behavior of the machine for all possible\\nsequences of coins. State q5 is represented by two circles, because it is a\\nspecial state: As soon as the machine reaches this state, the gate opens.\\nq0\\nq1\\nq2\\nq3\\nq4\\nq5\\n5\\n5\\n5\\n5\\n10\\n10\\n10\\n25\\n25\\n25\\n10, 25\\n5, 10, 25\\n5, 10\\n25\\nstart\\nObserve that the machine (or computer) only has to remember which\\nstate it is in at any given time. Thus, it needs only a very small amount\\nof memory: It has to be able to distinguish between any one of six possible\\ncases and, therefore, it only needs a memory of ⌈log 6⌉= 3 bits.\\n2.2.\\nDeterministic ﬁnite automata\\n23\\n2.2\\nDeterministic ﬁnite automata\\nLet us look at another example. Consider the following state diagram:\\nq1\\nq2\\nq3\\n0\\n0\\n1\\n1\\n0,1\\nWe say that q1 is the start state and q2 is an accept state. Consider the\\ninput string 1101. This string is processed in the following way:\\n• Initially, the machine is in the start state q1.\\n• After having read the ﬁrst 1, the machine switches from state q1 to\\nstate q2.\\n• After having read the second 1, the machine switches from state q2 to\\nstate q2. (So actually, it does not switch.)\\n• After having read the ﬁrst 0, the machine switches from state q2 to\\nstate q3.\\n• After having read the third 1, the machine switches from state q3 to\\nstate q2.\\nAfter the entire string 1101 has been processed, the machine is in state q2,\\nwhich is an accept state. We say that the string 1101 is accepted by the\\nmachine.\\nConsider now the input string 0101010. After having read this string\\nfrom left to right (starting in the start state q1), the machine is in state q3.\\nSince q3 is not an accept state, we say that the machine rejects the string\\n0101010.\\nWe hope you are able to see that this machine accepts every binary string\\nthat ends with a 1. In fact, the machine accepts more strings:\\n• Every binary string having the property that there are an even number\\nof 0s following the rightmost 1, is accepted by this machine.\\n24\\nChapter 2.\\nFinite Automata and Regular Languages\\n• Every other binary string is rejected by the machine. Observe that each\\nsuch string is either empty, consists of 0s only, or has an odd number\\nof 0s following the rightmost 1.\\nWe now come to the formal deﬁnition of a ﬁnite automaton:\\nDeﬁnition 2.2.1 A ﬁnite automaton is a 5-tuple M = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σ →Q is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\nYou can think of the transition function δ as being the “program” of the\\nﬁnite automaton M = (Q, Σ, δ, q, F). This function tells us what M can do\\nin “one step”:\\n• Let r be a state of Q and let a be a symbol of the alphabet Σ. If\\nthe ﬁnite automaton M is in state r and reads the symbol a, then it\\nswitches from state r to state δ(r, a). (In fact, δ(r, a) may be equal to\\nr.)\\nThe “computer” that we designed in the toll gate example in Section 2.1\\nis a ﬁnite automaton. For this example, we have Q = {q0, q1, q2, q3, q4, q5},\\nΣ = {5, 10, 25}, the start state is q0, F = {q5}, and δ is given by the following\\ntable:\\n5\\n10\\n25\\nq0\\nq1\\nq2\\nq5\\nq1\\nq2\\nq3\\nq5\\nq2\\nq3\\nq4\\nq5\\nq3\\nq4\\nq5\\nq5\\nq4\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nThe example given in the beginning of this section is also a ﬁnite automa-\\nton. For this example, we have Q = {q1, q2, q3}, Σ = {0, 1}, the start state\\nis q1, F = {q2}, and δ is given by the following table:\\n2.2.\\nDeterministic ﬁnite automata\\n25\\n0\\n1\\nq1\\nq1\\nq2\\nq2\\nq3\\nq2\\nq3\\nq2\\nq2\\nLet us denote this ﬁnite automaton by M. The language of M, denoted\\nby L(M), is the set of all binary strings that are accepted by M. As we have\\nseen before, we have\\nL(M) = {w : w contains at least one 1 and ends with an even number of 0s}.\\nWe now give a formal deﬁnition of the language of a ﬁnite automaton:\\nDeﬁnition 2.2.2 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton and let w =\\nw1w2 . . . wn be a string over Σ. Deﬁne the sequence r0, r1, . . . , rn of states, in\\nthe following way:\\n• r0 = q,\\n• ri+1 = δ(ri, wi+1), for i = 0, 1, . . . , n −1.\\n1. If rn ∈F, then we say that M accepts w.\\n2. If rn ̸∈F, then we say that M rejects w.\\nIn this deﬁnition, w may be the empty string, which we denote by ϵ, and\\nwhose length is zero; thus in the deﬁnition above, n = 0. In this case, the\\nsequence r0, r1, . . . , rn of states has length one; it consists of just the state\\nr0 = q. The empty string is accepted by M if and only if the start state q\\nbelongs to F.\\nDeﬁnition 2.2.3 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton. The lan-\\nguage L(M) accepted by M is deﬁned to be the set of all strings that are\\naccepted by M:\\nL(M) = {w : w is a string over Σ and M accepts w }.\\nDeﬁnition 2.2.4 A language A is called regular, if there exists a ﬁnite au-\\ntomaton M such that A = L(M).\\n26\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe ﬁnish this section by presenting an equivalent way of deﬁning the\\nlanguage accepted by a ﬁnite automaton. Let M = (Q, Σ, δ, q, F) be a ﬁnite\\nautomaton. The transition function δ : Q × Σ →Q tells us that, when M\\nis in state r ∈Q and reads symbol a ∈Σ, it switches from state r to state\\nδ(r, a). Let Σ∗denote the set of all strings over the alphabet Σ. (Σ∗includes\\nthe empty string ϵ.) We extend the function δ to a function\\nδ : Q × Σ∗→Q,\\nthat is deﬁned as follows. For any state r ∈Q and for any string w over the\\nalphabet Σ,\\nδ(r, w) =\\n\\x1a r\\nif w = ϵ,\\nδ(δ(r, v), a)\\nif w = va, where v is a string and a ∈Σ.\\nWhat is the meaning of this function δ? Let r be a state of Q and let w be\\na string over the alphabet Σ. Then\\n• δ(r, w) is the state that M reaches, when it starts in state r, reads the\\nstring w from left to right, and uses δ to switch from state to state.\\nUsing this notation, we have\\nL(M) = {w : w is a string over Σ and δ(q, w) ∈F}.\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton\\nLet\\nA = {w : w is a binary string containing an odd number of 1s}.\\nWe claim that this language A is regular. In order to prove this, we have to\\nconstruct a ﬁnite automaton M such that A = L(M).\\nHow to construct M? Here is a ﬁrst idea: The ﬁnite automaton reads the\\ninput string w from left to right and keeps track of the number of 1s it has\\nseen. After having read the entire string w, it checks whether this number\\nis odd (in which case w is accepted) or even (in which case w is rejected).\\nUsing this approach, the ﬁnite automaton needs a state for every integer\\ni ≥0, indicating that the number of 1s read so far is equal to i. Hence,\\nto design a ﬁnite automaton that follows this approach, we need an inﬁnite\\n2.2.\\nDeterministic ﬁnite automata\\n27\\nnumber of states. But, the deﬁnition of ﬁnite automaton requires the number\\nof states to be ﬁnite.\\nA better, and correct approach, is to keep track of whether the number\\nof 1s read so far is even or odd. This leads to the following ﬁnite automaton:\\n• The set of states is Q = {qe, qo}. If the ﬁnite automaton is in state qe,\\nthen it has read an even number of 1s; if it is in state qo, then it has\\nread an odd number of 1s.\\n• The alphabet is Σ = {0, 1}.\\n• The start state is qe, because at the start, the number of 1s read by the\\nautomaton is equal to 0, and 0 is even.\\n• The set F of accept states is F = {qo}.\\n• The transition function δ is given by the following table:\\n0\\n1\\nqe\\nqe\\nqo\\nqo\\nqo\\nqe\\nThis ﬁnite automaton M = (Q, Σ, δ, qe, F) can also be described by its state\\ndiagram, which is given in the ﬁgure below. The arrow that comes “out of\\nthe blue” and enters the state qe, indicates that qe is the start state. The\\nstate depicted with double circles indicates the accept state.\\nqe\\nqo\\n0\\n0\\n1\\n1\\nWe have constructed a ﬁnite automaton M that accepts the language A.\\nTherefore, A is a regular language.\\n28\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.2.2\\nA second example of a ﬁnite automaton\\nDeﬁne the language A as\\nA = {w : w is a binary string containing 101 as a substring}.\\nAgain, we claim that A is a regular language. In other words, we claim that\\nthere exists a ﬁnite automaton M that accepts A, i.e., A = L(M).\\nThe ﬁnite automaton M will do the following, when reading an input\\nstring from left to right:\\n• It skips over all 0s, and stays in the start state.\\n• At the ﬁrst 1, it switches to the state “maybe the next two symbols are\\n01”.\\n– If the next symbol is 1, then it stays in the state “maybe the next\\ntwo symbols are 01”.\\n– On the other hand, if the next symbol is 0, then it switches to the\\nstate “maybe the next symbol is 1”.\\n∗If the next symbol is indeed 1, then it switches to the accept\\nstate (but keeps on reading until the end of the string).\\n∗On the other hand, if the next symbol is 0, then it switches\\nto the start state, and skips 0s until it reads 1 again.\\nBy deﬁning the following four states, this process will become clear:\\n• q1: M is in this state if the last symbol read was 1, but the substring\\n101 has not been read.\\n• q10: M is in this state if the last two symbols read were 10, but the\\nsubstring 101 has not been read.\\n• q101: M is in this state if the substring 101 has been read in the input\\nstring.\\n• q: In all other cases, M is in this state.\\nHere is the formal description of the ﬁnite automaton that accepts the\\nlanguage A:\\n• Q = {q, q1, q10, q101},\\n2.2.\\nDeterministic ﬁnite automata\\n29\\n• Σ = {0, 1},\\n• the start state is q,\\n• the set F of accept states is equal to F = {q101}, and\\n• the transition function δ is given by the following table:\\n0\\n1\\nq\\nq\\nq1\\nq1\\nq10\\nq1\\nq10\\nq\\nq101\\nq101\\nq101\\nq101\\nThe ﬁgure below gives the state diagram of the ﬁnite automaton M =\\n(Q, Σ, δ, q, F).\\nq\\nq1\\nq10\\nq101\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nThis ﬁnite automaton accepts the language A consisting of all binary\\nstrings that contain the substring 101. As an exercise, how would you obtain\\na ﬁnite automaton that accepts the complement of A, i.e., the language\\nconsisting of all binary strings that do not contain the substring 101?\\n2.2.3\\nA third example of a ﬁnite automaton\\nThe ﬁnite automata we have seen so far have exactly one accept state. In\\nthis section, we will see an example of a ﬁnite automaton having more accept\\nstates.\\n30\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right},\\nwhere {0, 1}∗is the set of all binary strings, including the empty string ϵ. We\\nclaim that A is a regular language. To prove this, we have to construct a ﬁnite\\nautomaton M such that A = L(M). At ﬁrst sight, it seems diﬃcult (or even\\nimpossible?) to construct such a ﬁnite automaton: How does the automaton\\n“know” that it has reached the third symbol from the right? It is, however,\\npossible to construct such an automaton. The main idea is to remember the\\nlast three symbols that have been read. Thus, the ﬁnite automaton has eight\\nstates qijk, where i, j, and k range over the two elements of {0, 1}. If the\\nautomaton is in state qijk, then the following hold:\\n• If M has read at least three symbols, then the three most recently read\\nsymbols are ijk.\\n• If M has read only two symbols, then these two symbols are jk; more-\\nover, i = 0.\\n• If M has read only one symbol, then this symbol is k; moreover, i =\\nj = 0.\\n• If M has not read any symbol, then i = j = k = 0.\\nThe start state is q000 and the set of accept states is {q100, q110, q101, q111}.\\nThe transition function of M is given by the following state diagram.\\nq000\\nq100\\nq010\\nq110\\nq001\\nq101\\nq011\\nq111\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n2.3.\\nRegular operations\\n31\\n2.3\\nRegular operations\\nIn this section, we deﬁne three operations on languages. Later, we will answer\\nthe question whether the set of all regular languages is closed under these\\noperations. Let A and B be two languages over the same alphabet.\\n1. The union of A and B is deﬁned as\\nA ∪B = {w : w ∈A or w ∈B}.\\n2. The concatenation of A and B is deﬁned as\\nAB = {ww′ : w ∈A and w′ ∈B}.\\nIn words, AB is the set of all strings obtained by taking an arbitrary\\nstring w in A and an arbitrary string w′ in B, and gluing them together\\n(such that w is to the left of w′).\\n3. The star of A is deﬁned as\\nA∗= {u1u2 . . . uk : k ≥0 and ui ∈A for all i = 1, 2, . . . , k}.\\nIn words, A∗is obtained by taking any ﬁnite number of strings in A, and\\ngluing them together. Observe that k = 0 is allowed; this corresponds\\nto the empty string ϵ. Thus, ϵ ∈A∗.\\nTo give an example, let A = {0, 01} and B = {1, 10}. Then\\nA ∪B = {0, 01, 1, 10},\\nAB = {01, 010, 011, 0110},\\nand\\nA∗= {ϵ, 0, 01, 00, 001, 010, 0101, 000, 0001, 00101, . . .}.\\nAs another example, if Σ = {0, 1}, then Σ∗is the set of all binary strings\\n(including the empty string). Observe that a string always has a ﬁnite length.\\nBefore we proceed, we give an alternative (and equivalent) deﬁnition of\\nthe star of the language A: Deﬁne\\nA0 = {ϵ}\\n32\\nChapter 2.\\nFinite Automata and Regular Languages\\nand, for k ≥1,\\nAk = AAk−1,\\ni.e., Ak is the concatenation of the two languages A and Ak−1. Then we have\\nA∗=\\n∞\\n[\\nk=0\\nAk.\\nTheorem 2.3.1 The set of regular languages is closed under the union op-\\neration, i.e., if A and B are regular languages over the same alphabet Σ, then\\nA ∪B is also a regular language.\\nProof.\\nSince A and B are regular languages, there are ﬁnite automata\\nM1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2) that accept A and B,\\nrespectively. In order to prove that A ∪B is regular, we have to construct a\\nﬁnite automaton M that accepts A ∪B. In other words, M must have the\\nproperty that for every string w ∈Σ∗,\\nM accepts w ⇔M1 accepts w or M2 accepts w.\\nAs a ﬁrst idea, we may think that M could do the following:\\n• Starting in the start state q1 of M1, M “runs” M1 on w.\\n• If, after having read w, M1 is in a state of F1, then w ∈A, thus\\nw ∈A ∪B and, therefore, M accepts w.\\n• On the other hand, if, after having read w, M1 is in a state that is not\\nin F1, then w ̸∈A and M “runs” M2 on w, starting in the start state\\nq2 of M2. If, after having read w, M2 is in a state of F2, then we know\\nthat w ∈B, thus w ∈A ∪B and, therefore, M accepts w. Otherwise,\\nwe know that w ̸∈A ∪B, and M rejects w.\\nThis idea does not work, because the ﬁnite automaton M can read the input\\nstring w only once. The correct approach is to run M1 and M2 simulta-\\nneously. We deﬁne the set Q of states of M to be the Cartesian product\\nQ1 × Q2. If M is in state (r1, r2), this means that\\n• if M1 would have read the input string up to this point, then it would\\nbe in state r1, and\\n2.3.\\nRegular operations\\n33\\n• if M2 would have read the input string up to this point, then it would\\nbe in state r2.\\nThis leads to the ﬁnite automaton M = (Q, Σ, δ, q, F), where\\n• Q = Q1 × Q2 = {(r1, r2) : r1 ∈Q1 and r2 ∈Q2}.\\nObserve that\\n|Q| = |Q1| × |Q2|, which is ﬁnite.\\n• Σ is the alphabet of A and B (recall that we assume that A and B are\\nlanguages over the same alphabet).\\n• The start state q of M is equal to q = (q1, q2).\\n• The set F of accept states of M is given by\\nF = {(r1, r2) : r1 ∈F1 or r2 ∈F2} = (F1 × Q2) ∪(Q1 × F2).\\n• The transition function δ : Q × Σ →Q is given by\\nδ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)),\\nfor all r1 ∈Q1, r2 ∈Q2, and a ∈Σ.\\nTo ﬁnish the proof, we have to show that this ﬁnite automaton M indeed\\naccepts the language A∪B. Intuitively, this should be clear from the discus-\\nsion above. The easiest way to give a formal proof is by using the extended\\ntransition functions δ1 and δ2. (The extended transition function has been\\ndeﬁned after Deﬁnition 2.2.4.) Here we go: Recall that we have to prove that\\nM accepts w ⇔M1 accepts w or M2 accepts w,\\ni.e,\\nM accepts w ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\nIn terms of the extended transition function δ of the transition function δ of\\nM, this becomes\\nδ((q1, q2), w) ∈F ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\n(2.1)\\nBy applying the deﬁnition of the extended transition function, as given after\\nDeﬁnition 2.2.4, to δ, it can be seen that\\nδ((q1, q2), w) = (δ1(q1, w), δ2(q2, w)).\\n34\\nChapter 2.\\nFinite Automata and Regular Languages\\nThe latter equality implies that (2.1) is true and, therefore, M indeed accepts\\nthe language A ∪B.\\nWhat about the closure of the regular languages under the concatenation\\nand star operations? It turns out that the regular languages are closed under\\nthese operations. But how do we prove this?\\nLet A and B be two regular languages, and let M1 and M2 be ﬁnite\\nautomata that accept A and B, respectively. How do we construct a ﬁnite\\nautomaton M that accepts the concatenation AB? Given an input string\\nu, M has to decide whether or not u can be broken into two strings w and\\nw′ (i.e., write u as u = ww′), such that w ∈A and w′ ∈B. In words, M\\nhas to decide whether or not u can be broken into two substrings, such that\\nthe ﬁrst substring is accepted by M1 and the second substring is accepted by\\nM2. The diﬃculty is caused by the fact that M has to make this decision by\\nscanning the string u only once. If u ∈AB, then M has to decide, during\\nthis single scan, where to break u into two substrings. Similarly, if u ̸∈AB,\\nthen M has to decide, during this single scan, that u cannot be broken into\\ntwo substrings such that the ﬁrst substring is in A and the second substring\\nis in B.\\nIt seems to be even more diﬃcult to prove that A∗is a regular language,\\nif A itself is regular. In order to prove this, we need a ﬁnite automaton that,\\nwhen given an arbitrary input string u, decides whether or not u can be\\nbroken into substrings such that each substring is in A. The problem is that,\\nif u ∈A∗, the ﬁnite automaton has to determine into how many substrings,\\nand where, the string u has to be broken; it has to do this during one single\\nscan of the string u.\\nAs we mentioned already, if A and B are regular languages, then both\\nAB and A∗are also regular. In order to prove these claims, we will introduce\\na more general type of ﬁnite automaton.\\nThe ﬁnite automata that we have seen so far are deterministic.\\nThis\\nmeans the following:\\n• If the ﬁnite automaton M is in state r and if it reads the symbol a,\\nthen M switches from state r to the uniquely deﬁned state δ(r, a).\\nFrom now on, we will call such a ﬁnite automaton a deterministic ﬁnite\\nautomaton (DFA). In the next section, we will deﬁne the notion of a nonde-\\nterministic ﬁnite automaton (NFA). For such an automaton, there are zero\\nor more possible states to switch to. At ﬁrst sight, nondeterministic ﬁnite\\n2.4.\\nNondeterministic ﬁnite automata\\n35\\nautomata seem to be more powerful than their deterministic counterparts.\\nWe will prove, however, that DFAs have the same power as NFAs. As we will\\nsee, using this fact, it will be easy to prove that the class of regular languages\\nis closed under the concatenation and star operations.\\n2.4\\nNondeterministic ﬁnite automata\\nWe start by giving three examples of nondeterministic ﬁnite automata. These\\nexamples will show the diﬀerence between this type of automata and the\\ndeterministic versions that we have considered in the previous sections. After\\nthese examples, we will give a formal deﬁnition of a nondeterministic ﬁnite\\nautomaton.\\n2.4.1\\nA ﬁrst example\\nConsider the following state diagram:\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,ε\\n1\\n0,1\\nYou will notice three diﬀerences with the ﬁnite automata that we have\\nseen until now. First, if the automaton is in state q1 and reads the symbol 1,\\nthen it has two options: Either it stays in state q1, or it switches to state q2.\\nSecond, if the automaton is in state q2, then it can switch to state q3 without\\nreading a symbol; this is indicated by the edge having the empty string ϵ as\\nlabel. Third, if the automaton is in state q3 and reads the symbol 0, then it\\ncannot continue.\\nLet us see what this automaton can do when it gets the string 010110 as\\ninput. Initially, the automaton is in the start state q1.\\n• Since the ﬁrst symbol in the input string is 0, the automaton stays in\\nstate q1 after having read this symbol.\\n• The second symbol is 1, and the automaton can either stay in state q1\\nor switch to state q2.\\n36\\nChapter 2.\\nFinite Automata and Regular Languages\\n– If the automaton stays in state q1, then it is still in this state after\\nhaving read the third symbol.\\n– If the automaton switches to state q2, then it again has two op-\\ntions:\\n∗Either read the third symbol in the input string, which is 0,\\nand switch to state q3,\\n∗or switch to state q3, without reading the third symbol.\\nIf we continue in this way, then we see that, for the input string 010110,\\nthere are seven possible computations. All these computations are given in\\nthe ﬁgure below.\\nq1\\nq1\\n0\\n1\\nq1\\nq1\\n0\\n1\\n1\\nq1\\nq2\\n1\\n1\\nq1\\nq2\\nq1\\n0\\n0\\nε\\nq3\\nq3\\nhang\\nhang\\nε\\nq3\\nq4\\n1\\n0\\nq4\\n1\\nq2\\n0\\nε\\nq3\\nq3\\nhang\\n1\\nq4\\n1\\nq4\\nq4\\n0\\nConsider the lowest path in the ﬁgure above:\\n• When reading the ﬁrst symbol, the automaton stays in state q1.\\n• When reading the second symbol, the automaton switches to state q2.\\n• The automaton does not read the third symbol (equivalently, it “reads”\\nthe empty string ϵ), and switches to state q3. At this moment, the\\n2.4.\\nNondeterministic ﬁnite automata\\n37\\nautomaton cannot continue: The third symbol is 0, but there is no\\nedge leaving q3 that is labeled 0, and there is no edge leaving q3 that\\nis labeled ϵ. Therefore, the computation hangs at this point.\\nFrom the ﬁgure, you can see that, out of the seven possible computations,\\nexactly two end in the accept state q4 (after the entire input string 010110 has\\nbeen read). We say that the automaton accepts the string 010110, because\\nthere is at least one computation that ends in the accept state.\\nNow consider the input string 010. In this case, there are three possible\\ncomputations:\\n1. q1\\n0→q1\\n1→q1\\n0→q1\\n2. q1\\n0→q1\\n1→q2\\n0→q3\\n3. q1\\n0→q1\\n1→q2\\nϵ→q3 →hang\\nNone of these computations ends in the accept state (after the entire input\\nstring 010 has been read). Therefore, we say that the automaton rejects the\\ninput string 010.\\nThe state diagram given above is an example of a nondeterministic ﬁnite\\nautomaton (NFA). Informally, an NFA accepts a string, if there exists at least\\none path in the state diagram that (i) starts in the start state, (ii) does not\\nhang before the entire string has been read, and (iii) ends in an accept state.\\nA string for which (i), (ii), and (iii) does not hold is rejected by the NFA.\\nThe NFA given above accepts all binary strings that contain 101 or 11 as\\na substring. All other binary strings are rejected.\\n2.4.2\\nA second example\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right}.\\nThe following state diagram deﬁnes an NFA that accepts all strings that are\\nin A, and rejects all strings that are not in A.\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,1\\n0,1\\n38\\nChapter 2.\\nFinite Automata and Regular Languages\\nThis NFA does the following. If it is in the start state q1 and reads the\\nsymbol 1, then it either stays in state q1 or it “guesses” that this symbol\\nis the third symbol from the right in the input string. In the latter case,\\nthe NFA switches to state q2, and then it “veriﬁes” that there are indeed\\nexactly two remaining symbols in the input string. If there are more than\\ntwo remaining symbols, then the NFA hangs (in state q4) after having read\\nthe next two symbols.\\nObserve how this guessing mechanism is used: The automaton can only\\nread the input string once, from left to right. Hence, it does not know when\\nit reaches the third symbol from the right. When the NFA reads a 1, it can\\nguess that this is the third symbol from the right; after having made this\\nguess, it veriﬁes whether or not the guess was correct.\\nIn Section 2.2.3, we have seen a DFA for the same language A. Observe\\nthat the NFA has a much simpler structure than the DFA.\\n2.4.3\\nA third example\\nConsider the following state diagram, which deﬁnes an NFA whose alphabet\\nis {0}.\\nε\\nε\\n0\\n0\\n0\\n0\\n0\\nThis NFA accepts the language\\nA = {0k : k ≡0 mod 2 or k ≡0 mod 3},\\nwhere 0k is the string consisting of k many 0s. (If k = 0, then 0k = ϵ.)\\nObserve that A is the union of the two languages\\nA1 = {0k : k ≡0 mod 2}\\n2.4.\\nNondeterministic ﬁnite automata\\n39\\nand\\nA2 = {0k : k ≡0 mod 3}.\\nThe NFA basically consists of two DFAs: one of these accepts A1, whereas the\\nother accepts A2. Given an input string w, the NFA has to decide whether\\nor not w ∈A, which is equivalent to deciding whether or not w ∈A1 or\\nw ∈A2. The NFA makes this decision in the following way: At the start, it\\n“guesses” whether (i) it is going to check whether or not w ∈A1 (i.e., the\\nlength of w is even), or (ii) it is going to check whether or not w ∈A2 (i.e.,\\nthe length of w is a multiple of 3). After having made the guess, it veriﬁes\\nwhether or not the guess was correct. If w ∈A, then there exists a way of\\nmaking the correct guess and verifying that w is indeed an element of A (by\\nending in an accept state). If w ̸∈A, then no matter which guess is made,\\nthe NFA will never end in an accept state.\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\nThe previous examples give you an idea what nondeterministic ﬁnite au-\\ntomata are and how they work. In this section, we give a formal deﬁnition\\nof these automata.\\nFor any alphabet Σ, we deﬁne Σϵ to be the set\\nΣϵ = Σ ∪{ϵ}.\\nRecall the notion of a power set: For any set Q, the power set of Q, denoted\\nby P(Q), is the set of all subsets of Q, i.e.,\\nP(Q) = {R : R ⊆Q}.\\nDeﬁnition 2.4.1 A nondeterministic ﬁnite automaton (NFA) is a 5-tuple\\nM = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σϵ →P(Q) is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\n40\\nChapter 2.\\nFinite Automata and Regular Languages\\nAs for DFAs, the transition function δ can be thought of as the “program”\\nof the ﬁnite automaton M = (Q, Σ, δ, q, F):\\n• Let r ∈Q, and let a ∈Σϵ. Then δ(r, a) is a (possibly empty) subset of\\nQ. If the NFA M is in state r, and if it reads a (where a may be the\\nempty string ϵ), then M can switch from state r to any state in δ(r, a).\\nIf δ(r, a) = ∅, then M cannot continue and the computation hangs.\\nThe example given in Section 2.4.1 is an NFA, where Q = {q1, q2, q3, q4},\\nΣ = {0, 1}, the start state is q1, the set of accept states is F = {q4}, and the\\ntransition function δ is given by the following table:\\n0\\n1\\nϵ\\nq1\\n{q1}\\n{q1, q2}\\n∅\\nq2\\n{q3}\\n∅\\n{q3}\\nq3\\n∅\\n{q4}\\n∅\\nq4\\n{q4}\\n{q4}\\n∅\\nDeﬁnition 2.4.2 Let M = (Q, Σ, δ, q, F) be an NFA, and let w ∈Σ∗. We\\nsay that M accepts w, if1\\n• w = ϵ and the start state q is an accept state, or\\n• there exists an integer m ≥1, such that w can be written as w =\\ny1y2 . . . ym, where yi ∈Σϵ for all i with 1 ≤i ≤m, and there exists a\\nsequence r0, r1, . . . , rm of states in Q, such that\\n– r0 = q,\\n– ri+1 ∈δ(ri, yi+1), for i = 0, 1, . . . , m −1, and\\n– rm ∈F.\\nOtherwise, we say that M rejects the string w.\\nThe NFA in the example in Section 2.4.1 accepts the string 01100. This\\ncan be seen by taking\\n• m = 6,\\n1Thanks to Antoine Vigneron for pointing out an error in a previous version of this\\ndeﬁnition.\\n2.5.\\nEquivalence of DFAs and NFAs\\n41\\n• w = 01ϵ100 = y1y2y3y4y5y6, and\\n• r0 = q1, r1 = q1, r2 = q2, r3 = q3, r4 = q4, r5 = q4, and r6 = q4.\\nDeﬁnition 2.4.3 Let M = (Q, Σ, δ, q, F) be an NFA. The language L(M)\\naccepted by M is deﬁned as\\nL(M) = {w ∈Σ∗: M accepts w }.\\n2.5\\nEquivalence of DFAs and NFAs\\nYou may have the impression that nondeterministic ﬁnite automata are more\\npowerful than deterministic ﬁnite automata. In this section, we will show\\nthat this is not the case.\\nThat is, we will prove that a language can be\\naccepted by a DFA if and only if it can be accepted by an NFA. In order\\nto prove this, we will show how to convert an arbitrary NFA to a DFA that\\naccepts the same language.\\nWhat about converting a DFA to an NFA? Well, there is (almost) nothing\\nto do, because a DFA is also an NFA. This is not quite true, because\\n• the transition function of a DFA maps a state and a symbol to a state,\\nwhereas\\n• the transition function of an NFA maps a state and a symbol to a set\\nof zero or more states.\\nThe formal conversion of a DFA to an NFA is done as follows: Let M =\\n(Q, Σ, δ, q, F) be a DFA. Recall that δ is a function δ : Q × Σ →Q. We\\ndeﬁne the function δ′ : Q × Σϵ →P(Q) as follows. For any r ∈Q and for\\nany a ∈Σϵ,\\nδ′(r, a) =\\n\\x1a {δ(r, a)}\\nif a ̸= ϵ,\\n∅\\nif a = ϵ.\\nThen N = (Q, Σ, δ′, q, F) is an NFA, whose behavior is exactly the same as\\nthat of the DFA M; the easiest way to see this is by observing that the state\\ndiagrams of M and N are equal. Therefore, we have L(M) = L(N).\\nIn the rest of this section, we will show how to convert an NFA to a DFA:\\nTheorem 2.5.1 Let N = (Q, Σ, δ, q, F) be a nondeterministic ﬁnite automa-\\nton. There exists a deterministic ﬁnite automaton M, such that L(M) =\\nL(N).\\n42\\nChapter 2.\\nFinite Automata and Regular Languages\\nProof.\\nRecall that the NFA N can (in general) perform more than one\\ncomputation on a given input string. The idea of the proof is to construct a\\nDFA M that runs all these diﬀerent computations simultaneously. (We have\\nseen this idea already in the proof of Theorem 2.3.1.) To be more precise,\\nthe DFA M will have the following property:\\n• the state that M is in after having read an initial part of the input\\nstring corresponds exactly to the set of all states that N can reach\\nafter having read the same part of the input string.\\nWe start by presenting the conversion for the case when N does not\\ncontain ϵ-transitions. In other words, the state diagram of N does not contain\\nany edge that has ϵ as a label. (Later, we will extend the conversion to the\\ngeneral case.) Let the DFA M be deﬁned as M = (Q′, Σ, δ′, q′, F ′), where\\n• the set Q′ of states is equal to Q′ = P(Q); observe that |Q′| = 2|Q|,\\n• the start state q′ is equal to q′ = {q}; so M has the “same” start state\\nas N,\\n• the set F ′ of accept states is equal to the set of all elements R of Q′\\nhaving the property that R contains at least one accept state of N, i.e.,\\nF ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• the transition function δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each\\nR ∈Q′ and for each a ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nδ(r, a).\\nLet us see what the transition function δ′ of M does. First observe that,\\nsince N is an NFA, δ(r, a) is a subset of Q. This implies that δ′(R, a) is the\\nunion of subsets of Q and, therefore, also a subset of Q. Hence, δ′(R, a) is\\nan element of Q′.\\nThe set δ(r, a) is equal to the set of all states of the NFA N that can be\\nreached from state r by reading the symbol a. We take the union of these\\nsets δ(r, a), where r ranges over all elements of R, to obtain the new set\\nδ′(R, a). This new set is the state that the DFA M reaches from state R, by\\nreading the symbol a.\\n2.5.\\nEquivalence of DFAs and NFAs\\n43\\nIn this way, we obtain the correspondence that was given in the beginning\\nof this proof.\\nAfter this warming-up, we can consider the general case. In other words,\\nfrom now on, we allow ϵ-transitions in the NFA N. The DFA M is deﬁned as\\nabove, except that the start state q′ and the transition function δ′ have to be\\nmodiﬁed. Recall that a computation of the NFA N consists of the following:\\n1. Start in the start state q and make zero or more ϵ-transitions.\\n2. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n3. Make zero or more ϵ-transitions.\\n4. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n5. Make zero or more ϵ-transitions.\\n6. Etc.\\nThe DFA M will simulate this computation in the following way:\\n• Simulate 1. in one single step. As we will see below, this simulation is\\nimplicitly encoded in the deﬁnition of the start state q′ of M.\\n• Simulate 2. and 3. in one single step.\\n• Simulate 4. and 5. in one single step.\\n• Etc.\\nThus, in one step, the DFA M simulates the reading of one “real” symbol of\\nΣ, followed by making zero or more ϵ-transitions.\\nTo formalize this, we need the notion of ϵ-closure. For any state r of the\\nNFA N, the ϵ-closure of r, denoted by Cϵ(r), is deﬁned to be the set of all\\nstates of N that can be reached from r, by making zero or more ϵ-transitions.\\nFor any state R of the DFA M (hence, R ⊆Q), we deﬁne\\nCϵ(R) =\\n[\\nr∈R\\nCϵ(r).\\n44\\nChapter 2.\\nFinite Automata and Regular Languages\\nHow do we deﬁne the start state q′ of the DFA M? Before the NFA N\\nreads its ﬁrst “real” symbol of Σ, it makes zero or more ϵ-transitions. In\\nother words, at the moment when N reads the ﬁrst symbol of Σ, it can be\\nin any state of Cϵ(q). Therefore, we deﬁne q′ to be\\nq′ = Cϵ(q) = Cϵ({q}).\\nHow do we deﬁne the transition function δ′ of the DFA M? Assume that\\nM is in state R, and reads the symbol a. At this moment, the NFA N would\\nhave been in any state r of R. By reading the symbol a, N can switch to\\nany state in δ(r, a), and then make zero or more ϵ-transitions. Hence, the\\nNFA can switch to any state in the set Cϵ(δ(r, a)). Based on this, we deﬁne\\nδ′(R, a) to be\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nTo summarize, the NFA N = (Q, Σ, δ, q, F) is converted to the DFA\\nM = (Q′, Σ, δ′, q′, F ′), where\\n• Q′ = P(Q),\\n• q′ = Cϵ({q}),\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nThe results proved until now can be summarized in the following theorem.\\nTheorem 2.5.2 Let A be a language. Then A is regular if and only if there\\nexists a nondeterministic ﬁnite automaton that accepts A.\\n2.5.1\\nAn example\\nConsider the NFA N = (Q, Σ, δ, q, F), where Q = {1, 2, 3}, Σ = {a, b}, q = 1,\\nF = {2}, and δ is given by the following table:\\n2.5.\\nEquivalence of DFAs and NFAs\\n45\\na\\nb\\nϵ\\n1\\n{3}\\n∅\\n{2}\\n2\\n{1}\\n∅\\n∅\\n3\\n{2}\\n{2, 3}\\n∅\\nThe state diagram of N is as follows:\\n1\\n2\\n3\\na\\na\\nǫ\\nb\\na, b\\nWe will show how to convert this NFA N to a DFA M that accepts the\\nsame language. Following the proof of Theorem 2.5.1, the DFA M is speciﬁed\\nby M = (Q′, Σ, δ′, q′, F ′), where each of the components is deﬁned below.\\n• Q′ = P(Q). Hence,\\nQ′ = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}.\\n• q′ = Cϵ({q}). Hence, the start state q′ of M is the set of all states of\\nN that can be reached from N’s start state q = 1, by making zero or\\nmore ϵ-transitions. We obtain\\nq′ = Cϵ({q}) = Cϵ({1}) = {1, 2}.\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅}. Hence, the accept states of M are those\\nstates that contain the accept state 2 of N. We obtain\\nF ′ = {{2}, {1, 2}, {2, 3}, {1, 2, 3}}.\\n46\\nChapter 2.\\nFinite Automata and Regular Languages\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nIn this example δ′ is given by\\nδ′(∅, a) = ∅\\nδ′(∅, b) = ∅\\nδ′({1}, a) = {3}\\nδ′({1}, b) = ∅\\nδ′({2}, a) = {1, 2}\\nδ′({2}, b) = ∅\\nδ′({3}, a) = {2}\\nδ′({3}, b) = {2, 3}\\nδ′({1, 2}, a) = {1, 2, 3}\\nδ′({1, 2}, b) = ∅\\nδ′({1, 3}, a) = {2, 3}\\nδ′({1, 3}, b) = {2, 3}\\nδ′({2, 3}, a) = {1, 2}\\nδ′({2, 3}, b) = {2, 3}\\nδ′({1, 2, 3}, a) = {1, 2, 3}\\nδ′({1, 2, 3}, b) = {2, 3}\\nThe state diagram of the DFA M is as follows:\\n2.5.\\nEquivalence of DFAs and NFAs\\n47\\n/0\\n{1}\\n{2}\\n{3}\\n{1,2}\\n{2,3}\\n{1,3}\\n{1,2,3}\\na,b\\nb\\na\\nb\\na\\na\\nb\\na,b\\na\\nb\\nb\\na\\nb\\na\\nWe make the following observations:\\n• The states {1} and {1, 3} do not have incoming edges. Therefore, these\\ntwo states cannot be reached from the start state {1, 2}.\\n• The state {3} has only one incoming edge; it comes from the state\\n{1}. Since {1} cannot be reached from the start state, {3} cannot be\\nreached from the start state.\\n• The state {2} has only one incoming edge; it comes from the state\\n{3}. Since {3} cannot be reached from the start state, {2} cannot be\\nreached from the start state.\\nHence, we can remove the four states {1}, {2}, {3}, and {1, 3}. The\\nresulting DFA accepts the same language as the DFA above.\\nThis leads\\nto the following state diagram, which depicts a DFA that accepts the same\\nlanguage as the NFA N:\\n48\\nChapter 2.\\nFinite Automata and Regular Languages\\n/0\\n{1,2}\\n{2,3}\\n{1,2,3}\\na,b\\na\\nb\\nb\\na\\nb\\na\\n2.6\\nClosure under the regular operations\\nIn Section 2.3, we have deﬁned the regular operations union, concatenation,\\nand star. We proved in Theorem 2.3.1 that the union of two regular lan-\\nguages is a regular language. We also explained why it is not clear that the\\nconcatenation of two regular languages is regular, and that the star of a reg-\\nular language is regular. In this section, we will see that the concept of NFA,\\ntogether with Theorem 2.5.2, can be used to give a simple proof of the fact\\nthat the regular languages are indeed closed under the regular operations.\\nWe start by giving an alternative proof of Theorem 2.3.1:\\nTheorem 2.6.1 The set of regular languages is closed under the union op-\\neration, i.e., if A1 and A2 are regular languages over the same alphabet Σ,\\nthen A1 ∪A2 is also a regular language.\\n2.6.\\nClosure under the regular operations\\n49\\nq1\\nM1\\nM2\\nq2\\nq0\\nq1\\nq2\\nε\\nε\\nM\\nFigure 2.1: The NFA M accepts L(M1) ∪L(M2).\\nProof.\\nSince A1 is regular, there is, by Theorem 2.5.2, an NFA M1 =\\n(Q1, Σ, δ1, q1, F1), such that A1 = L(M1). Similarly, there is an NFA M2 =\\n(Q2, Σ, δ2, q2, F2), such that A2 = L(M2). We may assume that Q1 ∩Q2 = ∅,\\nbecause otherwise, we can give new “names” to the states of Q1 and Q2.\\nFrom these two NFAs, we will construct an NFA M = (Q, Σ, δ, q0, F), such\\nthat L(M) = A1 ∪A2. The construction is illustrated in Figure 2.1. The\\nNFA M is deﬁned as follows:\\n1. Q = {q0} ∪Q1 ∪Q2, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = F1 ∪F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\n50\\nChapter 2.\\nFinite Automata and Regular Languages\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1,\\nδ2(r, a)\\nif r ∈Q2,\\n{q1, q2}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nTheorem 2.6.2 The set of regular languages is closed under the concatena-\\ntion operation, i.e., if A1 and A2 are regular languages over the same alphabet\\nΣ, then A1A2 is also a regular language.\\nProof.\\nLet M1 = (Q1, Σ, δ1, q1, F1) be an NFA, such that A1 = L(M1).\\nSimilarly, let M2 = (Q2, Σ, δ2, q2, F2) be an NFA, such that A2 = L(M2).\\nAs in the proof of Theorem 2.6.1, we may assume that Q1 ∩Q2 = ∅. We\\nwill construct an NFA M = (Q, Σ, δ, q0, F), such that L(M) = A1A2. The\\nconstruction is illustrated in Figure 2.2. The NFA M is deﬁned as follows:\\n1. Q = Q1 ∪Q2.\\n2. q0 = q1.\\n3. F = F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q2}\\nif r ∈F1 and a = ϵ,\\nδ2(r, a)\\nif r ∈Q2.\\nTheorem 2.6.3 The set of regular languages is closed under the star oper-\\nation, i.e., if A is a regular language, then A∗is also a regular language.\\n2.6.\\nClosure under the regular operations\\n51\\nq1\\nM1\\nM2\\nq2\\nq2\\nε\\nε\\nε\\nq0\\nM\\nFigure 2.2: The NFA M accepts L(M1)L(M2).\\nq1\\nN\\nq1\\nq0\\nε\\nε\\nε\\nε\\nM\\nFigure 2.3: The NFA M accepts (L(N))∗.\\nProof. Let Σ be the alphabet of A and let N = (Q1, Σ, δ1, q1, F1) be an\\nNFA, such that A = L(N). We will construct an NFA M = (Q, Σ, δ, q0, F),\\nsuch that L(M) = A∗. The construction is illustrated in Figure 2.3. The\\nNFA M is deﬁned as follows:\\n52\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. Q = {q0} ∪Q1, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = {q0} ∪F1. (Since ϵ ∈A∗, q0 has to be an accept state.)\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ,\\n{q1}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nIn the ﬁnal theorem of this section, we mention (without proof) two more\\nclosure properties of the regular languages:\\nTheorem 2.6.4 The set of regular languages is closed under the complement\\nand intersection operations:\\n1. If A is a regular language over the alphabet Σ, then the complement\\nA = {w ∈Σ∗: w ̸∈A}\\nis also a regular language.\\n2. If A1 and A2 are regular languages over the same alphabet Σ, then the\\nintersection\\nA1 ∩A2 = {w ∈Σ∗: w ∈A1 and w ∈A2}\\nis also a regular language.\\n2.7\\nRegular expressions\\nIn this section, we present regular expressions, which are a means to describe\\nlanguages. As we will see, the class of languages that can be described by\\nregular expressions coincides with the class of regular languages.\\n2.7.\\nRegular expressions\\n53\\nBefore formally deﬁning the notion of a regular expression, we give some\\nexamples. Consider the expression\\n(0 ∪1)01∗.\\nThe language described by this expression is the set of all binary strings\\n1. that start with either 0 or 1 (this is indicated by (0 ∪1)),\\n2. for which the second symbol is 0 (this is indicated by 0), and\\n3. that end with zero or more 1s (this is indicated by 1∗).\\nThat is, the language described by this expression is\\n{00, 001, 0011, 00111, . . . , 10, 101, 1011, 10111, . . .}.\\nHere are some more examples (in all cases, the alphabet is {0, 1}):\\n• The language {w : w contains exactly two 0s} is described by the ex-\\npression\\n1∗01∗01∗.\\n• The language {w : w contains at least two 0s} is described by the ex-\\npression\\n(0 ∪1)∗0(0 ∪1)∗0(0 ∪1)∗.\\n• The language {w : 1011 is a substring of w} is described by the ex-\\npression\\n(0 ∪1)∗1011(0 ∪1)∗.\\n• The language {w : the length of w is even} is described by the expres-\\nsion\\n((0 ∪1)(0 ∪1))∗.\\n• The language {w : the length of w is odd} is described by the expres-\\nsion\\n(0 ∪1) ((0 ∪1)(0 ∪1))∗.\\n• The language {1011, 0} is described by the expression\\n1011 ∪0.\\n54\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The language {w :\\nthe ﬁrst and last symbols of w are equal} is de-\\nscribed by the expression\\n0(0 ∪1)∗0 ∪1(0 ∪1)∗1 ∪0 ∪1.\\nAfter these examples, we give a formal (and inductive) deﬁnition of regular\\nexpressions:\\nDeﬁnition 2.7.1 Let Σ be a non-empty alphabet.\\n1. ϵ is a regular expression.\\n2. ∅is a regular expression.\\n3. For each a ∈Σ, a is a regular expression.\\n4. If R1 and R2 are regular expressions, then R1 ∪R2 is a regular expres-\\nsion.\\n5. If R1 and R2 are regular expressions, then R1R2 is a regular expression.\\n6. If R is a regular expression, then R∗is a regular expression.\\nYou can regard 1., 2., and 3. as being the “building blocks” of regular\\nexpressions.\\nItems 4., 5., and 6. give rules that can be used to combine\\nregular expressions into new (and “larger”) regular expressions. To give an\\nexample, we claim that\\n(0 ∪1)∗101(0 ∪1)∗\\nis a regular expression (where the alphabet Σ is equal to {0, 1}). In order\\nto prove this, we have to show that this expression can be “built” using the\\n“rules” given in Deﬁnition 2.7.1. Here we go:\\n• By 3., 0 is a regular expression.\\n• By 3., 1 is a regular expression.\\n• Since 0 and 1 are regular expressions, by 4., 0∪1 is a regular expression.\\n• Since 0∪1 is a regular expression, by 6., (0∪1)∗is a regular expression.\\n• Since 1 and 0 are regular expressions, by 5., 10 is a regular expression.\\n2.7.\\nRegular expressions\\n55\\n• Since 10 and 1 are regular expressions, by 5., 101 is a regular expression.\\n• Since (0 ∪1)∗and 101 are regular expressions, by 5., (0 ∪1)∗101 is a\\nregular expression.\\n• Since (0 ∪1)∗101 and (0 ∪1)∗are regular expressions, by 5., (0 ∪\\n1)∗101(0 ∪1)∗is a regular expression.\\nNext we deﬁne the language that is described by a regular expression:\\nDeﬁnition 2.7.2 Let Σ be a non-empty alphabet.\\n1. The regular expression ϵ describes the language {ϵ}.\\n2. The regular expression ∅describes the language ∅.\\n3. For each a ∈Σ, the regular expression a describes the language {a}.\\n4. Let R1 and R2 be regular expressions and let L1 and L2 be the lan-\\nguages described by them, respectively. The regular expression R1∪R2\\ndescribes the language L1 ∪L2.\\n5. Let R1 and R2 be regular expressions and let L1 and L2 be the languages\\ndescribed by them, respectively. The regular expression R1R2 describes\\nthe language L1L2.\\n6. Let R be a regular expression and let L be the language described by\\nit. The regular expression R∗describes the language L∗.\\nWe consider some examples:\\n• The regular expression (0∪ϵ)(1∪ϵ) describes the language {01, 0, 1, ϵ}.\\n• The regular expression 0 ∪ϵ describes the language {0, ϵ}, whereas the\\nregular expression 1∗describes the language {ϵ, 1, 11, 111, . . .}. There-\\nfore, the regular expression (0 ∪ϵ)1∗describes the language\\n{0, 01, 011, 0111, . . . , ϵ, 1, 11, 111, . . .}.\\nObserve that this language is also described by the regular expression\\n01∗∪1∗.\\n56\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The regular expression 1∗∅describes the empty language, i.e., the lan-\\nguage ∅. (You should convince yourself that this is correct.)\\n• The regular expression ∅∗describes the language {ϵ}.\\nDeﬁnition 2.7.3 Let R1 and R2 be regular expressions and let L1 and L2\\nbe the languages described by them, respectively. If L1 = L2 (i.e., R1 and\\nR2 describe the same language), then we will write R1 = R2.\\nHence, even though (0∪ϵ)1∗and 01∗∪1∗are diﬀerent regular expressions,\\nwe write\\n(0 ∪ϵ)1∗= 01∗∪1∗,\\nbecause they describe the same language.\\nIn Section 2.8.2, we will show that every regular language can be described\\nby a regular expression. The proof of this fact is purely algebraic and uses\\nthe following algebraic identities involving regular expressions.\\nTheorem 2.7.4 Let R1, R2, and R3 be regular expressions. The following\\nidentities hold:\\n1. R1∅= ∅R1 = ∅.\\n2. R1ϵ = ϵR1 = R1.\\n3. R1 ∪∅= ∅∪R1 = R1.\\n4. R1 ∪R1 = R1.\\n5. R1 ∪R2 = R2 ∪R1.\\n6. R1(R2 ∪R3) = R1R2 ∪R1R3.\\n7. (R1 ∪R2)R3 = R1R3 ∪R2R3.\\n8. R1(R2R3) = (R1R2)R3.\\n9. ∅∗= ϵ.\\n10. ϵ∗= ϵ.\\n11. (ϵ ∪R1)∗= R∗\\n1.\\n2.8.\\nEquivalence of regular expressions and regular languages 57\\n12. (ϵ ∪R1)(ϵ ∪R1)∗= R∗\\n1.\\n13. R∗\\n1(ϵ ∪R1) = (ϵ ∪R1)R∗\\n1 = R∗\\n1.\\n14. R∗\\n1R2 ∪R2 = R∗\\n1R2.\\n15. R1(R2R1)∗= (R1R2)∗R1.\\n16. (R1 ∪R2)∗= (R∗\\n1R2)∗R∗\\n1 = (R∗\\n2R1)∗R∗\\n2.\\nWe will not present the (boring) proofs of these identities, but urge you\\nto convince yourself informally that they make perfect sense. To give an\\nexample, we mentioned above that\\n(0 ∪ϵ)1∗= 01∗∪1∗.\\nWe can verify this identity in the following way:\\n(0 ∪ϵ)1∗\\n=\\n01∗∪ϵ1∗\\n(by identity 7)\\n=\\n01∗∪1∗\\n(by identity 2)\\n2.8\\nEquivalence of regular expressions and reg-\\nular languages\\nIn the beginning of Section 2.7, we mentioned the following result:\\nTheorem 2.8.1 Let L be a language. Then L is regular if and only if there\\nexists a regular expression that describes L.\\nThe proof of this theorem consists of two parts:\\n• In Section 2.8.1, we will prove that every regular expression describes\\na regular language.\\n• In Section 2.8.2, we will prove that every DFA M can be converted to\\na regular expression that describes the language L(M).\\nThese two results will prove Theorem 2.8.1.\\n58\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.8.1\\nEvery regular expression describes a regular lan-\\nguage\\nLet R be an arbitrary regular expression over the alphabet Σ. We will prove\\nthat the language described by R is a regular language. The proof is by\\ninduction on the structure of R (i.e., by induction on the way R is “built”\\nusing the “rules” given in Deﬁnition 2.7.1).\\nThe ﬁrst base case: Assume that R = ϵ.\\nThen R describes the lan-\\nguage {ϵ}. In order to prove that this language is regular, it suﬃces, by\\nTheorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q, F) that accepts this\\nlanguage. This NFA is obtained by deﬁning Q = {q}, q is the start state,\\nF = {q}, and δ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state\\ndiagram of M:\\nq\\nThe second base case: Assume that R = ∅. Then R describes the language\\n∅. In order to prove that this language is regular, it suﬃces, by Theorem 2.5.2,\\nto construct an NFA M = (Q, Σ, δ, q, F) that accepts this language. This\\nNFA is obtained by deﬁning Q = {q}, q is the start state, F = ∅, and\\nδ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state diagram of M:\\nq\\nThe third base case: Let a ∈Σ and assume that R = a. Then R describes\\nthe language {a}. In order to prove that this language is regular, it suﬃces,\\nby Theorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q1, F) that accepts\\nthis language. This NFA is obtained by deﬁning Q = {q1, q2}, q1 is the start\\nstate, F = {q2}, and\\nδ(q1, a)\\n=\\n{q2},\\nδ(q1, b)\\n=\\n∅for all b ∈Σϵ \\\\ {a},\\nδ(q2, b)\\n=\\n∅for all b ∈Σϵ.\\nThe ﬁgure below gives the state diagram of M:\\n2.8.\\nEquivalence of regular expressions and regular languages 59\\nq1\\nq2\\na\\nThe ﬁrst case of the induction step: Assume that R = R1 ∪R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1 ∪L2, which, by Theorem 2.6.1, is regular.\\nThe second case of the induction step: Assume that R = R1R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1L2, which, by Theorem 2.6.2, is regular.\\nThe third case of the induction step: Assume that R = (R1)∗, where\\nR1 is a regular expression.\\nLet L1 be the language described by R1 and\\nassume that L1 is regular. Then R describes the language (L1)∗, which, by\\nTheorem 2.6.3, is regular.\\nThis concludes the proof of the claim that every regular expression de-\\nscribes a regular language.\\nTo give an example, consider the regular expression\\n(ab ∪a)∗,\\nwhere the alphabet is {a, b}. We will prove that this regular expression de-\\nscribes a regular language, by constructing an NFA that accepts the language\\ndescribed by this regular expression. Observe how the regular expression is\\n“built”:\\n• Take the regular expressions a and b, and combine them into the regular\\nexpression ab.\\n• Take the regular expressions ab and a, and combine them into the\\nregular expression ab ∪a.\\n• Take the regular expression ab ∪a, and transform it into the regular\\nexpression (ab ∪a)∗.\\nFirst, we construct an NFA M1 that accepts the language described by\\nthe regular expression a:\\n60\\nChapter 2.\\nFinite Automata and Regular Languages\\na\\nM1\\nNext, we construct an NFA M2 that accepts the language described by\\nthe regular expression b:\\nM2\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.2 to\\nM1 and M2. This gives an NFA M3 that accepts the language described by\\nthe regular expression ab:\\nM3\\na\\nε\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.1 to\\nM3 and M1. This gives an NFA M4 that accepts the language described by\\nthe regular expression ab ∪a:\\na\\nε\\nb\\na\\nε\\nε\\nM4\\nFinally, we apply the construction given in the proof of Theorem 2.6.3\\nto M4. This gives an NFA M5 that accepts the language described by the\\nregular expression (ab ∪a)∗:\\n2.8.\\nEquivalence of regular expressions and regular languages 61\\na\\nε\\nb\\na\\nε\\nε\\nε\\nε\\nε\\nM5\\n2.8.2\\nConverting a DFA to a regular expression\\nIn this section, we will prove that every DFA M can be converted to a regular\\nexpression that describes the language L(M). In order to prove this result,\\nwe need to solve recurrence relations involving languages.\\nSolving recurrence relations\\nLet Σ be an alphabet, let B and C be “known” languages in Σ∗such that\\nϵ ̸∈B, and let L be an “unknown” language such that\\nL = BL ∪C.\\nCan we “solve” this equation for L? That is, can we express L in terms of\\nB and C?\\nConsider an arbitrary string u in L. We are going to determine how u\\nlooks like. Since u ∈L and L = BL ∪C, we know that u is a string in\\nBL ∪C. Hence, there are two possibilities for u.\\n1. u is an element of C.\\n2. u is an element of BL. In this case, there are strings b ∈B and v ∈L\\nsuch that u = bv. Since ϵ ̸∈B, we have b ̸= ϵ and, therefore, |v| < |u|.\\n(Recall that |v| denotes the length, i.e., the number of symbols, of the\\nstring v.) Since v is a string in L, which is equal to BL ∪C, v is a\\nstring in BL ∪C. Hence, there are two possibilities for v.\\n62\\nChapter 2.\\nFinite Automata and Regular Languages\\n(a) v is an element of C. In this case,\\nu = bv, where b ∈B and v ∈C; thus, u ∈BC.\\n(b) v is an element of BL. In this case, there are strings b′ ∈B and\\nw ∈L such that v = b′w. Since ϵ ̸∈B, we have b′ ̸= ϵ and,\\ntherefore, |w| < |v|. Since w is a string in L, which is equal to\\nBL∪C, w is a string in BL∪C. Hence, there are two possibilities\\nfor w.\\ni. w is an element of C. In this case,\\nu = bb′w, where b, b′ ∈B and w ∈C; thus, u ∈BBC.\\nii. w is an element of BL. In this case, there are strings b′′ ∈B\\nand x ∈L such that w = b′′x. Since ϵ ̸∈B, we have b′′ ̸= ϵ\\nand, therefore, |x| < |w|. Since x is a string in L, which is\\nequal to BL ∪C, x is a string in BL ∪C. Hence, there are\\ntwo possibilities for x.\\nA. x is an element of C. In this case,\\nu = bb′b′′x, where b, b′, b′′ ∈B and x ∈C; thus, u ∈BBBC.\\nB. x is an element of BL. Etc., etc.\\nThis process hopefully convinces you that any string u in L can be written\\nas the concatenation of zero or more strings in B, followed by one string in\\nC. In fact, L consists of exactly those strings having this property:\\nLemma 2.8.2 Let Σ be an alphabet, and let B, C, and L be languages in\\nΣ∗such that ϵ ̸∈B and\\nL = BL ∪C.\\nThen\\nL = B∗C.\\nProof. First, we show that B∗C ⊆L. Let u be an arbitrary string in B∗C.\\nThen u is the concatenation of k strings of B, for some k ≥0, followed by\\none string of C. We proceed by induction on k.\\nThe base case is when k = 0. In this case, u is a string in C. Hence, u is\\na string in BL ∪C. Since BL ∪C = L, it follows that u is a string in L.\\n2.8.\\nEquivalence of regular expressions and regular languages 63\\nNow let k ≥1. Then we can write u = vwc, where v is a string in B,\\nw is the concatenation of k −1 strings of B, and c is a string of C. Deﬁne\\ny = wc. Observe that y is the concatenation of k −1 strings of B followed\\nby one string of C. Therefore, by induction, the string y is an element of L.\\nHence, u = vy, where v is a string in B and y is a string in L. This shows\\nthat u is a string in BL. Hence, u is a string in BL ∪C. Since BL ∪C = L,\\nit follows that u is a string in L. This completes the proof that B∗C ⊆L.\\nIt remains to show that L ⊆B∗C. Let u be an arbitrary string in L,\\nand let ℓbe its length (i.e., ℓis the number of symbols in u). We prove by\\ninduction on ℓthat u is a string in B∗C.\\nThe base case is when ℓ= 0. Then u = ϵ. Since u ∈L and L = BL ∪C,\\nu is a string in BL ∪C. Since ϵ ̸∈B, u cannot be a string in BL. Hence, u\\nmust be a string in C. Since C ⊆B∗C, it follows that u is a string in B∗C.\\nLet ℓ≥1. If u is a string in C, then u is a string in B∗C and we are done.\\nSo assume that u is not a string in C. Since u ∈L and L = BL ∪C, u is a\\nstring in BL. Hence, there are strings b ∈B and v ∈L such that u = bv.\\nSince ϵ ̸∈B, the length of b is at least one; hence, the length of v is less than\\nthe length of u. By induction, v is a string in B∗C. Hence, u = bv, where\\nb ∈B and v ∈B∗C. This shows that u ∈B(B∗C). Since B(B∗C) ⊆B∗C,\\nit follows that u ∈B∗C.\\nNote that Lemma 2.8.2 holds for any language B that does not contain\\nthe empty string ϵ. As an example, assume that B = ∅. Then the language\\nL satisﬁes the equation\\nL = BL ∪C = ∅L ∪C.\\nUsing Theorem 2.7.4, this equation becomes\\nL = ∅∪C = C.\\nWe now show that Lemma 2.8.2 also implies that L = C: Since ϵ ̸∈B,\\nLemma 2.8.2 implies that L = B∗C, which, using Theorem 2.7.4, becomes\\nL = B∗C = ∅∗C = ϵC = C.\\nThe conversion\\nWe will now use Lemma 2.8.2 to prove that every DFA can be converted to\\na regular expression.\\n64\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet M = (Q, Σ, δ, q, F) be an arbitrary deterministic ﬁnite automaton.\\nWe will show that there exists a regular expression that describes the lan-\\nguage L(M).\\nFor each state r ∈Q, we deﬁne\\nLr = {w ∈Σ∗:\\nthe path in the state diagram of M that starts\\nin state r and that corresponds to w ends in a\\nstate of F }.\\nIn words, Lr is the language accepted by M, if r were the start state.\\nWe will show that each such language Lr can be described by a regular\\nexpression. Since L(M) = Lq, this will prove that L(M) can be described by\\na regular expression.\\nThe basic idea is to set up equations for the languages Lr, which we then\\nsolve using Lemma 2.8.2. We claim that\\nLr =\\n[\\na∈Σ\\na · Lδ(r,a)\\nif r ̸∈F.\\n(2.2)\\nWhy is this true? Let w be a string in Lr. Then the path P in the state\\ndiagram of M that starts in state r and that corresponds to w ends in a\\nstate of F. Since r ̸∈F, this path contains at least one edge. Let r′ be the\\nstate that follows the ﬁrst state (i.e., r) of P. Then r′ = δ(r, b) for some\\nsymbol b ∈Σ. Hence, b is the ﬁrst symbol of w. Write w = bv, where v is\\nthe remaining part of w. Then the path P ′ = P \\\\ {r} in the state diagram\\nof M that starts in state r′ and that corresponds to v ends in a state of F.\\nTherefore, v ∈Lr′ = Lδ(r,b). Hence,\\nw ∈b · Lδ(r,b) ⊆\\n[\\na∈Σ\\na · Lδ(r,a).\\nConversely, let w be a string in S\\na∈Σ a · Lδ(r,a). Then there is a symbol b ∈Σ\\nand a string v ∈Lδ(r,b) such that w = bv. Let P ′ be the path in the state\\ndiagram of M that starts in state δ(r, b) and that corresponds to v. Since\\nv ∈Lδ(r,b), this path ends in a state of F. Let P be the path in the state\\ndiagram of M that starts in r, follows the edge to δ(r, b), and then follows P ′.\\nThis path P corresponds to w and ends in a state of F. Therefore, w ∈Lr.\\nThis proves the correctness of (2.2).\\n2.8.\\nEquivalence of regular expressions and regular languages 65\\nSimilarly, we can prove that\\nLr = ϵ ∪\\n [\\na∈Σ\\na · Lδ(r,a)\\n!\\nif r ∈F.\\n(2.3)\\nSo we now have a set of equations in the “unknowns” Lr, for r ∈Q. The\\nnumber of equations is equal to the size of Q. In other words, the number\\nof equations is equal to the number of unknowns. The regular expression for\\nL(M) = Lq is obtained by solving these equations using Lemma 2.8.2.\\nOf course, we have to convince ourselves that these equations have a so-\\nlution for any given DFA. Before we deal with this issue, we give an example.\\nAn example\\nConsider the deterministic ﬁnite automaton M = (Q, Σ, δ, q0, F), where Q =\\n{q0, q1, q2}, Σ = {a, b}, q0 is the start state, F = {q2}, and δ is given in the\\nstate diagram below. We show how to obtain the regular expression that\\ndescribes the language accepted by M.\\nq0\\nq1\\nq2\\na\\na\\na\\nb\\nb\\nb\\nFor this case, (2.2) and (2.3) give the following equations:\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nLq0\\n=\\na · Lq0 ∪b · Lq2\\nLq1\\n=\\na · Lq0 ∪b · Lq1\\nLq2\\n=\\nϵ ∪a · Lq1 ∪b · Lq0\\n66\\nChapter 2.\\nFinite Automata and Regular Languages\\nIn the third equation, Lq2 is expressed in terms of Lq0 and Lq1. Hence, if we\\nsubstitute the third equation into the ﬁrst one, and use Theorem 2.7.4, then\\nwe get\\nLq0\\n=\\na · Lq0 ∪b · (ϵ ∪a · Lq1 ∪b · Lq0)\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b.\\nWe obtain the following set of equations.\\n\\x1a Lq0\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b\\nLq1\\n=\\nb · Lq1 ∪a · Lq0\\nLet L = Lq1, B = b, and C = a · Lq0. Then ϵ ̸∈B and the second equation\\nreads L = BL ∪C. Hence, by Lemma 2.8.2,\\nLq1 = L = B∗C = b∗a · Lq0.\\nIf we substitute Lq1 into the ﬁrst equation, then we get (again using Theo-\\nrem 2.7.4)\\nLq0\\n=\\n(a ∪bb) · Lq0 ∪ba · b∗a · Lq0 ∪b\\n=\\n(a ∪bb ∪bab∗a)Lq0 ∪b.\\nAgain applying Lemma 2.8.2, this time with L = Lq0, B = a∪bb∪bab∗a and\\nC = b, gives\\nLq0 = (a ∪bb ∪bab∗a)∗b.\\nThus, the regular expression that describes the language accepted by M is\\n(a ∪bb ∪bab∗a)∗b.\\nCompleting the correctness of the conversion\\nIt remains to prove that, for any DFA, the system of equations (2.2) and (2.3)\\ncan be solved. This will follow from the following (more general) lemma.\\n(You should verify that the equations (2.2) and (2.3) are in the form as\\nspeciﬁed in this lemma.)\\n2.8.\\nEquivalence of regular expressions and regular languages 67\\nLemma 2.8.3 Let n ≥1 be an integer and, for 1 ≤i ≤n and 1 ≤j ≤n,\\nlet Bij and Ci be regular expressions such that ϵ ̸∈Bij. Let L1, L2, . . . , Ln be\\nlanguages that satisfy\\nLi =\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci for 1 ≤i ≤n.\\nThen L1 can be expressed as a regular expression only involving the regular\\nexpressions Bij and Ci.\\nProof. The proof is by induction on n. The base case is when n = 1. In\\nthis case, we have\\nL1 = B11L1 ∪C1.\\nSince ϵ ̸∈B11, it follows from Lemma 2.8.2 that L1 = B∗\\n11C1. This proves\\nthe base case.\\nLet n ≥2 and assume the lemma is true for n −1. We have\\nLn\\n=\\n n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n=\\nBnnLn ∪\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn.\\nSince ϵ ̸∈Bnn, it follows from Lemma 2.8.2 that\\nLn\\n=\\nB∗\\nnn\\n  n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n!\\n=\\nB∗\\nnn\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪B∗\\nnnCn\\n=\\n n−1\\n[\\nj=1\\nB∗\\nnnBnjLj\\n!\\n∪B∗\\nnnCn\\nBy substituting this equation for Ln into the equations for Li, 1 ≤i ≤n −1,\\n68\\nChapter 2.\\nFinite Automata and Regular Languages\\nwe obtain\\nLi\\n=\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\nBinLn ∪\\n n−1\\n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\n n−1\\n[\\nj=1\\n(BinB∗\\nnnBnj ∪Bij) Lj\\n!\\n∪BinB∗\\nnnCn ∪Ci.\\nThus, we have obtained n −1 equations in L1, L2, . . . , Ln−1.\\nSince ϵ ̸∈\\nBinB∗\\nnnBnj ∪Bij, it follows from the induction hypothesis that L1 can be\\nexpressed as a regular expression only involving the regular expressions Bij\\nand Ci.\\n2.9\\nThe pumping lemma and nonregular lan-\\nguages\\nIn the previous sections, we have seen that the class of regular languages is\\nclosed under various operations, and that these languages can be described by\\n(deterministic or nondeterministic) ﬁnite automata and regular expressions.\\nThese properties helped in developing techniques for showing that a language\\nis regular. In this section, we will present a tool that can be used to prove\\nthat certain languages are not regular. Observe that for a regular language,\\n1. the amount of memory that is needed to determine whether or not a\\ngiven string is in the language is ﬁnite and independent of the length\\nof the string, and\\n2. if the language consists of an inﬁnite number of strings, then this lan-\\nguage should contain inﬁnite subsets having a fairly repetitive struc-\\nture.\\nIntuitively, languages that do not follow 1. or 2. should be nonregular. For\\nexample, consider the language\\n{0n1n : n ≥0}.\\n2.9.\\nThe pumping lemma and nonregular languages\\n69\\nThis language should be nonregular, because it seems unlikely that a DFA can\\nremember how many 0s it has seen when it has reached the border between\\nthe 0s and the 1s. Similarly the language\\n{0n : n is a prime number}\\nshould be nonregular, because the prime numbers do not seem to have any\\nrepetitive structure that can be used by a DFA. To be more rigorous about\\nthis, we will establish a property that all regular languages must possess.\\nThis property is called the pumping lemma. If a language does not have this\\nproperty, then it must be nonregular.\\nThe pumping lemma states that any suﬃciently long string in a regular\\nlanguage can be pumped, i.e., there is a section in that string that can be\\nrepeated any number of times, so that the resulting strings are all in the\\nlanguage.\\nTheorem 2.9.1 (Pumping Lemma for Regular Languages) Let A be\\na regular language. Then there exists an integer p ≥1, called the pumping\\nlength, such that the following holds: Every string s in A, with |s| ≥p, can\\nbe written as s = xyz, such that\\n1. y ̸= ϵ (i.e., |y| ≥1),\\n2. |xy| ≤p, and\\n3. for all i ≥0, xyiz ∈A.\\nIn words, the pumping lemma states that by replacing the portion y in s\\nby zero or more copies of it, the resulting string is still in the language A.\\nProof. Let Σ be the alphabet of A. Since A is a regular language, there\\nexists a DFA M = (Q, Σ, δ, q, F) that accepts A. We deﬁne p to be the\\nnumber of states in Q.\\nLet s = s1s2 . . . sn be an arbitrary string in A such that n ≥p. Deﬁne\\nr1 = q, r2 = δ(r1, s1), r3 = δ(r2, s2), . . ., rn+1 = δ(rn, sn). Thus, when the\\nDFA M reads the string s from left to right, it visits the states r1, r2, . . . , rn+1.\\nSince s is a string in A, we know that rn+1 belongs to F.\\nConsider the ﬁrst p + 1 states r1, r2, . . . , rp+1 in this sequence. Since the\\nnumber of states of M is equal to p, the pigeonhole principle implies that\\nthere must be a state that occurs twice in this sequence. That is, there are\\nindices j and ℓsuch that 1 ≤j < ℓ≤p + 1 and rj = rℓ.\\n70\\nChapter 2.\\nFinite Automata and Regular Languages\\nq = r1\\nrn+1\\nr j = rℓ\\nread x\\nread y\\nread z\\nWe deﬁne x = s1s2 . . . sj−1, y = sj . . . sℓ−1, and z = sℓ. . . sn. Since j < ℓ,\\nwe have y ̸= ϵ, proving the ﬁrst claim in the theorem. Since ℓ≤p + 1, we\\nhave |xy| = ℓ−1 ≤p, proving the second claim in the theorem. To see that\\nthe third claim also holds, recall that the string s = xyz is accepted by M.\\nWhile reading x, M moves from the start state q to state rj. While reading\\ny, it moves from state rj to state rℓ= rj, i.e., after having read y, M is again\\nin state rj. While reading z, M moves from state rj to the accept state rn+1.\\nTherefore, the substring y can be repeated any number i ≥0 of times, and\\nthe corresponding string xyiz will still be accepted by M. It follows that\\nxyiz ∈A for all i ≥0.\\n2.9.1\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {0n1n : n ≥0}.\\nWe will prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. It is clear\\nthat s ∈A and |s| = 2p ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that, since |xy| ≤p, the string y contains only 0s. Moreover,\\nsince y ̸= ϵ, y contains at least one 0. But now we are in trouble: None of\\nthe strings xy0z = xz, xy2z = xyyz, xy3z = xyyyz, . . . , is contained in A.\\nHowever, by the pumping lemma, all these strings must be in A. Hence, we\\nhave a contradiction and we conclude that A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n71\\nSecond example\\nConsider the language\\nA = {w ∈{0, 1}∗: the number of 0s in w equals the number of 1s in w}.\\nAgain, we prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. Then s ∈A\\nand |s| = 2p ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz contains more 0s than 1s,\\nwhich implies that this string is not contained in A. But, by the pumping\\nlemma, this string is contained in A. This is a contradiction and, therefore,\\nA is not a regular language.\\nThird example\\nConsider the language\\nA = {ww : w ∈{0, 1}∗}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p10p1. Then s ∈A\\nand |s| = 2p + 2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz is not contained in A. But,\\nby the pumping lemma, this string is contained in A. This is a contradiction\\nand, therefore, A is not a regular language.\\nYou should convince yourself that by choosing s = 02p (which is a string\\nin A whose length is at least p), we do not obtain a contradiction. The reason\\nis that the string y may have an even length. Thus, 02p is the “wrong” string\\nfor showing that A is not regular. By choosing s = 0p10p1, we do obtain\\na contradiction; thus, this is the “correct” string for showing that A is not\\nregular.\\n72\\nChapter 2.\\nFinite Automata and Regular Languages\\nFourth example\\nConsider the language\\nA = {0m1n : m > n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p+11p. Then s ∈A\\nand |s| = 2p + 1 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Consider the string xy0z = xz. The number of 1s in this string\\nis equal to p, whereas the number of 0s is at most equal to p. Therefore, the\\nstring xy0z is not contained in A. But, by the pumping lemma, this string\\nis contained in A. This is a contradiction and, therefore, A is not a regular\\nlanguage.\\nFifth example\\nConsider the language\\nA = {1n2 : n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 1p2. Then s ∈A\\nand |s| = p2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that\\n|s| = |xyz| = p2\\nand\\n|xy2z| = |xyyz| = |xyz| + |y| = p2 + |y|.\\nSince |xy| ≤p, we have |y| ≤p. Since y ̸= ϵ, we have |y| ≥1. It follows that\\np2 < |xy2z| ≤p2 + p < (p + 1)2.\\nHence, the length of the string xy2z is strictly between two consecutive\\nsquares.\\nIt follows that this length is not a square and, therefore, xy2z\\nis not contained in A. But, by the pumping lemma, this string is contained\\nin A. This is a contradiction and, therefore, A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n73\\nSixth example\\nConsider the language\\nA = {1n : n is a prime number}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Let n ≥p be a prime number, and consider\\nthe string s = 1n. Then s ∈A and |s| = n ≥p. By the pumping lemma, s\\ncan be written as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nLet k be the integer such that y = 1k. Since y ̸= ϵ, we have k ≥1. For\\neach i ≥0, n + (i −1)k is a prime number, because xyiz = 1n+(i−1)k ∈A.\\nFor i = n + 1, however, we have\\nn + (i −1)k = n + nk = n(1 + k),\\nwhich is not a prime number, because n ≥2 and 1 + k ≥2.\\nThis is a\\ncontradiction and, therefore, A is not a regular language.\\nSeventh example\\nConsider the language\\nA = {w ∈{0, 1}∗:\\nthe number of occurrences of 01 in w is equal to\\nthe number of occurrences of 10 in w }.\\nSince this language has the same ﬂavor as the one in the second example,\\nwe may suspect that A is not a regular language. This is, however, not true:\\nAs we will show, A is a regular language.\\nThe key property is the following one: Let w be an arbitrary string in\\n{0, 1}∗. Then\\nthe absolute value of the number of occurrences of 01 in w minus\\nthe number of occurrences of 10 in w is at most one.\\nThis property holds, because between any two consecutive occurrences of\\n01, there must be exactly one occurrence of 10. Similarly, between any two\\nconsecutive occurrences of 10, there must be exactly one occurrence of 01.\\nWe will construct a DFA that accepts A. This DFA uses the following\\nﬁve states:\\n74\\nChapter 2.\\nFinite Automata and Regular Languages\\n• q: start state; no symbol has been read.\\n• q01: the last symbol read was 1; in the part of the string read so far, the\\nnumber of occurrences of 01 is one more than the number of occurrences\\nof 10.\\n• q10: the last symbol read was 0; in the part of the string read so far, the\\nnumber of occurrences of 10 is one more than the number of occurrences\\nof 01.\\n• q0\\nequal: the last symbol read was 0; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\n• q1\\nequal: the last symbol read was 1; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\nThe set of accept states is equal to {q, q0\\nequal, q1\\nequal}. The state diagram of\\nthe DFA is given below.\\nq0\\nequal\\nq1\\nequal\\nq01\\nq10\\nq\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\nIn fact, the key property mentioned above implies that the language A\\nconsists of the empty string ϵ and all non-empty binary strings that start\\n2.9.\\nThe pumping lemma and nonregular languages\\n75\\nand end with the same symbol. As a result, A is the language described by\\nthe regular expression\\nϵ ∪0 ∪1 ∪0(0 ∪1)∗0 ∪1(0 ∪1)∗1.\\nThis gives an alternative proof for the fact that A is a regular language.\\nEighth example\\nConsider the language\\nL = {w ∈{0, 1}∗: w is the binary representation of a prime number}.\\nWe assume that for any positive integer, the leftmost bit in its binary repre-\\nsentation is 1. In other words, we assume that there are no 0’s added to the\\nleft of such a binary representation. Thus,\\nL = {10, 11, 101, 111, 1011, 1101, 10001, . . .}.\\nWe will prove that L is not a regular language.\\nAssume that L is a regular language. Let p ≥1 be the pumping length.\\nLet N > 2p be a prime number and let s ∈{0, 1}∗be the binary representa-\\ntion of N. Observe that |s| ≥p + 1. Also, the leftmost and rightmost bits of\\ns are 1.\\nSince s ∈L and |s| ≥p + 1 ≥p, the Pumping Lemma implies that we\\ncan write s = xyz, such that\\n1. |y| ≥1,\\n2. |xy| ≤p (and, thus, |z| ≥1), and\\n3. for all i ≥0, xyiz ∈L, i.e., xyiz is the binary representation of a prime\\nnumber.\\nDeﬁne A, B, and C to be the integers whose binary representations are\\nx, y, and z, respectively. Note that both y and z may have leading 0’s. In\\nfact, y may be a string consisting of 0’s only, in which case B = 0. However,\\nsince the rightmost bit of z is 1, we have C ≥1. Observe that\\nN = C + B · 2|z| + A · 2|z|+|y|.\\n(2.4)\\n76\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet i = N, consider the bitstring xyiz = xyNz, and let M be the prime\\nnumber whose binary representation is given by this bitstring. Then,\\nM\\n=\\nC +\\nN−1\\nX\\nk=0\\nB · 2|z|+k|y| + A · 2|z|+N|y|\\n=\\nC + B · 2|z|\\nN−1\\nX\\nk=0\\n2k|y| + A · 2|z|+N|y|.\\nLet\\nT =\\nN−1\\nX\\nk=0\\n2k|y|.\\nThen\\n\\x002|y| −1\\n\\x01\\nT = 2N|y| −1.\\n(2.5)\\nBy Fermat’s Little Theorem, we have\\n2N ≡2\\n(mod N),\\nimplying that\\n2N|y| −1 =\\n\\x002N\\x01|y| −1 ≡2|y| −1\\n(mod N).\\nThus, (2.5) implies that\\n\\x002|y| −1\\n\\x01\\nT ≡2|y| −1\\n(mod N).\\n(2.6)\\nObserve that 2|y| ≤2p < N, because |y| ≤|xy| ≤p. Also, 2|y| ≥2, because\\ny ̸= ϵ. It follows that\\n1 ≤2|y| −1 < N,\\nimplying that\\n2|y| −1 ̸≡0\\n(mod N).\\nThis, together with (2.6), implies that\\nT ≡1\\n(mod N).\\nSince\\nM = C + B · 2|z| · T + A · 2|z|+N|y|,\\n2.10.\\nHigman’s Theorem\\n77\\nit follows that\\nM ≡C + B · 2|z| + A · 2|z|+|y|\\n(mod N).\\nThis, together with (2.4), implies that\\nM ≡0\\n(mod N),\\ni.e., N divides M. Since M > N, we conclude that M is not a prime number,\\nwhich is a contradiction. Thus, the language L is not regular.\\n2.10\\nHigman’s Theorem\\nLet Σ be a ﬁnite alphabet. For any two strings x and y in Σ∗, we say that x\\nis a subsequence of y, if x can be obtained by deleting zero or more symbols\\nfrom y. For example, 10110 is a subsequence of 0010010101010001. For any\\nlanguage L ⊆Σ∗, we deﬁne\\nSUBSEQ(L) := {x : there exists a y ∈L such that x is a subsequence of y}.\\nThat is, SUBSEQ(L) is the language consisting of the subsequences of all\\nstrings in L. In 1952, Higman proved the following result:\\nTheorem 2.10.1 (Higman) For any ﬁnite alphabet Σ and for any lan-\\nguage L ⊆Σ∗, the language SUBSEQ(L) is regular.\\n2.10.1\\nDickson’s Theorem\\nOur proof of Higman’s Theorem will use a theorem that was proved in 1913\\nby Dickson.\\nRecall that N denotes the set of positive integers. Let n ∈N. For any\\ntwo points p = (p1, p2, . . . , pn) and q = (q1, q2, . . . , qn) in Nn, we say that p is\\ndominated by q, if pi ≤qi for all i with 1 ≤i ≤n.\\nTheorem 2.10.2 (Dickson) Let S ⊆Nn, and let M be the set consisting of\\nall elements of S that are minimal in the relation “is dominated by”. Thus,\\nM = {q ∈S : there is no p in S \\\\ {q} such that p is dominated by q}.\\nThen, the set M is ﬁnite.\\n78\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe will prove this theorem by induction on the dimension n. If n = 1,\\nthen either M = ∅(if S = ∅) or M consists of exactly one element (if S ̸= ∅).\\nTherefore, the theorem holds if n = 1. Let n ≥2 and assume the theorem\\nholds for all subsets of Nn−1. Let S be a subset of Nn and consider the set\\nM of minimal elements in S. If S = ∅, then M = ∅and, thus, M is ﬁnite.\\nAssume that S ̸= ∅. We ﬁx an arbitrary element q in M. If p ∈M \\\\ {q},\\nthen q is not dominated by p. Therefore, there exists an index i such that\\npi ≤qi −1. It follows that\\nM \\\\ {q} ⊆\\nn[\\ni=1\\n\\x00Ni−1 × [1, qi −1] × Nn−i\\x01\\n.\\nFor all i and k with 1 ≤i ≤n and 1 ≤k ≤qi −1, we deﬁne\\nSik = {p ∈S : pi = k}\\nand\\nMik = {p ∈M : pi = k}.\\nThen,\\nM \\\\ {q} =\\nn[\\ni=1\\nqi−1\\n[\\nk=1\\nMik.\\n(2.7)\\nLemma 2.10.3 Mik is a subset of the set of all elements of Sik that are\\nminimal in the relation “is dominated by”.\\nProof. Let p be an element of Mik, and assume that p is not minimal in\\nSik. Then there is an element r in Sik, such that r ̸= p and r is dominated\\nby p. Since p and r are both elements of S, it follows that p ̸∈M. This is a\\ncontradiction.\\nSince the set Sik is basically a subset of Nn−1, it follows from the induction\\nhypothesis that Sik contains ﬁnitely many minimal elements. This, combined\\nwith Lemma 2.10.3, implies that Mik is a ﬁnite set. Thus, by (2.7), M \\\\ {q}\\nis the union of ﬁnitely many ﬁnite sets. Therefore, the set M is ﬁnite.\\n2.10.2\\nProof of Higman’s Theorem\\nWe give the proof of Theorem 2.10.1 for the case when Σ = {0, 1}. If L = ∅\\nor SUBSEQ(L) = {0, 1}∗, then SUBSEQ(L) is obviously a regular language.\\n2.10.\\nHigman’s Theorem\\n79\\nHence, we may assume that L is non-empty and SUBSEQ(L) is a proper\\nsubset of {0, 1}∗.\\nWe ﬁx a string z of length at least two in the complement SUBSEQ(L) of\\nthe language SUBSEQ(L). Observe that this is possible, because SUBSEQ(L)\\nis an inﬁnite language. We insert 0s and 1s into z, such that, in the result-\\ning string z′, 0s and 1s alternate. For example, if z = 0011101011, then\\nz′ = 01010101010101. Let n = |z′| −1, where |z′| denotes the length of z′.\\nThen, n ≥|z| −1 ≥1.\\nA (0, 1)-alternation in a binary string x is any occurrence of 01 or 10 in x.\\nFor example, the string 1101001 contains four (0, 1)-alternations. We deﬁne\\nA = {x ∈{0, 1}∗: x has at most n many (0, 1)-alternations}.\\nLemma 2.10.4 SUBSEQ(L) ⊆A.\\nProof. Let x ∈SUBSEQ(L) and assume that x ̸∈A. Then, x has at least\\nn + 1 = |z′| many (0, 1)-alternations and, therefore, z′ is a subsequence of x.\\nIn particular, z is a subsequence of x. Since x ∈SUBSEQ(L), it follows that\\nz ∈SUBSEQ(L), which is a contradiction.\\nLemma 2.10.5 SUBSEQ(L) =\\n\\x10\\nA ∩SUBSEQ(L)\\n\\x11\\n∪A.\\nProof. Follows from Lemma 2.10.4.\\nLemma 2.10.6 The language A is regular.\\nProof.\\nThe complement A of A is the language consisting of all binary\\nstrings with at least n + 1 many (0, 1)-alternations. If, for example, n = 3,\\nthen A is described by the regular expression\\n(00∗11∗00∗11∗0(0 ∪1)∗) ∪(11∗00∗11∗00∗1(0 ∪1)∗) .\\nThis should convince you that the claim is true for any value of n.\\nFor any b ∈{0, 1} and for any k ≥0, we deﬁne Abk to be the language\\nconsisting of all binary strings that start with a b and have exactly k many\\n(0, 1)-alternations. Then, we have\\nA = {ϵ} ∪\\n 1[\\nb=0\\nn[\\nk=0\\nAbk\\n!\\n.\\n80\\nChapter 2.\\nFinite Automata and Regular Languages\\nThus, if we deﬁne\\nFbk = Abk ∩SUBSEQ(L),\\nand use the fact that ϵ ∈SUBSEQ(L) (which is true because L ̸= ∅), then\\nA ∩SUBSEQ(L) =\\n1[\\nb=0\\nn[\\nk=0\\nFbk.\\n(2.8)\\nFor any b ∈{0, 1} and for any k ≥0, consider the relation “is a subse-\\nquence of” on the language Fbk. We deﬁne Mbk to be the language consisting\\nof all strings in Fbk that are minimal in this relation. Thus,\\nMbk = {x ∈Fbk : there is no x′ in Fbk \\\\ {x} such that x′ is a subsequence of x}.\\nIt is clear that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Fbk : x is a subsequence of y}.\\nIf x ∈Mbk, y ∈Abk, and x is a subsequence of y, then y must be in\\nSUBSEQ(L) and, therefore, y must be in Fbk. To prove this, assume that\\ny ∈SUBSEQ(L).\\nThen, x ∈SUBSEQ(L), contradicting the fact that\\nx ∈Mbk ⊆Fbk ⊆SUBSEQ(L). It follows that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Abk : x is a subsequence of y}.\\n(2.9)\\nLemma 2.10.7 Let b ∈{0, 1} and 0 ≤k ≤n, and let x be an element of\\nMbk. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis regular.\\nProof. We will prove the claim by means of an example. Assume that b = 1,\\nk = 3, and x = 11110001000. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis described by the regular expression\\n11111∗0000∗11∗0000∗.\\nThis should convince you that the claim is true in general.\\nExercises\\n81\\nLemma 2.10.8 For each b ∈{0, 1} and each 0 ≤k ≤n, the set Mbk is\\nﬁnite.\\nProof. Again, we will prove the claim by means of an example. Assume\\nthat b = 1 and k = 3. Any string in Fbk can be written as 1a0b1c0d, for some\\nintegers a, b, c, d ≥1. Consider the function ϕ : Fbk →N4 that is deﬁned by\\nϕ(1a0b1c0d) = (a, b, c, d). Then, ϕ is an injective function, and the following\\nis true, for any two strings x and x′ in Fbk:\\nx is a subsequence of x′ if and only if ϕ(x) is dominated by ϕ(x′).\\nIt follows that the elements of Mbk are in one-to-one correspondence with\\nthose elements of ϕ(Fbk) that are minimal in the relation “is dominated by”.\\nThe lemma thus follows from Dickson’s Theorem.\\nNow we can complete the proof of Higman’s Theorem:\\n• It follows from (2.9) and Lemmas 2.10.7 and 2.10.8, that Fbk is the\\nunion of ﬁnitely many regular languages. Therefore, by Theorem 2.3.1,\\nFbk is a regular language.\\n• It follows from (2.8) that A∩SUBSEQ(L) is the union of ﬁnitely many\\nregular languages. Therefore, again by Theorem 2.3.1, A∩SUBSEQ(L)\\nis a regular language.\\n• Since A ∩SUBSEQ(L) is regular and, by Lemma 2.10.6, A is regular,\\nit follows from Lemma 2.10.5 that SUBSEQ(L) is the union of two reg-\\nular languages. Therefore, by Theorem 2.3.1, SUBSEQ(L) is a regular\\nlanguage.\\n• Since SUBSEQ(L) is regular, it follows from Theorem 2.6.4 that the\\nlanguage SUBSEQ(L) is regular as well.\\nExercises\\n2.1 For each of the following languages, construct a DFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : the length of w is divisible by three}\\n82\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. {w : 110 is not a substring of w}\\n3. {w : w contains at least ﬁve 1s}\\n4. {w : w contains the substring 1011}\\n5. {w : w contains at least two 1s and at most two 0s}\\n6. {w : w contains an odd number of 1s or exactly two 0s}\\n7. {w : w begins with 1 and ends with 0}\\n8. {w : every odd position in w is 1}\\n9. {w : w has length at least 3 and its third symbol is 0}\\n10. {ϵ, 0}\\n2.2 For each of the following languages, construct an NFA, with the speciﬁed\\nnumber of states, that accepts the language. In all cases, the alphabet is\\n{0, 1}.\\n1. The language {w : w ends with 10} with three states.\\n2. The language {w : w contains the substring 1011} with ﬁve states.\\n3. The language {w : w contains an odd number of 1s or exactly two 0s}\\nwith six states.\\n2.3 For each of the following languages, construct an NFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : w contains the substring 11001}\\n2. {w : w has length at least 2 and does not end with 10}\\n3. {w : w begins with 1 or ends with 0}\\n2.4 Convert the following NFA to an equivalent DFA.\\nExercises\\n83\\n1\\n2\\na\\nb\\na, b\\n2.5 Convert the following NFA to an equivalent DFA.\\n1\\n3\\n2\\na\\na\\nb\\na\\nε,b\\n2.6 Convert the following NFA to an equivalent DFA.\\n0\\n1\\n2\\n3\\na, ǫ\\nb\\na\\nǫ\\nb\\n2.7 In the proof of Theorem 2.6.3, we introduced a new start state q0, which\\nis also an accept state. Explain why the following is not a valid proof of\\nTheorem 2.6.3:\\nLet N = (Q1, Σ, δ1, q1, F1) be an NFA, such that A = L(N). Deﬁne the\\nNFA M = (Q1, Σ, δ, q1, F), where\\n84\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. F = {q1} ∪F1.\\n2. δ : Q1 × Σϵ →P(Q1) is deﬁned as follows: For any r ∈Q1 and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ.\\nThen L(M) = A∗.\\n2.8 Prove Theorem 2.6.4.\\n2.9 Let A be a language over the alphabet Σ = {0, 1} and let A be the\\ncomplement of A. Thus, A is the language consisting of all binary strings\\nthat are not in A.\\nAssume that A is a regular language. Let M = (Q, Σ, δ, q, F) be a non-\\ndeterministic ﬁnite automaton (NFA) that accepts A.\\nConsider the NFA N = (Q, Σ, δ, q, F), where F = Q\\\\F is the complement\\nof F. Thus, N is obtained from M by turning all accept states into nonaccept\\nstates, and turning all nonaccept states into accept states.\\n1. Is it true that the language accepted by N is equal to A?\\n2. Assume now that M is a deterministic ﬁnite automaton (DFA) that\\naccepts A. Deﬁne N as above; thus, turn all accept states into nonac-\\ncept states, and turn all nonaccept states into accept states. Is it true\\nthat the language accepted by N is equal to A?\\n2.10 Recall the alternative deﬁnition for the star of a language A that we\\ngave just before Theorem 2.3.1.\\nIn Theorems 2.3.1 and 2.6.2, we have shown that the class of regular\\nlanguages is closed under the union and concatenation operations.\\nSince\\nA∗= S∞\\nk=0 Ak, why doesn’t this imply that the class of regular languages is\\nclosed under the star operation?\\n2.11 Let A and B be two regular languages over the same alphabet Σ. Prove\\nthat the diﬀerence of A and B, i.e., the language\\nA \\\\ B = {w : w ∈A and w ̸∈B}\\nis a regular language.\\nExercises\\n85\\n2.12 For each of the following regular expressions, give two strings that are\\nmembers and two strings that are not members of the language described by\\nthe expression. The alphabet is Σ = {a, b}.\\n1. a(ba)∗b.\\n2. (a ∪b)∗a(a ∪b)∗b(a ∪b)∗a(a ∪b)∗.\\n3. (a ∪ba ∪bb)(a ∪b)∗.\\n2.13 Give regular expressions describing the following languages.\\nIn all\\ncases, the alphabet is {0, 1}.\\n1. {w : w contains at least three 1s}.\\n2. {w : w contains at least two 1s and at most one 0},\\n3. {w : w contains an even number of 0s and exactly two 1s}.\\n4. {w : w contains exactly two 0s and at least two 1s}.\\n5. {w : w contains an even number of 0s and each 0 is followed by at least one 1}.\\n6. {w : every odd position in w is 1}.\\n2.14 Convert each of the following regular expressions to an NFA.\\n1. (0 ∪1)∗000(0 ∪1)∗\\n2. (((10)∗(00)) ∪10)∗\\n3. ((0 ∪1)(11)∗∪0)∗\\n2.15 Convert the following DFA to a regular expression.\\n86\\nChapter 2.\\nFinite Automata and Regular Languages\\n1\\n2\\n3\\na\\na\\nb\\nb\\na\\nb\\n2.16 Convert the following DFA to a regular expression.\\n1\\n2\\n3\\na, b\\na\\na\\nb\\nb\\n2.17 Convert the following DFA to a regular expression.\\na, b\\n2.18\\n1. Let A be a non-empty regular language. Prove that there exists\\nan NFA that accepts A and that has exactly one accept state.\\nExercises\\n87\\n2. For any string w = w1w2 . . . wn, we denote by wR the string obtained\\nby reading w backwards, i.e., wR = wnwn−1 . . . w2w1. For any language\\nA, we deﬁne AR to be the language obtained by reading all strings in\\nA backwards, i.e.,\\nAR = {wR : w ∈A}.\\nLet A be a non-empty regular language. Prove that the language AR\\nis also regular.\\n2.19 If n ≥1 is an integer and w = a1a2 . . . an is a string, then for any i\\nwith 0 ≤i < n, the string a1a2 . . . ai is called a proper preﬁx of w. (If i = 0,\\nthen a1a2 . . . ai = ϵ.)\\nFor any language L, we deﬁne MIN(L) to be the language\\nMIN(L) = {w ∈L : no proper preﬁx of w belongs to L}.\\nProve the following claim: If L is a regular language, then MIN(L) is regular\\nas well.\\n2.20 Use the pumping lemma to prove that the following languages are not\\nregular.\\n1. {anbmcn+m : n ≥0, m ≥0}.\\n2. {anbnc2n : n ≥0}.\\n3. {anbman : n ≥0, m ≥0}.\\n4. {a2n : n ≥0}. (Remark: a2n is the string consisting of 2n many a’s.)\\n5. {anbmck : n ≥0, m ≥0, k ≥0, n2 + m2 = k2}.\\n6. {uvu : u ∈{a, b}∗, u ̸= ϵ, v ∈{a, b}∗}.\\n2.21 Prove that the language\\n{ambn : m ≥0, n ≥0, m ̸= n}\\nis not regular. (Using the pumping lemma for this one is a bit tricky. You\\ncan avoid using the pumping lemma by combining results about the closure\\nunder regular operations.)\\n88\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.22\\n1. Give an example of a regular language A and a non-regular lan-\\nguage B for which A ⊆B.\\n2. Give an example of a non-regular language A and a regular language\\nB for which A ⊆B.\\n2.23 Let A be a language consisting of ﬁnitely many strings.\\n1. Prove that A is a regular language.\\n2. Let n be the maximum length of any string in A. Prove that every\\ndeterministic ﬁnite automaton (DFA) that accepts A has at least n+1\\nstates. (Hint: How is the pumping length chosen in the proof of the\\npumping lemma?)\\n2.24 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L ̸= ∅if and only\\nif L contains a string of length less than p.\\n2.25 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L is an inﬁnite\\nlanguage if and only if L contains a string w with p ≤|w| ≤2p −1.\\n2.26 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. We deﬁne a binary relation RL on Σ∗× Σ∗, in the following way:\\nFor any two strings u and u′ in Σ∗,\\nuRLu′ if and only if (∀v ∈Σ∗: uv ∈L ⇔u′v ∈L) .\\nProve that RL is an equivalence relation.\\n2.27 Let Σ = {0, 1}, let\\nL = {w ∈Σ∗: |w| is odd},\\nand consider the relation RL deﬁned in Exercise 2.26.\\n1. Prove that for any two strings u and u′ in Σ∗,\\nuRLu′ ⇔|u| −|u′| is even.\\nExercises\\n89\\n2. Determine all equivalence classes of the relation RL.\\n2.28 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. Recall the equivalence relation RL that was deﬁned in Exercise 2.26.\\n1. Assume that L is a regular language, and let M = (Q, Σ, δ, q0, F) be\\na DFA that accepts L. Let u and u′ be strings in Σ∗. Let q be the\\nstate reached, when following the path in the state diagram of M, that\\nstarts in q0 and that is obtained by reading the string u. Similarly, let\\nq′ be the state reached, when following the path in the state diagram\\nof M, that starts in q0 and that is obtained by reading the string u′.\\nProve the following: If q = q′, then uRLu′.\\n2. Prove the following claim: If L is a regular language, then the equiva-\\nlence relation RL has a ﬁnite number of equivalence classes.\\n2.29 Let L be the language deﬁned by\\nL = {uuR : u ∈{0, 1}∗}.\\nIn words, a string is in L if and only if its length is even, and the second half\\nis the reverse of the ﬁrst half. Consider the equivalence relation RL that was\\ndeﬁned in Exercise 2.26.\\n1. Let m and n be two distinct positive integers and consider the two\\nstrings u = 0m1 and u′ = 0n1. Prove that ¬(uRLu′).\\n2. Prove that L is not a regular language, without using the pumping\\nlemma.\\n3. Use the pumping lemma to prove that L is not a regular language.\\n2.30 In this exercise, we will show that the converse of the pumping lemma\\ndoes, in general, not hold. Consider the language\\nA = {ambncn : m ≥1, n ≥0} ∪{bnck : n ≥0, k ≥0}.\\n1. Show that A satisﬁes the conclusion of the pumping lemma for p = 1.\\nThus, show that every string s in A whose length is at least p can be\\nwritten as s = xyz, such that y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all\\ni ≥0.\\n90\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. Consider the equivalence relation RA that was deﬁned in Exercise 2.26.\\nLet n and n′ be two distinct non-negative integers and consider the two\\nstrings u = abn and u′ = abn′. Prove that ¬(uRAu′).\\n3. Prove that A is not a regular language.\\nChapter 3\\nContext-Free Languages\\nIn this chapter, we introduce the class of context-free languages.\\nAs we\\nwill see, this class contains all regular languages, as well as some nonregular\\nlanguages such as {0n1n : n ≥0}.\\nThe class of context-free languages consists of languages that have some\\nsort of recursive structure. We will see two equivalent methods to obtain this\\nclass. We start with context-free grammars, which are used for deﬁning the\\nsyntax of programming languages and their compilation. Then we introduce\\nthe notion of (nondeterministic) pushdown automata, and show that these\\nautomata have the same power as context-free grammars.\\n3.1\\nContext-free grammars\\nWe start with an example. Consider the following ﬁve (substitution) rules:\\nS\\n→\\nAB\\nA\\n→\\na\\nA\\n→\\naA\\nB\\n→\\nb\\nB\\n→\\nbB\\nHere, S, A, and B are variables, S is the start variable, and a and b are\\nterminals. We use these rules to derive strings consisting of terminals (i.e.,\\nelements of {a, b}∗), in the following manner:\\n1. Initialize the current string to be the string consisting of the start\\nvariable S.\\n92\\nChapter 3.\\nContext-Free Languages\\n2. Take any variable in the current string and take any rule that has this\\nvariable on the left-hand side. Then, in the current string, replace this\\nvariable by the right-hand side of the rule.\\n3. Repeat 2. until the current string only contains terminals.\\nFor example, the string aaaabb can be derived in the following way:\\nS\\n⇒\\nAB\\n⇒\\naAB\\n⇒\\naAbB\\n⇒\\naaAbB\\n⇒\\naaaAbB\\n⇒\\naaaabB\\n⇒\\naaaabb\\nThis derivation can also be represented using a parse tree, as in the ﬁgure\\nbelow:\\nS\\nA\\nA\\nA\\nA\\na\\na\\na\\na\\nb\\nb\\nB\\nB\\nThe ﬁve rules in this example constitute a context-free grammar. The\\nlanguage of this grammar is the set of all strings that\\n3.1.\\nContext-free grammars\\n93\\n• can be derived from the start variable and\\n• only contain terminals.\\nFor this example, the language is\\n{ambn : m ≥1, n ≥1},\\nbecause every string of the form ambn, for some m ≥1 and n ≥1, can be\\nderived from the start variable, whereas no other string over the alphabet\\n{a, b} can be derived from the start variable.\\nDeﬁnition 3.1.1 A context-free grammar is a 4-tuple G = (V, Σ, R, S),\\nwhere\\n1. V is a ﬁnite set, whose elements are called variables,\\n2. Σ is a ﬁnite set, whose elements are called terminals,\\n3. V ∩Σ = ∅,\\n4. S is an element of V ; it is called the start variable,\\n5. R is a ﬁnite set, whose elements are called rules. Each rule has the\\nform A →w, where A ∈V and w ∈(V ∪Σ)∗.\\nIn our example, we have V = {S, A, B}, Σ = {a, b}, and\\nR = {S →AB, A →a, A →aA, B →b, B →bB}.\\nDeﬁnition 3.1.2 Let G = (V, Σ, R, S) be a context-free grammar. Let A be\\nan element in V and let u, v, and w be strings in (V ∪Σ)∗such that A →w\\nis a rule in R. We say that the string uwv can be derived in one step from\\nthe string uAv, and write this as\\nuAv ⇒uwv.\\nIn other words, by applying the rule A →w to the string uAv, we obtain\\nthe string uwv. In our example, we see that aaAbb ⇒aaaAbb.\\nDeﬁnition 3.1.3 Let G = (V, Σ, R, S) be a context-free grammar. Let u\\nand v be strings in (V ∪Σ)∗. We say that v can be derived from u, and write\\nthis as u\\n∗⇒v, if one of the following two conditions holds:\\n94\\nChapter 3.\\nContext-Free Languages\\n1. u = v or\\n2. there exist an integer k ≥2 and a sequence u1, u2, . . . , uk of strings in\\n(V ∪Σ)∗, such that\\n(a) u = u1,\\n(b) v = uk, and\\n(c) u1 ⇒u2 ⇒. . . ⇒uk.\\nIn other words, by starting with the string u and applying rules zero or\\nmore times, we obtain the string v. In our example, we see that aaAbB\\n∗⇒\\naaaabbbB.\\nDeﬁnition 3.1.4 Let G = (V, Σ, R, S) be a context-free grammar.\\nThe\\nlanguage of G is deﬁned to be the set of all strings in Σ∗that can be derived\\nfrom the start variable S:\\nL(G) = {w ∈Σ∗: S\\n∗⇒w}.\\nDeﬁnition 3.1.5 A language L is called context-free, if there exists a context-\\nfree grammar G such that L(G) = L.\\n3.2\\nExamples of context-free grammars\\n3.2.1\\nProperly nested parentheses\\nConsider the context-free grammar G = (V, Σ, R, S), where V = {S}, Σ =\\n{a, b}, and\\nR = {S →ϵ, S →aSb, S →SS}.\\nWe write the three rules in R as\\nS →ϵ|aSb|SS,\\nwhere you can think of “|” as being a short-hand for “or”.\\n3.2.\\nExamples of context-free grammars\\n95\\nBy applying the rules in R, starting with the start variable S, we obtain,\\nfor example,\\nS\\n⇒\\nSS\\n⇒\\naSbS\\n⇒\\naSbSS\\n⇒\\naSSbSS\\n⇒\\naaSbSbSS\\n⇒\\naabSbSS\\n⇒\\naabbSS\\n⇒\\naabbaSbS\\n⇒\\naabbabS\\n⇒\\naabbabaSb\\n⇒\\naabbabab\\nWhat is the language L(G) of this context-free grammar G? If we think\\nof a as being a left-parenthesis “(”, and of b as being a right-parenthesis “)”,\\nthen L(G) is the language consisting of all strings of properly nested paren-\\ntheses. Here is the explanation: Any string of properly nested parentheses is\\neither\\n• empty (which we derive from S by the rule S →ϵ),\\n• consists of a left-parenthesis, followed by an arbitrary string of properly\\nnested parentheses, followed by a right-parenthesis (these are derived\\nfrom S by ﬁrst applying the rule S →aSb), or\\n• consists of an arbitrary string of properly nested parentheses, followed\\nby an arbitrary string of properly nested parentheses (these are derived\\nfrom S by ﬁrst applying the rule S →SS).\\n3.2.2\\nA context-free grammar for a nonregular lan-\\nguage\\nConsider the language L1 = {0n1n : n ≥0}. We have seen in Section 2.9.1\\nthat L1 is not a regular language. We claim that L1 is a context-free language.\\n96\\nChapter 3.\\nContext-Free Languages\\nIn order to prove this claim, we have to construct a context-free grammar\\nG1 such that L(G1) = L1.\\nObserve that any string in L1 is either\\n• empty or\\n• consists of a 0, followed by an arbitrary string in L1, followed by a 1.\\nThis leads to the context-free grammar G1 = (V1, Σ, R1, S1), where V1 =\\n{S1}, Σ = {0, 1}, and R1 consists of the rules\\nS1 →ϵ|0S11.\\nHence, R1 = {S1 →ϵ, S1 →0S11}.\\nTo derive the string 0n1n from the start variable S1, we do the following:\\n• Starting with S1, apply the rule S1 →0S11 exactly n times. This gives\\nthe string 0nS11n.\\n• Apply the rule S1 →ϵ. This gives the string 0n1n.\\nIt is not diﬃcult to see that these are the only strings that can be derived\\nfrom the start variable S1. Thus, L(G1) = L1.\\nIn a symmetric way, we see that the context-free grammar G2 = (V2, Σ, R2, S2),\\nwhere V2 = {S2}, Σ = {0, 1}, and R2 consists of the rules\\nS2 →ϵ|1S20,\\nhas the property that L(G2) = L2, where L2 = {1n0n : n ≥0}. Thus, L2 is\\na context-free language.\\nDeﬁne L = L1 ∪L2, i.e.,\\nL = {0n1n : n ≥0} ∪{1n0n : n ≥0}.\\nThe context-free grammar G = (V, Σ, R, S), where V = {S, S1, S2}, Σ =\\n{0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2\\nS1\\n→\\nϵ|0S11\\nS2\\n→\\nϵ|1S20,\\nhas the property that L(G) = L. Hence, L is a context-free language.\\n3.2.\\nExamples of context-free grammars\\n97\\n3.2.3\\nA context-free grammar for the complement of\\na nonregular language\\nLet L be the (nonregular) language L = {0n1n : n ≥0}. We want to prove\\nthat the complement L of L is a context-free language. Hence, we want to\\nconstruct a context-free grammar G whose language is equal to L. Observe\\nthat a binary string w is in L if and only if\\n1. w = 0m1n, for some integers m and n with 0 ≤m < n, or\\n2. w = 0m1n, for some integers m and n with 0 ≤n < m, or\\n3. w contains 10 as a substring.\\nThus, we can write L as the union of the languages of all strings of type 1.,\\ntype 2., and type 3.\\nAny string of type 1. is either\\n• the string 1,\\n• consists of a string of type 1., followed by one 1, or\\n• consists of one 0, followed by an arbitrary string of type 1., followed by\\none 1.\\nThus, using the rules\\nS1 →1|S11|0S11,\\nwe can derive, from S1, all strings of type 1.\\nSimilarly, using the rules\\nS2 →0|0S2|0S21,\\nwe can derive, from S2, all strings of type 2.\\nAny string of type 3.\\n• consists of an arbitrary binary string, followed by the string 10, followed\\nby an arbitrary binary string.\\nUsing the rules\\nX →ϵ|0X|1X,\\n98\\nChapter 3.\\nContext-Free Languages\\nwe can derive, from X, all binary strings. Thus, by combining these with\\nthe rule\\nS3 →X10X,\\nwe can derive, from S3, all strings of type 3.\\nWe arrive at the context-free grammar G = (V, Σ, R, S), where V =\\n{S, S1, S2, S3, X}, Σ = {0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2|S3\\nS1\\n→\\n1|S11|0S11\\nS2\\n→\\n0|0S2|0S21\\nS3\\n→\\nX10X\\nX\\n→\\nϵ|0X|1X\\nTo summarize, we have\\nS1\\n∗⇒0m1n, for all integers m and n with 0 ≤m < n,\\nS2\\n∗⇒0m1n, for all integers m and n with 0 ≤n < m,\\nX\\n∗⇒u, for each string u in {0, 1}∗,\\nand\\nS3\\n∗⇒w, for every binary string w that contains 10 as a substring.\\nFrom these observations, it follows that that L(G) = L.\\n3.2.4\\nA context-free grammar that veriﬁes addition\\nConsider the language\\nL = {anbmcn+m : n ≥0, m ≥0}.\\nUsing the pumping lemma for regular languages (Theorem 2.9.1), it can\\nbe shown that L is not a regular language. We will construct a context-\\nfree grammar G whose language is equal to L, thereby proving that L is a\\ncontext-free language.\\nFirst observe that ϵ ∈L. Therefore, we will take S →ϵ to be one of the\\nrules in the grammar.\\nLet us see how we can derive all strings in L from the start variable S:\\n3.3.\\nRegular languages are context-free\\n99\\n1. Every time we add an a, we also add a c. In this way, we obtain all\\nstrings of the form ancn, where n ≥0.\\n2. Given a string of the form ancn, we start adding bs. Every time we add\\na b, we also add a c. Observe that every b has to be added between\\nthe as and the cs. Therefore, we use a variable B as a “pointer” to\\nthe position in the current string where a b can be added: Instead of\\nderiving ancn from S, we derive the string anBcn. Then, from B, we\\nderive all strings of the form bmcm, where m ≥0.\\nWe obtain the context-free grammar G = (V, Σ, R, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R consists of the rules\\nS\\n→\\nϵ|A\\nA\\n→\\nϵ|aAc|B\\nB\\n→\\nϵ|bBc\\nThe facts that\\n• A\\n∗⇒anBcn, for every n ≥0,\\n• B\\n∗⇒bmcm, for every m ≥0,\\nimply that the following strings can be derived from the start variable S:\\n• S\\n∗⇒anBcn\\n∗⇒anbmcmcn = anbmcn+m, for all n ≥0 and m ≥0.\\nIn fact, no other strings in {a, b, c}∗can be derived from S. Therefore, we\\nhave L(G) = L. Since\\nS ⇒A ⇒B ⇒ϵ,\\nwe can simplify this grammar G, by eliminating the rules S →ϵ and A →ϵ.\\nThis gives the context-free grammar G′ = (V, Σ, R′, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R′ consists of the rules\\nS\\n→\\nA\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\nFinally, observe that we do not need S; instead, we can use A as start\\nvariable. This gives our ﬁnal context-free grammar G′′ = (V, Σ, R′′, A), where\\nV = {A, B}, Σ = {a, b, c}, and R′′ consists of the rules\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\n100\\nChapter 3.\\nContext-Free Languages\\n3.3\\nRegular languages are context-free\\nWe mentioned already that the class of context-free languages includes the\\nclass of regular languages. In this section, we will prove this claim.\\nTheorem 3.3.1 Let Σ be an alphabet and let L ⊆Σ∗be a regular language.\\nThen L is a context-free language.\\nProof.\\nSince L is a regular language, there exists a deterministic ﬁnite\\nautomaton M = (Q, Σ, δ, q, F) that accepts L.\\nTo prove that L is context-free, we have to deﬁne a context-free grammar\\nG = (V, Σ, R, S), such that L = L(M) = L(G). Thus, G must have the\\nfollowing property: For every string w ∈Σ∗,\\nw ∈L(M) if and only if w ∈L(G),\\nwhich can be reformulated as\\nM accepts w if and only if S\\n∗⇒w.\\nWe will deﬁne the context-free grammar G in such a way that the following\\ncorrespondence holds for any string w = w1w2 . . . wn:\\n• Assume that M is in state A just after it has read the substring\\nw1w2 . . . wi.\\n• Then in the context-free grammar G, we have S\\n∗⇒w1w2 . . . wiA.\\nIn the next step, M reads the symbol wi+1 and switches from state A to,\\nsay, state B; thus, δ(A, wi+1) = B. In order to guarantee that the above\\ncorrespondence still holds, we have to add the rule A →wi+1B to G.\\nConsider the moment when M has read the entire string w. Let A be the\\nstate M is in at that moment. By the above correspondence, we have\\nS\\n∗⇒w1w2 . . . wnA = wA.\\nRecall that G must have the property that\\nM accepts w if and only if S\\n∗⇒w,\\nwhich is equivalent to\\nA ∈F if and only if S\\n∗⇒w.\\n3.3.\\nRegular languages are context-free\\n101\\nWe guarantee this property by adding to G the rule A →ϵ for every accept\\nstate A of M.\\nWe are now ready to give the formal deﬁnition of the context-free gram-\\nmar G = (V, Σ, R, S):\\n• V = Q, i.e., the variables of G are the states of M.\\n• S = q, i.e., the start variable of G is the start state of M.\\n• R consists of the rules\\nA →aB, where A ∈Q, a ∈Σ, B ∈Q, and δ(A, a) = B,\\nand\\nA →ϵ, where A ∈F.\\nIn words,\\n• every transition δ(A, a) = B of M (i.e., when M is in the state A and\\nreads the symbol a, it switches to the state B) corresponds to a rule\\nA →aB in the grammar G,\\n• every accept state A of M corresponds to a rule A →ϵ in the grammar\\nG.\\nWe claim that L(G) = L. In order to prove this, we have to show that\\nL(G) ⊆L and L ⊆L(G).\\nWe prove that L ⊆L(G). Let w = w1w2 . . . wn be an arbitrary string\\nin L. When the ﬁnite automaton M reads the string w, it visits the states\\nr0, r1, . . . , rn, where\\n• r0 = q, and\\n• ri+1 = δ(ri, wi+1) for i = 0, 1, . . . , n −1.\\nSince w ∈L = L(M), we know that rn ∈F.\\nIt follows from the way we deﬁned the grammar G that\\n• for each i = 0, 1, . . . , n −1, ri →wi+1ri+1 is a rule in R, and\\n• rn →ϵ is a rule in R.\\n102\\nChapter 3.\\nContext-Free Languages\\nTherefore, we have\\nS = q = r0 ⇒w1r1 ⇒w1w2r2 ⇒. . . ⇒w1w2 . . . wnrn ⇒w1w2 . . . wn = w.\\nThis proves that w ∈L(G).\\nThe proof of the claim that L(G) ⊆L is left as an exercise.\\nIn Sections 2.9.1 and 3.2.2, we have seen that the language {0n1n : n ≥\\n0} is not regular, but context-free. Therefore, the class of all context-free\\nlanguages properly contains the class of regular languages.\\n3.3.1\\nAn example\\nLet L be the language deﬁned as\\nL = {w ∈{0, 1}∗: 101 is a substring of w}.\\nIn Section 2.2.2, we have seen that L is a regular language. In that section,\\nwe constructed the following deterministic ﬁnite automaton M that accepts\\nL (we have renamed the states):\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nS\\nA\\nB\\nC\\nWe apply the construction given in the proof of Theorem 3.3.1 to convert\\nM to a context-free grammar G whose language is equal to L. According\\nto this construction, we have G = (V, Σ, R, S), where V = {S, A, B, C},\\nΣ = {0, 1}, the start variable S is the start state of M, and R consists of the\\nrules\\nS\\n→\\n0S|1A\\nA\\n→\\n0B|1A\\nB\\n→\\n0S|1C\\nC\\n→\\n0C|1C|ϵ\\n3.4.\\nChomsky normal form\\n103\\nConsider the string 010011011, which is an element of L. When the ﬁnite\\nautomaton M reads this string, it visits the states\\nS, S, A, B, S, A, A, B, C, C.\\nIn the grammar G, this corresponds to the derivation\\nS\\n⇒\\n0S\\n⇒\\n01A\\n⇒\\n010B\\n⇒\\n0100S\\n⇒\\n01001A\\n⇒\\n010011A\\n⇒\\n0100110B\\n⇒\\n01001101C\\n⇒\\n010011011C\\n⇒\\n010011011.\\nHence,\\nS\\n∗⇒010011011,\\nimplying that the string 010011011 is in the language L(G) of the context-free\\ngrammar G.\\nThe string 10011 is not in the language L. When the ﬁnite automaton\\nM reads this string, it visits the states\\nS, A, B, S, A, A,\\ni.e., after the string has been read, M is in the non-accept state A. In the\\ngrammar G, reading the string 10011 corresponds to the derivation\\nS\\n⇒\\n1A\\n⇒\\n10B\\n⇒\\n100S\\n⇒\\n1001A\\n⇒\\n10011A.\\nSince A is not an accept state in M, the grammar G does not contain the\\nrule A →ϵ. This implies that the string 10011 cannot be derived from the\\nstart variable S. Thus, 10011 is not in the language L(G) of G.\\n104\\nChapter 3.\\nContext-Free Languages\\n3.4\\nChomsky normal form\\nThe rules in a context-free grammar G = (V, Σ, R, S) are of the form\\nA →w,\\nwhere A is a variable and w is a string over the alphabet V ∪Σ. In this\\nsection, we show that every context-free grammar G can be converted to a\\ncontext-free grammar G′, such that L(G) = L(G′), and the rules of G′ are of\\na restricted form, as speciﬁed in the following deﬁnition:\\nDeﬁnition 3.4.1 A context-free grammar G = (V, Σ, R, S) is said to be in\\nChomsky normal form, if every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are elements of V , B ̸= S, and C ̸= S.\\n2. A →a, where A is an element of V and a is an element of Σ.\\n3. S →ϵ, where S is the start variable.\\nYou should convince yourself that, for such a grammar, R contains the\\nrule S →ϵ if and only if ϵ ∈L(G).\\nTheorem 3.4.2 Let Σ be an alphabet and let L ⊆Σ∗be a context-free lan-\\nguage. There exists a context-free grammar in Chomsky normal form, whose\\nlanguage is L.\\nProof. Since L is a context-free language, there exists a context-free gram-\\nmar G = (V, Σ, R, S), such that L(G) = L. We will transform G into a\\ngrammar that is in Chomsky normal form and whose language is equal to\\nL(G). The transformation consists of ﬁve steps.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe deﬁne G1 = (V1, Σ, R1, S1), where S1 is the start variable (which is a\\nnew variable), V1 = V ∪{S1}, and R1 = R ∪{S1 →S}. This grammar has\\nthe property that\\n• the start variable S1 does not occur on the right-hand side of any rule\\nin R1, and\\n• L(G1) = L(G).\\n3.4.\\nChomsky normal form\\n105\\nStep 2: An ϵ-rule is a rule that is of the form A →ϵ, where A is a variable\\nthat is not equal to the start variable. In the second step, we eliminate all\\nϵ-rules from G1.\\nWe consider all ϵ-rules, one after another. Let A →ϵ be one such rule,\\nwhere A ∈V1 and A ̸= S1. We modify G1 as follows:\\n1. Remove the rule A →ϵ from the current set R1.\\n2. For each rule in the current set R1 that is of the form\\n(a) B →A, add the rule B →ϵ to R1, unless this rule has already\\nbeen deleted from R1; observe that in this way, we replace the two-\\nstep derivation B ⇒A ⇒ϵ by the one-step derivation B ⇒ϵ;\\n(b) B →uAv (where u and v are strings that are not both empty),\\nadd the rule B →uv to R1; observe that in this way, we replace\\nthe two-step derivation B ⇒uAv ⇒uv by the one-step derivation\\nB ⇒uv;\\n(c) B →uAvAw (where u, v, and w are strings), add the rules B →\\nuvw, B →uAvw, and B →uvAw to R1; if u = v = w = ϵ and\\nthe rule B →ϵ has already been deleted from R1, then we do not\\nadd the rule B →ϵ;\\n(d) treat rules in which A occurs more than twice on the right-hand\\nside in a similar fashion.\\nWe repeat this process until all ϵ-rules have been eliminated.\\nLet R2\\nbe the set of rules, after all ϵ-rules have been eliminated. We deﬁne G2 =\\n(V2, Σ, R2, S2), where V2 = V1 and S2 = S1. This grammar has the property\\nthat\\n• the start variable S2 does not occur on the right-hand side of any rule\\nin R2,\\n• R2 does not contain any ϵ-rule (it may contain the rule S2 →ϵ), and\\n• L(G2) = L(G1) = L(G).\\nStep 3: A unit-rule is a rule that is of the form A →B, where A and B are\\nvariables. In the third step, we eliminate all unit-rules from G2.\\n106\\nChapter 3.\\nContext-Free Languages\\nWe consider all unit-rules, one after another. Let A →B be one such\\nrule, where A and B are elements of V2. We know that B ̸= S2. We modify\\nG2 as follows:\\n1. Remove the rule A →B from the current set R2.\\n2. For each rule in the current set R2 that is of the form B →u, where\\nu ∈(V2 ∪Σ)∗, add the rule A →u to the current set R2, unless this is\\na unit-rule that has already been eliminated.\\nObserve that in this way, we replace the two-step derivation A ⇒B ⇒\\nu by the one-step derivation A ⇒u.\\nWe repeat this process until all unit-rules have been eliminated.\\nLet\\nR3 be the set of rules, after all unit-rules have been eliminated. We deﬁne\\nG3 = (V3, Σ, R3, S3), where V3 = V2 and S3 = S2. This grammar has the\\nproperty that\\n• the start variable S3 does not occur on the right-hand side of any rule\\nin R3,\\n• R3 does not contain any ϵ-rule (it may contain the rule S3 →ϵ),\\n• R3 does not contain any unit-rule, and\\n• L(G3) = L(G2) = L(G1) = L(G).\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside.\\nFor each rule in the current set R3 that is of the form A →u1u2 . . . uk,\\nwhere k ≥3 and each ui is an element of V3 ∪Σ, we modify G3 as follows:\\n1. Remove the rule A →u1u2 . . . uk from the current set R3.\\n2. Add the following rules to the current set R3:\\nA\\n→\\nu1A1\\nA1\\n→\\nu2A2\\nA2\\n→\\nu3A3\\n...\\nAk−3\\n→\\nuk−2Ak−2\\nAk−2\\n→\\nuk−1uk\\n3.4.\\nChomsky normal form\\n107\\nwhere A1, A2, . . . , Ak−2 are new variables that are added to the current\\nset V3.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 . . . uk by the (k −1)-step derivation\\nA ⇒u1A1 ⇒u1u2A2 ⇒. . . ⇒u1u2 . . . uk−2Ak−2 ⇒u1u2 . . . uk.\\nLet R4 be the set of rules, and let V4 be the set of variables, after all rules\\nwith more than two symbols on the right-hand side have been eliminated. We\\ndeﬁne G4 = (V4, Σ, R4, S4), where S4 = S3. This grammar has the property\\nthat\\n• the start variable S4 does not occur on the right-hand side of any rule\\nin R4,\\n• R4 does not contain any ϵ-rule (it may contain the rule S4 →ϵ),\\n• R4 does not contain any unit-rule,\\n• R4 does not contain any rule with more than two symbols on the right-\\nhand side, and\\n• L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nStep 5: Eliminate all rules of the form A →u1u2, where u1 and u2 are not\\nboth variables.\\nFor each rule in the current set R4 that is of the form A →u1u2, where\\nu1 and u2 are elements of V4 ∪Σ, but u1 and u2 are not both contained in\\nV4, we modify G3 as follows:\\n1. If u1 ∈Σ and u2 ∈V4, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →U1u2 and U1 →u1, where U1 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒U1u2 ⇒u1u2.\\n2. If u1 ∈V4 and u2 ∈Σ, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →u1U2 and U2 →u2, where U2 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒u1U2 ⇒u1u2.\\n108\\nChapter 3.\\nContext-Free Languages\\n3. If u1 ∈Σ, u2 ∈Σ, and u1 ̸= u2, then replace the rule A →u1u2 in the\\ncurrent set R4 by the three rules A →U1U2, U1 →u1, and U2 →u2,\\nwhere U1 and U2 are new variables that are added to the current set\\nV4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the three-step derivation A ⇒U1U2 ⇒u1U2 ⇒u1u2.\\n4. If u1 ∈Σ, u2 ∈Σ, and u1 = u2, then replace the rule A →u1u2 = u1u1\\nin the current set R4 by the two rules A →U1U1 and U1 →u1, where\\nU1 is a new variable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 = u1u1 by the three-step derivation A ⇒U1U1 ⇒u1U1 ⇒u1u1.\\nLet R5 be the set of rules, and let V5 be the set of variables, after Step 5\\nhas been completed. We deﬁne G5 = (V5, Σ, R5, S5), where S5 = S4. This\\ngrammar has the property that\\n• the start variable S5 does not occur on the right-hand side of any rule\\nin R5,\\n• R5 does not contain any ϵ-rule (it may contain the rule S5 →ϵ),\\n• R5 does not contain any unit-rule,\\n• R5 does not contain any rule with more than two symbols on the right-\\nhand side,\\n• R5 does not contain any rule of the form A →u1u2, where u1 and u2\\nare not both variables of V5, and\\n• L(G5) = L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nSince the grammar G5 is in Chomsky normal form, the proof is complete.\\n3.4.\\nChomsky normal form\\n109\\n3.4.1\\nAn example\\nConsider the context-free grammar G = (V, Σ, R, A), where V = {A, B},\\nΣ = {0, 1}, A is the start variable, and R consists of the rules\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nWe apply the construction given in the proof of Theorem 3.4.2 to convert\\nthis grammar to a context-free grammar in Chomsky normal form whose\\nlanguage is the same as that of G. Throughout the construction, upper case\\nletters will denote variables.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe introduce a new start variable S, and add the rule S →A. This gives\\nthe following grammar:\\nS\\n→\\nA\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nStep 2: Eliminate all ϵ-rules.\\nWe take the ϵ-rule A →ϵ, and remove it. Then we consider all rules that\\ncontain A on the right-hand side. There are two such rules:\\n• S →A; we add the rule S →ϵ;\\n• A →BAB; we add the rule A →BB.\\nThis gives the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB\\nB\\n→\\n00|ϵ\\nWe take the ϵ-rule B →ϵ, and remove it. Then we consider all rules that\\ncontain B on the right-hand side. There are three such rules:\\n• A →BAB; we add the rules A →AB, A →BA, and A →A;\\n• A →B; we do not add the rule A →ϵ, because it has already been\\nremoved;\\n110\\nChapter 3.\\nContext-Free Languages\\n• A →BB; we add the rule A →B, but not the rule A →ϵ (because it\\nhas already been removed).\\nAt this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA|A\\nB\\n→\\n00\\nSince all ϵ-rules have been eliminated, this completes Step 2. (Observe that\\nthe rule S →ϵ is allowed, because S is the start variable.)\\nStep 3: Eliminate all unit-rules.\\nWe take the unit-rule A →A. We can remove this rule, without adding\\nany new rule. At this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →A, remove it, and add the rules\\nS →BAB|B|BB|AB|BA.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BAB|B|BB|AB|BA\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →B, remove it, and add the rule S →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule A →B, remove it, and add the rule A →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|BB|AB|BA|00\\nB\\n→\\n00\\n3.5.\\nPushdown automata\\n111\\nSince all unit-rules have been eliminated, this concludes Step 3.\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside. There are two such rules:\\n• We take the rule S →BAB, remove it, and add the rules S →BA1\\nand A1 →AB.\\n• We take the rule A →BAB, remove it, and add the rules A →BA2\\nand A2 →AB.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BB|AB|BA|00|BA1\\nA\\n→\\nBB|AB|BA|00|BA2\\nB\\n→\\n00\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nStep 4 is now completed.\\nStep 5: Eliminate all rules, whose right-hand side contains exactly two\\nsymbols, which are not both variables. There are three such rules:\\n• We replace the rule S →00 by the rules S →A3A3 and A3 →0.\\n• We replace the rule A →00 by the rules A →A4A4 and A4 →0.\\n• We replace the rule B →00 by the rules B →A5A5 and A5 →0.\\nThis gives the following grammar, which is in Chomsky normal form:\\nS\\n→\\nϵ|BB|AB|BA|BA1|A3A3\\nA\\n→\\nBB|AB|BA|BA2|A4A4\\nB\\n→\\nA5A5\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nA3\\n→\\n0\\nA4\\n→\\n0\\nA5\\n→\\n0\\n112\\nChapter 3.\\nContext-Free Languages\\n3.5\\nPushdown automata\\nIn this section, we introduce nondeterministic pushdown automata. As we\\nwill see, the class of languages that can be accepted by these automata is\\nexactly the class of context-free languages.\\nWe start with an informal description of a deterministic pushdown au-\\ntomaton. Such an automaton consists of the following, see also Figure 3.1.\\n1. There is a tape which is divided into cells. Each cell stores a symbol\\nbelonging to a ﬁnite set Σ, called the tape alphabet. There is a special\\nsymbol 2 that is not contained in Σ; this symbol is called the blank\\nsymbol. If a cell contains 2, then this means that the cell is actually\\nempty.\\n2. There is a tape head which can move along the tape, one cell to the\\nright per move. This tape head can also read the cell it currently scans.\\n3. There is a stack containing symbols from a ﬁnite set Γ, called the stack\\nalphabet. This set contains a special symbol $.\\n4. There is a stack head which can read the top symbol of the stack. This\\nhead can also pop the top symbol, and it can push symbols of Γ onto\\nthe stack.\\n5. There is a state control, which can be in any one of a ﬁnite number\\nof states. The set of states is denoted by Q. The set Q contains one\\nspecial state q, called the start state.\\nThe input for a pushdown automaton is a string in Σ∗. This input string\\nis stored on the tape of the pushdown automaton and, initially, the tape head\\nis on the leftmost symbol of the input string. Initially, the stack only contains\\nthe special symbol $, and the pushdown automaton is in the start state q.\\nIn one computation step, the pushdown automaton does the following:\\n1. Assume that the pushdown automaton is currently in state r. Let a be\\nthe symbol of Σ that is read by the tape head, and let A be the symbol\\nof Γ that is on top of the stack.\\n2. Depending on the current state r, the tape symbol a, and the stack\\nsymbol A,\\n3.5.\\nPushdown automata\\n113\\nstate control\\na a b a b b a b a b 2\\ntape\\n6\\n$\\nA\\nA\\nB\\nA\\nstack\\n-\\nFigure 3.1: A pushdown automaton.\\n(a) the pushdown automaton switches to a state r′ of Q (which may\\nbe equal to r),\\n(b) the tape head either moves one cell to the right or stays at the\\ncurrent cell, and\\n(c) the top symbol A is replaced by a string w that belongs to Γ∗. To\\nbe more precise,\\ni. if w = ϵ, then A is popped from the stack, whereas\\nii. if w = B1B2 . . . Bk, with k ≥1 and B1, B2, . . . , Bk ∈Γ, then\\nA is replaced by w, and Bk becomes the new top symbol of\\nthe stack.\\nLater, we will specify when the pushdown automaton accepts the input\\nstring.\\nWe now give a formal deﬁnition of a deterministic pushdown automaton.\\nDeﬁnition 3.5.1 A deterministic pushdown automaton is a 5-tuple M =\\n(Σ, Γ, Q, δ, q), where\\n114\\nChapter 3.\\nContext-Free Languages\\n1. Σ is a ﬁnite set, called the tape alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the stack alphabet; this alphabet contains the\\nspecial symbol $,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. δ is called the transition function, which is a function\\nδ : Q × (Σ ∪{2}) × Γ →Q × {N, R} × Γ∗.\\nThe transition function δ can be thought of as being the “program” of the\\npushdown automaton. This function tells us what the automaton can do in\\none “computation step”: Let r ∈Q, a ∈Σ ∪{2}, and A ∈Γ. Furthermore,\\nlet r′ ∈Q, σ ∈{R, N}, and w ∈Γ∗be such that\\nδ(r, a, A) = (r′, σ, w).\\n(3.1)\\nThis transition means that if\\n• the pushdown automaton is in state r,\\n• the tape head reads the symbol a, and\\n• the top symbol on the stack is A,\\nthen\\n• the pushdown automaton switches to state r′,\\n• the tape head moves according to σ: if σ = R, then it moves one cell\\nto the right; if σ = N, then it does not move, and\\n• the top symbol A on the stack is replaced by the string w.\\nWe will write the computation step (3.1) in the form of the instruction\\nraA →r′σw.\\nWe now specify the computation of the pushdown automaton M = (Σ, Γ, Q, δ, q).\\n3.6.\\nExamples of pushdown automata\\n115\\nStart conﬁguration: Initially, the pushdown automaton is in the start state\\nq, the tape head is on the leftmost symbol of the input string a1a2 . . . an, and\\nthe stack only contains the special symbol $.\\nComputation and termination: Starting in the start conﬁguration, the\\npushdown automaton performs a sequence of computation steps as described\\nabove. It terminates at the moment when the stack becomes empty. (Hence,\\nif the stack never gets empty, the pushdown automaton does not terminate.)\\nAcceptance: The pushdown automaton accepts the input string a1a2 . . . an ∈\\nΣ∗, if\\n1. the automaton terminates on this input, and\\n2. at the time of termination (i.e., at the moment when the stack gets\\nempty), the tape head is on the cell immediately to the right of the cell\\ncontaining the symbol an (this cell must contain the blank symbol 2).\\nIn all other cases, the pushdown automaton rejects the input string. Thus,\\nthe pushdown automaton rejects this string if\\n1. the automaton does not terminate on this input (i.e., the computation\\n“loops forever”) or\\n2. at the time of termination, the tape head is not on the cell immediately\\nto the right of the cell containing the symbol an.\\nWe denote by L(M) the language accepted by the pushdown automaton\\nM. Thus,\\nL(M) = {w ∈Σ∗: M accepts w}.\\nThe pushdown automaton described above is deterministic. For a non-\\ndeterministic pushdown automata, the current computation step may not\\nbe uniquely deﬁned, but the automaton can make a choice out of a ﬁnite\\nnumber of possibilities. In this case, the transition function δ is a function\\nδ : Q × (Σ ∪{2}) × Γ →Pf(Q × {N, R} × Γ∗),\\nwhere Pf(K) is the set of all ﬁnite subsets of the set K.\\nWe say that a nondeterministic pushdown automaton M accepts an input\\nstring, if there exists an accepting computation, in the sense as described for\\ndeterministic pushdown automata. We say that M rejects an input string, if\\nevery computation on this string is rejecting. As before, we denote by L(M)\\nthe set of all strings in Σ∗that are accepted by M.\\n116\\nChapter 3.\\nContext-Free Languages\\n3.6\\nExamples of pushdown automata\\n3.6.1\\nProperly nested parentheses\\nWe will show how to construct a deterministic pushdown automaton, that\\naccepts the set of all strings of properly nested parentheses. Observe that a\\nstring w in {(, )}∗is properly nested if and only if\\n• in every preﬁx of w, the number of “(” is greater than or equal to the\\nnumber of “)”, and\\n• in the complete string w, the number of “(” is equal to the number of\\n“)”.\\nWe will use the tape symbol a for “(”, and the tape symbol b for “)”.\\nThe idea is as follows. Recall that initially, the stack only contains the\\nspecial symbol $. The pushdown automaton reads the input string from left\\nto right. For every a it reads, it pushes the symbol S onto the stack, and\\nfor every b it reads, it pops the top symbol from the stack. In this way, the\\nnumber of symbols S on the stack will always be equal to the number of as\\nthat have been read minus the number of bs that have been read; additionally,\\nthe bottom of the stack will contain the special symbol $. The input string\\nis properly nested if and only if (i) this diﬀerence is always non-negative and\\n(ii) this diﬀerence is zero once the entire input string has been read. Hence,\\nthe input string is accepted if and only if during this process, (i) the stack\\nalways contains at least the special symbol $ and (ii) at the end, the stack\\nonly contains the special symbol $ (which will then be popped in the ﬁnal\\nstep).\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q), where Σ = {a, b}, Γ = {$, S}, Q = {q}, and the\\ntransition function δ is speciﬁed by the following instructions:\\n3.6.\\nExamples of pushdown automata\\n117\\nqa$ →qR$S\\nbecause of the a, S is pushed onto the stack\\nqaS →qRSS\\nbecause of the a, S is pushed onto the stack\\nqbS →qRϵ\\nbecause of the b, the top element is popped\\nfrom the stack\\nqb$ →qNϵ\\nthe number of bs read is larger than the number\\nof as read; the stack is made empty (hence,\\nthe computation terminates before the entire\\nstring has been read), and the input string is rejected\\nq2$ →qNϵ\\nthe entire input string has been read; the stack is\\nmade empty, and the input string is accepted\\nq2S →qNS\\nthe entire input string has been read, it contains\\nmore as than bs; no changes are made (thus, the\\nautomaton does not terminate), and the input string\\nis rejected\\n3.6.2\\nStrings of the form 0n1n\\nWe construct a deterministic pushdown automata that accepts the language\\n{0n1n : n ≥0}.\\nThe automaton uses two states q0 and q1, where q0 is the start state.\\nInitially, the automaton is in state q0.\\n• For each 0 that it reads, the automaton pushes one symbol S onto the\\nstack and stays in state q0.\\n• When the ﬁrst 1 is read, the automaton switches to state q1. From that\\nmoment,\\n– for each 1 that is read, the automaton pops the top symbol from\\nthe stack and stays in state q1;\\n– if a 0 is read, the automaton does not make any change and,\\ntherefore, does not terminate.\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q0), where Σ = {0, 1}, Γ = {$, S}, Q = {q0, q1}, q0 is\\nthe start state, and the transition function δ is speciﬁed by the following\\ninstructions:\\n118\\nChapter 3.\\nContext-Free Languages\\nq00$ →q0R$S\\npush S onto the stack\\nq00S →q0RSS\\npush S onto the stack\\nq01$ →q0N$\\nﬁrst symbol in the input is 1; loop forever\\nq01S →q1Rϵ\\nﬁrst 1 is encountered\\nq02$ →q0Nϵ\\ninput string is empty; accept\\nq02S →q0NS\\ninput only consists of 0s; loop forever\\nq10$ →q1N$\\n0 to the right of 1; loop forever\\nq10S →q1NS\\n0 to the right of 1; loop forever\\nq11$ →q1N$\\ntoo many 1s; loop forever\\nq11S →q1Rϵ\\npop top symbol from the stack\\nq12$ →q1Nϵ\\naccept\\nq12S →q1NS\\ntoo many 0s; loop forever\\n3.6.3\\nStrings with b in the middle\\nWe will construct a nondeterministic pushdown automaton that accepts the\\nset L of all strings in {a, b}∗having an odd length and whose middle symbol\\nis b, i.e.,\\nL = {vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|}.\\nThe idea is as follows. The automaton uses two states q and q′, where q\\nis the start state. These states have the following meaning:\\n• If the automaton is in state q, then it has not reached the middle symbol\\nb of the input string.\\n• If the automaton is in state q′, then it has read the middle symbol b.\\nObserve that since the automaton can only make one single pass over the\\ninput string, it has to “guess” (i.e., use nondeterminism) when it reaches the\\nmiddle of the string.\\n• If the automaton is in state q, then, when reading the current tape\\nsymbol,\\n– it either pushes one symbol S onto the stack and stays in state q\\n– or, in case the current tape symbol is b, it “guesses” that it has\\nreached the middle of the input string, by switching to state q′.\\n• If the automaton is in state q′, then, when reading the current tape\\nsymbol, it pops the top symbol S from the stack and stays in state q′.\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n119\\nIn this way, the number of symbols S on the stack will always be equal to the\\ndiﬀerence of (i) the number of symbols in the part to the left of the middle\\nsymbol b that have been read and (ii) the number of symbols in the part\\nto the right of the middle symbol b that have been read; additionally, the\\nbottom of the stack will contain the special symbol $.\\nThe input string is accepted if and only if, at the moment when the blank\\nsymbol 2 is read, the automaton is in state q′ and the top symbol on the\\nstack is $. In this case, the stack is made empty and, thus, the computation\\nterminates.\\nWe obtain the nondeterministic pushdown automaton M = (Σ, Γ, Q, δ, q),\\nwhere Σ = {a, b}, Γ = {$, S}, Q = {q, q′}, q is the start state, and the\\ntransition function δ is speciﬁed by the following instructions:\\nqa$ →qR$S\\npush S onto the stack\\nqaS →qRSS\\npush S onto the stack\\nqb$ →q′R$\\nreached the middle\\nqb$ →qR$S\\ndid not reach the middle; push S onto the stack\\nqbS →q′RS\\nreached the middle\\nqbS →qRSS\\ndid not reach the middle; push S onto the stack\\nq2$ →qN$\\ninput string is empty; loop forever\\nq2S →qNS\\nloop forever\\nq′a$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′aS →q′Rϵ\\npop top symbol from stack\\nq′b$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′bS →q′Rϵ\\npop top symbol from stack\\nq′2$ →q′Nϵ\\naccept\\nq′2S →q′NS\\nloop forever\\nRemark 3.6.1 It can be shown that there is no deterministic pushdown\\nautomaton that accepts the language L. The reason is that a deterministic\\npushdown automaton cannot determine when it reaches the middle of the\\ninput string. Thus, unlike as for ﬁnite automata, nondeterministic pushdown\\nautomata are more powerful than their deterministic counterparts.\\n120\\nChapter 3.\\nContext-Free Languages\\n3.7\\nEquivalence of pushdown automata and\\ncontext-free grammars\\nThe main result of this section is that nondeterministic pushdown automata\\nand context-free grammars are equivalent in power:\\nTheorem 3.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then\\nA is context-free if and only if there exists a nondeterministic pushdown\\nautomaton that accepts A.\\nWe will only prove one direction of this theorem. That is, we will show\\nhow to convert an arbitrary context-free grammar to a nondeterministic push-\\ndown automaton.\\nLet G = (V, Σ, R, $) be a context-free grammar, where V is the set of\\nvariables, Σ is the set of terminals, R is the set of rules, and $ is the start\\nvariable. By Theorem 3.4.2, we may assume that G is in Chomsky normal\\nform. Hence, every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are variables, B ̸= $, and C ̸= $.\\n2. A →a, where A is a variable and a is a terminal.\\n3. $ →ϵ.\\nWe will construct a nondeterministic pushdown automaton M that ac-\\ncepts the language L(G) of this grammar G. Observe that M must have the\\nfollowing property: For every string w = a1a2 . . . an ∈Σ∗,\\nw ∈L(G) if and only if M accepts w.\\nThis can be reformulated as follows:\\n$\\n∗⇒a1a2 . . . an\\nif and only if there exists a computation of M that starts in the initial\\nconﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n121\\nand ends in the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nwhere ∅indicates that the stack is empty.\\nAssume that $\\n∗⇒a1a2 . . . an. Then there exists a derivation (using the\\nrules of R) of the string a1a2 . . . an from the start variable $. We may assume\\nthat in each step in this derivation, a rule is applied to the leftmost variable\\nin the current string. Hence, because the grammar G is in Chomsky normal\\nform, at any moment during the derivation, the current string has the form\\na1a2 . . . ai−1AkAk−1 . . . A1,\\n(3.2)\\nfor some integers i and k with 1 ≤i ≤n + 1 and k ≥0, and variables\\nA1, A2, . . . , Ak. (In particular, at the start of the derivation, we have i = 1\\nand k = 1, and the current string is Ak = $. At the end of the derivation,\\nwe have i = n + 1 and k = 0, and the current string is a1a2 . . . an.)\\nWe will deﬁne the pushdown automaton M in such a way that the current\\nstring (3.2) corresponds to the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nBased on this discussion, we obtain the nondeterministic pushdown au-\\ntomaton M = (Σ, V, {q}, δ, q), where\\n• the tape alphabet is the set Σ of terminals of G,\\n• the stack alphabet is the set V of variables of G,\\n• the set of states consists of one state q, which is the start state, and\\n• the transition function δ is obtained from the rules in R, in the following\\nway:\\n122\\nChapter 3.\\nContext-Free Languages\\n– For each rule in R that is of the form A →BC, with A, B, C ∈V ,\\nthe pushdown automaton M has the instructions\\nqaA →qNCB, for all a ∈Σ.\\n– For each rule in R that is of the form A →a, with A ∈V and\\na ∈Σ, the pushdown automaton M has the instruction\\nqaA →qRϵ.\\n– If R contains the rule $ →ϵ, then the pushdown automaton M\\nhas the instruction\\nq2$ →qNϵ.\\nThis concludes the deﬁnition of M. It remains to prove that L(M) =\\nL(G), i.e., the language of the nondeterministic pushdown automaton M is\\nequal to the language of the context-free grammar G. Hence, we have to\\nshow that for every string w ∈Σ∗,\\nw ∈L(G) if and only if w ∈L(M),\\nwhich can be rewritten as\\n$\\n∗⇒w if and only if M accepts w.\\nClaim 3.7.2 Let a1a2 . . . an be a string in Σ∗, let A1, A2, . . . , Ak be variables\\nin V , and let i and k be integers with 1 ≤i ≤n + 1 and k ≥0. Then the\\nfollowing holds:\\n$\\n∗⇒a1a2 . . . ai−1AkAk−1 . . . A1\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n123\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nProof. The claim can be proved by induction. Let\\nw = a1a2 . . . ai−1AkAk−1 . . . A1.\\nAssume that k ≥1 and assume that the claim is true for the string w. Then\\nwe have to show that the claim is still true after applying a rule in R to the\\nleftmost variable Ak in w. Since the grammar is in Chomsky normal form,\\nthe rule to be applied is either of the form Ak →BC or of the form Ak →ai.\\nIn both cases, the property mentioned in the claim is maintained.\\nWe now use Claim 3.7.2 to prove that L(M) = L(G). Let w = a1a2 . . . an\\nbe an arbitrary string in Σ∗. By applying Claim 3.7.2, with i = n + 1 and\\nk = 0, we see that w ∈L(G), i.e.,\\n$\\n∗⇒a1a2 . . . an,\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nBut this means that w ∈L(G) if and only if the automaton M accepts the\\nstring w.\\nThis concludes the proof of the fact that every context-free grammar can\\nbe converted to a nondeterministic pushdown automaton.\\nAs mentioned\\nalready, we will not give the conversion in the other direction. We ﬁnish this\\nsection with the following observation:\\n124\\nChapter 3.\\nContext-Free Languages\\nTheorem 3.7.3 Let Σ be an alphabet and let A ⊆Σ∗be a context-free lan-\\nguage. Then there exists a nondeterministic pushdown automaton that ac-\\ncepts A and has only one state.\\nProof. Since A is context-free, there exists a context-free grammar G0 such\\nthat L(G0) = A. By Theorem 3.4.2, there exists a context-free grammar G\\nthat is in Chomsky normal form and for which L(G) = L(G0). The construc-\\ntion given above converts G to a nondeterministic pushdown automaton M\\nthat has only one state and for which L(M) = L(G).\\n3.8\\nThe pumping lemma for context-free lan-\\nguages\\nIn Section 2.9, we proved the pumping lemma for regular languages and\\nused it to prove that certain languages are not regular. In this section, we\\ngeneralize the pumping lemma to context-free languages.\\nThe idea is to\\nconsider the parse tree (see Section 3.1) that describes the derivation of a\\nsuﬃciently long string in the context-free language L. Since the number of\\nvariables in the corresponding context-free grammar G is ﬁnite, there is at\\nleast one variable, say Aj, that occurs more than once on the longest root-\\nto-leaf path in the parse tree. The subtree which is sandwiched between two\\noccurrences of Aj on this path can be copied any number of times. This will\\nresult in a legal parse tree and, hence, in a “pumped” string that is in the\\nlanguage L.\\nTheorem 3.8.1 (Pumping Lemma for Context-Free Languages) Let\\nL be a context-free language. Then there exists an integer p ≥1, called the\\npumping length, such that the following holds: Every string s in L, with\\n|s| ≥p, can be written as s = uvxyz, such that\\n1. |vy| ≥1 (i.e., v and y are not both empty),\\n2. |vxy| ≤p, and\\n3. uvixyiz ∈L, for all i ≥0.\\n3.8.\\nThe pumping lemma for context-free languages\\n125\\n3.8.1\\nProof of the pumping lemma\\nThe proof of the pumping lemma will use the following result about parse\\ntrees:\\nLemma 3.8.2 Let G be a context-free grammar in Chomsky normal form,\\nlet s be a non-empty string in L(G), and let T be a parse tree for s. Let ℓbe\\nthe height of T, i.e., ℓis the number of edges on a longest root-to-leaf path\\nin T. Then\\n|s| ≤2ℓ−1.\\nProof. The claim can be proved by induction on ℓ. By looking at some\\nsmall values of ℓand using the fact that G is in Chomsky normal form, you\\nshould be able to verify the claim.\\nNow we can start with the proof of the pumping lemma. Let L be a\\ncontext-free language and let Σ be the alphabet of L. By Theorem 3.4.2, there\\nexists a context-free grammar in Chomsky normal form, G = (V, Σ, R, S),\\nsuch that L = L(G).\\nDeﬁne r to be the number of variables of G and deﬁne p = 2r. We will\\nprove that the value of p can be used as the pumping length. Consider an\\narbitrary string s in L such that |s| ≥p, and let T be a parse tree for s. Let\\nℓbe the height of T. Then, by Lemma 3.8.2, we have\\n|s| ≤2ℓ−1.\\nOn the other hand, we have\\n|s| ≥p = 2r.\\nBy combining these inequalities, we see that 2r ≤2ℓ−1, which can be rewrit-\\nten as\\nℓ≥r + 1.\\nConsider the nodes on a longest root-to-leaf path in T.\\nSince this path\\nconsists of ℓedges, it consists of ℓ+ 1 nodes. The ﬁrst ℓof these nodes store\\nvariables, which we denote by A0, A1, . . . , Aℓ−1 (where A0 = S), and the last\\nnode (which is a leaf) stores a terminal, which we denote by a.\\nSince ℓ−1 −r ≥0, the sequence\\nAℓ−1−r, Aℓ−r, . . . , Aℓ−1\\n126\\nChapter 3.\\nContext-Free Languages\\nof variables is well-deﬁned.\\nObserve that this sequence consists of r + 1\\nvariables. Since the number of variables in the grammar G is equal to r,\\nthe pigeonhole principle implies that there is a variable that occurs at least\\ntwice in this sequence. In other words, there are indices j and k, such that\\nℓ−1 −r ≤j < k ≤ℓ−1 and Aj = Ak. Refer to the ﬁgure below for an\\nillustration.\\nS\\nA j\\nAk\\nu\\nv\\nx\\ny\\nz\\ns\\nA0 = S\\nA1\\nAℓ−1−r\\nAℓ−r\\nAℓ−2\\nAℓ−1\\na\\nr +1\\nvariables\\nRecall that T is a parse tree for the string s. Therefore, the terminals\\nstored at the leaves of T, in the order from left to right, form s. As indicated\\nin the ﬁgure above, the nodes storing the variables Aj and Ak partition s\\ninto ﬁve substrings u, v, x, y, and z, such that s = uvxyz.\\nIt remains to prove that the three properties stated in the pumping lemma\\n3.8.\\nThe pumping lemma for context-free languages\\n127\\nhold. We start with the third property, i.e., we prove that\\nuvixyiz ∈L, for all i ≥0.\\nIn the grammar G, we have\\nS\\n∗⇒uAjz.\\n(3.3)\\nSince Aj\\n∗⇒vAky and Ak = Aj, we have\\nAj\\n∗⇒vAjy.\\n(3.4)\\nFinally, since Ak\\n∗⇒x and Ak = Aj, we have\\nAj\\n∗⇒x.\\n(3.5)\\nFrom (3.3) and (3.5), it follows that\\nS\\n∗⇒uAjz\\n∗⇒uxz,\\nwhich implies that the string uxz is in the language L. Similarly, it follows\\nfrom (3.3), (3.4), and (3.5) that\\nS\\n∗⇒uAjz\\n∗⇒uvAjyz\\n∗⇒uvvAjyyz\\n∗⇒uvvxyyz.\\nHence, the string uv2xy2z is in the language L. In general, for each i ≥0,\\nthe string uvixyiz is in the language L, because\\nS\\n∗⇒uAjz\\n∗⇒uviAjyiz\\n∗⇒uvixyiz.\\nThis proves that the third property in the pumping lemma holds.\\nNext we show that the second property holds. That is, we prove that\\n|vxy| ≤p.\\nConsider the subtree rooted at the node storing the variable\\nAj.\\nThe path from the node storing Aj to the leaf storing the terminal\\na is a longest path in this subtree. (Convince yourself that this is true.)\\nMoreover, this path consists of ℓ−j edges. Since Aj\\n∗⇒vxy, this subtree\\nis a parse tree for the string vxy (where Aj is used as the start variable).\\nTherefore, by Lemma 3.8.2, we can conclude that |vxy| ≤2ℓ−j−1. We know\\nthat ℓ−1 −r ≤j, which is equivalent to ℓ−j −1 ≤r. It follows that\\n|vxy| ≤2ℓ−j−1 ≤2r = p.\\n128\\nChapter 3.\\nContext-Free Languages\\nFinally, we show that the ﬁrst property in the pumping lemma holds.\\nThat is, we prove that |vy| ≥1. Recall that\\nAj\\n∗⇒vAky.\\nLet the ﬁrst rule used in this derivation be Aj →BC. (Since the variables\\nAj and Ak, even though they are equal, are stored at diﬀerent nodes of the\\nparse tree, and since the grammar G is in Chomsky normal form, this ﬁrst\\nrule exists.) Then\\nAj ⇒BC\\n∗⇒vAky.\\nObserve that the string BC has length two. Moreover, by applying rules of\\na grammar in Chomsky normal form, strings cannot become shorter. (Here,\\nwe use the fact that the start variable does not occur on the right-hand side\\nof any rule.) Therefore, we have |vAky| ≥2. But this implies that |vy| ≥1.\\nThis completes the proof of the pumping lemma.\\n3.8.2\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {anbncn : n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. Consider the string s = apbpcp.\\nObserve that s ∈A and |s| = 3p ≥p. Hence, by the pumping lemma, s can\\nbe written as s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all\\ni ≥0.\\nObserve that the pumping lemma does not tell us the location of the\\nsubstring vxy in the string s, it only gives us an upper bound on the length\\nof this substring. Therefore, we have to consider three cases, depending on\\nthe location of vxy in s.\\nCase 1: The substring vxy does not contain any c.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many as or more than p many bs. Since it contains\\nexactly p many cs, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\n3.8.\\nThe pumping lemma for context-free languages\\n129\\nCase 2: The substring vxy does not contain any a.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many bs or more than p many cs. Since it contains\\nexactly p many as, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\nCase 3: The substring vxy contains at least one a and at least one c.\\nSince s = apbpcp, this implies that |vxy| > p, which again contradicts the\\npumping lemma.\\nThus, in all of the three cases, we have obtained a contradiction. There-\\nfore, we have shown that the language A is not context-free.\\nSecond example\\nConsider the languages\\nA = {wwR : w ∈{a, b}∗},\\nwhere wR is the string obtained by writing w backwards, and\\nB = {ww : w ∈{a, b}∗}.\\nEven though these languages look similar, we will show that A is context-free\\nand B is not context-free.\\nConsider the following context-free grammar, in which S is the start vari-\\nable:\\nS →ϵ|aSa|bSb.\\nIt is easy to see that the language of this grammar is exactly the language A.\\nTherefore, A is context-free. Alternatively, we can show that A is context-\\nfree, by constructing a (nondeterministic) pushdown automaton that accepts\\nA. This automaton has two states q and q′, where q is the start state. If the\\nautomaton is in state q, then it did not yet ﬁnish reading the leftmost half of\\nthe input string; it pushes all symbols read onto the stack. If the automaton\\nis in state q′, then it is reading the rightmost half of the input string; for each\\nsymbol read, it checks whether it is equal to the symbol on top of the stack\\nand, if so, pops the top symbol from the stack. The pushdown automaton\\nuses nondeterminism to “guess” when to switch from state q to state q′ (i.e.,\\nwhen it has completed reading the leftmost half of the input string).\\n130\\nChapter 3.\\nContext-Free Languages\\nAt this point, you should convince yourself that the two approaches above,\\nwhich showed that A is context-free, do not work for B. The reason why\\nthey do not work is that the language B is not context-free, as we will prove\\nnow.\\nAssume that B is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. At this point, we must choose a\\nstring s in B, whose length is at least p, and that does not satisfy the three\\nproperties stated in the pumping lemma. Let us try the string s = apbapb.\\nThen s ∈B and |s| = 2p + 2 ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = uvxyz, where (i) |vy| ≥1, (ii) |vxy| ≤p, and (iii) uvixyiz ∈B\\nfor all i ≥0. It may happen that p ≥3, u = ap−1, v = a, x = b, y = a,\\nand z = ap−1b. If this is the case, then properties (i), (ii), and (iii) hold,\\nand, thus, we do not get a contradiction. In other words, we have chosen\\nthe “wrong” string s. This string is “wrong”, because there is only one b\\nbetween the as. Because of this, v can be in the leftmost block of as, and\\ny can be in the rightmost block of as. Observe that if there were at least p\\nmany bs between the as, then this would not happen, because |vxy| ≤p.\\nBased on the discussion above, we choose s = apbpapbp. Observe that\\ns ∈B and |s| = 4p ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈B for all i ≥0. Based\\non the location of vxy in the string s, we distinguish three cases:\\nCase 1: The substring vxy overlaps both the leftmost half and the rightmost\\nhalf of s.\\nSince |vxy| ≤p, the substring vxy is contained in the “middle” part of s,\\ni.e., vxy is contained in the block bpap. Consider the string uv0xy0z = uxz.\\nSince |vy| ≥1, we know that at least one of v and y is non-empty.\\n• If v ̸= ϵ, then v contains at least one b from the leftmost block of bs in\\ns, whereas y does not contain any b from the rightmost block of bs in s.\\nTherefore, in the string uxz, the leftmost block of bs contains fewer bs\\nthan the rightmost block of bs. Hence, the string uxz is not contained\\nin B.\\n• If y ̸= ϵ, then y contains at least one a from the rightmost block of\\nas in s, whereas v does not contain any a from the leftmost block of\\nas in s. Therefore, in the string uxz, the leftmost block of as contains\\nmore as than the rightmost block of as. Hence, the string uxz is not\\ncontained in B.\\n3.8.\\nThe pumping lemma for context-free languages\\n131\\nIn both cases, we conclude that the string uxz is not an element of the\\nlanguage B. But, by the pumping lemma, this string is contained in B.\\nCase 2: The substring vxy is in the leftmost half of s.\\nIn this case, none of the strings uxz, uv2xy2z, uv3xy3z, uv4xy4z, etc.,\\nis contained in B.\\nBut, by the pumping lemma, each of these strings is\\ncontained in B.\\nCase 3: The substring vxy is in the rightmost half of s.\\nThis case is symmetric to Case 2: None of the strings uxz, uv2xy2z,\\nuv3xy3z, uv4xy4z, etc., is contained in B. But, by the pumping lemma, each\\nof these strings is contained in B.\\nTo summarize, in each of the three cases, we have obtained a contradic-\\ntion. Therefore, the language B is not context-free.\\nThird example\\nWe have seen in Section 3.2.4 that the language\\n{ambncm+n : m ≥0, n ≥0}\\nis context-free. Using the pumping lemma for regular languages, it is easy to\\nprove that this language is not regular. In other words, context-free gram-\\nmars can verify addition, whereas ﬁnite automata are not powerful enough\\nfor this. We now consider the problem of verifying multiplication: Let A be\\nthe language deﬁned as\\nA = {ambncmn : m ≥0, n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is context-free. Let p ≥1 be the pumping length, as\\ngiven by the pumping lemma. Consider the string s = apbpcp2. Then, s ∈A\\nand |s| = 2p + p2 ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all i ≥0.\\nThere are three possible cases, depending on the locations of v and y in\\nthe string s.\\nCase 1: The substring v does not contain any a and does not contain any\\nb, and the substring y does not contain any a and does not contain any b.\\n132\\nChapter 3.\\nContext-Free Languages\\nConsider the string uv2xy2z. Since |vy| ≥1, this string consists of p\\nmany as, p many bs, but more than p2 many cs. Therefore, this string is not\\ncontained in A. But, by the pumping lemma, it is contained in A.\\nCase 2: The substring v does not contain any c and the substring y does\\nnot contain any c.\\nConsider again the string uv2xy2z. This string consists of p2 many cs.\\nSince |vy| ≥1, in this string,\\n• the number of as is at least p + 1 and the number of bs is at least p, or\\n• the number of as is at least p and the number of bs is at least p + 1.\\nTherefore, the number of as multiplied by the number of bs is at least p(p+1),\\nwhich is larger than p2. Therefore, uv2xy2z is not contained in A. But, by\\nthe pumping lemma, this string is contained in A.\\nCase 3: The substring v contains at least one b and the substring y contains\\nat least one c.\\nSince |vxy| ≤p, the substring vy does not contain any a. Thus, we can\\nwrite vy = bjck, where j ≥1 and k ≥1. Consider the string uxz. We can\\nwrite this string as uxz = apbp−jcp2−k. Since, by the pumping lemma, this\\nstring is contained in A, we have p(p−j) = p2−k, which implies that jp = k.\\nThus,\\n|vxy| ≥|vy| = j + k = j + jp ≥1 + p.\\nBut, by the pumping lemma, we have |vxy| ≤p.\\nObserve that, since |vxy| ≤p, the above three cases cover all possibilities\\nfor the locations of v and y in the string s. In each of the three cases, we\\nhave obtained a contradiction. Therefore, the language A is not context-free.\\nExercises\\n3.1 Construct context-free grammars that generate the following languages.\\nIn all cases, Σ = {0, 1}.\\n• {02n1n : n ≥0}\\n• {w : w contains at least three 1s}\\n• {w : the length of w is odd and its middle symbol is 0}\\nExercises\\n133\\n• {w : w is a palindrome}.\\nA palindrome is a string w having the property that w = wR, i.e.,\\nreading w from left to right gives the same result as reading w from\\nright to left.\\n• {w : w starts and ends with the same symbol}\\n• {w : w starts and ends with diﬀerent symbols}\\n3.2 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {0, 1}, S is the start variable, and R consists of the rules\\nS\\n→\\n0S|1A|ϵ\\nA\\n→\\n0B|1S\\nB\\n→\\n0A|1B\\nDeﬁne the following language L:\\nL = {w ∈{0, 1}∗:\\nw is the binary representation of a non-negative\\ninteger that is divisible by three } ∪{ϵ}.\\nProve that L = L(G). (Hint: The variables S, A, and B are used to\\nremember the remainder after division by three.)\\n3.3 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {a, b}, S is the start variable, and R consists of the rules\\nS\\n→\\naB|bA\\nA\\n→\\na|aS|BAA\\nB\\n→\\nb|bS|ABB\\n• Prove that ababba ∈L(G).\\n• Prove that L(G) is the set of all non-empty strings w over the alphabet\\n{a, b} such that the number of as in w is equal to the number of bs in\\nw.\\n3.4 Let A and B be context-free languages over the same alphabet Σ.\\n• Prove that the union A ∪B of A and B is also context-free.\\n• Prove that the concatenation AB of A and B is also context-free.\\n134\\nChapter 3.\\nContext-Free Languages\\n• Prove that the star A∗of A is also context-free.\\n3.5 Deﬁne the following two languages A and B:\\nA = {ambncn : m ≥0, n ≥0}\\nand\\nB = {ambmcn : m ≥0, n ≥0}.\\n• Prove that both A and B are context-free, by constructing two context-\\nfree grammars, one that generates A and one that generates B.\\n• We have seen in Section 3.8.2 that the language\\n{anbncn : n ≥0}\\nis not context-free. Explain why this implies that the intersection of\\ntwo context-free languages is not necessarily context-free.\\n• Use De Morgan’s Law to conclude that the complement of a context-\\nfree language is not necessarily context-free.\\n3.6 Let A be a context-free language and let B be a regular language.\\n• Prove that the intersection A ∩B of A and B is context-free.\\n• Prove that the set-diﬀerence\\nA \\\\ B = {w : w ∈A, w ̸∈B}\\nof A and B is context-free.\\n• Is the set-diﬀerence of two context-free languages necessarily context-\\nfree?\\n3.7 Let L be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that\\n• the number of as in w is equal to the number of bs in w,\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\nExercises\\n135\\nIn this exercise, you will prove that L is context-free.\\nLet A be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that the number of as in w is equal to the number of bs\\nin w. In Exercise 3.3, you have shown that A is context-free.\\nLet B be the language consisting of all strings w over the alphabet {a, b}\\nsuch that\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\n1. Give a regular expression that describes the complement of B.\\n2. Argue that B is a regular language.\\n3. Use Exercise 3.6 to argue that L is a context-free language.\\n3.8 Construct (deterministic or nondeterministic) pushdown automata that\\naccept the following languages.\\n1. {02n1n : n ≥0}.\\n2. {0n1m0n : n ≥1, m ≥1}.\\n3. {w ∈{0, 1}∗: w contains more 1s than 0s}.\\n4. {wwR : w ∈{0, 1}∗}.\\n(If w = w1 . . . wn, then wR = wn . . . w1.)\\n5. {w ∈{0, 1}∗: w is a palindrome}.\\n3.9 Let L be the language\\nL = {ambn : 0 ≤m ≤n ≤2m}.\\n1. Prove that L is context-free, by constructing a context-free grammar\\nwhose language is equal to L.\\n2. Prove that L is context-free, by constructing a nondeterministic push-\\ndown automaton that accepts L.\\n3.10 Prove that the following languages are not context-free.\\n136\\nChapter 3.\\nContext-Free Languages\\n• {an b a2n b a3n : n ≥0}.\\n• {anbnanbn : n ≥0}.\\n• {ambnck : m ≥0, n ≥0, k = max(m, n)}.\\n• {w#x : w is a substring of x, and w, x ∈{a, b}∗}.\\nFor example, the string aba#abbababbb is in the language, whereas the\\nstring aba#baabbaabb is not in the language. The alphabet is {a, b, #}.\\n•\\n{ w ∈{a, b, c}∗\\n:\\nw contains more b’s than a’s and\\nw contains more c’s than a’s }.\\n• {1n : n is a prime number}.\\n• {(abn)n : n ≥0}. (The parentheses are not part of the alphabet; thus,\\nthe alphabet is {a, b, }.)\\n3.11 Let L be a language consisting of ﬁnitely many strings. Show that L\\nis regular and, therefore, context-free. Let k be the maximum length of any\\nstring in L.\\n• Prove that every context-free grammar in Chomsky normal form that\\ngenerates L has more than log k variables. (The logarithm is in base\\n2.)\\n• Prove that there is a context-free grammar that generates L and that\\nhas only one variable.\\n3.12 Let L be a context-free language. Prove that there exists an integer\\np ≥1, such that the following is true: For every string s in L with |s| ≥p,\\nthere exists a string s′ in L such that |s| < |s′| ≤|s| + p.\\nChapter 4\\nTuring Machines and the\\nChurch-Turing Thesis\\nIn the previous chapters, we have seen several computational devices that\\ncan be used to accept or generate regular and context-free languages. Even\\nthough these two classes of languages are fairly large, we have seen in Sec-\\ntion 3.8.2 that these devices are not powerful enough to accept simple lan-\\nguages such as A = {ambncmn : m ≥0, n ≥0}. In this chapter, we introduce\\nthe Turing machine, which is a simple model of a real computer. Turing ma-\\nchines can be used to accept all context-free languages, but also languages\\nsuch as A. We will argue that every problem that can be solved on a real\\ncomputer can also be solved by a Turing machine (this statement is known\\nas the Church-Turing Thesis). In Chapter 5, we will consider the limitations\\nof Turing machines and, hence, of real computers.\\n4.1\\nDeﬁnition of a Turing machine\\nWe start with an informal description of a Turing machine. Such a machine\\nconsists of the following, see also Figure 4.1.\\n1. There are k tapes, for some ﬁxed k ≥1. Each tape is divided into\\ncells, and is inﬁnite both to the left and to the right. Each cell stores\\na symbol belonging to a ﬁnite set Γ, which is called the tape alphabet.\\nThe tape alphabet contains the blank symbol 2. If a cell contains 2,\\nthen this means that the cell is actually empty.\\n138\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nstate control\\n. . . 2 2 2 a a b a b b a b a b 2 2 2\\n. . .\\n?\\n. . . 2 2 2 b a a b 2 a b 2 2 2\\n. . .\\n?\\nFigure 4.1: A Turing machine with k = 2 tapes.\\n2. Each tape has a tape head which can move along the tape, one cell\\nper move. It can also read the cell it currently scans and replace the\\nsymbol in this cell by another symbol.\\n3. There is a state control, which can be in any one of a ﬁnite number of\\nstates. The ﬁnite set of states is denoted by Q. The set Q contains\\nthree special states: a start state, an accept state, and a reject state.\\nThe Turing machine performs a sequence of computation steps. In one\\nsuch step, it does the following:\\n1. Immediately before the computation step, the Turing machine is in a\\nstate r of Q, and each of the k tape heads is on a certain cell.\\n2. Depending on the current state r and the k symbols that are read by\\nthe tape heads,\\n(a) the Turing machine switches to a state r′ of Q (which may be\\nequal to r),\\n(b) each tape head writes a symbol of Γ in the cell it is currently\\nscanning (this symbol may be equal to the symbol currently stored\\nin the cell), and\\n4.1.\\nDeﬁnition of a Turing machine\\n139\\n(c) each tape head either moves one cell to the left, moves one cell to\\nthe right, or stays at the current cell.\\nWe now give a formal deﬁnition of a deterministic Turing machine.\\nDeﬁnition 4.1.1 A deterministic Turing machine is a 7-tuple\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject),\\nwhere\\n1. Σ is a ﬁnite set, called the input alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the tape alphabet; this alphabet contains the\\nblank symbol 2, and Σ ⊆Γ,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. qaccept is an element of Q; it is called the accept state,\\n6. qreject is an element of Q; it is called the reject state,\\n7. δ is called the transition function, which is a function\\nδ : Q × Γk →Q × Γk × {L, R, N}k.\\nThe transition function δ is basically the “program” of the Turing ma-\\nchine. This function tells us what the machine can do in “one computation\\nstep”: Let r ∈Q, and let a1, a2, . . . , ak ∈Γ.\\nFurthermore, let r′ ∈Q,\\na′\\n1, a′\\n2, . . . , a′\\nk ∈Γ, and σ1, σ2, . . . , σk ∈{L, R, N} be such that\\nδ(r, a1, a2, . . . , ak) = (r′, a′\\n1, a′\\n2, . . . , a′\\nk, σ1, σ2, . . . , σk).\\n(4.1)\\nThis transition means that if\\n• the Turing machine is in state r, and\\n• the head of the i-th tape reads the symbol ai, 1 ≤i ≤k,\\nthen\\n140\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• the Turing machine switches to state r′,\\n• the head of the i-th tape replaces the scanned symbol ai by the symbol\\na′\\ni, 1 ≤i ≤k, and\\n• the head of the i-th tape moves according to σi, 1 ≤i ≤k: if σi = L,\\nthen the tape head moves one cell to the left; if σi = R, then it moves\\none cell to the right; if σi = N, then the tape head does not move.\\nWe will write the computation step (4.1) in the form of the instruction\\nra1a2 . . . ak →r′a′\\n1a′\\n2 . . . a′\\nkσ1σ2 . . . σk.\\nWe now specify the computation of the Turing machine\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject).\\nStart conﬁguration: The input is a string over the input alphabet Σ.\\nInitially, this input string is stored on the ﬁrst tape, and the head of this\\ntape is on the leftmost symbol of the input string. Initially, all other k −1\\ntapes are empty, i.e., only contain blank symbols, and the Turing machine is\\nin the start state q.\\nComputation and termination: Starting in the start conﬁguration, the\\nTuring machine performs a sequence of computation steps as described above.\\nThe computation terminates at the moment when the Turing machine en-\\nters the accept state qaccept or the reject state qreject. (Hence, if the Turing\\nmachine never enters the states qaccept and qreject, the computation does not\\nterminate.)\\nAcceptance: The Turing machine M accepts the input string w ∈Σ∗, if the\\ncomputation on this input terminates in the state qaccept. If the computation\\non this input terminates in the state qreject, then M rejects the input string\\nw.\\nWe denote by L(M) the language accepted by the Turing machine M.\\nThus, L(M) is the set of all strings in Σ∗that are accepted by M.\\nObserve that a string w ∈Σ∗does not belong to L(M) if and only if on\\ninput w,\\n• the computation of M terminates in the state qreject or\\n• the computation of M does not terminate.\\n4.2.\\nExamples of Turing machines\\n141\\n4.2\\nExamples of Turing machines\\n4.2.1\\nAccepting palindromes using one tape\\nWe will show how to construct a Turing machine with one tape, that decides\\nwhether or not any input string w ∈{a, b}∗is a palindrome. Recall that the\\nstring w is called a palindrome, if reading w from left to right gives the same\\nresult as reading w from right to left. Examples of palindromes are abba,\\nbaabbbbaab, and the empty string ϵ.\\nStart of the computation: The tape contains the input string w, the tape\\nhead is on the leftmost symbol of w, and the Turing machine is in the start\\nstate q0.\\nIdea: The tape head reads the leftmost symbol of w, deletes this symbol\\nand “remembers” it by means of a state.\\nThen the tape head moves to\\nthe rightmost symbol and tests whether it is equal to the (already deleted)\\nleftmost symbol.\\n• If they are equal, then the rightmost symbol is deleted, the tape head\\nmoves to the new leftmost symbol, and the whole process is repeated.\\n• If they are not equal, the Turing machine enters the reject state, and\\nthe computation terminates.\\nThe Turing machine enters the accept state as soon as the string currently\\nstored on the tape is empty.\\nWe will use the input alphabet Σ = {a, b} and the tape alphabet Γ =\\n{a, b, 2}. The set Q of states consists of the following eight states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost symbol was a; tape head is moving to the right\\nqb :\\nleftmost symbol was b; tape head is moving to the right\\nq′\\na :\\nreached rightmost symbol; test whether it is equal to a, and delete it\\nq′\\nb :\\nreached rightmost symbol; test whether it is equal to b, and delete it\\nqL :\\ntest was positive; tape head is moving to the left\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n142\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nqba →qbaR\\nq0b →qb2R\\nqab →qabR\\nqbb →qbbR\\nq02 →qaccept\\nqa2 →q′\\na2L\\nqb2 →q′\\nb2L\\nq′\\naa →qL2L\\nq′\\nba →qreject\\nqLa →qLaL\\nq′\\nab →qreject\\nq′\\nbb →qL2L\\nqLb →qLbL\\nq′\\na2 →qaccept\\nq′\\nb2 →qaccept\\nqL2 →q02R\\nYou should go through the computation of this Turing machine for some\\nsample inputs, for example abba, b, abb and the empty string (which is a\\npalindrome).\\n4.2.2\\nAccepting palindromes using two tapes\\nWe again consider the palindrome problem, but now we use a Turing machine\\nwith two tapes.\\nStart of the computation: The ﬁrst tape contains the input string w and\\nthe head of the ﬁrst tape is on the leftmost symbol of w. The second tape is\\nempty and its tape head is at an arbitrary position. The Turing machine is\\nin the start state q0.\\nIdea: First, the input string w is copied to the second tape. Then the head\\nof the ﬁrst tape moves back to the leftmost symbol of w, while the head of\\nthe second tape stays at the rightmost symbol of w. Finally, the actual test\\nstarts: The head of the ﬁrst tape moves to the right and, at the same time,\\nthe head of the second tape moves to the left. While moving, the Turing\\nmachine tests whether the two tape heads read the same symbol in each\\nstep.\\nThe input alphabet is Σ = {a, b} and the tape alphabet is Γ = {a, b, 2}.\\nThe set Q of states consists of the following ﬁve states:\\nq0 :\\nstart state; copy w to the second tape\\nq1 :\\nw has been copied; head of ﬁrst tape moves to the left\\nq2 :\\nhead of ﬁrst tape moves to the right; head of second tape moves\\nto the left; until now, all tests were positive\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n4.2.\\nExamples of Turing machines\\n143\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a2 →q0aaRR\\nq1aa →q1aaLN\\nq0b2 →q0bbRR\\nq1ab →q1abLN\\nq022 →q122LL\\nq1ba →q1baLN\\nq1bb →q1bbLN\\nq12a →q22aRN\\nq12b →q22bRN\\nq122 →qaccept\\nq2aa →q2aaRL\\nq2ab →qreject\\nq2ba →qreject\\nq2bb →q2bbRL\\nq222 →qaccept\\nAgain, you should run this Turing machine for some sample inputs.\\n4.2.3\\nAccepting anbncn using one tape\\nWe will construct1 a Turing machine with one tape that accepts the language\\n{anbncn : n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: In the previous examples, the tape alphabet Γ was equal to the union\\nof the input alphabet Σ and {2}. In this example, we will add one symbol\\nd to the tape alphabet. As we will see, this simpliﬁes the construction of\\nthe Turing machine. Thus, the input alphabet is Σ = {a, b, c} and the tape\\nalphabet is Γ = {a, b, c, d, 2}. Recall that the input string w belongs to Σ∗.\\nThe general approach is to split the computation into two stages.\\n1Thanks to Michael Fleming for pointing out an error in a previous version of this\\nconstruction.\\n144\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nStage 1: In this stage, we check if the string w is in the language described\\nby the regular expression a∗b∗c∗. If this is the case, then we walk back to\\nthe leftmost symbol. For this stage, we use the following states, besides the\\nstates qaccept and qreject:\\nqa :\\nstart state; we are reading the block of a’s\\nqb :\\nwe are reading the block of b’s\\nqc :\\nwe are reading the block of c’s\\nqL :\\nwalk to the leftmost symbol\\nStage 2: In this stage, we repeat the following: Walk along the string from\\nleft to right, replace the leftmost a by d, replace the leftmost b by d, replace\\nthe leftmost c by d, and walk back to the leftmost symbol.\\nFor this stage, we use the following states:\\nq′\\na :\\nstart state of Stage 2; search for the leftmost a\\nq′\\nb :\\nleftmost a has been replaced by d;\\nsearch for the leftmost b\\nq′\\nc :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nsearch for the leftmost c\\nq′\\nL :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nleftmost c has been replaced by d;\\nwalk to the leftmost symbol\\nThe transition function δ is speciﬁed by the following instructions:\\nqaa →qaaR\\nqba →qreject\\nqab →qbbR\\nqbb →qbbR\\nqac →qccR\\nqbc →qccR\\nqad →cannot happen\\nqbd →cannot happen\\nqa2 →qL2L\\nqb2 →qL2L\\nqca →qreject\\nqLa →qLaL\\nqcb →qreject\\nqLb →qLbL\\nqcc →qccR\\nqLc →qLcL\\nqcd →cannot happen\\nqLd →cannot happen\\nqc2 →qL2L\\nqL2 →q′\\na2R\\n4.2.\\nExamples of Turing machines\\n145\\nq′\\naa →q′\\nbdR\\nq′\\nba →q′\\nbaR\\nq′\\nab →qreject\\nq′\\nbb →q′\\ncdR\\nq′\\nac →qreject\\nq′\\nbc →qreject\\nq′\\nad →q′\\nadR\\nq′\\nbd →q′\\nbdR\\nq′\\na2 →qaccept\\nq′\\nb2 →qreject\\nq′\\nca →qreject\\nq′\\nLa →q′\\nLaL\\nq′\\ncb →q′\\ncbR\\nq′\\nLb →q′\\nLbL\\nq′\\ncc →q′\\nLdL\\nq′\\nLc →q′\\nLcL\\nq′\\ncd →q′\\ncdR\\nq′\\nLd →q′\\nLdL\\nq′\\nc2 →qreject\\nq′\\nL2 →q′\\na2R\\nWe remark that Stage 1 is really necessary for this Turing machine: If we\\nomit this stage, and use only Stage 2, then the string aabcbc will be accepted.\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2}\\nWe consider again the language {anbncn : n ≥0}. In the previous section,\\nwe presented a Turing machine that uses an extra symbol d. The reader may\\nwonder if we can construct a Turing machine for this language that does not\\nuse any extra symbols. We will show below that this is indeed possible.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate q0.\\nIdea: Repeat the following Stages 1 and 2, until the string is empty.\\nStage 1. Walk along the string from left to right, delete the leftmost a,\\ndelete the leftmost b, and delete the rightmost c.\\nStage 2. Shift the substring of bs and cs one position to the left; then walk\\nback to the leftmost symbol.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, 2}.\\n146\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nFor Stage 1, we use the following states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost a has been deleted; have not read b\\nqb :\\nleftmost b has been deleted; have not read c\\nqc :\\nleftmost c has been read; tape head moves to the right\\nq′\\nc :\\ntape head is on the rightmost c\\nq1 :\\nrightmost c has been deleted; tape head is on the rightmost\\nsymbol or 2\\nqaccept :\\naccept state\\nqreject :\\nreject state\\nThe transitions for Stage 1 are speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nq0b →qreject\\nqab →qb2R\\nq0c →qreject\\nqac →qreject\\nq02 →qaccept\\nqa2 →qreject\\nqba →qreject\\nqca →qreject\\nqbb →qbbR\\nqcb →qreject\\nqbc →qccR\\nqcc →qccR\\nqb2 →qreject\\nqc2 →q′\\nc2L\\nq′\\ncc →q12L\\nFor Stage 2, we use the following states:\\nq1 :\\nas above; tape head is on the rightmost symbol or on 2\\nqc :\\ncopy c one cell to the left\\nqb :\\ncopy b one cell to the left\\nq2 :\\ndone with shifting; head moves to the left\\nAdditionally, we use a state q′\\n1 which has the following meaning: If the input\\nstring is of the form aibc, for some i ≥1, then after Stage 1, the tape contains\\nthe string ai−122, the tape head is on the 2 immediately to the right of the\\nas, and the Turing machine is in state q1. In this case, we move one cell to\\nthe left; if we then read 2, then i = 1, and we accept; otherwise, we read a,\\nand we reject.\\n4.2.\\nExamples of Turing machines\\n147\\nThe transitions for Stage 2 are speciﬁed by the following instructions:\\nq1a →cannot happen\\nq′\\n1a →qreject\\nq1b →qreject\\nq′\\n1b →cannot happen\\nq1c →qc2L\\nq′\\n1c →cannot happen\\nq12 →q′\\n12L\\nq′\\n12 →qaccept\\nqca →cannot happen\\nqba →cannot happen\\nqcb →qbcL\\nqbb →qbbL\\nqcc →qccL\\nqbc →cannot happen\\nqc2 →qreject\\nqb2 →q2bL\\nq2a →q2aL\\nq2b →cannot happen\\nq2c →cannot happen\\nq22 →q02R\\n4.2.5\\nAccepting ambncmn using one tape\\nWe will sketch how to construct a Turing machine with one tape that accepts\\nthe language\\n{ambncmn : m ≥0, n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, $, 2},\\nwhere the purpose of the symbol $ will become clear below.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: Observe that a string ambnck is in the language if and only if for every\\na, the string contains n many cs. Based on this, the computation consists of\\nthe following stages:\\nStage 1. Walk along the input string w from left to right and check whether\\nw is an element of the language described by the regular expression a∗b∗c∗.\\nIf this is not the case, then reject the input string. Otherwise, go to Stage 2.\\nStage 2. Walk back to the leftmost symbol of w. Go to Stage 3.\\nStage 3. In this stage, the Turing machine does the following:\\n148\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• Replace the leftmost a by the blank symbol 2.\\n• Walk to the leftmost b.\\n• Zigzag between the bs and cs; each time, replace the leftmost b by the\\nsymbol $, and replace the rightmost c by the blank symbol 2. If, for\\nsome b, there is no c left, the Turing machine rejects the input string.\\n• Continue zigzagging until there are no bs left. Then go to Stage 4.\\nObserve that in this third stage, the string ambnck is transformed to the\\nstring am−1$nck−n.\\nStage 4. In this stage, the Turing machine does the following:\\n• Replace each $ by b.\\n• Walk to the leftmost a.\\nHence, in this fourth stage, the string am−1$nck−n is transformed to the string\\nam−1bnck−n.\\nObserve that the input string ambnck is in the language if and only if the\\nstring am−1bnck−n is in the language. Therefore, the Turing machine repeats\\nStages 3 and 4, until there are no as left. At that moment, it checks whether\\nthere are any cs left; if so, it rejects the input string; otherwise, it accepts\\nthe input string.\\nWe hope that you believe that this description of the algorithm can be\\nturned into a formal description of a Turing machine.\\n4.3\\nMulti-tape Turing machines\\nIn Section 4.2, we have seen two Turing machines that accept palindromes;\\nthe ﬁrst Turing machine has one tape, whereas the second one has two tapes.\\nYou will have noticed that the two-tape Turing machine was easier to obtain\\nthan the one-tape Turing machine. This leads to the question whether multi-\\ntape Turing machines are more powerful than their one-tape counterparts.\\nThe answer is “no”:\\nTheorem 4.3.1 Let k ≥1 be an integer. Any k-tape Turing machine can\\nbe converted to an equivalent one-tape Turing machine.\\n4.3.\\nMulti-tape Turing machines\\n149\\nProof.2\\nWe will sketch the proof for the case when k = 2.\\nLet M =\\n(Σ, Γ, Q, δ, q, qaccept, qreject) be a two-tape Turing machine.\\nOur goal is to\\nconvert M to an equivalent one-tape Turing machine N. That is, N should\\nhave the property that for all strings w ∈Σ∗,\\n• M accepts w if and only if N accepts w,\\n• M rejects w if and only if N rejects w,\\n• M does not terminate on input w if and only if N does not terminate\\non input w.\\nThe tape alphabet of the one-tape Turing machine N is\\nΓ ∪{ ˙x : x ∈Γ} ∪{#}.\\nIn words, we take the tape alphabet Γ of M, and add, for each x ∈Γ, the\\nsymbol ˙x. Moreover, we add a special symbol #.\\nThe Turing machine N will be deﬁned in such a way that any conﬁgura-\\ntion of the two-tape Turing machine M, for example\\n. . . 2 1 0 0 1 2 . . .\\n6\\n. . . 2 a a b a 2 . . .\\n6\\ncorresponds to the following conﬁguration of the one-tape Turing machine\\nN:\\n. . .\\n2\\n#\\n1\\n0\\n˙0\\n1\\n#\\na\\n˙a\\nb\\na\\n#\\n2 . . .\\n6\\n2Thanks to Sergio Cabello for pointing out an error in a previous version of this proof.\\n150\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThus, the contents of the two tapes of M are encoded on the single tape of\\nN. The dotted symbols are used to indicate the positions of the two tape\\nheads of M, whereas the three occurrences of the special symbol # are used\\nto mark the boundaries of the strings on the two tapes of M.\\nThe Turing machine N simulates one computation step of M, in the\\nfollowing way:\\n• Throughout the simulation of this step, N “remembers” the current\\nstate of M.\\n• At the start of the simulation, the tape head of N is on the leftmost\\nsymbol #.\\n• N walks along the string to the right until it ﬁnds the ﬁrst dotted\\nsymbol. (This symbol indicates the location of the head on the ﬁrst tape\\nof M.) N remembers this ﬁrst dotted symbol and continues walking\\nto the right until it ﬁnds the second dotted symbol.\\n(This symbol\\nindicates the location of the head on the second tape of M.) Again, N\\nremembers this second dotted symbol.\\n• At this moment, N is still at the second dotted symbol. N updates\\nthis part of the tape, by making the change that M would make on its\\nsecond tape. (This change is given by the transition function of M; it\\ndepends on the current state of M and the two symbols that M reads\\non its two tapes.)\\n• N walks to the left until it ﬁnds the ﬁrst dotted symbol.\\nThen, it\\nupdates this part of the tape, by making the change that M would\\nmake on its ﬁrst tape.\\n• In the previous two steps, in which the tape is updated, it may be\\nnecessary to shift a part of the tape.\\n• Finally, N remembers the new state of M and walks back to the left-\\nmost symbol #.\\nIt should be clear that the Turing machine N can be constructed by\\nintroducing appropriate states.\\n4.4.\\nThe Church-Turing Thesis\\n151\\n4.4\\nThe Church-Turing Thesis\\nWe all have some intuitive notion of what an algorithm is. This notion will\\nprobably be something like “an algorithm is a procedure consisting of com-\\nputation steps that can be speciﬁed in a ﬁnite amount of text”. For example,\\nany “computational process” that can be speciﬁed by a Java program, should\\nbe considered an algorithm. Similarly, a Turing machine speciﬁes a “com-\\nputational process” and, therefore, should be considered an algorithm. This\\nleads to the question of whether it is possible to give a mathematical deﬁni-\\ntion of an “algorithm”. We just saw that every Java program represents an\\nalgorithm and that every Turing machine also represents an algorithm. Are\\nthese two notions of an algorithm equivalent? The answer is “yes”. In fact,\\nthe following theorem states that many diﬀerent notions of “computational\\nprocess” are equivalent. (We hope that you have gained suﬃcient intuition,\\nso that none of the claims in this theorem comes as a surprise to you.)\\nTheorem 4.4.1 The following computation models are equivalent, i.e., any\\none of them can be converted to any other one:\\n1. One-tape Turing machines.\\n2. k-tape Turing machines, for any k ≥1.\\n3. Non-deterministic Turing machines.\\n4. Java programs.\\n5. C++ programs.\\n6. Lisp programs.\\nIn other words, if we deﬁne the notion of an algorithm using any of the\\nmodels in this theorem, then it does not matter which model we take: All\\nthese models give the same notion of an algorithm.\\nThe problem of deﬁning the notion of an algorithm goes back to David\\nHilbert. On August 8, 1900, at the Second International Congress of Math-\\nematicians in Paris, Hilbert presented a list of problems that he considered\\ncrucial for the further development of mathematics. Hilbert’s 10th problem\\nis the following:\\n152\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nDoes there exist a ﬁnite process that decides whether or not any\\ngiven polynomial with integer coeﬃcients has integral roots?\\nOf course, in our language, Hilbert asked whether or not there exists an\\nalgorithm that decides, when given an arbitrary polynomial equation (with\\ninteger coeﬃcients) such as\\n12x3y7z5 + 7x2y4z −x4 + y2z7 −z3 + 10 = 0,\\nwhether or not this equation has a solution in integers. In 1970, Matiyasevich\\nproved that such an algorithm does not exist. Of course, in order to prove\\nthis claim, we ﬁrst have to agree on what an algorithm is. In the beginning\\nof the twentieth century, mathematicians gave several deﬁnitions, such as\\nTuring machines (1936) and the λ-calculus (1936), and they proved that all\\nthese are equivalent. Later, after programming languages were invented, it\\nwas shown that these older notions of an algorithm are equivalent to notions\\nof an algorithm that are based on C programs, Java programs, Lisp programs,\\nPascal programs, etc.\\nIn other words, all attempts to give a rigorous deﬁnition of the notion of\\nan algorithm led to the same concept. Because of this, computer scientists\\nnowadays agree on what is called the Church-Turing Thesis:\\nChurch-Turing Thesis:\\nEvery computational process that is intuitively\\nconsidered to be an algorithm can be converted to a Turing machine.\\nIn other words, this basically states that we deﬁne an algorithm to be a\\nTuring machine. At this point, you should ask yourself, whether the Church-\\nTuring Thesis can be proved. Alternatively, what has to be done in order to\\ndisprove this thesis?\\nExercises\\n4.1 Construct a Turing machine with one tape, that accepts the language\\n{02n1n : n ≥0}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\nExercises\\n153\\n4.2 Construct a Turing machine with one tape, that accepts the language\\n{w : w contains twice as many 0s as 1s}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\n4.3 Let A be the language\\nA\\n=\\n{ w ∈{a, b, c}∗\\n:\\nw contains more bs than as and\\nw contains more cs than as }.\\nGive an informal description (in plain English) of a Turing machine with one\\ntape, that accepts the language A.\\n4.4 Construct a Turing machine with one tape that receives as input a non-\\nnegative integer x and returns as output the integer x + 1.\\nIntegers are\\nrepresented as binary strings.\\nStart of the computation: The tape contains the binary representation\\nof the input x. The tape head is on the leftmost symbol and the Turing\\nmachine is in the start state q0. For example, if x = 431, the tape looks as\\nfollows:\\n. . . 2 2 2 1 1 0 1 0 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x + 1. The tape head is on the leftmost symbol and the Turing\\nmachine is in the ﬁnal state q1. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 0 1 1 0 0 0 0 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state q1. As soon as state q1 is entered,\\nthe Turing machine terminates. At termination, the contents of the tape is\\nthe output of the Turing machine.\\n154\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n4.5 Construct a Turing machine with two tapes that receives as input two\\nnon-negative integers x and y, and returns as output the integer x + y.\\nIntegers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost bit\\nof y. At the start, the Turing machine is in the start state q0.\\nEnd of the computation: The ﬁrst tape contains the binary representation\\nof x and its head is on the rightmost symbol of x. The second tape contains\\nthe binary representation of the integer x + y (thus, the integer y is “gone”).\\nThe head of the second tape is on the rightmost bit of x + y. The Turing\\nmachine is in the ﬁnal state q1.\\n4.6 Give an informal description (in plain English) of a Turing machine with\\none tape that receives as input two non-negative integers x and y, and returns\\nas output the integer x+y. Integers are represented as binary strings. If you\\nare an adventurous student, you may give a formal deﬁnition of your Turing\\nmachine.\\n4.7 Construct a Turing machine with one tape that receives as input an\\ninteger x ≥1 and returns as output the integer x−1. Integers are represented\\nin binary.\\nStart of the computation: The tape contains the binary representation of\\nthe input x. The tape head is on the rightmost symbol of x and the Turing\\nmachine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x −1. The tape head is on the rightmost bit of x −1 and the\\nTuring machine is in the ﬁnal state q1.\\n4.8 Give an informal description (in plain English) of a Turing machine with\\nthree tapes that receives as input two non-negative integers x and y, and\\nreturns as output the integer xy. Integers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost sym-\\nbol of y. The third tape is empty and its head is at an arbitrary location.\\nThe Turing machine is in the start state q0.\\nExercises\\n155\\nEnd of the computation: The ﬁrst and second tapes are empty. The third\\ntape contains the binary representation of the product xy and its head is on\\nthe rightmost bit of xy. The Turing machine is in the ﬁnal state q1.\\nHint: Use the Turing machines of Exercises 4.5 and 4.7.\\n4.9 Construct a Turing machine with one tape that receives as input a string\\nof the form 1n for some integer n ≥0; thus, the input is a string of n many\\n1s. The output of the Turing machine is the string 1n21n. Thus, this Turing\\nmachine makes a copy of its input.\\nThe input alphabet is Σ = {1} and the tape alphabet is Γ = {1, 2}.\\nStart of the computation: The tape contains a string of the form 1n, for\\nsome integer n ≥0, the tape head is on the leftmost symbol, and the Turing\\nmachine is in the start state. For example, if n = 4, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the string 1n21n, the tape\\nhead is on the 2 in the middle of this string, and the Turing machine is in\\nthe ﬁnal state. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 1 1 1 1 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this state is entered, the\\nTuring machine terminates. At termination, the contents of the tape is the\\noutput of the Turing machine.\\n156\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nChapter 5\\nDecidable and Undecidable\\nLanguages\\nWe have seen in Chapter 4 that Turing machines form a model for “everything\\nthat is intuitively computable”. In this chapter, we consider the limitations\\nof Turing machines. That is, we ask ourselves the question whether or not\\n“everything” is computable. As we will see, the answer is “no”. In fact, we\\nwill even see that “most” problems are not solvable by Turing machines and,\\ntherefore, not solvable by computers.\\n5.1\\nDecidability\\nIn Chapter 4, we have deﬁned when a Turing machine accepts an input string\\nand when it rejects an input string. Based on this, we deﬁne the following\\nclass of languages.\\nDeﬁnition 5.1.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is decidable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the reject state.\\n158\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is decidable, if there exists an algorithm\\nthat (i) terminates on every input string w, and (ii) correctly tells us whether\\nw ∈A or w ̸∈A.\\nA language A that is not decidable is called undecidable.\\nFor such a\\nlanguage, there does not exist an algorithm that satisﬁes (i) and (ii) above.\\nIn Section 4.2, we have seen several examples of languages that are de-\\ncidable.\\nIn the following subsections, we will give some examples of decidable and\\nundecidable languages. These examples involve languages A whose elements\\nare pairs of the form (C, w), where C is some computation model (for ex-\\nample, a deterministic ﬁnite automaton) and w is a string over the alphabet\\nΣ. The pair (C, w) is in the language A if and only if the string w is in the\\nlanguage of the computation model C. For diﬀerent computation models C,\\nwe will ask the question whether A is decidable, i.e., whether an algorithm\\nexists that decides, for any input (C, w), whether or not this input belongs\\nto the language A. Since the input to any algorithm is a string over some\\nalphabet, we must encode the pair (C, w) as a string. In all cases that we\\nconsider, such a pair can be described using a ﬁnite amount of text. There-\\nfore, we assume, without loss of generality, that binary strings are used for\\nthese encodings. Throughout the rest of this chapter, we will denote the\\nbinary encoding of a pair (C, w) by\\n⟨C, w⟩.\\n5.1.1\\nThe language ADFA\\nWe deﬁne the following language:\\nADFA = {⟨M, w⟩:\\nM is a deterministic ﬁnite automaton that\\naccepts the string w}.\\nKeep in mind that ⟨M, w⟩denotes the binary string that forms an en-\\ncoding of the ﬁnite automaton M and the string w that is given as input to\\nM.\\nWe claim that the language ADFA is decidable. In order to prove this,\\nwe have to construct an algorithm with the following property, for any given\\ninput string u:\\n• If u is the encoding of a deterministic ﬁnite automaton M and a string\\nw (i.e., u is in the correct format ⟨M, w⟩), and if M accepts w, then\\n5.1.\\nDecidability\\n159\\nthe algorithm terminates in its accept state.\\n• In all other cases, the algorithm terminates in its reject state.\\nAn algorithm that exactly does this, is easy to obtain: On input u, the algo-\\nrithm ﬁrst checks whether or not u encodes a deterministic ﬁnite automaton\\nM and a string w. If this is not the case, then it terminates and rejects\\nthe input string u. Otherwise, the algorithm “constructs” M and w, and\\nthen simulates the computation of M on the input string w. If M accepts\\nw, then the algorithm terminates and accepts the input string u. If M does\\nnot accept w, then the algorithm terminates and rejects the input string u.\\nThus, we have proved the following result:\\nTheorem 5.1.2 The language ADFA is decidable.\\n5.1.2\\nThe language ANFA\\nWe deﬁne the following language:\\nANFA = {⟨M, w⟩:\\nM is a nondeterministic ﬁnite automaton that\\naccepts the string w}.\\nTo prove that this language is decidable, consider the algorithm that\\ndoes the following: On input u, the algorithm ﬁrst checks whether or not\\nu encodes a nondeterministic ﬁnite automaton M and a string w. If this is\\nnot the case, then it terminates and rejects the input string u. Otherwise,\\nthe algorithm constructs M and w. Since a computation of M (on input w)\\nis not unique, the algorithm ﬁrst converts M to an equivalent deterministic\\nﬁnite automaton N. Then, it proceeds as in Section 5.1.1.\\nObserve that the construction for converting a nondeterministic ﬁnite au-\\ntomaton to a deterministic ﬁnite automaton (see Section 2.5) is algorithmic,\\nin the sense that it can be described by an algorithm. Because of this, the\\nalgorithm described above is a valid algorithm; it accepts all strings u that\\nare in ANFA, and it rejects all strings u that are not in ANFA. Thus, we have\\nproved the following result:\\nTheorem 5.1.3 The language ANFA is decidable.\\n160\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.1.3\\nThe language ACFG\\nWe deﬁne the following language:\\nACFG = {⟨G, w⟩: G is a context-free grammar such that w ∈L(G)}.\\nWe claim that this language is decidable. In order to prove this claim, con-\\nsider a string u that encodes a context-free grammar G = (V, Σ, S, R) and a\\nstring w ∈Σ∗. Deciding whether or not w ∈L(G) is equivalent to deciding\\nwhether or not S\\n∗⇒w. A ﬁrst idea to decide this is by trying all possible\\nderivations that start with the start variable S and that use rules of R. The\\nproblem is that, in case w ̸∈L(G), it is not clear how many such derivations\\nhave to be checked before we can be sure that w is not in the language of\\nG: If w ∈L(G), then it may be that w can be derived from S, only by ﬁrst\\nderiving a very long string, say v, and then use rules to shorten it so as to\\nobtain the string w. Since there is no obvious upper bound on the length of\\nthe string v, we have to be careful.\\nThe trick is to do the following. First, convert the grammar G to an\\nequivalent grammar G′ in Chomsky normal form. (The construction given\\nin Section 3.4 can be described by an algorithm.) Let n be the length of the\\nstring w. Then, if w ∈L(G) = L(G′), any derivation of w in G′, from the\\nstart variable of G′, consists of exactly 2n−1 steps (where a “step” is deﬁned\\nas applying one rule of G′). Hence, we can decide whether or not w ∈L(G),\\nby trying all possible derivations, in G′, consisting of 2n −1 steps. If one of\\nthese (ﬁnite number of) derivations leads to the string w, then w ∈L(G).\\nOtherwise, w ̸∈L(G). Thus, we have proved the following result:\\nTheorem 5.1.4 The language ACFG is decidable.\\nIn fact, the arguments above imply the following result:\\nTheorem 5.1.5 Every context-free language is decidable.\\nProof. Let Σ be an alphabet and let A ⊆Σ∗be an arbitrary context-free\\nlanguage. There exists a context-free grammar in Chomsky normal form,\\nwhose language is equal to A. Given an arbitrary string w ∈Σ∗, we have\\nseen above how we can decide whether or not w can be derived from the\\nstart variable of this grammar.\\n5.1.\\nDecidability\\n161\\n5.1.4\\nThe language ATM\\nAfter having seen the languages ADFA, ANFA, and ACFG, it is natural to\\nconsider the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nWe will prove that this language is undecidable. Before we give the proof,\\nlet us mention what this means:\\nThere is no algorithm that, when given an arbitrary algorithm M\\nand an arbitrary input string w for M, decides in a ﬁnite amount\\nof time, whether or not M accepts w.\\nThe proof of the claim that ATM is undecidable is by contradiction. Thus,\\nwe assume that ATM is decidable. Then there exists a Turing machine H\\nthat has the following property. For every input string ⟨M, w⟩for H:\\n• If ⟨M, w⟩∈ATM (i.e., M accepts w), then H terminates in its accept\\nstate.\\n• If ⟨M, w⟩̸∈ATM (i.e., M rejects w or M does not terminate on input\\nw), then H terminates in its reject state.\\n• In particular, H terminates on any input ⟨M, w⟩.\\nWe construct a new Turing machine D, that does the following: On input\\n⟨M⟩, the Turing machine D uses H as a subroutine to determine what M\\ndoes when it is given its own description as input. Once D has determined\\nthis information, it does the opposite of what H does.\\nTuring machine D: On input ⟨M⟩, where M is a Turing machine,\\nthe new Turing machine D does the following:\\nStep 1: Run the Turing machine H on the input ⟨M, ⟨M⟩⟩.\\nStep 2:\\n• If H terminates in its accept state, then D terminates in its\\nreject state.\\n• If H terminates in its reject state, then D terminates in its\\naccept state.\\n162\\nChapter 5.\\nDecidable and Undecidable Languages\\nFirst observe that this new Turing machine D terminates on any input\\nstring ⟨M⟩, because H terminates on every input. Next observe that, for any\\ninput string ⟨M⟩for D:\\n• If ⟨M, ⟨M⟩⟩∈ATM (i.e., M accepts ⟨M⟩), then D terminates in its\\nreject state.\\n• If ⟨M, ⟨M⟩⟩̸∈ATM (i.e., M rejects ⟨M⟩or M does not terminate on\\ninput ⟨M⟩), then D terminates in its accept state.\\nThis means that for any string ⟨M⟩:\\n• If M accepts ⟨M⟩, then D rejects ⟨M⟩.\\n• If M rejects ⟨M⟩or M does not terminate on input ⟨M⟩, then D\\naccepts ⟨M⟩.\\nWe now consider what happens if we give the Turing machine D the string\\n⟨D⟩as input, i.e., we take M = D:\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩or D does not terminate on input ⟨D⟩, then D accepts\\n⟨D⟩.\\nSince D terminates on every input string, this means that\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩, then D accepts ⟨D⟩.\\nThis is clearly a contradiction. Therefore, the Turing machine H that decides\\nthe language ATM cannot exist and, thus, ATM is undecidable. We have\\nproved the following result:\\nTheorem 5.1.6 The language ATM is undecidable.\\n5.1.\\nDecidability\\n163\\n5.1.5\\nThe Halting Problem\\nWe deﬁne the following language:\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}.\\nTheorem 5.1.7 The language Halt is undecidable.\\nProof. The proof is by contradiction. Thus, we assume that the language\\nHalt is decidable. Then there exists a Java program H that takes as input a\\nstring of the form ⟨P, w⟩, where P is an arbitrary Java program and w is an\\narbitrary input for P. The program H has the following property:\\n• If ⟨P, w⟩∈Halt (i.e., program P terminates on input w), then H\\noutputs true.\\n• If ⟨P, w⟩̸∈Halt (i.e., program P does not terminate on input w), then\\nH outputs false.\\n• In particular, H terminates on any input ⟨P, w⟩.\\nWe will write the output of H as H(P, w). Moreover, we will denote by P(w)\\nthe computation obtained by running the program P on the input w. Hence,\\nH(P, w) =\\n\\x1a true\\nif P(w) terminates,\\nfalse\\nif P(w) does not terminate.\\nConsider the following algorithm Q, which takes as input the encoding\\n⟨P⟩of an arbitrary Java program P:\\nAlgorithm Q(⟨P⟩):\\nwhile H(P, ⟨P⟩) = true\\ndo have a beer\\nendwhile\\nSince H is a Java program, this new algorithm Q can also be written as\\na Java program. Observe that\\nQ(⟨P⟩) terminates if and only if H(P, ⟨P⟩) = false.\\n164\\nChapter 5.\\nDecidable and Undecidable Languages\\nThis means that for every Java program P,\\nQ(⟨P⟩) terminates if and only if P(⟨P⟩) does not terminate.\\n(5.1)\\nWhat happens if we run the Java program Q on the input string ⟨Q⟩?\\nIn other words, what happens if we run Q(⟨Q⟩)? Then, in (5.1), we have to\\nreplace all occurrences of P by Q. Hence,\\nQ(⟨Q⟩) terminates if and only if Q(⟨Q⟩) does not terminate.\\nThis is obviously a contradiction, and we can conclude that the Java program\\nH does not exist. Therefore, the language Halt is undecidable.\\nRemark 5.1.8 In this proof, we run the Java program Q on the input ⟨Q⟩.\\nThis means that the input to Q is a description of itself. In other words, we\\ngive Q itself as input. This is an example of what is called self-reference. An-\\nother example of self-reference can be found in Remark 5.1.8 of the textbook\\nIntroduction to Theory of Computation by A. Maheshwari and M. Smid.\\n5.2\\nCountable sets\\nThe proofs that we gave in Sections 5.1.4 and 5.1.5 seem to be bizarre. In\\nthis section, we will convince you that these proofs in fact use a technique\\nthat you have seen in the course COMP 1805: Cantor’s Diagonalization.\\nLet A and B be two sets and let f : A →B be a function. Recall that f\\nis called a bijection, if\\n• f is one-to-one (or injective), i.e., for any two distinct elements a and\\na′ in A, we have f(a) ̸= f(a′), and\\n• f is onto (or surjective), i.e., for each element b ∈B, there exists an\\nelement a ∈A, such that f(a) = b.\\nThe set of natural numbers is denoted by N. That is, N = {1, 2, 3, . . .}.\\nDeﬁnition 5.2.1 Let A and B be two sets. We say that A and B have the\\nsame size, if there exists a bijection f : A →B.\\nDeﬁnition 5.2.2 Let A be a set. We say that A is countable, if A is ﬁnite,\\nor A and N have the same size.\\n5.2.\\nCountable sets\\n165\\nIn other words, if A is an inﬁnite and countable set, then there exists a\\nbijection f : N →A, and we can write A as\\nA = {f(1), f(2), f(3), f(4), . . .}.\\nSince f is a bijection, every element of A occurs exactly once in the set on\\nthe right-hand side. This means that we can number the elements of A using\\nthe positive integers: Every element of A receives a unique number.\\nTheorem 5.2.3 The following sets are countable:\\n1. The set Z of integers:\\nZ = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n2. The Cartesian product N × N:\\nN × N = {(m, n) : m ∈N, n ∈N}.\\n3. The set Q of rational numbers:\\nQ = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\nProof. To prove that the set Z is countable, we have to give each element of\\nZ a unique number in N. We obtain this numbering, by listing the elements\\nof Z in the following order:\\n0, 1, −1, 2, −2, 3, −3, 4, −4, . . .\\nIn this (inﬁnite) list, every element of Z occurs exactly once. The number of\\nan element of Z is given by its position in this list.\\nFormally, deﬁne the function f : N →Z by\\nf(n) =\\n\\x1a n/2\\nif n is even,\\n−(n −1)/2\\nif n is odd.\\nThis function f is a bijection and, therefore, the sets N and Z have the same\\nsize. Hence, the set Z is countable.\\nFor the proofs of the other two claims, we refer to the course COMP 1805.\\nWe now use Cantor’s Diagonalization principle to prove that the set of\\nreal numbers is not countable:\\n166\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.2.4 The set R of real numbers is not countable.\\nProof. Deﬁne\\nA = {x ∈R : 0 ≤x < 1}.\\nWe will prove that the set A is not countable. This will imply that the set\\nR is not countable, because A ⊆R.\\nThe proof that A is not countable is by contradiction. So we assume that\\nA is countable. Then there exists a bijection f : N →A. Thus, for each\\nn ∈N, f(n) is a real number between zero and one. We can write\\nA = {f(1), f(2), f(3), . . .},\\n(5.2)\\nwhere every element of A occurs exactly once in the set on the right-hand\\nside.\\nConsider the real number f(1). We can write this number in decimal\\nnotation as\\nf(1) = 0.d11d12d13 . . . ,\\nwhere each d1i is a digit in the set {0, 1, 2, . . . , 9}. In general, for every n ∈N,\\nwe can write the real number f(n) as\\nf(n) = 0.dn1dn2dn3 . . . ,\\nwhere, again, each dni is a digit in {0, 1, 2, . . . , 9}.\\nWe deﬁne the real number\\nx = 0.d1d2d3 . . . ,\\nwhere, for each integer n ≥1,\\ndn =\\n\\x1a 4\\nif dnn ̸= 4,\\n5\\nif dnn = 4.\\nObserve that x is a real number between zero and one, i.e., x ∈A. Therefore,\\nby (5.2), there is an element n ∈N, such that f(n) = x. We compare the\\nn-th digits of f(n) and x:\\n• The n-th digit of f(n) is equal to dnn.\\n• The n-th digit of x is equal to dn.\\n5.2.\\nCountable sets\\n167\\nSince f(n) and x are equal, their n-th digits must be equal, i.e., dnn = dn.\\nBut, by the deﬁnition of dn, we have dnn ̸= dn. This is a contradiction and,\\ntherefore, the set A is not countable.\\nNotice how we deﬁned the real number x: For each n ≥1, the n-th digit\\nof x is not equal to the n-th digit of f(n). Therefore, for each n ≥1, x ̸= f(n)\\nand, thus, x ̸∈A.\\nThe ﬁnal result of this section is the fact that for every set A, its power\\nset\\nP(A) = {B : B ⊆A}\\nis “strictly larger” than A. Deﬁne the function f : A →P(A) by\\nf(a) = {a},\\nfor any a in A. Since f is one-to-one, we can say that P(A) is “at least as\\nlarge as” A.\\nTheorem 5.2.5 Let A be an arbitrary set. Then A and P(A) do not have\\nthe same size.\\nProof. The proof is by contradiction. Thus, we assume that there exists a\\nbijection g : A →P(A). Deﬁne the set B as\\nB = {a ∈A : a ̸∈g(a)}.\\nSince B ∈P(A) and g is a bijection, there exists an element a in A such that\\ng(a) = B.\\nFirst assume that a ∈B. Since g(a) = B, we have a ∈g(a). But then,\\nfrom the deﬁnition of the set B, we have a ̸∈B, which is a contradiction.\\nNext assume that a ̸∈B.\\nSince g(a) = B, we have a ̸∈g(a).\\nBut\\nthen, from the deﬁnition of the set B, we have a ∈B, which is again a\\ncontradiction.\\nWe conclude that the bijection g does not exist. Therefore, A and P(A)\\ndo not have the same size.\\n168\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.2.1\\nThe Halting Problem revisited\\nNow that we know about countability, we give a diﬀerent way to look at the\\nproof in Section 5.1.5 of the fact that the language\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}\\nis undecidable.\\nYou should convince yourself that the proof given below\\nfollows the same reasoning as the one used in the proof of Theorem 5.2.4.\\nWe ﬁrst argue that the set of all Java programs is countable. Indeed,\\nevery Java program P can be described by a ﬁnite amount of text. In fact,\\nwe have been using ⟨P⟩to denote such a description by a binary string. For\\nany integer n ≥0, there are at most 2n (i.e., ﬁnitely many) Java programs\\nP whose description ⟨P⟩has length n. Therefore, to obtain a list of all Java\\nprograms, we do the following:\\n• List all Java programs P whose description ⟨P⟩has length 0. (Well,\\nthe empty string does not describe any Java program, so in this step,\\nnothing happens.)\\n• List all Java programs P whose description ⟨P⟩has length 1.\\n• List all Java programs P whose description ⟨P⟩has length 2.\\n• List all Java programs P whose description ⟨P⟩has length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every Java program occurs exactly once. Therefore, the\\nset of all Java programs is countable.\\nConsider an inﬁnite list\\nP1, P2, P3, . . .\\nin which every Java program occurs exactly once.\\nAssume that the language Halt is decidable. Then there exists a Java\\nprogram H that decides this language. We may assume that, on input ⟨P, w⟩,\\nH returns true if P terminates on input w, and false if P does not terminate\\non input w.\\nWe construct a new Java program D that does the following:\\n5.3.\\nRice’s Theorem\\n169\\nAlgorithm D:\\nOn input ⟨Pn⟩, where n is a positive integer, the\\nnew Java program D does the following:\\nStep 1: Run the Java program H on the input ⟨Pn, ⟨Pn⟩⟩.\\nStep 2:\\n• If H returns true, then D goes into an inﬁnite loop.\\n• If H returns false, then D returns true and terminates its com-\\nputation.\\nObserve that D can be written as a Java program. Therefore, there exists\\nan integer n ≥1 such that D = Pn. The next two observations follow from\\nthe pseudocode:\\n• If D terminates on input ⟨Pn⟩, then H returns false on input ⟨Pn, ⟨Pn⟩⟩,\\ni.e., Pn does not terminate on input ⟨Pn⟩.\\n• If D does not terminate on input ⟨Pn⟩, then H returns true on input\\n⟨Pn, ⟨Pn⟩⟩, i.e., Pn terminates on input ⟨Pn⟩.\\nThus,\\n• D terminates on input ⟨Pn⟩if and only if Pn does not terminate on\\ninput ⟨Pn⟩.\\nSince D = Pn, this becomes\\n• D terminates on input ⟨D⟩if and only if D does not terminate on input\\n⟨D⟩.\\nThus, we have obtained a contradiction.\\nRemark 5.2.6 We deﬁned the Java program D in such a way that, for each\\nn ≥1, the computation of D on input ⟨Pn⟩diﬀers from the computation of\\nPn on input ⟨Pn⟩. Hence, for each n ≥1, D ̸= Pn. However, since D is a\\nJava program, there must be an integer n ≥1 such that D = Pn.\\n170\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.3\\nRice’s Theorem\\nWe have seen two examples of undecidable languages: ATM and Halt. In this\\nsection, we prove that many languages involving Turing machines (or Java\\nprograms) are undecidable.\\nDeﬁne T to be the set of binary encodings of all Turing machines, i.e.,\\nT = {⟨M⟩: M is a Turing machine with input alphabet {0,1}}.\\nTheorem 5.3.1 (Rice) Let P be a subset of T such that\\n1. P ̸= ∅, i.e., there exists a Turing machine M such that ⟨M⟩∈P,\\n2. P is a proper subset of T , i.e., there exists a Turing machine N such\\nthat ⟨N⟩̸∈P, and\\n3. for any two Turing machines M1 and M2 with L(M1) = L(M2),\\n(a) either both ⟨M1⟩and ⟨M2⟩are in P or\\n(b) none of ⟨M1⟩and ⟨M2⟩is in P.\\nThen the language P is undecidable.\\nYou can think of P as the set of all Turing machines that satisfy a certain\\nproperty. The ﬁrst two conditions state that at least one Turing machine\\nsatisﬁes this property and not all Turing machines satisfy this property. The\\nthird condition states that, for any Turing machine M, whether or not M\\nsatisﬁes this property only depends on the language L(M) of M.\\nHere are some examples of languages that satisfy the conditions in Rice’s\\nTheorem:\\nP1 = {⟨M⟩: M is a Turing machine and ϵ ∈L(M)},\\nP2 = {⟨M⟩: M is a Turing machine and L(M) = {1011, 001100}},\\nP3 = {⟨M⟩: M is a Turing machine and L(M) is a regular language}.\\nYou are encouraged to verify that Rice’s Theorem indeed implies that each\\nof P1, P2, and P3 is undecidable.\\n5.3.\\nRice’s Theorem\\n171\\n5.3.1\\nProof of Rice’s Theorem\\nThe strategy of the proof is as follows: Assuming that the language P is\\ndecidable, we show that the language\\nHalt = {⟨M, w⟩:\\nM is a Turing machine that terminates on\\nthe input string w}\\nis decidable. This will contradict Theorem 5.1.7.\\nThe assumption that P is decidable implies the existence of a Turing\\nmachine H that decides P. Observe that H takes as input a binary string\\n⟨M⟩encoding a Turing machine M. In order to show that Halt is decidable,\\nwe need a Turing machine that takes as input a binary string ⟨M, w⟩encoding\\na Turing machine M and a binary string w. In the rest of this section, we\\nwill explain how this Turing machine can be obtained.\\nLet M1 be a Turing machine that, for any input string, switches in its\\nﬁrst computation step from its start state to its reject state. In other words,\\nM1 is a Turing machine with L(M1) = ∅. We assume that\\n⟨M1⟩̸∈P.\\n(At the end of the proof, we will consider the case when ⟨M1⟩∈P.) We also\\nchoose a Turing machine M2 such that\\n⟨M2⟩∈P.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nif M terminates\\nthen run M2 on input x;\\nif M2 terminates in the accept state\\nthen terminate in the accept state\\nelse if M2 terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\n172\\nChapter 5.\\nDecidable and Undecidable Languages\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that for any string x,\\nx is accepted by TMw if and only if x is accepted by M2.\\nThus, L(TMw) = L(M2).\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩̸∈Halt.\\nThen it follows from the pseudocode that for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅. In particular, L(TMw) =\\nL(M1).\\nRecall that ⟨M1⟩̸∈P, whereas ⟨M2⟩∈P. Then the following follows from\\nthe third condition in Rice’s Theorem:\\n• If ⟨M, w⟩∈Halt, then ⟨TMw⟩∈P.\\n• If ⟨M, w⟩̸∈Halt, then ⟨TMw⟩̸∈P.\\nThus, we have obtained a connection between the languages P and Halt.\\nThis suggests that we proceed as follows.\\nAssume that the language P is decidable. Let H be a Turing machine\\nthat decides P. Then, for any Turing machine M,\\n• if ⟨M⟩∈P, then H accepts the string ⟨M⟩,\\n• if ⟨M⟩̸∈P, then H rejects the string ⟨M⟩, and\\n• H terminates on any input string.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\n5.4.\\nEnumerability\\n173\\nIt follows from the pseudocode that H′ terminates on any input. We\\nobserve the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that ⟨TMw⟩∈P.\\nSince H decides the language P, it follows that H accepts the string\\n⟨TMw⟩. Therefore, from the pseudocode, H′ accepts the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then we have seen before that ⟨TMw⟩̸∈\\nP. Since H decides the language P, it follows that H rejects (and\\nterminates on) the string ⟨TMw⟩. Therefore, from the pseudocode, H′\\nrejects (and terminates on) the string ⟨M, w⟩.\\nWe have shown that the Turing machine H′ decides the language Halt.\\nThis is a contradiction and, therefore, we conclude that the language P is\\nundecidable.\\nUntil now, we assumed that ⟨M1⟩̸∈P. If ⟨M1⟩∈P, then we repeat the\\nproof with P replaced by its complement P. This revised proof then shows\\nthat P is undecidable. Since for every language L,\\nL is decidable if and only if L is decidable,\\nwe again conclude that P is undecidable.\\n5.4\\nEnumerability\\nWe now come to the last class of languages in this chapter:\\nDeﬁnition 5.4.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is enumerable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, does not terminate in the accept state. That is, either the\\ncomputation terminates in the reject state or the computation does not\\nterminate.\\n174\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is enumerable, if there exists an algorithm\\nhaving the following property. If w ∈A, then the algorithm terminates on\\nthe input string w and tells us that w ∈A. On the other hand, if w ̸∈A,\\nthen either (i) the algorithm terminates on the input string w and tells us\\nthat w ̸∈A or (ii) the algorithm does not terminate on the input string w,\\nin which case it does not tell us that w ̸∈A.\\nIn Section 5.5, we will show where the term “enumerable” comes from.\\nThe following theorem follows immediately from Deﬁnitions 5.1.1 and 5.4.1.\\nTheorem 5.4.2 Every decidable language is enumerable.\\nIn the following subsections, we will give some examples of enumerable\\nlanguages.\\n5.4.1\\nHilbert’s problem\\nWe have seen Hilbert’s problem in Section 4.4: Is there an algorithm that\\ndecides, for any given polynomial p with integer coeﬃcients, whether or not\\np has integral roots? If we formulate this problem in terms of languages,\\nthen Hilbert asked whether or not the language\\nHilbert = {⟨p⟩:\\np is a polynomial with integer coeﬃcients\\nthat has an integral root}\\nis decidable. As usual, ⟨p⟩denotes the binary string that forms an encoding\\nof the polynomial p.\\nAs we mentioned in Section 4.4, it was proven by Matiyasevich in 1970\\nthat the language Hilbert is not decidable. We claim, that this language\\nis enumerable.\\nIn order to prove this claim, we have to construct an al-\\ngorithm Hilbert with the following property: For any input polynomial p\\nwith integer coeﬃcients,\\n• if p has an integral root, then algorithm Hilbert will ﬁnd one in a\\nﬁnite amount of time,\\n• if p does not have an integral root, then either algorithm Hilbert ter-\\nminates and tells us that p does not have an integral root, or algorithm\\nHilbert does not terminate.\\n5.4.\\nEnumerability\\n175\\nRecall that Z denotes the set of integers. Algorithm Hilbert does the\\nfollowing, on any input polynomial p with integer coeﬃcients.\\nLet n de-\\nnote the number of variables in p. Algorithm Hilbert tries all elements\\n(x1, x2, . . . , xn) ∈Zn, in a systematic way, and for each such element, it\\ncomputes p(x1, x2, . . . , xn). If this value is zero, then algorithm Hilbert\\nterminates and accepts the input.\\nWe observe the following:\\n• If ⟨p⟩∈Hilbert, then algorithm Hilbert terminates and accepts p,\\nprovided we are able to visit all elements (x1, x2, . . . , xn) ∈Zn in a\\n“systematic way”.\\n• If ⟨p⟩̸∈Hilbert, then p(x1, x2, . . . , xn) ̸= 0 for all (x1, x2, . . . , xn) ∈Zn\\nand, therefore, algorithm Hilbert does not terminate.\\nThese are exactly the requirements for the language Hilbert to be enumerable.\\nIt remains to explain how we visit all elements (x1, x2, . . . , xn) ∈Zn in a\\nsystematic way. For any integer d ≥0, let Hd denote the hypercube in Zn\\nwith sides of length 2d that is centered at the origin. That is, Hd consists\\nof the set of all points (x1, x2, . . . , xn) in Zn, such that −d ≤xi ≤d for all\\n1 ≤i ≤n and there exists at least one index j for which xj = d or xj = −d.\\nWe observe that Hd contains a ﬁnite number of elements. In fact, if d ≥1,\\nthen this number is equal to (2d + 1)n −(2d −1)n. The algorithm will visit\\nall elements (x1, x2, . . . , xn) ∈Zn, in the following order: First, it visits the\\norigin, which is the only element of H0. Then, it visits all elements of H1,\\nfollowed by all elements of H2, etc., etc.\\nTo summarize, we obtain the following algorithm, proving that the lan-\\nguage Hilbert is enumerable:\\n176\\nChapter 5.\\nDecidable and Undecidable Languages\\nAlgorithm Hilbert(⟨p⟩):\\nn := the number of variables in p;\\nd := 0;\\nwhile d ≥0\\ndo for each (x1, x2, . . . , xn) ∈Hd\\ndo R := p(x1, x2, . . . , xn);\\nif R = 0\\nthen terminate and accept\\nendif\\nendfor;\\nd := d + 1\\nendwhile\\nTheorem 5.4.3 The language Hilbert is enumerable.\\n5.4.2\\nThe language ATM\\nWe have shown in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nis undecidable. In this section, we will prove that this language is enumerable.\\nThus, we have to construct an algorithm P having the following property,\\nfor any given input string u:\\n• If\\n– u encodes a Turing machine M and an input string w for M (i.e.,\\nu is in the correct format ⟨M, w⟩) and\\n– ⟨M, w⟩∈ATM (i.e., M accepts w),\\nthen algorithm P terminates in its accept state.\\n• In all other cases, either algorithm P terminates in its reject state, or\\nalgorithm P does not terminate.\\nOn input string u = ⟨M, w⟩, which is in the correct format, algorithm P does\\nthe following:\\n5.5.\\nWhere does the term “enumerable” come from?\\n177\\n1. It simulates the computation of M on input w.\\n2. If M terminates in its accept state, then P terminates in its accept\\nstate.\\n3. If M terminates in its reject state, then P terminates in its reject state.\\n4. If M does not terminate, then P does not terminate.\\nHence, if u = ⟨M, w⟩∈ATM, then M accepts w and, therefore, P accepts\\nu. On the other hand, if u = ⟨M, w⟩̸∈ATM, then M does not accept w. This\\nmeans that, on input w, M either terminates in its reject state or does not\\nterminate. But this implies that, on input u, P either terminates in its reject\\nstate or does not terminate. This proves that algorithm P has the properties\\nthat are needed in order to show that the language ATM is enumerable. We\\nhave proved the following result:\\nTheorem 5.4.4 The language ATM is enumerable.\\n5.5\\nWhere does the term “enumerable” come\\nfrom?\\nIn Deﬁnition 5.4.1, we have deﬁned what it means for a language to be\\nenumerable. In this section, we will see where this term comes from.\\nDeﬁnition 5.5.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. An\\nenumerator for A is a Turing machine E having the following properties:\\n1. Besides the standard features as in Section 4.1, E has a print tape and\\na print state. During its computation, E writes symbols of Σ on the\\nprint tape. Each time, E enters the print state, the current string on\\nthe print tape is sent to the printer and the print tape is made empty.\\n2. At the start of the computation, all tapes are empty and E is in the\\nstart state.\\n3. Every string w in A is sent to the printer at least once.\\n4. Every string w that is not in A is never sent to the printer.\\n178\\nChapter 5.\\nDecidable and Undecidable Languages\\nThus, an enumerator E for A really enumerates all strings in the language\\nA. There is no particular order in which the strings of A are sent to the\\nprinter. Moreover, a string in A may be sent to the printer multiple times.\\nIf the language A is inﬁnite, then the Turing machine E obviously does not\\nterminate; however, every string in A (and only strings in A) will be sent to\\nthe printer at some time during the computation.\\nTo give an example, let A = {0n : n ≥0}. The following Turing machine\\nis an enumerator for A.\\nTuring machine StringsOfZeros:\\nn := 0;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo write 0 on the print tape\\nendfor;\\nenter the print state;\\nn := n + 1\\nendwhile\\nIn the rest of this section, we will prove the following result.\\nTheorem 5.5.2 A language is enumerable if and only if it has an enumer-\\nator.\\nFor the ﬁrst part of the proof, assume that the language A has an enu-\\nmerator E. We construct the following Turing machine M, which takes an\\narbitrary string w as input:\\nTuring machine M(w):\\nrun E; every time E enters the print state:\\nlet v be the string on the print tape;\\nif w = v\\nthen terminate in the accept state\\nendif\\nThe Turing machine M has the following properties:\\n• If w ∈A, then w will be sent to the printer at some time during the\\n5.5.\\nWhere does the term “enumerable” come from?\\n179\\ncomputation of E. It follows from the pseudocode that, on input w,\\nM terminates in the accept state.\\n• If w ̸∈A, then E will never sent w to the printer. It follows from the\\npseudocode that, on input w, M does not terminate.\\nThus, M satisﬁes the conditions in Deﬁnition 5.4.1. We conclude that the\\nlanguage A is enumerable.\\nTo prove the converse, we now assume that A is enumerable. Let M be\\na Turing machine that satisﬁes the conditions in Deﬁnition 5.4.1.\\nWe ﬁx an inﬁnite list\\ns1, s2, s3, . . .\\nof all strings in Σ∗. For example, if Σ = {0, 1}, then we can take this list to\\nbe\\nϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .\\nWe construct the following Turing machine E, which takes the empty\\nstring as input:\\nTuring machine E:\\nn := 1;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo run M for n steps on the input string si;\\nif M accepts si within n steps\\nthen write si on the print tape;\\nenter the print state\\nendif\\nendfor;\\nn := n + 1\\nendwhile\\nWe claim that E is an enumerator for the language A. To prove this, it\\nis obvious that any string that is sent to the printer by E belongs to A.\\nIt remains to prove that every string in A will be sent to the printer by E.\\nLet w be a string in A. Then, on input w, the Turing machine M terminates\\nin the accept state. Let m be the number of steps made by M on input w.\\nLet i be the index such that w = si. Deﬁne n = max(m, i). Consider the\\n180\\nChapter 5.\\nDecidable and Undecidable Languages\\nn-th iteration of the while-loop and the i-th iteration of the for-loop. In this\\niteration, M accepts si = w in m ≤n steps and, therefore, w is sent to the\\nprinter.\\n5.6\\nMost languages are not enumerable\\nIn this section, we will prove that most languages are not enumerable. The\\nproof is based on the following two facts:\\n• The set consisting of all enumerable languages is countable; we will\\nprove this in Section 5.6.1.\\n• The set consisting of all languages is not countable; we will prove this\\nin Section 5.6.2.\\n5.6.1\\nThe set of enumerable languages is countable\\nWe deﬁne the set E as\\nE = {A : A ⊆{0, 1}∗is an enumerable language}.\\nIn words, E is the set whose elements are the enumerable languages. Every\\nelement of E is an enumerable language. Hence, every element of the set E\\nis itself a set consisting of strings.\\nLemma 5.6.1 The set E is countable.\\nProof. Let A ⊆{0, 1}∗be an enumerable language. There exists a Turing\\nmachine TA that satisﬁes the conditions in Deﬁnition 5.4.1.\\nThis Turing\\nmachine TA can be uniquely speciﬁed by a string in English. This string can\\nbe converted to a binary string sA. Hence, the binary string sA is a unique\\nencoding of the Turing machine TA.\\nConsider the set\\nS = {sA : A ⊆{0, 1}∗is an enumerable language}.\\nObserve that the function f : E →S, deﬁned by f(A) = sA for each A ∈E,\\nis a bijection. Therefore, the sets E and S have the same size. Hence, in\\norder to prove that the set E is countable, it is suﬃcient to prove that the\\nset S is countable.\\n5.6.\\nMost languages are not enumerable\\n181\\nWhy is the set S countable? For each integer n ≥0, there are exactly 2n\\nbinary strings of length n. Since there are binary strings that are not encod-\\nings of Turing machines, the set S contains at most 2n strings of length n.\\nIn particular, the number of strings in S having length n is ﬁnite. Therefore,\\nwe obtain an inﬁnite list of the elements of S in the following way:\\n• List all strings in S having length 0. (Well, the empty string is not in\\nS, so in this step, nothing happens.)\\n• List all strings in S having length 1.\\n• List all strings in S having length 2.\\n• List all strings in S having length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every element of S occurs exactly once. Therefore, S is\\ncountable.\\n5.6.2\\nThe set of all languages is not countable\\nWe deﬁne the set L as\\nL = {A : A ⊆{0, 1}∗is a language}.\\nIn words, L is the set consisting of all languages. Every element of the set L\\nis a set consisting of strings.\\nLemma 5.6.2 The set L is not countable.\\nProof. We deﬁne the set B as\\nB = {w : w is an inﬁnite binary sequence}.\\nWe claim that this set is not countable. The proof of this claim is almost\\nidentical to the proof of Theorem 5.2.4. We assume that the set B is count-\\nable. Then there exists a bijection f : N →B. Thus, for each n ∈N, f(n) is\\nan inﬁnite binary sequence. We can write\\nB = {f(1), f(2), f(3), . . .},\\n(5.3)\\n182\\nChapter 5.\\nDecidable and Undecidable Languages\\nwhere every element of B occurs exactly once in the set on the right-hand\\nside.\\nWe deﬁne the inﬁnite binary sequence w = w1w2w3 . . ., where, for each\\ninteger n ≥1,\\nwn =\\n\\x1a 1\\nif the n-th bit of f(n) is 0,\\n0\\nif the n-th bit of f(n) is 1.\\nSince w ∈B, it follows from (5.3) that there is an element n ∈N, such that\\nf(n) = w. Hence, the n-th bits of f(n) and w are equal. But, by deﬁnition,\\nthese n-th bits are not equal. This is a contradiction and, therefore, the set\\nB is not countable.\\nIn the rest of the proof, we will show that the sets L and B have the same\\nsize. Since B is not countable, this will imply that L is not countable.\\nIn order to prove that L and B have the same size, we have to show that\\nthere exists a bijection\\ng : L →B.\\nWe ﬁrst observe that the set {0, 1}∗is countable, because for each integer\\nn ≥0, there are only ﬁnitely many (to be precise, exactly 2n) binary strings\\nof length n. In fact, we can write\\n{0, 1}∗= {ϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .}.\\nFor each integer n ≥1, we denote by sn the n-th string in this list. Hence,\\n{0, 1}∗= {s1, s2, s3, . . .}.\\n(5.4)\\nNow we are ready to deﬁne the bijection g : L →B: Let A ∈L, i.e.,\\nA ⊆{0, 1}∗is a language. We deﬁne the inﬁnite binary sequence g(A) as\\nfollows: For each integer n ≥1, the n-th bit of g(A) is equal to\\n\\x1a 1\\nif sn ∈A,\\n0\\nif sn ̸∈A.\\nIn words, the inﬁnite binary sequence g(A) contains a 1 exactly in those\\npositions n for which the string sn in (5.4) is in the language A.\\nTo give an example, assume that A is the language consisting of all binary\\nstrings that start with 0. The following table gives the corresponding inﬁnite\\nbinary sequence g(A) (this sequence is obtained by reading the rightmost\\ncolumn from top to bottom):\\n5.6.\\nMost languages are not enumerable\\n183\\n{0, 1}∗\\nA\\ng(A)\\nϵ\\nnot in A\\n0\\n0\\nin A\\n1\\n1\\nnot in A\\n0\\n00\\nin A\\n1\\n01\\nin A\\n1\\n10\\nnot in A\\n0\\n11\\nnot in A\\n0\\n000\\nin A\\n1\\n001\\nin A\\n1\\n010\\nin A\\n1\\n100\\nnot in A\\n0\\n011\\nin A\\n1\\n101\\nnot in A\\n0\\n110\\nnot in A\\n0\\n111\\nnot in A\\n0\\n...\\n...\\n...\\nThe function g deﬁned above has the following properties:\\n• If A and A′ are two diﬀerent languages in L, then g(A) ̸= g(A′).\\n• For every inﬁnite binary sequence w in B, there exists a language A in\\nL, such that g(A) = w.\\nThis means that the function g is a bijection from L to B.\\n5.6.3\\nThere are languages that are not enumerable\\nWe have proved that the set\\nE = {A : A ⊆{0, 1}∗is an enumerable language}\\nis countable, whereas the set\\nL = {A : A ⊆{0, 1}∗is a language}\\nis not countable. This means that there are “more” languages in L than\\nthere are in E, proving the following result:\\n184\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.6.3 There exist languages that are not enumerable.\\nThe proof given above shows the existence of languages that are not\\nenumerable. However, the proof does not give us a speciﬁc example of a\\nlanguage that is not enumerable. In the next sections, we will see examples\\nof such languages. Before we move on to these examples, we mention the\\ndiﬀerence between being countable and being enumerable:\\n• Any language A is countable, i.e., we can number the elements of A\\nand, thus, write\\nA = {s1, s2, s3, s4, . . .}.\\n• If the language A is enumerable, then, by Theorem 5.5.2, there is an\\nalgorithm that produces this numbering.\\n• If the language A is not enumerable, then, again by Theorem 5.5.2,\\nthere does not exist an algorithm that produces this numbering.\\n5.7\\nThe relationship between decidable and\\nenumerable languages\\nWe know from Theorem 5.4.2 that every decidable language is enumerable.\\nOn the other hand, we know from Theorems 5.1.6 and 5.4.4 that the converse\\nis not true. The following result should not come as a surprise:\\nTheorem 5.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then,\\nA is decidable if and only if both A and its complement A are enumerable.\\nProof. We ﬁrst assume that A is decidable. Then, by Theorem 5.4.2, A\\nis enumerable. Since A is decidable, it is not diﬃcult to see that A is also\\ndecidable. Then, again by Theorem 5.4.2, A is enumerable.\\nTo prove the converse, we assume that both A and A are enumerable.\\nSince A is enumerable, there exists a Turing machine M1, such that for any\\nstring w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M1, on the input string w, terminates\\nin the accept state of M1.\\n5.7.\\nDecidable versus enumerable languages\\n185\\n• If w ̸∈A, then the computation of M1, on the input string w, terminates\\nin the reject state of M1 or does not terminate.\\nSimilarly, since A is enumerable, there exists a Turing machine M2, such that\\nfor any string w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M2, on the input string w, terminates\\nin the accept state of M2.\\n• If w ̸∈A, then the computation of M2, on the input string w, terminates\\nin the reject state of M2 or does not terminate.\\nWe construct a two-tape Turing machine M:\\nTwo-tape Turing machine M: For any input string w ∈Σ∗, M\\ndoes the following:\\n• M simulates the computation of M1, on input w, on the ﬁrst\\ntape, and, simultaneously, it simulates the computation of M2,\\non input w, on the second tape.\\n• If the simulation of M1 terminates in the accept state of M1,\\nthen M terminates in its accept state.\\n• If the simulation of M2 terminates in the accept state of M2,\\nthen M terminates in its reject state.\\nObserve the following:\\n• If w ∈A, then M1 terminates in its accept state and, therefore, M\\nterminates in its accept state.\\n• If w ̸∈A, then M2 terminates in its accept state and, therefore, M\\nterminates in its reject state.\\nWe conclude that the Turing machine M accepts all strings in A, and rejects\\nall strings that are not in A. This proves that the language A is decidable.\\nWe now use Theorem 5.7.1 to give examples of languages that are not\\nenumerable:\\n186\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.7.2 The language ATM is not enumerable.\\nProof. We know from Theorems 5.4.4 and 5.1.6 that the language ATM is\\nenumerable but not decidable. Combining these facts with Theorem 5.7.1\\nimplies that the language ATM is not enumerable.\\nThe following result can be proved in exactly the same way:\\nTheorem 5.7.3 The language Halt is not enumerable.\\n5.8\\nA language A such that both A and A are\\nnot enumerable\\nIn Theorem 5.7.2, we have seen that the complement of the language ATM\\nis not enumerable.\\nIn Theorem 5.4.4, however, we have shown that the\\nlanguage ATM itself is enumerable. In this section, we consider the language\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\nWe will show the following result:\\nTheorem 5.8.1 Both EQTM and its complement EQTM are not enumer-\\nable.\\n5.8.1\\nEQTM is not enumerable\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We construct\\na new Turing machine TMw that takes as input an arbitrary binary string x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nterminate in the accept state\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that every string x is accepted by TMw.\\nThus, L(TMw) = {0, 1}∗.\\n5.8.\\nBoth A and A not enumerable\\n187\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩∈Halt.\\nThen it follows from the pseudocode that, for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that rejects every input string;\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. By our choice of M1, we have L(M1) = ∅as well. Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt, i.e., ⟨M, w⟩∈Halt. Then we have seen\\nbefore that L(TMw) ̸= ∅= L(M1). Therefore, on input ⟨M1, TMw⟩, H\\neither terminates in the reject state or does not terminate. It follows\\n188\\nChapter 5.\\nDecidable and Undecidable Languages\\nfrom the pseudocode that, on input ⟨M, w⟩, H′ either terminates in the\\nreject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\n5.8.2\\nEQTM is not enumerable\\nThis proof is symmetric to the one in Section 5.8.1.\\nFor a ﬁxed Turing\\nmachine M and a ﬁxed binary string w, we will use the same Turing machine\\nTMw as in Section 5.8.1.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that accepts every input string;\\nconstruct the Turing machine TMw of Section 5.8.1;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\nExercises\\n189\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. Thus, by our choice of M1, we have L(TMw) ̸= L(M1). Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then L(TMw) = {0, 1}∗= L(M1) and, on\\ninput ⟨M1, TMw⟩, H either terminates in the reject state or does not\\nterminate. It follows from the pseudocode that, on input ⟨M, w⟩, H′\\neither terminates in the reject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\nExercises\\n5.1 Prove that the language\\n{w ∈{0, 1}∗: w is the binary representation of 2n for some n ≥0}\\nis decidable. In other words, construct a Turing machine that gets as input\\nan arbitrary number x ∈N, represented in binary as a string w, and that\\ndecides whether or not x is a power of two.\\n5.2 Let F be the set of all functions f : N →N.\\nProve that F is not\\ncountable.\\n5.3 A function f : N →N is called computable, if there exists a Turing\\nmachine, that gets as input an arbitrary positive integer n, written in binary,\\nand gives as output the value of f(n), again written in binary. This Turing\\nmachine has a ﬁnal state. As soon as the Turing machine enters this ﬁnal\\nstate, the computation terminates, and the output is the binary string that\\nis written on its tape.\\nProve that there exist functions f : N →N that are not computable.\\n5.4 Let n be a ﬁxed positive integer, and let k be the number of bits in the\\nbinary representation of n. (Hence, k = 1 + ⌊log n⌋.) Construct a Turing\\nmachine with one tape, tape alphabet {0, 1, 2}, and exactly k + 1 states\\nq0, q1, . . . , qk, that does the following:\\n190\\nChapter 5.\\nDecidable and Undecidable Languages\\nStart of the computation: The tape is empty, i.e., every cell of the tape\\ncontains 2, and the Turing machine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer n, the tape head is on the rightmost bit of the binary represen-\\ntation of n, and the Turing machine is in the ﬁnal state qk.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state qk. As soon as state qk is entered,\\nthe Turing machine terminates.\\n5.5 Give an informal description (in plain English) of a Turing machine\\nwith three tapes, that gets as input the binary representation of an arbitrary\\ninteger m ≥1, and returns as output the unary representation of m.\\nStart of the computation: The ﬁrst tape contains the binary representa-\\ntion of the input m. The other two tapes are empty (i.e., contain only 2s).\\nThe Turing machine is in the start state.\\nEnd of the computation: The third tape contains the unary representation\\nof m, i.e., a string consisting of m many ones. The Turing machine is in the\\nﬁnal state.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this ﬁnal state is entered,\\nthe Turing machine terminates.\\nHint: Use the second tape to maintain a string of ones, whose length is\\na power of two.\\n5.6 In this exercise, you are asked to prove that the busy beaver function\\nBB : N →N is not computable.\\nFor any integer n ≥1, we deﬁne TM n to be the set of all Turing machines\\nM, such that\\n• M has one tape,\\n• M has exactly n states,\\n• the tape alphabet of M is {0, 1, 2}, and\\n• M terminates, when given the empty string ϵ as input.\\nExercises\\n191\\nFor every Turing machine M ∈TM n, we deﬁne f(M) to be the number of\\nones on the tape, after the computation of M, on the empty input string,\\nhas terminated.\\nThe busy beaver function BB : N →N is deﬁned as\\nBB(n) := max{f(M) : M ∈TM n}, for every n ≥1.\\nIn words, BB(n) is the maximum number of ones that any Turing machine\\nwith n states can produce, when given the empty string as input, and as-\\nsuming the Turing machine terminates on this input.\\nProve that the function BB is not computable.\\nHint: Assume that BB is computable. Then there exists a Turing ma-\\nchine M that, for any given n ≥1, computes the value of BB(n). Fix a large\\ninteger n ≥1. Deﬁne (in plain English) a Turing machine that, when given\\nthe empty string as input, terminates and outputs a string consisting of more\\nthan BB(n) many ones. Use Exercises 5.4 and 5.5 to argue that there exists\\nsuch a Turing machine having O(log n) states. Then, if you assume that n\\nis large enough, the number of states is at most n.\\n5.7 Since the set\\nT = {M : M is a Turing machine}\\nis countable, there is an inﬁnite list\\nM1, M2, M3, M4, . . . ,\\nsuch that every Turing machine occurs exactly once in this list.\\nFor any positive integer n, let ⟨n⟩denote the binary representation of n;\\nobserve that ⟨n⟩is a binary string.\\nLet A be the language deﬁned as\\nA = {⟨n⟩:\\nthe Turing machine Mn terminates on the input string ⟨n⟩,\\nand it rejects this string}.\\nProve that the language A is undecidable.\\n5.8 Consider the three languages\\nEmpty = {⟨M⟩: M is a Turing machine for which L(M) = ∅},\\n192\\nChapter 5.\\nDecidable and Undecidable Languages\\nUselessState = {⟨M, q⟩:\\nM is a Turing machine, q is a state of M,\\nfor every input string w, the computation of M on\\ninput w never visits state q},\\nand\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\n• Use Rice’s Theorem to show that Empty is undecidable.\\n• Use the ﬁrst part to show that UselessState is undecidable.\\n• Use the ﬁrst part to show that EQTM is undecidable.\\n5.9 Consider the language\\nREGTM = {⟨M⟩: M is a Turing machine whose language L(M) is regular}.\\nUse Rice’s Theorem to prove that REGTM is undecidable.\\n5.10 We have seen in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts w}\\nis undecidable. Consider the language REGTM of Exercise 5.9. The questions\\nbelow will lead you through a proof of the claim that the language REGTM\\nis undecidable.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nif x = 0n1n for some n ≥0\\nthen terminate in the accept state\\nelse run M on the input string w;\\nif M terminates in the accept state\\nthen terminate in the accept state\\nelse if M terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\nExercises\\n193\\nAnswer the following two questions:\\n• Assume that M accepts the string w. What is the language L(TMw) of\\nthe new Turing machine TMw?\\n• Assume that M does not accept the string w. What is the language\\nL(TMw) of the new Turing machine TMw?\\nThe goal is to prove that the language REGTM is undecidable. We will\\nprove this by contradiction. Thus, we assume that R is a Turing machine\\nthat decides REGTM. Recall what this means:\\n• If M is a Turing machine whose language is regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the accept state.\\n• If M is a Turing machine whose language is not regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the reject state.\\nWe construct a new Turing machine R′ which takes as input an arbitrary\\nTuring machine M and an arbitrary binary string w:\\nTuring machine R′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun R on the input ⟨TMw⟩;\\nif R terminates in the accept state\\nthen terminate in the accept state\\nelse if R terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nProve that M accepts w if and only if R′ (when given ⟨M, w⟩as input),\\nterminates in the accept state.\\nNow ﬁnish the proof by arguing that the language REGTM is undecidable.\\n5.11 A Java program P is called a Hello-World-program, if the following is\\ntrue: When given the empty string ϵ as input, P outputs the string Hello\\nWorld and then terminates. (We do not care what P does when the input\\nstring is non-empty.)\\n194\\nChapter 5.\\nDecidable and Undecidable Languages\\nConsider the language\\nHW = {⟨P⟩: P is a Hello-World-program}.\\nThe questions below will lead you through a proof of the claim that the\\nlanguage HW is undecidable.\\nConsider a ﬁxed Java program P and a ﬁxed binary string w. We write\\na new Java program JPw which takes as input an arbitrary binary string x:\\nJava program JPw(x):\\nrun P on the input w;\\nprint Hello World\\n• Argue that P terminates on input w if and only if ⟨JPw⟩∈HW .\\nThe goal is to prove that the language HW is undecidable. We will prove this\\nby contradiction. Thus, we assume that H is a Java program that decides\\nHW . Recall what this means:\\n• If P is a Hello-World-program, then H, when given ⟨P⟩as input, will\\nterminate in the accept state.\\n• If P is not a Hello-World-program, then H, when given ⟨P⟩as input,\\nwill terminate in the reject state.\\nWe write a new Java program H′ which takes as input the binary encoding\\n⟨P, w⟩of an arbitrary Java program P and an arbitrary binary string w:\\nJava program H′(⟨P, w⟩):\\nconstruct the Java program JPw described above;\\nrun H on the input ⟨JPw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\nArgue that the following are true:\\nExercises\\n195\\n• For any input ⟨P, w⟩, H′ terminates.\\n• If P terminates on input w, then H′ (when given ⟨P, w⟩as input),\\nterminates in the accept state.\\n• If P does not terminate on input w, then H′ (when given ⟨P, w⟩as\\ninput), terminates in the reject state.\\nNow ﬁnish the proof by arguing that the language HW is undecidable.\\n5.12 Prove that the language Halt, see Section 5.1.5, is enumerable.\\n5.13 We deﬁne the following language:\\nL = {u\\n:\\nu = ⟨0, M, w⟩for some ⟨M, w⟩∈ATM,\\nor u = ⟨1, M, w⟩for some ⟨M, w⟩̸∈ATM } .\\nProve that neither L nor its complement L is enumerable.\\nHint: There are two ways to solve this exercise. In the ﬁrst solution, (i)\\nyou assume that L is enumerable, and then prove that ATM is decidable, and\\n(ii) you assume that L is enumerable, and then prove that ATM is decidable.\\nIn the second solution, (i) you assume that L is enumerable, and then prove\\nthat ATM is enumerable, and (ii) you assume that L is enumerable, and then\\nprove that ATM is enumerable.\\n196\\nChapter 5.\\nDecidable and Undecidable Languages\\nChapter 6\\nComplexity Theory\\nIn the previous chapters, we have considered the problem of what can be\\ncomputed by Turing machines (i.e., computers) and what cannot be com-\\nputed. We did not, however, take the eﬃciency of the computations into\\naccount. In this chapter, we introduce a classiﬁcation of decidable languages\\nA, based on the running time of the “best” algorithm that decides A. That\\nis, given a decidable language A, we are interested in the “fastest” algorithm\\nthat, for any given string w, decides whether or not w ∈A.\\n6.1\\nThe running time of algorithms\\nLet M be a Turing machine, and let w be an input string for M. We deﬁne\\nthe running time tM(w) of M on input w as\\ntM(w) := the number of computation steps made by M on input w.\\nAs usual, we denote by |w|, the number of symbols in the string w. We\\ndenote the set of non-negative integers by N0.\\nDeﬁnition 6.1.1 Let Σ be an alphabet, let T : N0 →N0 be a function, let\\nA ⊆Σ∗be a decidable language, and let F : Σ∗→Σ∗be a computable\\nfunction.\\n• We say that the Turing machine M decides the language A in time T,\\nif\\ntM(w) ≤T(|w|)\\nfor all strings w in Σ∗.\\n198\\nChapter 6.\\nComplexity Theory\\n• We say that the Turing machine M computes the function F in time\\nT, if\\ntM(w) ≤T(|w|)\\nfor all strings w ∈Σ∗.\\nIn other words, the “running time function” T is a function of the length\\nof the input, which we usually denote by n. For any n, the value of T(n) is\\nan upper bound on the running time of the Turing machine M, on any input\\nstring of length n.\\nTo give an example, consider the Turing machine of Section 4.2.1 that\\ndecides, using one tape, the language consisting of all palindromes. The tape\\nhead of this Turing machine moves from the left to the right, then back to\\nthe left, then to the right again, back to the left, etc. Each time it reaches\\nthe leftmost or rightmost symbol, it deletes this symbol. The running time\\nof this Turing machine, on any input string of length n, is\\nO(1 + 2 + 3 + . . . + n) = O(n2).\\nOn the other hand, the running time of the Turing machine of Section 4.2.2,\\nwhich also decides the palindromes, but using two tapes instead of just one,\\nis O(n).\\nIn Section 4.4, we mentioned that all computation models listed there are\\nequivalent, in the sense that if a language can be decided in one model, it\\ncan be decided in any of the other models. We just saw, however, that the\\nlanguage consisting of all palindromes allows a faster algorithm on a two-\\ntape Turing machine than on one-tape Turing machines. (Even though we\\ndid not prove this, it is true that Ω(n2) is a lower bound on the running\\ntime to decide palindromes on a one-tape Turing machine.) The following\\ntheorem can be proved.\\nTheorem 6.1.2 Let A be a language (resp. let F be a function) that can be\\ndecided (resp. computed) in time T by an algorithm of type M. Then there is\\nan algorithm of type N that decides A (resp. computes F) in time T ′, where\\nM\\nN\\nT ′\\nk-tape Turing machine\\none-tape Turing machine\\nO(T 2)\\none-tape Turing machine\\nJava program\\nO(T 2)\\nJava program\\nk-tape Turing machine\\nO(T 4)\\n6.2.\\nThe complexity class P\\n199\\n6.2\\nThe complexity class P\\nDeﬁnition 6.2.1 We say that algorithm M decides the language A (resp.\\ncomputes the function F) in polynomial time, if there exists an integer k ≥1,\\nsuch that the running time of M is O(nk), for any input string of length n.\\nIt follows from Theorem 6.1.2 that this notion of “polynomial time” does\\nnot depend on the model of computation:\\nTheorem 6.2.2 Consider the models of computation “Java program”, “k-\\ntape Turing machine”, and “one-tape Turing machine”. If a language can\\nbe decided (resp. a function can be computed) in polynomial time in one of\\nthese models, then it can be decided (resp. computed) in polynomial time in\\nall of these models.\\nBecause of this theorem, we can deﬁne the following two complexity\\nclasses:\\nP := {A : the language A is decidable in polynomial time},\\nand\\nFP := {F : the function F is computable in polynomial time}.\\n6.2.1\\nSome examples\\nPalindromes\\nLet Pal be the language\\nPal := {w ∈{a, b}∗: w is a palindrome}.\\nWe have seen that there exists a one-tape Turing machine that decides Pal\\nin O(n2) time. Therefore, Pal ∈P.\\nSome functions in FP\\nThe following functions are in the class FP:\\n• F1 : N0 →N0 deﬁned by F1(x) := x + 1,\\n• F2 : N2\\n0 →N0 deﬁned by F2(x, y) := x + y,\\n• F3 : N2\\n0 →N0 deﬁned by F3(x, y) := xy.\\n200\\nChapter 6.\\nComplexity Theory\\nr\\nb\\nb\\nb\\nr\\nr\\nb\\nG1\\nG2\\nFigure 6.1: The graph G1 is 2-colorable; r stands for red; b stands for blue.\\nThe graph G2 is not 2-colorable.\\nContext-free languages\\nWe have shown in Section 5.1.3 that every context-free language is decid-\\nable. The algorithm presented there, however, does not run in polynomial\\ntime. Using a technique called dynamic programming (which you will learn\\nin COMP 3804), the following result can be shown:\\nTheorem 6.2.3 Let Σ be an alphabet, and let A ⊆Σ∗be a context-free\\nlanguage. Then A ∈P.\\nObserve that, obviously, every language in P is decidable.\\nThe 2-coloring problem\\nLet G be a graph with vertex set V and edge set E.\\nWe say that G is\\n2-colorable, if it is possible to give each vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. only two colors are used to color all vertices.\\nSee Figure 6.1 for two examples. We deﬁne the following language:\\n2Color := {⟨G⟩: the graph G is 2-colorable},\\nwhere ⟨G⟩denotes the binary string that encodes the graph G.\\n6.2.\\nThe complexity class P\\n201\\nWe claim that 2Color ∈P. In order to show this, we have to construct an\\nalgorithm that decides in polynomial time, whether or not any given graph\\nis 2-colorable.\\nLet G be an arbitrary graph with vertex set V = {1, 2, . . . , m}. The edge\\nset of G is given by an adjacency matrix. This matrix, which we denote by\\nE, is a two-dimensional array with m rows and m columns. For all i and j\\nwith 1 ≤i ≤m and 1 ≤j ≤m, we have\\nE(i, j) =\\n\\x1a 1\\nif (i, j) is an edge of G,\\n0\\notherwise.\\nThe length of the input G, i.e., the number of bits needed to specify G, is\\nequal to m2 =: n. We will present an algorithm that decides, in O(n) time,\\nwhether or not the graph G is 2-colorable.\\nThe algorithm uses the colors red and blue. It gives the ﬁrst vertex the\\ncolor red. Then, the algorithm considers all vertices that are connected by\\nan edge to the ﬁrst vertex, and colors them blue. Now the algorithm is done\\nwith the ﬁrst vertex; it marks this ﬁrst vertex.\\nNext, the algorithm chooses a vertex i that already has a color, but that\\nhas not been marked. Then it considers all vertices j that are connected by\\nan edge to i. If j has the same color as i, then the input graph G is not\\n2-colorable. Otherwise, if vertex j does not have a color yet, the algorithm\\ngives j the color that is diﬀerent from i’s color. After having done this for\\nall neighbors j of i, the algorithm is done with vertex i, so it marks i.\\nIt may happen that there is no vertex i that already has a color but that\\nhas not been marked. (In other words, each vertex i that is not marked does\\nnot have a color yet.) In this case, the algorithm chooses an arbitrary vertex\\ni having this property, and colors it red. (This vertex i is the ﬁrst vertex in\\nits connected component that gets a color.)\\nThis procedure is repeated until all vertices of G have been marked.\\nWe now give a formal description of this algorithm. Vertex i has been\\nmarked, if\\n1. i has a color,\\n2. all vertices that are connected by an edge to i have a color, and\\n3. the algorithm has veriﬁed that each vertex that is connected by an edge\\nto i has a color diﬀerent from i’s color.\\n202\\nChapter 6.\\nComplexity Theory\\nThe algorithm uses two arrays f(1 . . . m) and a(1 . . . m), and a variable\\nM. The value of f(i) is equal to the color (red or blue) of vertex i; if i does\\nnot have a color yet, then f(i) = 0. The value of a(i) is equal to\\na(i) =\\n\\x1a 1\\nif vertex i has been marked,\\n0\\notherwise.\\nThe value of M is equal to the number of marked vertices. The algorithm\\nis presented in Figure 6.2. You are encouraged to convince yourself of the\\ncorrectness of this algorithm. That is, you should convince yourself that this\\nalgorithm returns YES if the graph G is 2-colorable, whereas it returns NO\\notherwise.\\nWhat is the running time of this algorithm? First we count the number\\nof iterations of the outer while-loop. In one iteration, either M increases by\\none, or a vertex i, for which a(i) = 0, gets the color red. In the latter case,\\nthe variable M is increased during the next iteration of the outer while-loop.\\nSince, during the entire outer while-loop, the value of M is increased from\\nzero to m, it follows that there are at most 2m iterations of the outer while-\\nloop. (In fact, the number of iterations is equal to m plus the number of\\nconnected components of G minus one.)\\nOne iteration of the outer while-loop takes O(m) time. Hence, the total\\nrunning time of the algorithm is O(m2), which is O(n). Therefore, we have\\nshown that 2Color ∈P.\\n6.3\\nThe complexity class NP\\nBefore we deﬁne the class NP, we consider some examples.\\nExample 6.3.1 Let G be a graph with vertex set V and edge set E, and\\nlet k ≥1 be an integer. We say that G is k-colorable, if it is possible to give\\neach vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. at most k diﬀerent colors are used to color all vertices.\\nWe deﬁne the following language:\\nkColor := {⟨G⟩: the graph G is k-colorable}.\\n6.3.\\nThe complexity class NP\\n203\\nAlgorithm 2Color\\nfor i := 1 to m do f(i) := 0; a(i) := 0 endfor;\\nf(1) := red; M := 0;\\nwhile M ̸= m\\ndo (∗Find the minimum index i for which vertex i has not\\nbeen marked, but has a color already ∗)\\nbool := false; i := 1;\\nwhile bool = false and i ≤m\\ndo if a(i) = 0 and f(i) ̸= 0 then bool := true else i := i + 1 endif;\\nendwhile;\\n(∗If bool = true, then i is the smallest index such that\\na(i) = 0 and f(i) ̸= 0.\\nIf bool = false, then for all i, the following holds: if a(i) = 0, then\\nf(i) = 0; because M < m, there is at least one such i. ∗)\\nif bool = true\\nthen for j := 1 to m\\ndo if E(i, j) = 1\\nthen if f(i) = f(j)\\nthen return NO and terminate\\nelse if f(j) = 0\\nthen if f(i) = red\\nthen f(j) := blue\\nelse f(j) := red\\nendif\\nendif\\nendif\\nendif\\nendfor;\\na(i) := 1; M := M + 1;\\nelse i := 1;\\nwhile a(i) ̸= 0 do i := i + 1 endwhile;\\n(∗an unvisited connected component starts at vertex i ∗)\\nf(i) := red\\nendif\\nendwhile;\\nreturn YES\\nFigure 6.2:\\nAn algorithm that decides whether or not a graph G is 2-\\ncolorable.\\nWe have seen that for k = 2, this problem is in the class P. For k ≥3, it\\nis not known whether there exists an algorithm that decides, in polynomial\\ntime, whether or not any given graph is k-colorable. In other words, for\\n204\\nChapter 6.\\nComplexity Theory\\nk ≥3, it is not known whether or not kColor is in the class P.\\nExample 6.3.2 Let G be a graph with vertex set V = {1, 2, . . . , m} and\\nedge set E. A Hamilton cycle is a cycle in G that visits each vertex exactly\\nonce. Formally, it is a sequence v1, v2, . . . , vm of vertices such that\\n1. {v1, v2, . . . , vm} = V , and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nWe deﬁne the following language:\\nHC := {⟨G⟩: the graph G contains a Hamilton cycle}.\\nIt is not known whether or not HC is in the class P.\\nExample 6.3.3 The sum of subset language is deﬁned as follows:\\nSOS := {⟨a1, a2, . . . , am, b⟩:\\nm, a1, a2, . . . , am, b ∈N0 and\\n∃I ⊆{1, 2, . . . , m}, P\\ni∈I ai = b}.\\nAlso in this case, no polynomial-time algorithm is known that decides the\\nlanguage SOS. That is, it is not known whether or not SOS is in the class\\nP.\\nExample 6.3.4 An integer x ≥2 is a prime number, if there are no a, b ∈N\\nsuch that a ̸= x, b ̸= x, and x = ab. Hence, the language of all non-primes\\nthat are greater than or equal to two, is\\nNPrim := {⟨x⟩: x ≥2 and x is not a prime number}.\\nIt is not obvious at all, whether or not NPrim is in the class P. In fact, it\\nwas shown only in 2002 that NPrim is in the class P.\\nObservation 6.3.5 The four languages above have the following in com-\\nmon: If someone gives us a “solution” for any given input, then we can\\neasily, i.e., in polynomial time, verify whether or not this “solution” is a cor-\\nrect solution. Moreover, for any input to each of these four problems, there\\nexists a “solution” whose length is polynomial in the length of the input.\\n6.3.\\nThe complexity class NP\\n205\\nLet us again consider the language kColor. Let G be a graph with vertex\\nset V = {1, 2, . . . , m} and edge set E, and let k be a positive integer. We\\nwant to decide whether or not G is k-colorable. A “solution” is a coloring of\\nthe nodes using at most k diﬀerent colors. That is, a solution is a sequence\\nf1, f2, . . . , fm. (Interpret this as: vertex i receives color fi, 1 ≤i ≤m). This\\nsequence is a correct solution if and only if\\n1. fi ∈{1, 2, . . . , k}, for all i with 1 ≤i ≤m, and\\n2. for all i with 1 ≤i ≤m, and for all j with 1 ≤j ≤m, if (i, j) ∈E,\\nthen fi ̸= fj.\\nIf someone gives us this solution (i.e., the sequence f1, f2, . . . , fm), then\\nwe can verify in polynomial time whether or not these two conditions are\\nsatisﬁed. The length of this solution is O(m log k): for each i, we need about\\nlog k bits to represent fi. Hence, the length of the solution is polynomial in\\nthe length of the input, i.e., it is polynomial in the number of bits needed to\\nrepresent the graph G and the number k.\\nFor the Hamilton cycle problem, a solution consists of a sequence v1,\\nv2, . . . , vm of vertices. This sequence is a correct solution if and only if\\n1. {v1, v2, . . . , vm} = {1, 2, . . . , m} and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nThese two conditions can be veriﬁed in polynomial time.\\nMoreover, the\\nlength of the solution is polynomial in the length of the input graph.\\nConsider the sum of subset problem. A solution is a sequence c1, c2, . . . , cm.\\nIt is a correct solution if and only if\\n1. ci ∈{0, 1}, for all i with 1 ≤i ≤m, and\\n2. Pm\\ni=1 ciai = b.\\nHence, the set I ⊆{1, 2, . . . , m} in the deﬁnition of SOS is the set of indices\\ni for which ci = 1. Again, these two conditions can be veriﬁed in polynomial\\ntime, and the length of the solution is polynomial in the length of the input.\\nFinally, let us consider the language NPrim. Let x ≥2 be an integer.\\nThe integers a and b form a “solution” for x if and only if\\n206\\nChapter 6.\\nComplexity Theory\\n1. 2 ≤a < x,\\n2. 2 ≤b < x, and\\n3. x = ab.\\nClearly, these three conditions can be veriﬁed in polynomial time. Moreover,\\nthe length of this solution, i.e., the total number of bits in the binary rep-\\nresentations of a and b, is polynomial in the number of bits in the binary\\nrepresentation of x.\\nLanguages having the property that the correctness of a proposed “solu-\\ntion” can be veriﬁed in polynomial time, form the class NP:\\nDeﬁnition 6.3.6 A language A belongs to the class NP, if there exist a\\npolynomial p and a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\nIn words, a language A is in the class NP, if for every string w, w ∈A if\\nand only if the following two conditions are satisﬁed:\\n1. There is a “solution” s, whose length |s| is polynomial in the length of\\nw (i.e., |s| ≤p(|w|), where p is a polynomial).\\n2. In polynomial time, we can verify whether or not s is a correct “solu-\\ntion” for w (i.e., ⟨w, s⟩∈B and B ∈P).\\nHence, the language B can be regarded to be the “veriﬁcation language”:\\nB = {⟨w, s⟩: s is a correct “solution” for w}.\\nWe have given already informal proofs of the fact that the languages\\nkColor, HC, SOS, and NPrim are all contained in the class NP. Below, we\\nformally prove that NPrim ∈NP. To prove this claim, we have to specify\\nthe polynomial p and the language B ∈P. First, we observe that\\nNPrim = {⟨x⟩:\\nthere exist a and b in N such that\\n2 ≤a < x, 2 ≤b < x and x = ab }.\\n(6.1)\\nWe deﬁne the polynomial p by p(n) := n + 2, and the language B as\\nB := {⟨x, a, b⟩: x ≥2, 2 ≤a < x, 2 ≤b < x and x = ab}.\\n6.3.\\nThe complexity class NP\\n207\\nIt is obvious that B ∈P: For any three positive integers x, a, and b, we\\ncan verify in polynomial time whether or not ⟨x, a, b⟩∈B. In order to do\\nthis, we only have to verify whether or not x ≥2, 2 ≤a < x, 2 ≤b < x,\\nand x = ab. If all these four conditions are satisﬁed, then ⟨x, a, b⟩∈B. If at\\nleast one of them is not satisﬁed, then ⟨x, a, b⟩̸∈B.\\nIt remains to show that for all x ∈N:\\n⟨x⟩∈NPrim ⇐⇒∃a, b : |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B.\\n(6.2)\\n(Remember that |⟨x⟩| denotes the number of bits in the binary representation\\nof x; |⟨a, b⟩| denotes the total number of bits of a and b, i.e., |⟨a, b⟩| =\\n|⟨a⟩| + |⟨b⟩|.)\\nLet x ∈NPrim. It follows from (6.1) that there exist a and b in N, such\\nthat 2 ≤a < x, 2 ≤b < x, and x = ab. Since x = ab ≥2 · 2 = 4 ≥2, it\\nfollows that ⟨x, a, b⟩∈B. Hence, it remains to show that\\n|⟨a, b⟩| ≤|⟨x⟩| + 2.\\nThe binary representation of x contains ⌊log x⌋+1 bits, i.e., |⟨x⟩| = ⌊log x⌋+1.\\nWe have\\n|⟨a, b⟩|\\n=\\n|⟨a⟩| + |⟨b⟩|\\n=\\n(⌊log a⌋+ 1) + (⌊log b⌋+ 1)\\n≤\\nlog a + log b + 2\\n=\\nlog ab + 2\\n=\\nlog x + 2\\n≤\\n⌊log x⌋+ 3\\n=\\n|⟨x⟩| + 2.\\nThis proves one direction of (6.2).\\nTo prove the other direction, we assume that there are positive integers\\na and b, such that |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B. Then it follows\\nimmediately from (6.1) and the deﬁnition of the language B, that x ∈NPrim.\\nHence, we have proved the other direction of (6.2). This completes the proof\\nof the claim that\\nNPrim ∈NP.\\n208\\nChapter 6.\\nComplexity Theory\\n6.3.1\\nP is contained in NP\\nIntuitively, it is clear that P ⊆NP, because a language is\\n• in P, if for every string w, it is possible to compute the “solution” s in\\npolynomial time,\\n• in NP, if for every string w and for any given “solution” s, it is possible\\nto verify in polynomial time whether or not s is a correct solution for\\nw (hence, we do not need to compute the solution s ourselves, we only\\nhave to verify it).\\nWe give a formal proof of this:\\nTheorem 6.3.7 P ⊆NP.\\nProof. Let A ∈P. We will prove that A ∈NP. Deﬁne the polynomial p\\nby p(n) := 0 for all n ∈N0, and deﬁne\\nB := {⟨w, ϵ⟩: w ∈A}.\\nSince A ∈P, the language B is also contained in P. It is easy to see that\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) = 0 and ⟨w, s⟩∈B.\\nThis completes the proof.\\n6.3.2\\nDeciding NP-languages in exponential time\\nLet us look again at the deﬁnition of the class NP. Let A be a language in\\nthis class. Then there exist a polynomial p and a language B ∈P, such that\\nfor all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.3)\\nHow do we decide whether or not any given string w belongs to the language\\nA? If we can ﬁnd a string s that satisﬁes the right-hand side in (6.3), then\\nwe know that w ∈A. On the other hand, if there is no such string s, then\\nw ̸∈A. How much time do we need to decide whether or not such a string s\\nexists?\\n6.3.\\nThe complexity class NP\\n209\\nAlgorithm NonPrime\\n(∗decides whether or not ⟨x⟩∈NPrim ∗)\\nif x = 0 or x = 1 or x = 2\\nthen return NO and terminate\\nelse a := 2;\\nwhile a < x\\ndo if x mod a = 0\\nthen return YES and terminate\\nelse a := a + 1\\nendif\\nendwhile;\\nreturn NO\\nendif\\nFigure 6.3: An algorithm that decides whether or not a number x is contained\\nin the language NPrim.\\nFor example, let A be the language\\nNPrim = {⟨x⟩: x ≥2 and x is not a prime number},\\nand let x ∈N. The algorithm in Figure 6.3 decides whether or not ⟨x⟩∈\\nNPrim.\\nIt is clear that this algorithm is correct. Let n be the length of the binary\\nrepresentation of x, i.e., n = ⌊log x⌋+ 1. If x > 2 and x is a prime number,\\nthen the while-loop makes x−2 iterations. Therefore, since n−1 = ⌊log x⌋≤\\nlog x, the running time of this algorithm is at least\\nx −2 ≥2n−1 −2,\\ni.e., it is at least exponential in the length of the input.\\nWe now prove that every language in NP can be decided in exponential\\ntime. Let A be an arbitrary language in NP. Let p be the polynomial, and\\nlet B ∈P be the language such that for all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.4)\\nThe following algorithm decides, for any given string w, whether or not\\nw ∈A. It does so by looking at all possible strings s for which |s| ≤p(|w|):\\n210\\nChapter 6.\\nComplexity Theory\\nfor all s with |s| ≤p(|w|)\\ndo if ⟨w, s⟩∈B\\nthen return YES and terminate\\nendif\\nendfor;\\nreturn NO\\nThe correctness of the algorithm follows from (6.4). What is the running\\ntime? We assume that w and s are represented as binary strings. Let n be\\nthe length of the input, i.e., n = |w|.\\nHow many binary strings s are there whose length is at most p(|w|)? Any\\nsuch s can be described by a sequence of length p(|w|) = p(n), consisting of\\nthe symbols “0”, “1”, and the blank symbol. Hence, there are at most 3p(n)\\nmany binary strings s with |s| ≤p(n). Therefore, the for-loop makes at most\\n3p(n) iterations.\\nSince B ∈P, there is an algorithm and a polynomial q, such that this\\nalgorithm, when given any input string z, decides in q(|z|) time, whether or\\nnot z ∈B. This input z has the form ⟨w, s⟩, and we have\\n|z| = |w| + |s| ≤|w| + p(|w|) = n + p(n).\\nIt follows that the total running time of our algorithm that decides whether\\nor not w ∈A, is bounded from above by\\n3p(n) · q(n + p(n))\\n≤\\n22p(n) · q(n + p(n))\\n≤\\n22p(n) · 2q(n+p(n))\\n=\\n2p′(n),\\nwhere p′ is the polynomial that is deﬁned by p′(n) := 2p(n) + q(n + p(n)).\\nIf we deﬁne the class EXP as\\nEXP :=\\n{A :\\nthere exists a polynomial p, such that A can be\\ndecided in time 2p(n) } ,\\nthen we have proved the following theorem.\\nTheorem 6.3.8 NP ⊆EXP.\\n6.4.\\nNon-deterministic algorithms\\n211\\n6.3.3\\nSummary\\n• P ⊆NP. It is not known whether P is a proper subclass of NP, or\\nwhether P = NP. This is one of the most important open problems in\\ncomputer science. If you can solve this problem, then you will get one\\nmillion dollars; not from us, but from the Clay Mathematics Institute,\\nsee\\nhttp://www.claymath.org/prizeproblems/index.htm\\nMost people believe that P is a proper subclass of NP.\\n• NP ⊆EXP, i.e., each language in NP can be decided in exponential\\ntime. It is not known whether NP is a proper subclass of EXP, or\\nwhether NP = EXP.\\n• It follows from P ⊆NP and NP ⊆EXP, that P ⊆EXP. It can\\nbe shown that P is a proper subset of EXP, i.e., there exist languages\\nthat can be decided in exponential time, but that cannot be decided in\\npolynomial time.\\n• P is the class of those languages that can be decided eﬃciently, i.e., in\\npolynomial time. Sets that are not in P, are not eﬃciently decidable.\\n6.4\\nNon-deterministic algorithms\\nThe abbreviation NP stands for Non-deterministic Polynomial time. The al-\\ngorithms that we have considered so far are deterministic, which means that\\nat any time during the computation, the next computation step is uniquely\\ndetermined. In a non-deterministic algorithm, there are one or more possi-\\nbilities for being the next computation step, and the algorithm chooses one\\nof them.\\nTo give an example, we consider the language SOS, see Example 6.3.3.\\nLet m, a1, a2, . . . , am, and b be elements of N0. Then\\n⟨a1, a2, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, c2, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciai = b.\\nThe following non-deterministic algorithm decides the language SOS:\\n212\\nChapter 6.\\nComplexity Theory\\nAlgorithm SOS(m, a1, a2, . . . , am, b):\\ns := 0;\\nfor i := 1 to m\\ndo s := s | s := s + ai\\nendfor;\\nif s = b\\nthen return YES\\nelse return NO\\nendif\\nThe line\\ns := s | s := s + ai\\nmeans that either the instruction “s := s” or the instruction “s := s + ai” is\\nexecuted.\\nLet us assume that ⟨a1, a2, . . . , am, b⟩∈SOS. Then there are c1, c2, . . . , cm ∈\\n{0, 1} such that Pm\\ni=1 ciai = b. Assume our algorithm does the following, for\\neach i with 1 ≤i ≤m: In the i-th iteration,\\n• if ci = 0, then it executes the instruction “s := s”,\\n• if ci = 1, then it executes the instruction “s := s + ai”.\\nThen after the for-loop, we have s = b, and the algorithm returns YES;\\nhence, the algorithm has correctly found out that ⟨a1, a2, . . . , am, b⟩∈SOS.\\nIn other words, in this case, there exists at least one accepting computation.\\nOn the other hand, if ⟨a1, a2, . . . , am, b⟩̸∈SOS, then the algorithm always\\nreturns NO, no matter which of the two instructions is executed in each\\niteration of the for-loop. In this case, there is no accepting computation.\\nDeﬁnition 6.4.1 Let M be a non-deterministic algorithm. We say that M\\naccepts a string w, if there exists at least one computation that, on input w,\\nreturns YES.\\nDeﬁnition 6.4.2 We say that a non-deterministic algorithm M decides a\\nlanguage A in time T, if for every string w, the following holds: w ∈A if\\nand only if there exists at least one computation that, on input w, returns\\nYES and that takes at most T(|w|) time.\\n6.5.\\nNP-complete languages\\n213\\nThe non-deterministic algorithm that we have seen above decides the\\nlanguage SOS in linear time: Let ⟨a1, a2, . . . , am, b⟩∈SOS, and let n be the\\nlength of this input. Then\\nn = |⟨a1⟩| + |⟨a2⟩| + . . . + |⟨am⟩| + |⟨b⟩| ≥m.\\nFor this input, there is a computation that returns YES and that takes\\nO(m) = O(n) time.\\nAs in Section 6.2, we deﬁne the notion of “polynomial time” for non-\\ndeterministic algorithms. The following theorem relates this notion to the\\nclass NP that we deﬁned in Deﬁnition 6.3.6.\\nTheorem 6.4.3 A language A is in the class NP if and only if there exists\\na non-deterministic Turing machine (or Java program) that decides A in\\npolynomial time.\\n6.5\\nNP-complete languages\\nLanguages in the class P are considered easy, i.e., they can be decided in\\npolynomial time. People believe (but cannot prove) that P is a proper sub-\\nclass of NP. If this is true, then there are languages in NP that are hard,\\ni.e., cannot be decided in polynomial time.\\nIntuition tells us that if P ̸= NP, then the hardest languages in NP are\\nnot contained in P. These languages are called NP-complete. In this section,\\nwe will give a formal deﬁnition of this concept.\\nIf we want to talk about the “hardest” languages in NP, then we have to\\nbe able to compare two languages according to their “diﬃculty”. The idea is\\nas follows: We say that a language B is “at least as hard” as a language A,\\nif the following holds: If B can be decided in polynomial time, then A can\\nalso be decided in polynomial time.\\nDeﬁnition 6.5.1 Let A ⊆{0, 1}∗and B ⊆{0, 1}∗be languages. We say\\nthat A ≤P B, if there exists a function\\nf : {0, 1}∗→{0, 1}∗\\nsuch that\\n1. f ∈FP and\\n214\\nChapter 6.\\nComplexity Theory\\n2. for all strings w in {0, 1}∗,\\nw ∈A ⇐⇒f(w) ∈B.\\nIf A ≤P B, then we also say that “B is at least as hard as A”, or “A is\\npolynomial-time reducible to B”.\\nWe ﬁrst show that this formal deﬁnition is in accordance with the intuitive\\ndeﬁnition given above.\\nTheorem 6.5.2 Let A and B be languages such that B ∈P and A ≤P B.\\nThen A ∈P.\\nProof. Let f : {0, 1}∗→{0, 1}∗be the function in FP for which\\nw ∈A ⇐⇒f(w) ∈B.\\n(6.5)\\nThe following algorithm decides whether or not any given binary string w is\\nin A:\\nu := f(w);\\nif u ∈B\\nthen return YES\\nelse return NO\\nendif\\nThe correctness of this algorithm follows immediately from (6.5). So it\\nremains to show that the running time is polynomial in the length of the\\ninput string w.\\nSince f ∈FP, there exists a polynomial p such that the function f can\\nbe computed in time p. Similarly, since B ∈P, there exists a polynomial q,\\nsuch that the language B can be decided in time q.\\nLet n be the length of the input string w, i.e., n = |w|. Then the length\\nof the string u is less than or equal to p(|w|) = p(n). (Why?) Therefore, the\\nrunning time of our algorithm is bounded from above by\\np(|w|) + q(|u|) ≤p(n) + q(p(n)).\\nSince the function p′, deﬁned by p′(n) := p(n)+q(p(n)), is a polynomial, this\\nproves that A ∈P.\\nThe following theorem states that the relation ≤P is reﬂexive and tran-\\nsitive. We leave the proof as an exercise.\\n6.5.\\nNP-complete languages\\n215\\nTheorem 6.5.3 Let A, B, and C be languages. Then\\n1. A ≤P A, and\\n2. if A ≤P B and B ≤P C, then A ≤P C.\\nWe next show that the languages in P are the easiest languages in NP:\\nTheorem 6.5.4 Let A be a language in P, and let B be an arbitrary lan-\\nguage such that B ̸= ∅and B ̸= {0, 1}∗. Then A ≤P B.\\nProof. We choose two strings u and v in {0, 1}∗, such that u ∈B and v ̸∈B.\\n(Observe that this is possible.) Deﬁne the function f : {0, 1}∗→{0, 1}∗by\\nf(w) :=\\n\\x1a u\\nif w ∈A,\\nv\\nif w ̸∈A.\\nThen it is clear that for any binary string w,\\nw ∈A ⇐⇒f(w) ∈B.\\nSince A ∈P, the function f can be computed in polynomial time, i.e.,\\nf ∈FP.\\n6.5.1\\nTwo examples of reductions\\nSum of subsets and knapsacks\\nWe start with a simple reduction. Consider the two languages\\nSOS := {⟨a1, . . . , am, b⟩:\\nm, a1, . . . , am, b ∈N0 and there exist\\nc1, . . . , cm ∈{0, 1}, such that Pm\\ni=1 ciai = b}\\nand\\nKS\\n:=\\n{⟨w1, . . . , wm, k1, . . . , km, W, K⟩:\\nm, w1, . . . , wm, k1, . . . , km, W, K ∈N0\\nand there exist c1, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciwi ≤W and Pm\\ni=1 ciki ≥K}.\\nThe notation KS stands for knapsack: We have m pieces of food. The\\ni-th piece has weight wi and contains ki calories. We want to decide whether\\nor not we can ﬁll our knapsack with a subset of the pieces of food such that\\nthe total weight is at most W, and the total amount of calories is at least K.\\n216\\nChapter 6.\\nComplexity Theory\\nTheorem 6.5.5 SOS ≤P KS.\\nProof. Let us ﬁrst see what we have to show. According to Deﬁnition 6.5.1,\\nwe need a function f ∈FP, that maps input strings for SOS to input strings\\nfor KS, in such a way that\\n⟨a1, . . . , am, b⟩∈SOS ⇐⇒f(⟨a1, . . . , am, b⟩) ∈KS.\\nIn order for f(⟨a1, . . . , am, b⟩) to be an input string for KS, this function\\nvalue has to be of the form\\nf(⟨a1, . . . , am, b⟩) = ⟨w1, . . . , wm, k1, . . . , km, W, K⟩.\\nWe deﬁne\\nf(⟨a1, . . . , am, b⟩) := ⟨a1, . . . , am, a1, . . . , am, b, b⟩.\\nIt is clear that f ∈FP. We have\\n⟨a1, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai = b\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai ≤b and Pm\\ni=1 ciai ≥b\\n⇐⇒\\n⟨a1, . . . , am, a1, . . . , am, b, b⟩∈KS\\n⇐⇒\\nf(⟨a1, . . . , am, b⟩) ∈KS.\\nCliques and Boolean formulas\\nWe will deﬁne two languages A = 3SAT and B = Clique that have, at\\nﬁrst sight, nothing to do with each other. Then we show that, nevertheless,\\nA ≤P B.\\nLet G be a graph with vertex set V and edge set E. A subset V ′ of V is\\ncalled a clique, if each pair of distinct vertices in V ′ is connected by an edge\\nin E. We deﬁne the following language:\\nClique := {⟨G, k⟩: k ∈N and G has a clique with k vertices}.\\nWe encourage you to prove the following claim:\\n6.5.\\nNP-complete languages\\n217\\nTheorem 6.5.6 Clique ∈NP.\\nNext we consider Boolean formulas ϕ, with variables x1, x2, . . . , xm, hav-\\ning the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.6)\\nwhere each Ci, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nEach ℓi\\na is either a variable or the negation of a variable. An example of such\\na formula is\\nϕ = (x1 ∨¬x1 ∨¬x2) ∧(x3 ∨x2 ∨x4) ∧(¬x1 ∨¬x3 ∨¬x4).\\nA formula ϕ of the form (6.6) is said to be satisﬁable, if there exists a truth-\\nvalue in {0, 1} for each of the variables x1, x2, . . . , xm, such that the entire\\nformula ϕ is true. Our example formula is satisﬁable: If we take x1 = 0 and\\nx2 = 1, and give x3 and x4 an arbitrary value, then\\nϕ = (0 ∨1 ∨0) ∧(x3 ∨1 ∨x4) ∧(1 ∨¬x3 ∨¬x4) = 1.\\nWe deﬁne the following language:\\n3SAT := {⟨ϕ⟩: ϕ is of the form (6.6) and is satisﬁable}.\\nAgain, we encourage you to prove the following claim:\\nTheorem 6.5.7 3SAT ∈NP.\\nObserve that the elements of Clique (which are pairs consisting of a graph\\nand a positive integer) are completely diﬀerent from the elements of 3SAT\\n(which are Boolean formulas). We will show that 3SAT ≤P Clique. Recall\\nthat this means the following: If the language Clique can be decided in\\npolynomial time, then the language 3SAT can also be decided in polynomial\\ntime. In other words, any polynomial-time algorithm that decides Clique can\\nbe converted to a polynomial-time algorithm that decides 3SAT.\\nTheorem 6.5.8 3SAT ≤P Clique.\\n218\\nChapter 6.\\nComplexity Theory\\nProof. We have to show that there exists a function f ∈FP, that maps\\ninput strings for 3SAT to input strings for Clique, such that for each Boolean\\nformula ϕ that is of the form (6.6),\\n⟨ϕ⟩∈3SAT ⇐⇒f(⟨ϕ⟩) ∈Clique.\\nThe function f maps the binary string encoding an arbitrary Boolean formula\\nϕ to a binary string encoding a pair (G, k), where G is a graph and k is a\\npositive integer. We have to deﬁne this function f in such a way that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\nLet\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an arbitrary Boolean formula in the variables x1, x2, . . . , xm, where each\\nCi, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nRemember that each ℓi\\na is either a variable or the negation of a variable.\\nThe formula ϕ is mapped to the pair (G, k), where the vertex set V and\\nthe edge set E of the graph G are deﬁned as follows:\\n• V = {v1\\n1, v1\\n2, v1\\n3, . . . , vk\\n1, vk\\n2, vk\\n3}. The idea is that each vertex vi\\na corre-\\nsponds to one term ℓi\\na.\\n• The pair (vi\\na, vj\\nb) of vertices form an edge in E if and only if\\n– i ̸= j and\\n– ℓi\\na is not the negation of ℓj\\nb.\\nTo give an example, let ϕ be the Boolean formula\\nϕ = (x1 ∨¬x2 ∨¬x3) ∧(¬x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3),\\n(6.7)\\ni.e., k = 3, C1 = x1 ∨¬x2 ∨¬x3, C2 = ¬x1 ∨x2 ∨x3, and C3 = x1 ∨x2 ∨x3.\\nThe graph G that corresponds to this formula is given in Figure 6.4.\\nIt is not diﬃcult to see that the function f can be computed in polynomial\\ntime. So it remains to prove that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\n(6.8)\\n6.5.\\nNP-complete languages\\n219\\n¬x2\\n¬x3\\nx1\\n¬x1\\nx2\\nx3\\nx1\\nx2\\nx3\\nFigure 6.4: The formula ϕ in (6.7) is mapped to this graph. The vertices on\\nthe top represent C1; the vertices on the left represent C2; the vertices on\\nthe right represent C3.\\nTo prove this, we ﬁrst assume that the formula\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nis satisﬁable. Then there exists a truth-value in {0, 1} for each of the variables\\nx1, x2, . . . , xm, such that the entire formula ϕ is true. Hence, for each i with\\n1 ≤i ≤k, there is at least one term ℓi\\na in\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3\\nthat is true (i.e., has value 1).\\nLet V ′ be the set of vertices obtained by choosing for each i, 1 ≤i ≤k,\\nexactly one vertex vi\\na such that ℓi\\na has value 1.\\nIt is clear that V ′ contains exactly k vertices. We claim that this set is\\na clique in G. To prove this claim, let vi\\na and vj\\nb be two distinct vertices in\\nV ′. It follows from the deﬁnition of V ′ that i ̸= j and ℓi\\na = ℓj\\nb = 1. Hence,\\nℓi\\na is not the negation of ℓj\\nb. But this means that the vertices vi\\na and vj\\nb are\\nconnected by an edge in G.\\nThis proves one direction of (6.8). To prove the other direction, we assume\\nthat the graph G contains a clique V ′ with k vertices.\\n220\\nChapter 6.\\nComplexity Theory\\nThe vertices of G consist of k groups, where each group contains exactly\\nthree vertices. Since vertices within the same group are not connected by\\nedges, the clique V ′ contains exactly one vertex from each group. Hence, for\\neach i with 1 ≤i ≤k, there is exactly one a, such that vi\\na ∈V ′. Consider\\nthe corresponding term ℓi\\na. We know that this term is either a variable or\\nthe negation of a variable, i.e., ℓi\\na is either of the form xj or of the form ¬xj.\\nIf ℓi\\na = xj, then we give xj the truth-value 1. Otherwise, we have ℓi\\na = ¬xj,\\nin which case we give xj the truth-value 0. Since V ′ is a clique, each variable\\ngets at most one truth-value. If a variable has no truth-value yet, then we\\ngive it an arbitrary truth-value.\\nIf we substitute these truth-values into ϕ, then the entire formula has\\nvalue 1. Hence, ϕ is satisﬁable.\\nIn order to get a better understanding of this proof, you should verify the\\nproof for the formula ϕ in (6.7) and the graph G in Figure 6.4.\\n6.5.2\\nDeﬁnition of NP-completeness\\nReductions, as deﬁned in Deﬁnition 6.5.1, allow us to compare two language\\naccording to their diﬃculty. A language B in NP is called NP-complete,\\nif B belongs to the most diﬃcult languages in NP; in other words, B is at\\nleast as hard as any other language in NP.\\nDeﬁnition 6.5.9 Let B ⊆{0, 1}∗be a language. We say that B is NP-\\ncomplete, if\\n1. B ∈NP and\\n2. A ≤P B, for every language A in NP.\\nTheorem 6.5.10 Let B be an NP-complete language. Then\\nB ∈P ⇐⇒P = NP.\\nProof. Intuitively, this theorem should be true: If the language B is in P,\\nthen B is an easy language. On the other hand, since B is NP-complete,\\nit belongs to the most diﬃcult languages in NP. Hence, the most diﬃcult\\nlanguage in NP is easy. But then all languages in NP must be easy, i.e.,\\nP = NP.\\n6.5.\\nNP-complete languages\\n221\\nWe give a formal proof. Let us ﬁrst assume that B ∈P. We already\\nknow that P ⊆NP. Hence, it remains to show that NP ⊆P. Let A be an\\narbitrary language in NP. Since B is NP-complete, we have A ≤P B. Then,\\nby Theorem 6.5.2, we have A ∈P.\\nTo prove the converse, assume that P = NP. Since B ∈NP, it follows\\nimmediately that B ∈P.\\nTheorem 6.5.11 Let B and C be languages, such that C ∈NP and B ≤P\\nC. If B is NP-complete, then C is also NP-complete.\\nProof. First, we give an intuitive explanation of the claim: By assumption,\\nB belongs to the most diﬃcult languages in NP, and C is at least as hard as\\nB. Since C ∈NP, it follows that C belongs to the most diﬃcult languages\\nin NP. Hence, C is NP-complete.\\nTo give a formal proof, we have to show that A ≤P C, for all languages A\\nin NP. Let A be an arbitrary language in NP. Since B is NP-complete, we\\nhave A ≤P B. Since B ≤P C, it follows from Theorem 6.5.3, that A ≤P C.\\nTherefore, C is NP-complete.\\nTheorem 6.5.11 can be used to prove the NP-completeness of languages:\\nLet C be a language, and assume that we want to prove that C is NP-\\ncomplete. We can do this in the following way:\\n1. We ﬁrst prove that C ∈NP.\\n2. Then we ﬁnd a language B that looks “similar” to C, and for which\\nwe already know that it is NP-complete.\\n3. Finally, we prove that B ≤P C.\\n4. Then, Theorem 6.5.11 tells us that C is NP-complete.\\nOf course, this leads to the question “How do we know that the language\\nB is NP-complete?” In order to apply Theorem 6.5.11, we need a “ﬁrst” NP-\\ncomplete language; the NP-completeness of this language must be proven\\nusing Deﬁnition 6.5.9.\\nObserve that it is not clear at all that there exist NP-complete languages!\\nFor example, consider the language 3SAT. If we want to use Deﬁnition 6.5.9\\nto show that this language is NP-complete, then we have to show that\\n222\\nChapter 6.\\nComplexity Theory\\n• 3SAT ∈NP. We know from Theorem 6.5.7 that this is true.\\n• A ≤P 3SAT, for every language A ∈NP. Hence, we have to show this\\nfor languages A such as kColor, HC, SOS, NPrim, KS, Clique, and\\nfor inﬁnitely many other languages.\\nIn 1971, Cook has exactly done this: He showed that the language 3SAT\\nis NP-complete. Since his proof is rather technical, we will prove the NP-\\ncompleteness of another language.\\n6.5.3\\nAn NP-complete domino game\\nWe are given a ﬁnite collection of tile types. For each such type, there are\\narbitrarily many tiles of this type. A tile is a square that is partitioned into\\nfour triangles. Each of these triangles contains a symbol that belongs to a\\nﬁnite alphabet Σ. Hence, a tile looks as follows:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\na\\nb\\nc\\nd\\nWe are also given a square frame, consisting of cells. Each cell has the same\\nsize as a tile, and contains a symbol of Σ.\\nThe problem is to decide whether or not this domino game has a solution.\\nThat is, can we completely ﬁll the frame with tiles such that\\n• for any two neighboring tiles s and s′, the two triangles of s and s′ that\\ntouch each other contain the same symbol, and\\n• each triangle that touches the frame contains the same symbol as the\\ncell of the frame that is touched by this triangle.\\nThere is one ﬁnal restriction: The orientation of the tiles is ﬁxed, they cannot\\nbe rotated.\\nLet us give a formal deﬁnition of this problem. We assume that the sym-\\nbols belong to the ﬁnite alphabet Σ = {0, 1}m, i.e., each symbol is encoded\\nas a bit-string of length m. Then, a tile type can be encoded as a tuple of\\nfour bit-strings, i.e., as an element of Σ4. A frame consisting of t rows and t\\ncolumns can be encoded as a string in Σ4t.\\n6.5.\\nNP-complete languages\\n223\\nWe denote the language of all solvable domino games by Domino:\\nDomino\\n:=\\n{⟨m, k, t, R, T1, . . . , Tk⟩:\\nm ≥1, k ≥1, t ≥1, R ∈Σ4t, Ti ∈Σ4, 1 ≤i ≤k,\\nframe R can be ﬁlled using tiles of types\\nT1, . . . , Tk.}\\nWe will prove the following theorem.\\nTheorem 6.5.12 The language Domino is NP-complete.\\nProof. It is clear that Domino ∈NP: A solution consists of a t × t matrix,\\nin which the (i, j)-entry indicates the type of the tile that occupies position\\n(i, j) in the frame. The number of bits needed to specify such a solution is\\npolynomial in the length of the input. Moreover, we can verify in polynomial\\ntime whether or not any given “solution” is correct.\\nIt remains to show that\\nA ≤P Domino, for every language A in NP.\\nLet A be an arbitrary language in NP. Then there exist a polynomial p and\\na non-deterministic Turing machine M, that decides the language A in time\\np. We may assume that this Turing machine has only one tape.\\nOn input w = a1a2 . . . an, the Turing machine M starts in the start state\\nz0, with its tape head on the cell containing the symbol a1. We may assume\\nthat during the entire computation, the tape head never moves to the left of\\nthis initial cell. Hence, the entire computation “takes place” in and to the\\nright of the initial cell. We know that\\nw ∈A\\n⇐⇒\\non input w, there exists an accepting computation\\nthat makes at most p(n) computation steps.\\nAt the end of such an accepting computation, the tape only contains the\\nsymbol 1, which we may assume to be in the initial cell, and M is in the ﬁnal\\nstate z1. In this case, we may assume that the accepting computation makes\\nexactly p(n) computation steps. (If this is not the case, then we extend the\\ncomputation using the instruction z11 →z11N.)\\nWe need one more technical detail: We may assume that za →z′bR and\\nza′ →z′′b′L are not both instructions of M. Hence, the state of the Turing\\nmachine uniquely determines the direction in which the tape head moves.\\n224\\nChapter 6.\\nComplexity Theory\\nWe have to deﬁne a domino game, that depends on the input string w\\nand the Turing machine M, such that\\nw ∈A ⇐⇒this domino game is solvable.\\nThe idea is to encode an accepting computation of the Turing machine M as\\na solution of the domino game. In order to do this, we use a frame in which\\neach row corresponds to one computation step. This frame consists of p(n)\\nrows. Since an accepting computation makes exactly p(n) computation steps,\\nand since the tape head never moves to the left of the initial cell, this tape\\nhead can visit only p(n) cells. Therefore, our frame will have p(n) columns.\\nThe domino game will use the following tile types:\\n1. For each symbol a in the alphabet of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\n#\\na\\nIntuition: Before and after the computation step, the tape head is not\\non this cell.\\n2. For each instruction za →z′bR of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\nz′\\nb\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the right.\\n3. For each instruction za →z′bL of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz′\\n(z, a)\\n#\\nb\\n6.5.\\nNP-complete languages\\n225\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the left.\\n4. For each instruction za →z′bN of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\n#\\n(z′, b)\\nIntuition: Before and after the computation step, the tape head is on\\nthis cell.\\n5. For each state z and for each symbol a in the alphabet of the Turing\\nmachine M, there are two tile types:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz\\na\\n#\\n(z, a)\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\nz\\n(z, a)\\nIntuition: The leftmost tile indicates that the tape head enters this cell\\nfrom the left; the righmost tile indicates that the tape head enters this\\ncell from the right.\\nThis speciﬁes all tile types. The p(n) × p(n) frame is given in Figure 6.5.\\nThe top row corresponds to the start of the computation, whereas the bottom\\nrow corresponds to the end of the computation. The left and right columns\\ncorrespond to the part of the tape in which the tape head can move.\\nThe encodings of these tile types and the frame can be computed in\\npolynomial time.\\nIt can be shown that, for any input string w, any accepting computation\\nof length p(n) of the Turing machine M can be encoded as a solution of\\nthis domino game. Conversely, any solution of this domino game can be\\n“translated” to an accepting computation of length p(n) of M, on input\\nstring w. Hence, the following holds.\\nw ∈A\\n⇐⇒\\nthere exists an accepting computation that makes\\np(n) computation steps\\n⇐⇒\\nthe domino game is solvable.\\n226\\nChapter 6.\\nComplexity Theory\\n(z0, a1)\\na2\\n. . .\\nan\\n✷\\n. . .\\n✷\\n#\\n#\\n#\\n#\\n#\\n...\\n#\\n...\\n✷\\n✷\\n✷\\n✷\\n✷\\n. . .\\n(z1, 1)\\np(n)\\np(n)\\nFigure 6.5: The p(n) × p(n) frame for the domino game.\\nTherefore, we have A ≤P Domino. Hence, the language Domino is NP-\\ncomplete.\\nAn example of a domino game\\nWe have deﬁned the domino game corresponding to a Turing machine that\\nsolves a decision problem. Of course, we can also do this for Turing machines\\nthat compute functions. In this section, we will exactly do this for a Turing\\nmachine that computes the successor function x →x + 1.\\nWe will design a Turing machine with one tape, that gets as input the\\nbinary representation of a natural number x, and that computes the binary\\nrepresentation of x + 1.\\nStart of the computation: The tape contains a 0 followed by the binary\\nrepresentation of the integer x ∈N0. The tape head is on the leftmost bit\\n(which is 0), and the Turing machine is in the start state z0. Here is an\\nexample, where x = 431:\\n6.5.\\nNP-complete languages\\n227\\n0 1 1 0 1 0 1 1 1 1 2\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe number x + 1. The tape head is on the rightmost 1, and the Turing\\nmachine is in the ﬁnal state z1. For our example, the tape looks as follows:\\n0 1 1 0 1 1 0 0 0 0 2\\n6\\nOur Turing machine will use the following states:\\nz0 :\\nstart state; tape head moves to the right\\nz1 :\\nﬁnal state\\nz2 :\\ntape head moves to the left; on its way to the left, it has not read 0\\nThe Turing machine has the following instructions:\\nz00 →z00R\\nz21 →z20L\\nz01 →z01R\\nz20 →z11N\\nz02 →z22L\\nIn Figure 6.6, you can see the sequence of states and tape contents of this\\nTuring machine on input x = 11.\\nWe now construct the domino game that corresponds to the computation\\nof this Turing machine on input x = 11. Following the general construction\\nin Section 6.5.3, we obtain the following tile types:\\n1. The three symbols of the alphabet yield three tile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n2\\n2\\n2. The ﬁve instructions of the Turing machine yield ﬁve tile types:\\n228\\nChapter 6.\\nComplexity Theory\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n0\\n(z0, 1)\\n0\\n1\\n1\\n2\\n0\\n1\\n(z0, 0)\\n1\\n1\\n2\\n0\\n1\\n0\\n(z0, 1)\\n1\\n2\\n0\\n1\\n0\\n1\\n(z0, 1)\\n2\\n0\\n1\\n0\\n1\\n1\\n(z0, 2)\\n0\\n1\\n0\\n1\\n(z2, 1)\\n2\\n0\\n1\\n0\\n(z2, 1)\\n0\\n2\\n0\\n1\\n(z2, 0)\\n0\\n0\\n2\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\nFigure 6.6: The computation of the Turing machine on input x = 11. The\\npair (state,symbol) indicates the position of the tape head.\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n3. The states z0 and z2, and the three symbols of the alphabet yield twelve\\ntile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 2)\\n2\\nThe computation of the Turing machine on input x = 11 consists of nine\\ncomputation steps. During this computation, the tape head visits exactly\\nsix cells. Therefore, the frame for the domino game has nine rows and six\\ncolumns.\\nThis frame is given in Figure 6.7.\\nIn Figure 6.8, you ﬁnd the\\nsolution of the domino game.\\nObserve that this solution is nothing but\\nan equivalent way of writing the computation of Figure 6.6.\\nHence, the\\ncomputation of the Turing machine corresponds to a solution of the domino\\ngame; in fact, the converse also holds.\\n6.5.\\nNP-complete languages\\n229\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\nFigure 6.7: The frame for the domino game for input x = 11.\\n230\\nChapter 6.\\nComplexity Theory\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\nFigure 6.8: The solution for the domino game for input x = 11.\\n6.5.\\nNP-complete languages\\n231\\n6.5.4\\nExamples of NP-complete languages\\nIn Section 6.5.3, we have shown that Domino is NP-complete. Using this\\nresult, we will apply Theorem 6.5.11 to prove the NP-completeness of some\\nother languages.\\nSatisﬁability\\nWe consider Boolean formulas ϕ, in the variables x1, x2, . . . , xm, having the\\nform\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.9)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nEach ℓi\\nj is either a variable or the negation of a variable. Such a formula ϕ\\nis said to be satisﬁable, if there exists a truth-value in {0, 1} for each of the\\nvariables x1, x2, . . . , xm, such that the entire formula ϕ is true. We deﬁne the\\nfollowing language:\\nSAT := {⟨ϕ⟩: ϕ is of the form (6.9) and is satisﬁable}.\\nWe will prove that SAT is NP-complete.\\nIt is clear that SAT ∈NP. If we can show that\\nDomino ≤P SAT,\\nthen it follows from Theorem 6.5.11 that SAT is NP-complete. (In Theo-\\nrem 6.5.11, take B := Domino and C := SAT.)\\nHence, we need a function f ∈FP, that maps input strings for Domino\\nto input strings for SAT, in such a way that for every domino game D, the\\nfollowing holds:\\ndomino game D is solvable ⇐⇒the formula encoded by the\\nstring f(⟨D⟩) is satisﬁable.\\n(6.10)\\nLet us consider an arbitrary domino game D. Let k be the number of\\ntile types, and let the frame have t rows and t columns. We denote the tile\\ntypes by T1, T2, . . . , Tk.\\n232\\nChapter 6.\\nComplexity Theory\\nWe map this domino game D to a Boolean formula ϕ, such that (6.10)\\nholds. The formula ϕ will have variables\\nxijℓ, 1 ≤i ≤t, 1 ≤j ≤t, 1 ≤ℓ≤k.\\nThese variables can be interpretated as follows:\\nxijℓ= 1 ⇐⇒there is a tile of type Tℓat position (i, j) of the frame.\\nWe deﬁne:\\n• For all i and j with 1 ≤i ≤t and 1 ≤j ≤t:\\nC1\\nij := xij1 ∨xij2 ∨. . . ∨xijk.\\nThis formula expresses the condition that there is at least one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j ≤t, and 1 ≤ℓ< ℓ′ ≤k:\\nC2\\nijℓℓ′ := ¬xijℓ∨¬xijℓ′.\\nThis formula expresses the condition that there is at most one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j < t, 1 ≤ℓ≤k and 1 ≤ℓ′ ≤k,\\nsuch that i < t and the right symbol on a tile of type Tℓis not equal\\nto the left symbol on a tile of type Tℓ′:\\nC3\\nijℓℓ′ := ¬xijℓ∨¬xi,j+1,ℓ′.\\nThis formula expresses the condition that neighboring tiles in the same\\nrow “ﬁt” together. There are symmetric formulas for neighboring tiles\\nin the same column.\\n• For all j and ℓwith 1 ≤j ≤t and 1 ≤ℓ≤k, such that the top symbol\\non a tile of type Tℓis not equal to the symbol at position j of the upper\\nboundary of the frame:\\nC4\\njℓ:= ¬x1jℓ.\\nThis formula expresses the condition that tiles that touch the upper\\nboundary of the frame “ﬁt” there. There are symmetric formulas for\\nthe lower, left, and right boundaries of the frame.\\n6.5.\\nNP-complete languages\\n233\\nThe formula ϕ is the conjunction of all these formulas C1\\nij, C2\\nijℓℓ′, C3\\nijℓℓ′, and\\nC4\\njℓ. The complete formula ϕ consists of\\nO(t2k + t2k2 + t2k2 + tk) = O(t2k2)\\nterms, i.e., its length is polynomial in the length of the domino game. This\\nimplies that ϕ can be constructed in polynomial time. Hence, the function\\nf that maps the domino game D to the Boolean formula ϕ, is in the class\\nFP. It is not diﬃcult to see that (6.10) holds for this function f. Therefore,\\nwe have proved the following result.\\nTheorem 6.5.13 The language SAT is NP-complete.\\nIn Section 6.5.1, we have deﬁned the language 3SAT.\\nTheorem 6.5.14 The language 3SAT is NP-complete.\\nProof. It is clear that 3SAT ∈NP. If we can show that\\nSAT ≤P 3SAT,\\nthen the claim follows from Theorem 6.5.11. Let\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an input for SAT, in the variables x1, x2, . . . , xm. We map ϕ, in polynomial\\ntime, to an input ϕ′ for 3SAT, such that\\nϕ is satisﬁable ⇐⇒ϕ′ is satisﬁable.\\n(6.11)\\nFor each i with 1 ≤i ≤k, we do the following. Consider\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\n• If ki = 1, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n1 ∨ℓi\\n1.\\n• If ki = 2, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n2.\\n234\\nChapter 6.\\nComplexity Theory\\n• If ki = 3, then we deﬁne\\nC′\\ni := Ci.\\n• If ki ≥4, then we deﬁne\\nC′\\ni\\n:=\\n(ℓi\\n1 ∨ℓi\\n2 ∨zi\\n1) ∧(¬zi\\n1 ∨ℓi\\n3 ∨zi\\n2) ∧(¬zi\\n2 ∨ℓi\\n4 ∨zi\\n3) ∧. . .\\n∧(¬zi\\nki−3 ∨ℓi\\nki−1 ∨ℓi\\nki),\\nwhere zi\\n1, . . . , zi\\nki−3 are new variables.\\nLet\\nϕ′ := C′\\n1 ∧C′\\n2 ∧. . . ∧C′\\nk.\\nThen ϕ′ is an input for 3SAT, and (6.11) holds.\\nTheorems 6.5.6, 6.5.8, 6.5.11, and 6.5.14 imply:\\nTheorem 6.5.15 The language Clique is NP-complete.\\nThe traveling salesperson problem\\nWe are given two positive integers k and m, a set of m cities, and an integer\\nm × m matrix M, where\\nM(i, j) = the cost of driving from city i to city j,\\nfor all i, j ∈{1, 2, . . . , m}. We want to decide whether or not there is a tour\\nthrough all cities whose total cost is less than or equal to k. This problem is\\nNP-complete.\\nBin packing\\nWe are given three positive integers m, k, and ℓ, a set of m objects having\\nvolumes a1, a2, . . . , am, and k bins. Each bin has volume ℓ. We want to\\ndecide whether or not all objects ﬁt within these bins. This problem is NP-\\ncomplete.\\nHere is another interpretation of this problem: We are given m jobs that\\nneed time a1, a2, . . . , am to complete. We are also given k processors, and an\\ninteger ℓ. We want to decide whether or not it is possible to divide the jobs\\nover the k processors, such that no processor needs more than ℓtime.\\nExercises\\n235\\nTime tables\\nWe are given a set of courses, class rooms, and professors.\\nWe want to\\ndecide whether or not there exists a time table such that all courses are\\nbeing taught, no two courses are taught at the same time in the same class\\nroom, no professor teaches two courses at the same time, and conditions such\\nas “Prof. L. Azy does not teach before 1pm” are satisﬁed. This problem is\\nNP-complete.\\nMotion planning\\nWe are given two positive integers k and ℓ, a set of k polyhedra, and two\\npoints s and t in Q3. We want to decide whether or not there exists a path\\nbetween s and t, that does not intersect any of the polyhedra, and whose\\nlength is less than or equal to ℓ. This problem is NP-complete.\\nMap labeling\\nWe are given a map with m cities, where each city is represented by a point.\\nFor each city, we are given a rectangle that is large enough to contain the\\nname of the city. We want to decide whether or not these rectangles can be\\nplaced on the map, such that\\n• no two rectangles overlap,\\n• For each i with 1 ≤i ≤m, the point that represents city i is a corner\\nof its rectangle.\\nThis problem is NP-complete.\\nThis list of NP-complete problems can be extended almost arbitrarily:\\nFor thousands of problems, it is known that they are NP-complete. For all\\nof these, it is not known, whether or not they can be solved eﬃciently (i.e.,\\nin polynomial time). Collections of NP-complete problems can be found in\\nthe book\\n• M.R. Garey and D.S. Johnson. Computers and Intractability: A Guide\\nto the Theory of NP-Completeness. W.H. Freeman, New York, 1979,\\nand on the web page\\nhttp://www.nada.kth.se/~viggo/wwwcompendium/\\n236\\nChapter 6.\\nComplexity Theory\\nExercises\\n6.1 Prove that the function F : N →N, deﬁned by F(x) := 2x, is not in FP.\\n6.2 Prove Theorem 6.5.3.\\n6.3 Prove that the language Clique is in the class NP.\\n6.4 Prove that the language 3SAT is in the class NP.\\n6.5 We deﬁne the following languages:\\n• Sum of subset:\\nSOS := {⟨a1, a2, . . . , am, b⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai = b}.\\n• Set partition:\\nSP := {⟨a1, a2, . . . , am⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai =\\nX\\ni̸∈I\\nai}.\\n• Bin packing: BP is the set of all strings ⟨s1, s2, . . . , sm, B⟩for which\\n1. 0 < si < 1, for all i,\\n2. B ∈N,\\n3. the numbers s1, s2, . . . , sm ﬁt into B bins, where each bin has size\\none, i.e., there exists a partition of {1, 2, . . . , m} into subsets Ik,\\n1 ≤k ≤B, such that P\\ni∈Ik si ≤1 for all k, 1 ≤k ≤B.\\nFor example, ⟨1/6, 1/2, 1/5, 1/9, 3/5, 1/5, 1/2, 11/18, 3⟩∈BP, because\\nthe eight fractions ﬁt into three bins:\\n1/6 + 1/9 + 11/18 ≤1, 1/2 + 1/2 = 1, and 1/5 + 3/5 + 1/5 = 1.\\n1. Prove that SOS ≤P SP.\\n2. Prove that the language SOS is NP-complete. You may use the fact\\nthat the language SP is NP-complete.\\nExercises\\n237\\n3. Prove that the language BP is NP-complete. Again, you may use the\\nfact that the language SP is NP-complete.\\n6.6 Prove that 3Color ≤P 3SAT.\\nHint: For each vertex i, and for each of the three colors k, introduce a\\nBoolean variable xik.\\n6.7 The (0, 1)-integer programming language IP is deﬁned as follows:\\nIP := {⟨A, c⟩:\\nA is an integer m × n matrix for some m, n ∈N,\\nc is an integer vector of length m, and\\n∃x ∈{0, 1}n such that Ax ≤c (componentwise) }.\\nProve that the language IP is NP-complete. You may use the fact that\\nthe language SOS is NP-complete.\\n6.8 Let ϕ be a Boolean formula in the variables x1, x2, . . . , xm.\\nWe say that ϕ is in disjunctive normal form (DNF) if it is of the form\\nϕ = C1 ∨C2 ∨. . . ∨Ck,\\n(6.12)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∧ℓi\\n2 ∧. . . ∧ℓi\\nki.\\nEach ℓi\\nj is a literal, which is either a variable or the negation of a variable.\\nWe say that ϕ is in conjunctive normal form (CNF) if it is of the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.13)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nAgain, each ℓi\\nj is a literal.\\nWe deﬁne the following two languages:\\nDNFSAT := {⟨ϕ⟩: ϕ is in DNF-form and is satisﬁable},\\nand\\nCNFSAT := {⟨ϕ⟩: ϕ is in CNF-form and is satisﬁable}.\\n238\\nChapter 6.\\nComplexity Theory\\n1. Prove that the language DNFSAT is in P.\\n2. What is wrong with the following argument: Since we can rewrite\\nany Boolean formula in DNF-form, we have CNFSAT ≤P DNFSAT.\\nHence, since CNFSAT is NP-complete and since DNFSAT ∈P, we\\nhave P = NP.\\n3. Prove directly that for every language A in P, A ≤P CNFSAT. “Di-\\nrectly” means that you should not use the fact that CNFSAT is NP-\\ncomplete.\\n6.9 1 Prove that the polynomial upper bound on the length of the string y\\nin the deﬁnition of NP is necessary, in the sense that if it is left out, then\\nany enumerable language would satisfy the condition.\\nMore precisely, we say that the language A belongs to the class E, if there\\nexists a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃y : ⟨w, y⟩∈B.\\nProve that E is equal to the class of all enumerable languages.\\n1Thanks to Antoine Vigneron for poining out an error in a previous version of this\\nexercise.\\nChapter 7\\nSummary\\nWe have seen several diﬀerent models for “processing” languages, i.e., pro-\\ncessing sets of strings over some ﬁnite alphabet. For each of these models,\\nwe have asked the question which types of languages can be processed, and\\nwhich types of languages cannot be processed. In this ﬁnal chapter, we give\\na brief summary of these results.\\nRegular languages:\\nThis class of languages was considered in Chapter 2.\\nThe following statements are equivalent:\\n1. The language A is regular, i.e., there exists a deterministic ﬁnite au-\\ntomaton that accepts A.\\n2. There exists a nondeterministic ﬁnite automaton that accepts A.\\n3. There exists a regular expression that describes A.\\nThis claim was proved by the following conversions:\\n1. Every nondeterministic ﬁnite automaton can be converted to an equiv-\\nalent deterministic ﬁnite automaton.\\n2. Every deterministic ﬁnite automaton can be converted to an equivalent\\nregular expression.\\n3. Every regular expression can be converted to an equivalent nondeter-\\nministic ﬁnite automaton.\\n240\\nChapter 7.\\nSummary\\nWe have seen that the class of regular languages is closed under the regular\\noperations: If A and B are regular languages, then\\n1. A ∪B is regular,\\n2. AB is regular,\\n3. A∗is regular,\\n4. A is regular, and\\n5. A ∩B is regular.\\nFinally, the pumping lemma for regular languages gives a property that\\nevery regular language possesses. We have used this to prove that languages\\nsuch as {anbn : n ≥0} are not regular.\\nContext-free languages:\\nThis class of languages was considered in Chap-\\nter 3. We have seen that every regular language is context-free. Moreover,\\nthere exist languages, for example {anbn : n ≥0}, that are context-free, but\\nnot regular. The following statements are equivalent:\\n1. The language A is context-free, i.e., there exists a context-free grammar\\nwhose language is A.\\n2. There exists a context-free grammar in Chomsky normal form whose\\nlanguage is A.\\n3. There exists a nondeterministic pushdown automaton that accepts A.\\nThis claim was proved by the following conversions:\\n1. Every context-free grammar can be converted to an equivalent context-\\nfree grammar in Chomsky normal form.\\n2. Every context-free grammar in Chomsky normal form can be converted\\nto an equivalent nondeterministic pushdown automaton.\\n3. Every nondeterministic pushdown automaton can be converted to an\\nequivalent context-free grammar. (This conversion was not covered in\\nthis book.)\\nChapter 7.\\nSummary\\n241\\nNondeterministic pushdown automata are more powerful than determin-\\nistic pushdown automata: There exists a nondeterministic pushdown au-\\ntomaton that accepts the language\\n{vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|},\\nbut there is no deterministic pushdown automaton that accepts this language.\\n(We did not prove this in this book.)\\nWe have seen that the class of context-free languages is closed under\\nthe union, concatenation, and star operations: If A and B are context-free\\nlanguages, then\\n1. A ∪B is context-free,\\n2. AB is context-free, and\\n3. A∗is context-free.\\nHowever,\\n1. the intersection of two context-free languages is not necessarily context-\\nfree, and\\n2. the complement of a context-free language is not necessarily context-\\nfree.\\nFinally, the pumping lemma for context-free languages gives a property\\nthat every context-free language possesses. We have used this to prove that\\nlanguages such as {anbncn : n ≥0} are not context-free.\\nThe Church-Turing Thesis:\\nIn Chapter 4, we considered “reasonable”\\ncomputational devices that model real computers. Examples of such devices\\nare Turing machines (with one or more tapes) and Java programs. It turns\\nout that all known “reasonable” devices are equivalent, i.e., can be converted\\nto each other. This led to the Church-Turing Thesis:\\n• Every computational process that is intuitively considered to be an\\nalgorithm can be converted to a Turing machine.\\n242\\nChapter 7.\\nSummary\\nDecidable and enumerable languages:\\nThese classes of languages were\\nconsidered in Chapter 5. They are deﬁned based on “reasonable” computa-\\ntional devices, such as Turing machines and Java programs. We have seen\\nthat\\n1. every context-free language is decidable, and\\n2. every decidable language is enumerable.\\nMoreover,\\n1. there exist languages, for example {anbncn : n ≥0}, that are decidable,\\nbut not context-free,\\n2. there exist languages, for example the Halting Problem, that are enu-\\nmerable, but not decidable,\\n3. there exist languages, for example the complement of the Halting Prob-\\nlem, that are not enumerable.\\nIn fact,\\n1. the class of all languages is not countable, whereas\\n2. the class of all enumerable languages is countable.\\nThe following statements are equivalent:\\n1. The language A is decidable.\\n2. Both A and its complement A are enumerable.\\nComplexity classes:\\nThese classes of languages were considered in Chap-\\nter 6.\\n1. The class P consists of all languages that can be decided in polynomial\\ntime by a deterministic Turing machine.\\n2. The class NP consists of all languages that can be decided in poly-\\nnomial time by a nondeterministic Turing machine. Equivalently, a\\nlanguage A is in the class NP, if for every string w ∈A, there exists a\\n“solution” s, such that (i) the length of s is polynomial in the length\\nof w, and (ii) the correctness of s can be veriﬁed in polynomial time.\\nChapter 7.\\nSummary\\n243\\nThe following properties hold:\\n1. Every context-free language is in P. (We did not prove this).\\n2. Every language in P is also in NP.\\n3. It is not known if there exist languages that are in NP, but not in P.\\n4. Every language in NP is decidable.\\nWe have introduced reductions to deﬁne the notion of a language B to be\\n“at least as hard” as a language A. A language B is called NP-complete, if\\n1. B belongs to the class NP, and\\n2. B is at least as hard as every language in the class NP.\\nWe have seen that NP-complete exist.\\nThe ﬁgure below summarizes the relationships among the various classes\\nof languages.\\n244\\nChapter 7.\\nSummary\\nregular\\ncontext-free\\nP\\nNP\\ndecidable\\nenumerable\\nall languages\\n', 'Introduction to Theory of Computation\\nAnil Maheshwari\\nMichiel Smid\\nSchool of Computer Science\\nCarleton University\\nOttawa\\nCanada\\n{anil,michiel}@scs.carleton.ca\\nAugust 29, 2024\\nii\\nContents\\nContents\\nPreface\\nvi\\n1\\nIntroduction\\n1\\n1.1\\nPurpose and motivation\\n. . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1.1\\nComplexity theory\\n. . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.2\\nComputability theory . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.3\\nAutomata theory . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.1.4\\nThis course\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.2\\nMathematical preliminaries\\n. . . . . . . . . . . . . . . . . . .\\n4\\n1.3\\nProof techniques\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.3.1\\nDirect proofs\\n. . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.3.2\\nConstructive proofs . . . . . . . . . . . . . . . . . . . .\\n9\\n1.3.3\\nNonconstructive proofs . . . . . . . . . . . . . . . . . .\\n10\\n1.3.4\\nProofs by contradiction . . . . . . . . . . . . . . . . . .\\n11\\n1.3.5\\nThe pigeon hole principle . . . . . . . . . . . . . . . . .\\n12\\n1.3.6\\nProofs by induction . . . . . . . . . . . . . . . . . . . .\\n13\\n1.3.7\\nMore examples of proofs . . . . . . . . . . . . . . . . .\\n15\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2\\nFinite Automata and Regular Languages\\n21\\n2.1\\nAn example: Controling a toll gate . . . . . . . . . . . . . . .\\n21\\n2.2\\nDeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . . . .\\n23\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton . . . . . . . . . .\\n26\\n2.2.2\\nA second example of a ﬁnite automaton\\n. . . . . . . .\\n28\\n2.2.3\\nA third example of a ﬁnite automaton\\n. . . . . . . . .\\n29\\n2.3\\nRegular operations . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n2.4\\nNondeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . .\\n35\\n2.4.1\\nA ﬁrst example . . . . . . . . . . . . . . . . . . . . . .\\n35\\niv\\nContents\\n2.4.2\\nA second example . . . . . . . . . . . . . . . . . . . . .\\n37\\n2.4.3\\nA third example . . . . . . . . . . . . . . . . . . . . . .\\n38\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\n. . . .\\n39\\n2.5\\nEquivalence of DFAs and NFAs . . . . . . . . . . . . . . . . .\\n41\\n2.5.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n2.6\\nClosure under the regular operations\\n. . . . . . . . . . . . . .\\n48\\n2.7\\nRegular expressions . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n2.8\\nEquivalence of regular expressions and regular languages . . .\\n57\\n2.8.1\\nEvery regular expression describes a regular language .\\n58\\n2.8.2\\nConverting a DFA to a regular expression\\n. . . . . . .\\n61\\n2.9\\nThe pumping lemma and nonregular languages . . . . . . . . .\\n68\\n2.9.1\\nApplications of the pumping lemma . . . . . . . . . . .\\n70\\n2.10 Higman’s Theorem . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.1 Dickson’s Theorem . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.2 Proof of Higman’s Theorem . . . . . . . . . . . . . . .\\n78\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n3\\nContext-Free Languages\\n91\\n3.1\\nContext-free grammars . . . . . . . . . . . . . . . . . . . . . .\\n91\\n3.2\\nExamples of context-free grammars . . . . . . . . . . . . . . .\\n94\\n3.2.1\\nProperly nested parentheses . . . . . . . . . . . . . . .\\n94\\n3.2.2\\nA context-free grammar for a nonregular language . . .\\n95\\n3.2.3\\nA context-free grammar for the complement of a non-\\nregular language\\n. . . . . . . . . . . . . . . . . . . . .\\n97\\n3.2.4\\nA context-free grammar that veriﬁes addition\\n. . . . .\\n98\\n3.3\\nRegular languages are context-free . . . . . . . . . . . . . . . . 100\\n3.3.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 102\\n3.4\\nChomsky normal form\\n. . . . . . . . . . . . . . . . . . . . . . 104\\n3.4.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 109\\n3.5\\nPushdown automata\\n. . . . . . . . . . . . . . . . . . . . . . . 112\\n3.6\\nExamples of pushdown automata\\n. . . . . . . . . . . . . . . . 116\\n3.6.1\\nProperly nested parentheses . . . . . . . . . . . . . . . 116\\n3.6.2\\nStrings of the form 0n1n\\n. . . . . . . . . . . . . . . . . 117\\n3.6.3\\nStrings with b in the middle . . . . . . . . . . . . . . . 118\\n3.7\\nEquivalence of pushdown automata and context-free grammars 120\\n3.8\\nThe pumping lemma for context-free languages\\n. . . . . . . . 124\\n3.8.1\\nProof of the pumping lemma . . . . . . . . . . . . . . . 125\\n3.8.2\\nApplications of the pumping lemma . . . . . . . . . . . 128\\nContents\\nv\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n4\\nTuring Machines and the Church-Turing Thesis\\n137\\n4.1\\nDeﬁnition of a Turing machine . . . . . . . . . . . . . . . . . . 137\\n4.2\\nExamples of Turing machines\\n. . . . . . . . . . . . . . . . . . 141\\n4.2.1\\nAccepting palindromes using one tape\\n. . . . . . . . . 141\\n4.2.2\\nAccepting palindromes using two tapes . . . . . . . . . 142\\n4.2.3\\nAccepting anbncn using one tape . . . . . . . . . . . . . 143\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2} . . . . 145\\n4.2.5\\nAccepting ambncmn using one tape . . . . . . . . . . . . 147\\n4.3\\nMulti-tape Turing machines . . . . . . . . . . . . . . . . . . . 148\\n4.4\\nThe Church-Turing Thesis . . . . . . . . . . . . . . . . . . . . 151\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n5\\nDecidable and Undecidable Languages\\n157\\n5.1\\nDecidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n5.1.1\\nThe language ADFA . . . . . . . . . . . . . . . . . . . . 158\\n5.1.2\\nThe language ANFA . . . . . . . . . . . . . . . . . . . . 159\\n5.1.3\\nThe language ACFG . . . . . . . . . . . . . . . . . . . . 160\\n5.1.4\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 161\\n5.1.5\\nThe Halting Problem . . . . . . . . . . . . . . . . . . . 163\\n5.2\\nCountable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n5.2.1\\nThe Halting Problem revisited . . . . . . . . . . . . . . 168\\n5.3\\nRice’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n5.3.1\\nProof of Rice’s Theorem . . . . . . . . . . . . . . . . . 171\\n5.4\\nEnumerability . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n5.4.1\\nHilbert’s problem . . . . . . . . . . . . . . . . . . . . . 174\\n5.4.2\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 176\\n5.5\\nWhere does the term “enumerable” come from? . . . . . . . . 177\\n5.6\\nMost languages are not enumerable . . . . . . . . . . . . . . . 180\\n5.6.1\\nThe set of enumerable languages is countable\\n. . . . . 180\\n5.6.2\\nThe set of all languages is not countable . . . . . . . . 181\\n5.6.3\\nThere are languages that are not enumerable . . . . . . 183\\n5.7\\nThe relationship between decidable and enumerable languages 184\\n5.8\\nA language A such that both A and A are not enumerable . . 186\\n5.8.1\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 186\\n5.8.2\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 188\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\nvi\\nContents\\n6\\nComplexity Theory\\n197\\n6.1\\nThe running time of algorithms . . . . . . . . . . . . . . . . . 197\\n6.2\\nThe complexity class P . . . . . . . . . . . . . . . . . . . . . . 199\\n6.2.1\\nSome examples . . . . . . . . . . . . . . . . . . . . . . 199\\n6.3\\nThe complexity class NP . . . . . . . . . . . . . . . . . . . . . 202\\n6.3.1\\nP is contained in NP . . . . . . . . . . . . . . . . . . . 208\\n6.3.2\\nDeciding NP-languages in exponential time\\n. . . . . . 208\\n6.3.3\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . 211\\n6.4\\nNon-deterministic algorithms\\n. . . . . . . . . . . . . . . . . . 211\\n6.5\\nNP-complete languages\\n. . . . . . . . . . . . . . . . . . . . . 213\\n6.5.1\\nTwo examples of reductions . . . . . . . . . . . . . . . 215\\n6.5.2\\nDeﬁnition of NP-completeness . . . . . . . . . . . . . . 220\\n6.5.3\\nAn NP-complete domino game\\n. . . . . . . . . . . . . 222\\n6.5.4\\nExamples of NP-complete languages . . . . . . . . . . 231\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\n7\\nSummary\\n239\\nPreface\\nThis is a free textbook for an undergraduate course on the Theory of Com-\\nputation, which we have been teaching at Carleton University since 2002.\\nUntil the 2011/2012 academic year, this course was oﬀered as a second-year\\ncourse (COMP 2805) and was compulsory for all Computer Science students.\\nStarting with the 2012/2013 academic year, the course has been downgraded\\nto a third-year optional course (COMP 3803).\\nWe have been developing this book since we started teaching this course.\\nCurrently, we cover most of the material from Chapters 2–5 during a 12-week\\nterm with three hours of classes per week.\\nThe material from Chapter 6, on Complexity Theory, is taught in the\\nthird-year course COMP 3804 (Design and Analysis of Algorithms). In the\\nearly years of COMP 2805, we gave a two-lecture overview of Complexity\\nTheory at the end of the term. Even though this overview has disappeared\\nfrom the course, we decided to keep Chapter 6. This chapter has not been\\nrevised/modiﬁed for a long time.\\nThe course as we teach it today has been inﬂuenced by the following two\\ntextbooks:\\n• Introduction to the Theory of Computation (second edition), by Michael\\nSipser, Thomson Course Technnology, Boston, 2006.\\n• Einf¨uhrung in die Theoretische Informatik, by Klaus Wagner, Springer-\\nVerlag, Berlin, 1994.\\nBesides reading this text, we recommend that you also take a look at\\nthese excellent textbooks, as well as one or more of the following ones:\\n• Elements of the Theory of Computation (second edition), by Harry\\nLewis and Christos Papadimitriou, Prentice-Hall, 1998.\\nviii\\n• Introduction to Languages and the Theory of Computation (third edi-\\ntion), by John Martin, McGraw-Hill, 2003.\\n• Introduction to Automata Theory, Languages, and Computation (third\\nedition), by John Hopcroft, Rajeev Motwani, Jeﬀrey Ullman, Addison\\nWesley, 2007.\\nPlease let us know if you ﬁnd errors, typos, simpler proofs, comments,\\nomissions, or if you think that some parts of the book “need improvement”.\\nChapter 1\\nIntroduction\\n1.1\\nPurpose and motivation\\nThis course is on the Theory of Computation, which tries to answer the\\nfollowing questions:\\n• What are the mathematical properties of computer hardware and soft-\\nware?\\n• What is a computation and what is an algorithm? Can we give rigorous\\nmathematical deﬁnitions of these notions?\\n• What are the limitations of computers?\\nCan “everything” be com-\\nputed? (As we will see, the answer to this question is “no”.)\\nPurpose of the Theory of Computation: Develop formal math-\\nematical models of computation that reﬂect real-world computers.\\nThis ﬁeld of research was started by mathematicians and logicians in the\\n1930’s, when they were trying to understand the meaning of a “computation”.\\nA central question asked was whether all mathematical problems can be\\nsolved in a systematic way. The research that started in those days led to\\ncomputers as we know them today.\\nNowadays, the Theory of Computation can be divided into the follow-\\ning three areas: Complexity Theory, Computability Theory, and Automata\\nTheory.\\n2\\nChapter 1.\\nIntroduction\\n1.1.1\\nComplexity theory\\nThe main question asked in this area is “What makes some problems com-\\nputationally hard and other problems easy?”\\nInformally, a problem is called “easy”, if it is eﬃciently solvable. Exam-\\nples of “easy” problems are (i) sorting a sequence of, say, 1,000,000 numbers,\\n(ii) searching for a name in a telephone directory, and (iii) computing the\\nfastest way to drive from Ottawa to Miami. On the other hand, a problem is\\ncalled “hard”, if it cannot be solved eﬃciently, or if we don’t know whether\\nit can be solved eﬃciently. Examples of “hard” problems are (i) time table\\nscheduling for all courses at Carleton, (ii) factoring a 300-digit integer into\\nits prime factors, and (iii) computing a layout for chips in VLSI.\\nCentral Question in Complexity Theory: Classify problems ac-\\ncording to their degree of “diﬃculty”. Give a rigorous proof that\\nproblems that seem to be “hard” are really “hard”.\\n1.1.2\\nComputability theory\\nIn the 1930’s, G¨odel, Turing, and Church discovered that some of the fun-\\ndamental mathematical problems cannot be solved by a “computer”. (This\\nmay sound strange, because computers were invented only in the 1940’s).\\nAn example of such a problem is “Is an arbitrary mathematical statement\\ntrue or false?” To attack such a problem, we need formal deﬁnitions of the\\nnotions of\\n• computer,\\n• algorithm, and\\n• computation.\\nThe theoretical models that were proposed in order to understand solvable\\nand unsolvable problems led to the development of real computers.\\nCentral Question in Computability Theory: Classify problems\\nas being solvable or unsolvable.\\n1.1.\\nPurpose and motivation\\n3\\n1.1.3\\nAutomata theory\\nAutomata Theory deals with deﬁnitions and properties of diﬀerent types of\\n“computation models”. Examples of such models are:\\n• Finite Automata. These are used in text processing, compilers, and\\nhardware design.\\n• Context-Free Grammars. These are used to deﬁne programming lan-\\nguages and in Artiﬁcial Intelligence.\\n• Turing Machines.\\nThese form a simple abstract model of a “real”\\ncomputer, such as your PC at home.\\nCentral Question in Automata Theory: Do these models have\\nthe same power, or can one model solve more problems than the\\nother?\\n1.1.4\\nThis course\\nIn this course, we will study the last two areas in reverse order: We will start\\nwith Automata Theory, followed by Computability Theory. The ﬁrst area,\\nComplexity Theory, will be covered in COMP 3804.\\nActually, before we start, we will review some mathematical proof tech-\\nniques. As you may guess, this is a fairly theoretical course, with lots of\\ndeﬁnitions, theorems, and proofs. You may guess this course is fun stuﬀfor\\nmath lovers, but boring and irrelevant for others. You guessed it wrong, and\\nhere are the reasons:\\n1. This course is about the fundamental capabilities and limitations of\\ncomputers. These topics form the core of computer science.\\n2. It is about mathematical properties of computer hardware and software.\\n3. This theory is very much relevant to practice, for example, in the design\\nof new programming languages, compilers, string searching, pattern\\nmatching, computer security, artiﬁcial intelligence, etc., etc.\\n4. This course helps you to learn problem solving skills. Theory teaches\\nyou how to think, prove, argue, solve problems, express, and abstract.\\n4\\nChapter 1.\\nIntroduction\\n5. This theory simpliﬁes the complex computers to an abstract and simple\\nmathematical model, and helps you to understand them better.\\n6. This course is about rigorously analyzing capabilities and limitations\\nof systems.\\nWhere does this course ﬁt in the Computer Science Curriculum at Car-\\nleton University? It is a theory course that is the third part in the series\\nCOMP 1805, COMP 2804, COMP 3803, COMP 3804, and COMP 4804.\\nThis course also widens your understanding of computers and will inﬂuence\\nother courses including Compilers, Programming Languages, and Artiﬁcial\\nIntelligence.\\n1.2\\nMathematical preliminaries\\nThroughout this course, we will assume that you know the following mathe-\\nmatical concepts:\\n1. A set is a collection of well-deﬁned objects. Examples are (i) the set of\\nall Dutch Olympic Gold Medallists, (ii) the set of all pubs in Ottawa,\\nand (iii) the set of all even natural numbers.\\n2. The set of natural numbers is N = {1, 2, 3, . . .}.\\n3. The set of integers is Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n4. The set of rational numbers is Q = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\n5. The set of real numbers is denoted by R.\\n6. If A and B are sets, then A is a subset of B, written as A ⊆B, if every\\nelement of A is also an element of B. For example, the set of even\\nnatural numbers is a subset of the set of all natural numbers. Every\\nset A is a subset of itself, i.e., A ⊆A. The empty set is a subset of\\nevery set A, i.e., ∅⊆A.\\n7. If B is a set, then the power set P(B) of B is deﬁned to be the set of\\nall subsets of B:\\nP(B) = {A : A ⊆B}.\\nObserve that ∅∈P(B) and B ∈P(B).\\n1.2.\\nMathematical preliminaries\\n5\\n8. If A and B are two sets, then\\n(a) their union is deﬁned as\\nA ∪B = {x : x ∈A or x ∈B},\\n(b) their intersection is deﬁned as\\nA ∩B = {x : x ∈A and x ∈B},\\n(c) their diﬀerence is deﬁned as\\nA \\\\ B = {x : x ∈A and x ̸∈B},\\n(d) the Cartesian product of A and B is deﬁned as\\nA × B = {(x, y) : x ∈A and y ∈B},\\n(e) the complement of A is deﬁned as\\nA = {x : x ̸∈A}.\\n9. A binary relation on two sets A and B is a subset of A × B.\\n10. A function f from A to B, denoted by f : A →B, is a binary relation\\nR, having the property that for each element a ∈A, there is exactly\\none ordered pair in R, whose ﬁrst component is a. We will also say\\nthat f(a) = b, or f maps a to b, or the image of a under f is b. The\\nset A is called the domain of f, and the set\\n{b ∈B : there is an a ∈A with f(a) = b}\\nis called the range of f.\\n11. A function f : A →B is one-to-one (or injective), if for any two distinct\\nelements a and a′ in A, we have f(a) ̸= f(a′). The function f is onto\\n(or surjective), if for each element b ∈B, there exists an element a ∈A,\\nsuch that f(a) = b; in other words, the range of f is equal to the set\\nB. A function f is a bijection, if f is both injective and surjective.\\n12. A binary relation R ⊆A × A is an equivalence relation, if it satisﬁes\\nthe following three conditions:\\n6\\nChapter 1.\\nIntroduction\\n(a) R is reﬂexive: For every element in a ∈A, we have (a, a) ∈R.\\n(b) R is symmetric: For all a and b in A, if (a, b) ∈R, then also\\n(b, a) ∈R.\\n(c) R is transitive: For all a, b, and c in A, if (a, b) ∈R and (b, c) ∈R,\\nthen also (a, c) ∈R.\\n13. A graph G = (V, E) is a pair consisting of a set V , whose elements are\\ncalled vertices, and a set E, where each element of E is a pair of distinct\\nvertices. The elements of E are called edges. The ﬁgure below shows\\nsome well-known graphs: K5 (the complete graph on ﬁve vertices), K3,3\\n(the complete bipartite graph on 2 × 3 = 6 vertices), and the Peterson\\ngraph.\\nK5\\nK3,3\\nPeterson graph\\nThe degree of a vertex v, denoted by deg(v), is deﬁned to be the number\\nof edges that are incident on v.\\nA path in a graph is a sequence of vertices that are connected by edges.\\nA path is a cycle, if it starts and ends at the same vertex. A simple\\npath is a path without any repeated vertices. A graph is connected, if\\nthere is a path between every pair of vertices.\\n14. In the context of strings, an alphabet is a ﬁnite set, whose elements\\nare called symbols. Examples of alphabets are Σ = {0, 1} and Σ =\\n{a, b, c, . . . , z}.\\n15. A string over an alphabet Σ is a ﬁnite sequence of symbols, where each\\nsymbol is an element of Σ. The length of a string w, denoted by |w|, is\\nthe number of symbols contained in w. The empty string, denoted by\\n1.3.\\nProof techniques\\n7\\nϵ, is the string having length zero. For example, if the alphabet Σ is\\nequal to {0, 1}, then 10, 1000, 0, 101, and ϵ are strings over Σ, having\\nlengths 2, 4, 1, 3, and 0, respectively.\\n16. A language is a set of strings.\\n17. The Boolean values are 1 and 0, that represent true and false, respec-\\ntively. The basic Boolean operations include\\n(a) negation (or NOT), represented by ¬,\\n(b) conjunction (or AND), represented by ∧,\\n(c) disjunction (or OR), represented by ∨,\\n(d) exclusive-or (or XOR), represented by ⊕,\\n(e) equivalence, represented by ↔or ⇔,\\n(f) implication, represented by →or ⇒.\\nThe following table explains the meanings of these operations.\\nNOT\\nAND\\nOR\\nXOR\\nequivalence\\nimplication\\n¬0 = 1\\n0 ∧0 = 0\\n0 ∨0 = 0\\n0 ⊕0 = 0\\n0 ↔0 = 1\\n0 →0 = 1\\n¬1 = 0\\n0 ∧1 = 0\\n0 ∨1 = 1\\n0 ⊕1 = 1\\n0 ↔1 = 0\\n0 →1 = 1\\n1 ∧0 = 0\\n1 ∨0 = 1\\n1 ⊕0 = 1\\n1 ↔0 = 0\\n1 →0 = 0\\n1 ∧1 = 1\\n1 ∨1 = 1\\n1 ⊕1 = 0\\n1 ↔1 = 1\\n1 →1 = 1\\n1.3\\nProof techniques\\nIn mathematics, a theorem is a statement that is true. A proof is a sequence\\nof mathematical statements that form an argument to show that a theorem is\\ntrue. The statements in the proof of a theorem include axioms (assumptions\\nabout the underlying mathematical structures), hypotheses of the theorem\\nto be proved, and previously proved theorems. The main question is “How\\ndo we go about proving theorems?” This question is similar to the question\\nof how to solve a given problem. Of course, the answer is that ﬁnding proofs,\\nor solving problems, is not easy; otherwise life would be dull! There is no\\nspeciﬁed way of coming up with a proof, but there are some generic strategies\\nthat could be of help. In this section, we review some of these strategies,\\nthat will be suﬃcient for this course. The best way to get a feeling of how\\nto come up with a proof is by solving a large number of problems. Here are\\n8\\nChapter 1.\\nIntroduction\\nsome useful tips. (You may take a look at the book How to Solve It, by G.\\nP´olya).\\n1. Read and completely understand the statement of the theorem to be\\nproved. Most often this is the hardest part.\\n2. Sometimes, theorems contain theorems inside them.\\nFor example,\\n“Property A if and only if property B”, requires showing two state-\\nments:\\n(a) If property A is true, then property B is true (A ⇒B).\\n(b) If property B is true, then property A is true (B ⇒A).\\nAnother example is the theorem “Set A equals set B.” To prove this,\\nwe need to prove that A ⊆B and B ⊆A. That is, we need to show\\nthat each element of set A is in set B, and that each element of set B\\nis in set A.\\n3. Try to work out a few simple cases of the theorem just to get a grip on\\nit (i.e., crack a few simple cases ﬁrst).\\n4. Try to write down the proof once you have it. This is to ensure the\\ncorrectness of your proof. Often, mistakes are found at the time of\\nwriting.\\n5. Finding proofs takes time, we do not come prewired to produce proofs.\\nBe patient, think, express and write clearly and try to be precise as\\nmuch as possible.\\nIn the next sections, we will go through some of the proof strategies.\\n1.3.1\\nDirect proofs\\nAs the name suggests, in a direct proof of a theorem, we just approach the\\ntheorem directly.\\nTheorem 1.3.1 If n is an odd positive integer, then n2 is odd as well.\\n1.3.\\nProof techniques\\n9\\nProof. An odd positive integer n can be written as n = 2k + 1, for some\\ninteger k ≥0. Then\\nn2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1.\\nSince 2(2k2 + 2k) is even, and “even plus one is odd”, we can conclude that\\nn2 is odd.\\nTheorem 1.3.2 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is an even integer, i.e.,\\nX\\nv∈V\\ndeg(v)\\nis even.\\nProof. If you do not see the meaning of this statement, then ﬁrst try it out\\nfor a few graphs. The reason why the statement holds is very simple: Each\\nedge contributes 2 to the summation (because an edge is incident on exactly\\ntwo distinct vertices).\\nActually, the proof above proves the following theorem.\\nTheorem 1.3.3 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2|E|.\\n1.3.2\\nConstructive proofs\\nThis technique not only shows the existence of a certain object, it actually\\ngives a method of creating it. Here is how a constructive proof looks like:\\nTheorem 1.3.4 There exists an object with property P.\\nProof. Here is the object: [. . .]\\nAnd here is the proof that the object satisﬁes property P: [. . .]\\nHere is an example of a constructive proof. A graph is called 3-regular, if\\neach vertex has degree three.\\n10\\nChapter 1.\\nIntroduction\\nTheorem 1.3.5 For every even integer n ≥4, there exists a 3-regular graph\\nwith n vertices.\\nProof. Deﬁne\\nV = {0, 1, 2, . . . , n −1},\\nand\\nE = {{i, i+1} : 0 ≤i ≤n−2}∪{{n−1, 0}}∪{{i, i+n/2} : 0 ≤i ≤n/2−1}.\\nThen the graph G = (V, E) is 3-regular.\\nConvince yourself that this graph is indeed 3-regular. It may help to draw\\nthe graph for, say, n = 8.\\n1.3.3\\nNonconstructive proofs\\nIn a nonconstructive proof, we show that a certain object exists, without\\nactually creating it. Here is an example of such a proof:\\nTheorem 1.3.6 There exist irrational numbers x and y such that xy is ra-\\ntional.\\nProof. There are two possible cases.\\nCase 1:\\n√\\n2\\n√\\n2 ∈Q.\\nIn this case, we take x = y =\\n√\\n2. In Theorem 1.3.9 below, we will prove\\nthat\\n√\\n2 is irrational.\\nCase 2:\\n√\\n2\\n√\\n2 ̸∈Q.\\nIn this case, we take x =\\n√\\n2\\n√\\n2 and y =\\n√\\n2. Since\\nxy =\\n\\x12√\\n2\\n√\\n2\\x13√\\n2\\n=\\n√\\n2\\n2 = 2,\\nthe claim in the theorem follows.\\nObserve that this proof indeed proves the theorem, but it does not give\\nan example of a pair of irrational numbers x and y such that xy is rational.\\n1.3.\\nProof techniques\\n11\\n1.3.4\\nProofs by contradiction\\nThis is how a proof by contradiction looks like:\\nTheorem 1.3.7 Statement S is true.\\nProof. Assume that statement S is false. Then, derive a contradiction (such\\nas 1 + 1 = 3).\\nIn other words, show that the statement “¬S ⇒false” is true. This is\\nsuﬃcient, because the contrapositive of the statement “¬S ⇒false” is the\\nstatement “true ⇒S”. The latter logical formula is equivalent to S, and\\nthat is what we wanted to show.\\nBelow, we give two examples of proofs by contradiction.\\nTheorem 1.3.8 Let n be a positive integer. If n2 is even, then n is even.\\nProof. We will prove the theorem by contradiction. So we assume that n2\\nis even, but n is odd. Since n is odd, we know from Theorem 1.3.1 that n2\\nis odd. This is a contradiction, because we assumed that n2 is even.\\nTheorem 1.3.9\\n√\\n2 is irrational, i.e.,\\n√\\n2 cannot be written as a fraction of\\ntwo integers m and n.\\nProof. We will prove the theorem by contradiction. So we assume that\\n√\\n2\\nis rational. Then\\n√\\n2 can be written as a fraction of two integers,\\n√\\n2 = m/n,\\nwhere m ≥1 and n ≥1. We may assume that m and n do not share any\\ncommon factors, i.e., the greatest common divisor of m and n is equal to\\none; if this is not the case, then we can get rid of the common factors. By\\nsquaring\\n√\\n2 = m/n, we get 2n2 = m2. This implies that m2 is even. Then,\\nby Theorem 1.3.8, m is even, which means that we can write m as m = 2k,\\nfor some positive integer k. It follows that 2n2 = m2 = 4k2, which implies\\nthat n2 = 2k2. Hence, n2 is even. Again by Theorem 1.3.8, it follows that n\\nis even.\\nWe have shown that m and n are both even. But we know that m and\\nn are not both even. Hence, we have a contradiction. Our assumption that\\n√\\n2 is rational is wrong. Thus, we can conclude that\\n√\\n2 is irrational.\\nThere is a nice discussion of this proof in the book My Brain is Open:\\nThe Mathematical Journeys of Paul Erd˝os by B. Schechter.\\n12\\nChapter 1.\\nIntroduction\\n1.3.5\\nThe pigeon hole principle\\nThis is a simple principle with surprising consequences.\\nPigeon Hole Principle: If n + 1 or more objects are placed into n\\nboxes, then there is at least one box containing two or more objects.\\nIn other words, if A and B are two sets such that |A| > |B|, then\\nthere is no one-to-one function from A to B.\\nTheorem 1.3.10 Let n be a positive integer. Every sequence of n2 + 1 dis-\\ntinct real numbers contains a subsequence of length n + 1 that is either in-\\ncreasing or decreasing.\\nProof. For example consider the sequence (20, 10, 9, 7, 11, 2, 21, 1, 20, 31) of\\n10 = 32 + 1 numbers. This sequence contains an increasing subsequence of\\nlength 4 = 3 + 1, namely (10, 11, 21, 31).\\nThe proof of this theorem is by contradiction, and uses the pigeon hole\\nprinciple.\\nLet (a1, a2, . . . , an2+1) be an arbitrary sequence of n2 + 1 distinct real\\nnumbers. For each i with 1 ≤i ≤n2 + 1, let inci denote the length of\\nthe longest increasing subsequence that starts at ai, and let deci denote the\\nlength of the longest decreasing subsequence that starts at ai.\\nUsing this notation, the claim in the theorem can be formulated as follows:\\nThere is an index i such that inci ≥n + 1 or deci ≥n + 1.\\nWe will prove the claim by contradiction. So we assume that inci ≤n\\nand deci ≤n for all i with 1 ≤i ≤n2 + 1.\\nConsider the set\\nB = {(b, c) : 1 ≤b ≤n, 1 ≤c ≤n},\\nand think of the elements of B as being boxes. For each i with 1 ≤i ≤n2+1,\\nthe pair (inci, deci) is an element of B. So we have n2+1 elements (inci, deci),\\nwhich are placed in the n2 boxes of B. By the pigeon hole principle, there\\nmust be a box that contains two (or more) elements. In other words, there\\nexist two integers i and j such that i < j and\\n(inci, deci) = (incj, decj).\\nRecall that the elements in the sequence are distinct. Hence, ai ̸= aj. We\\nconsider two cases.\\n1.3.\\nProof techniques\\n13\\nFirst assume that ai < aj. Then the length of the longest increasing\\nsubsequence starting at ai must be at least 1+incj, because we can append ai\\nto the longest increasing subsequence starting at aj. Therefore, inci ̸= incj,\\nwhich is a contradiction.\\nThe second case is when ai > aj. Then the length of the longest decreasing\\nsubsequence starting at ai must be at least 1+decj, because we can append ai\\nto the longest decreasing subsequence starting at aj. Therefore, deci ̸= decj,\\nwhich is again a contradiction.\\n1.3.6\\nProofs by induction\\nThis is a very powerful and important technique for proving theorems.\\nFor each positive integer n, let P(n) be a mathematical statement that\\ndepends on n. Assume we wish to prove that P(n) is true for all positive\\nintegers n. A proof by induction of such a statement is carried out as follows:\\nBasis: Prove that P(1) is true.\\nInduction step: Prove that for all n ≥1, the following holds: If P(n) is\\ntrue, then P(n + 1) is also true.\\nIn the induction step, we choose an arbitrary integer n ≥1 and assume\\nthat P(n) is true; this is called the induction hypothesis. Then we prove that\\nP(n + 1) is also true.\\nTheorem 1.3.11 For all positive integers n, we have\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\nProof. We start with the basis of the induction. If n = 1, then the left-hand\\nside is equal to 1, and so is the right-hand side. So the theorem is true for\\nn = 1.\\nFor the induction step, let n ≥1 and assume that the theorem is true for\\nn, i.e., assume that\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\n14\\nChapter 1.\\nIntroduction\\nWe have to prove that the theorem is true for n + 1, i.e., we have to prove\\nthat\\n1 + 2 + 3 + . . . + (n + 1) = (n + 1)(n + 2)\\n2\\n.\\nHere is the proof:\\n1 + 2 + 3 + . . . + (n + 1)\\n=\\n1 + 2 + 3 + . . . + n\\n|\\n{z\\n}\\n= n(n+1)\\n2\\n+(n + 1)\\n=\\nn(n + 1)\\n2\\n+ (n + 1)\\n=\\n(n + 1)(n + 2)\\n2\\n.\\nBy the way, here is an alternative proof of the theorem above: Let S =\\n1 + 2 + 3 + . . . + n. Then,\\nS\\n=\\n1\\n+\\n2\\n+\\n3\\n+\\n. . .\\n+\\n(n −2)\\n+\\n(n −1)\\n+\\nn\\nS\\n=\\nn\\n+\\n(n −1)\\n+\\n(n −2)\\n+\\n. . .\\n+\\n3\\n+\\n2\\n+\\n1\\n2S\\n=\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n. . .\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\nSince there are n terms on the right-hand side, we have 2S = n(n + 1). This\\nimplies that S = n(n + 1)/2.\\nTheorem 1.3.12 For every positive integer n, a −b is a factor of an −bn.\\nProof. A direct proof can be given by providing a factorization of an −bn:\\nan −bn = (a −b)(an−1 + an−2b + an−3b2 + . . . + abn−2 + bn−1).\\nWe now prove the theorem by induction. For the basis, let n = 1. The claim\\nin the theorem is “a −b is a factor of a −b”, which is obviously true.\\nLet n ≥1 and assume that a −b is a factor of an −bn. We have to prove\\nthat a −b is a factor of an+1 −bn+1. We have\\nan+1 −bn+1 = an+1 −anb + anb −bn+1 = an(a −b) + (an −bn)b.\\nThe ﬁrst term on the right-hand side is divisible by a −b. By the induction\\nhypothesis, the second term on the right-hand side is divisible by a −b as\\nwell. Therefore, the entire right-hand side is divisible by a −b. Since the\\nright-hand side is equal to an+1 −bn+1, it follows that a −b is a factor of\\nan+1 −bn+1.\\nWe now give an alternative proof of Theorem 1.3.3:\\n1.3.\\nProof techniques\\n15\\nTheorem 1.3.13 Let G = (V, E) be a graph with m edges. Then the sum\\nof the degrees of all vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2m.\\nProof. The proof is by induction on the number m of edges. For the basis of\\nthe induction, assume that m = 0. Then the graph G does not contain any\\nedges and, therefore, P\\nv∈V deg(v) = 0. Thus, the theorem is true if m = 0.\\nLet m ≥0 and assume that the theorem is true for every graph with m\\nedges. Let G be an arbitrary graph with m+1 edges. We have to prove that\\nP\\nv∈V deg(v) = 2(m + 1).\\nLet {a, b} be an arbitrary edge in G, and let G′ be the graph obtained\\nfrom G by removing the edge {a, b}. Since G′ has m edges, we know from\\nthe induction hypothesis that the sum of the degrees of all vertices in G′ is\\nequal to 2m. Using this, we obtain\\nX\\nv∈G\\ndeg(v) =\\nX\\nv∈G′\\ndeg(v) + 2 = 2m + 2 = 2(m + 1).\\n1.3.7\\nMore examples of proofs\\nRecall Theorem 1.3.5, which states that for every even integer n ≥4, there\\nexists a 3-regular graph with n vertices. The following theorem explains why\\nwe stated this theorem for even values of n.\\nTheorem 1.3.14 Let n ≥5 be an odd integer. There is no 3-regular graph\\nwith n vertices.\\nProof. The proof is by contradiction. So we assume that there exists a\\ngraph G = (V, E) with n vertices that is 3-regular. Let m be the number of\\nedges in G. Since deg(v) = 3 for every vertex, we have\\nX\\nv∈V\\ndeg(v) = 3n.\\nOn the other hand, by Theorem 1.3.3, we have\\nX\\nv∈V\\ndeg(v) = 2m.\\n16\\nChapter 1.\\nIntroduction\\nIt follows that 3n = 2m, which can be rewritten as m = 3n/2. Since m is an\\ninteger, and since gcd(2, 3) = 1, n/2 must be an integer. Hence, n is even,\\nwhich is a contradiction.\\nLet Kn be the complete graph on n vertices. This graph has a vertex set\\nof size n, and every pair of distinct vertices is joined by an edge.\\nIf G = (V, E) is a graph with n vertices, then the complement G of G is\\nthe graph with vertex set V that consists of those edges of Kn that are not\\npresent in G.\\nTheorem 1.3.15 Let n ≥2 and let G be a graph on n vertices. Then G is\\nconnected or G is connected.\\nProof. We prove the theorem by induction on the number n of vertices. For\\nthe basis, assume that n = 2. There are two possibilities for the graph G:\\n1. G contains one edge. In this case, G is connected.\\n2. G does not contain an edge. In this case, the complement G contains\\none edge and, therefore, G is connected.\\nSo for n = 2, the theorem is true.\\nLet n ≥2 and assume that the theorem is true for every graph with n\\nvertices. Let G be graph with n + 1 vertices. We have to prove that G is\\nconnected or G is connected. We consider three cases.\\nCase 1: There is a vertex v whose degree in G is equal to n.\\nSince G has n+1 vertices, v is connected by an edge to every other vertex\\nof G. Therefore, G is connected.\\nCase 2: There is a vertex v whose degree in G is equal to 0.\\nIn this case, the degree of v in the graph G is equal to n. Since G has n+1\\nvertices, v is connected by an edge to every other vertex of G. Therefore, G\\nis connected.\\nCase 3: For every vertex v, the degree of v in G is in {1, 2, . . . , n −1}.\\nLet v be an arbitrary vertex of G.\\nLet G′ be the graph obtained by\\ndeleting from G the vertex v, together with all edges that are incident on v.\\nSince G′ has n vertices, we know from the induction hypothesis that G′ is\\nconnected or G′ is connected.\\n1.3.\\nProof techniques\\n17\\nLet us ﬁrst assume that G′ is connected. Then the graph G is connected\\nas well, because there is at least one edge in G between v and some vertex\\nof G′.\\nIf G′ is not connected, then G′ must be connected. Since we are in Case 3,\\nwe know that the degree of v in G is in the set {1, 2, . . . , n −1}. It follows\\nthat the degree of v in the graph G is in this set as well. Hence, there is at\\nleast one edge in G between v and some vertex in G′. This implies that G is\\nconnected.\\nThe previous theorem can be rephrased as follows:\\nTheorem 1.3.16 Let n ≥2 and consider the complete graph Kn on n ver-\\ntices. Color each edge of this graph as either red or blue. Let R be the graph\\nconsisting of all the red edges, and let B be the graph consisting of all the\\nblue edges. Then R is connected or B is connected.\\nA graph is said to be planar, if it can be drawn (a better term is “embed-\\nded”) in the plane in such a way that no two edges intersect, except possibly\\nat their endpoints.\\nAn embedding of a planar graph consists of vertices,\\nedges, and faces. In the example below, there are 11 vertices, 18 edges, and\\n9 faces (including the unbounded face).\\nThe following theorem is known as Euler’s theorem for planar graphs.\\nApparently, this theorem was discovered by Euler around 1750. Legendre\\ngave the ﬁrst proof in 1794, see\\nhttp://www.ics.uci.edu/~eppstein/junkyard/euler/\\nTheorem 1.3.17 (Euler) Consider an embedding of a planar graph G. Let\\nv, e, and f be the number of vertices, edges, and faces (including the single\\n18\\nChapter 1.\\nIntroduction\\nunbounded face) of this embedding, respectively. Moreover, let c be the number\\nof connected components of G. Then\\nv −e + f = c + 1.\\nProof. The proof is by induction on the number of edges of G. To be more\\nprecise, we start with a graph having no edges, and prove that the theorem\\nholds for this case. Then, we add the edges one by one, and show that the\\nrelation v −e + f = c + 1 is maintained.\\nSo we ﬁrst assume that G has no edges, i.e., e = 0. Then the embedding\\nconsists of a collection of v points. In this case, we have f = 1 and c = v.\\nHence, the relation v −e + f = c + 1 holds.\\nLet e > 0 and assume that Euler’s formula holds for a subgraph of G\\nhaving e −1 edges. Let {u, v} be an edge of G that is not in the subgraph,\\nand add this edge to the subgraph. There are two cases depending on whether\\nthis new edge joins two connected components or joins two vertices in the\\nsame connected component.\\nCase 1: The new edge {u, v} joins two connected components.\\nIn this case, the number of vertices and the number of faces do not change,\\nthe number of connected components goes down by 1, and the number of\\nedges increases by 1. It follows that the relation in the theorem is still valid.\\nCase 2: The new edge {u, v} joins two vertices in the same connected com-\\nponent.\\nIn this case, the number of vertices and the number of connected com-\\nponents do not change, the number of edges increases by 1, and the number\\nof faces increases by 1 (because the new edge splits one face into two faces).\\nTherefore, the relation in the theorem is still valid.\\nEuler’s theorem is usually stated as follows:\\nTheorem 1.3.18 (Euler) Consider an embedding of a connected planar\\ngraph G. Let v, e, and f be the number of vertices, edges, and faces (in-\\ncluding the single unbounded face) of this embedding, respectively. Then\\nv −e + f = 2.\\nIf you like surprising proofs of various mathematical results, you should\\nread the book Proofs from THE BOOK by Aigner and Ziegler.\\nExercises\\n19\\nExercises\\n1.1 Use induction to prove that every integer n ≥2 can be written as a\\nproduct of prime numbers.\\n1.2 For every prime number p, prove that √p is irrational.\\n1.3 Let n be a positive integer that is not a perfect square. Prove that √n\\nis irrational.\\n1.4 Prove by induction that n4 −4n2 is divisible by 3, for all integers n ≥1.\\n1.5 Prove that\\nn\\nX\\ni=1\\n1\\ni2 < 2 −1/n,\\nfor every integer n ≥2.\\n1.6 Prove that 9 divides n3 + (n + 1)3 + (n + 2)3, for every integer n ≥0.\\n1.7 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers that are consecutive.\\n1.8 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers such that one divides the other.\\n20\\nChapter 1.\\nIntroduction\\nChapter 2\\nFinite Automata and Regular\\nLanguages\\nIn this chapter, we introduce and analyze the class of languages that are\\nknown as regular languages. Informally, these languages can be “processed”\\nby computers having a very small amount of memory.\\n2.1\\nAn example: Controling a toll gate\\nBefore we give a formal deﬁnition of a ﬁnite automaton, we consider an\\nexample in which such an automaton shows up in a natural way. We consider\\nthe problem of designing a “computer” that controls a toll gate.\\nWhen a car arrives at the toll gate, the gate is closed. The gate opens as\\nsoon as the driver has payed 25 cents. We assume that we have only three\\ncoin denominations: 5, 10, and 25 cents. We also assume that no excess\\nchange is returned.\\nAfter having arrived at the toll gate, the driver inserts a sequence of coins\\ninto the machine. At any moment, the machine has to decide whether or not\\nto open the gate, i.e., whether or not the driver has paid 25 cents (or more).\\nIn order to decide this, the machine is in one of the following six states, at\\nany moment during the process:\\n• The machine is in state q0, if it has not collected any money yet.\\n• The machine is in state q1, if it has collected exactly 5 cents.\\n• The machine is in state q2, if it has collected exactly 10 cents.\\n22\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The machine is in state q3, if it has collected exactly 15 cents.\\n• The machine is in state q4, if it has collected exactly 20 cents.\\n• The machine is in state q5, if it has collected 25 cents or more.\\nInitially (when a car arrives at the toll gate), the machine is in state q0.\\nAssume, for example, that the driver presents the sequence (10,5,5,10) of\\ncoins.\\n• After receiving the ﬁrst 10 cents coin, the machine switches from state\\nq0 to state q2.\\n• After receiving the ﬁrst 5 cents coin, the machine switches from state\\nq2 to state q3.\\n• After receiving the second 5 cents coin, the machine switches from state\\nq3 to state q4.\\n• After receiving the second 10 cents coin, the machine switches from\\nstate q4 to state q5. At this moment, the gate opens. (Remember that\\nno change is given.)\\nThe ﬁgure below represents the behavior of the machine for all possible\\nsequences of coins. State q5 is represented by two circles, because it is a\\nspecial state: As soon as the machine reaches this state, the gate opens.\\nq0\\nq1\\nq2\\nq3\\nq4\\nq5\\n5\\n5\\n5\\n5\\n10\\n10\\n10\\n25\\n25\\n25\\n10, 25\\n5, 10, 25\\n5, 10\\n25\\nstart\\nObserve that the machine (or computer) only has to remember which\\nstate it is in at any given time. Thus, it needs only a very small amount\\nof memory: It has to be able to distinguish between any one of six possible\\ncases and, therefore, it only needs a memory of ⌈log 6⌉= 3 bits.\\n2.2.\\nDeterministic ﬁnite automata\\n23\\n2.2\\nDeterministic ﬁnite automata\\nLet us look at another example. Consider the following state diagram:\\nq1\\nq2\\nq3\\n0\\n0\\n1\\n1\\n0,1\\nWe say that q1 is the start state and q2 is an accept state. Consider the\\ninput string 1101. This string is processed in the following way:\\n• Initially, the machine is in the start state q1.\\n• After having read the ﬁrst 1, the machine switches from state q1 to\\nstate q2.\\n• After having read the second 1, the machine switches from state q2 to\\nstate q2. (So actually, it does not switch.)\\n• After having read the ﬁrst 0, the machine switches from state q2 to\\nstate q3.\\n• After having read the third 1, the machine switches from state q3 to\\nstate q2.\\nAfter the entire string 1101 has been processed, the machine is in state q2,\\nwhich is an accept state. We say that the string 1101 is accepted by the\\nmachine.\\nConsider now the input string 0101010. After having read this string\\nfrom left to right (starting in the start state q1), the machine is in state q3.\\nSince q3 is not an accept state, we say that the machine rejects the string\\n0101010.\\nWe hope you are able to see that this machine accepts every binary string\\nthat ends with a 1. In fact, the machine accepts more strings:\\n• Every binary string having the property that there are an even number\\nof 0s following the rightmost 1, is accepted by this machine.\\n24\\nChapter 2.\\nFinite Automata and Regular Languages\\n• Every other binary string is rejected by the machine. Observe that each\\nsuch string is either empty, consists of 0s only, or has an odd number\\nof 0s following the rightmost 1.\\nWe now come to the formal deﬁnition of a ﬁnite automaton:\\nDeﬁnition 2.2.1 A ﬁnite automaton is a 5-tuple M = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σ →Q is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\nYou can think of the transition function δ as being the “program” of the\\nﬁnite automaton M = (Q, Σ, δ, q, F). This function tells us what M can do\\nin “one step”:\\n• Let r be a state of Q and let a be a symbol of the alphabet Σ. If\\nthe ﬁnite automaton M is in state r and reads the symbol a, then it\\nswitches from state r to state δ(r, a). (In fact, δ(r, a) may be equal to\\nr.)\\nThe “computer” that we designed in the toll gate example in Section 2.1\\nis a ﬁnite automaton. For this example, we have Q = {q0, q1, q2, q3, q4, q5},\\nΣ = {5, 10, 25}, the start state is q0, F = {q5}, and δ is given by the following\\ntable:\\n5\\n10\\n25\\nq0\\nq1\\nq2\\nq5\\nq1\\nq2\\nq3\\nq5\\nq2\\nq3\\nq4\\nq5\\nq3\\nq4\\nq5\\nq5\\nq4\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nThe example given in the beginning of this section is also a ﬁnite automa-\\nton. For this example, we have Q = {q1, q2, q3}, Σ = {0, 1}, the start state\\nis q1, F = {q2}, and δ is given by the following table:\\n2.2.\\nDeterministic ﬁnite automata\\n25\\n0\\n1\\nq1\\nq1\\nq2\\nq2\\nq3\\nq2\\nq3\\nq2\\nq2\\nLet us denote this ﬁnite automaton by M. The language of M, denoted\\nby L(M), is the set of all binary strings that are accepted by M. As we have\\nseen before, we have\\nL(M) = {w : w contains at least one 1 and ends with an even number of 0s}.\\nWe now give a formal deﬁnition of the language of a ﬁnite automaton:\\nDeﬁnition 2.2.2 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton and let w =\\nw1w2 . . . wn be a string over Σ. Deﬁne the sequence r0, r1, . . . , rn of states, in\\nthe following way:\\n• r0 = q,\\n• ri+1 = δ(ri, wi+1), for i = 0, 1, . . . , n −1.\\n1. If rn ∈F, then we say that M accepts w.\\n2. If rn ̸∈F, then we say that M rejects w.\\nIn this deﬁnition, w may be the empty string, which we denote by ϵ, and\\nwhose length is zero; thus in the deﬁnition above, n = 0. In this case, the\\nsequence r0, r1, . . . , rn of states has length one; it consists of just the state\\nr0 = q. The empty string is accepted by M if and only if the start state q\\nbelongs to F.\\nDeﬁnition 2.2.3 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton. The lan-\\nguage L(M) accepted by M is deﬁned to be the set of all strings that are\\naccepted by M:\\nL(M) = {w : w is a string over Σ and M accepts w }.\\nDeﬁnition 2.2.4 A language A is called regular, if there exists a ﬁnite au-\\ntomaton M such that A = L(M).\\n26\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe ﬁnish this section by presenting an equivalent way of deﬁning the\\nlanguage accepted by a ﬁnite automaton. Let M = (Q, Σ, δ, q, F) be a ﬁnite\\nautomaton. The transition function δ : Q × Σ →Q tells us that, when M\\nis in state r ∈Q and reads symbol a ∈Σ, it switches from state r to state\\nδ(r, a). Let Σ∗denote the set of all strings over the alphabet Σ. (Σ∗includes\\nthe empty string ϵ.) We extend the function δ to a function\\nδ : Q × Σ∗→Q,\\nthat is deﬁned as follows. For any state r ∈Q and for any string w over the\\nalphabet Σ,\\nδ(r, w) =\\n\\x1a r\\nif w = ϵ,\\nδ(δ(r, v), a)\\nif w = va, where v is a string and a ∈Σ.\\nWhat is the meaning of this function δ? Let r be a state of Q and let w be\\na string over the alphabet Σ. Then\\n• δ(r, w) is the state that M reaches, when it starts in state r, reads the\\nstring w from left to right, and uses δ to switch from state to state.\\nUsing this notation, we have\\nL(M) = {w : w is a string over Σ and δ(q, w) ∈F}.\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton\\nLet\\nA = {w : w is a binary string containing an odd number of 1s}.\\nWe claim that this language A is regular. In order to prove this, we have to\\nconstruct a ﬁnite automaton M such that A = L(M).\\nHow to construct M? Here is a ﬁrst idea: The ﬁnite automaton reads the\\ninput string w from left to right and keeps track of the number of 1s it has\\nseen. After having read the entire string w, it checks whether this number\\nis odd (in which case w is accepted) or even (in which case w is rejected).\\nUsing this approach, the ﬁnite automaton needs a state for every integer\\ni ≥0, indicating that the number of 1s read so far is equal to i. Hence,\\nto design a ﬁnite automaton that follows this approach, we need an inﬁnite\\n2.2.\\nDeterministic ﬁnite automata\\n27\\nnumber of states. But, the deﬁnition of ﬁnite automaton requires the number\\nof states to be ﬁnite.\\nA better, and correct approach, is to keep track of whether the number\\nof 1s read so far is even or odd. This leads to the following ﬁnite automaton:\\n• The set of states is Q = {qe, qo}. If the ﬁnite automaton is in state qe,\\nthen it has read an even number of 1s; if it is in state qo, then it has\\nread an odd number of 1s.\\n• The alphabet is Σ = {0, 1}.\\n• The start state is qe, because at the start, the number of 1s read by the\\nautomaton is equal to 0, and 0 is even.\\n• The set F of accept states is F = {qo}.\\n• The transition function δ is given by the following table:\\n0\\n1\\nqe\\nqe\\nqo\\nqo\\nqo\\nqe\\nThis ﬁnite automaton M = (Q, Σ, δ, qe, F) can also be described by its state\\ndiagram, which is given in the ﬁgure below. The arrow that comes “out of\\nthe blue” and enters the state qe, indicates that qe is the start state. The\\nstate depicted with double circles indicates the accept state.\\nqe\\nqo\\n0\\n0\\n1\\n1\\nWe have constructed a ﬁnite automaton M that accepts the language A.\\nTherefore, A is a regular language.\\n28\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.2.2\\nA second example of a ﬁnite automaton\\nDeﬁne the language A as\\nA = {w : w is a binary string containing 101 as a substring}.\\nAgain, we claim that A is a regular language. In other words, we claim that\\nthere exists a ﬁnite automaton M that accepts A, i.e., A = L(M).\\nThe ﬁnite automaton M will do the following, when reading an input\\nstring from left to right:\\n• It skips over all 0s, and stays in the start state.\\n• At the ﬁrst 1, it switches to the state “maybe the next two symbols are\\n01”.\\n– If the next symbol is 1, then it stays in the state “maybe the next\\ntwo symbols are 01”.\\n– On the other hand, if the next symbol is 0, then it switches to the\\nstate “maybe the next symbol is 1”.\\n∗If the next symbol is indeed 1, then it switches to the accept\\nstate (but keeps on reading until the end of the string).\\n∗On the other hand, if the next symbol is 0, then it switches\\nto the start state, and skips 0s until it reads 1 again.\\nBy deﬁning the following four states, this process will become clear:\\n• q1: M is in this state if the last symbol read was 1, but the substring\\n101 has not been read.\\n• q10: M is in this state if the last two symbols read were 10, but the\\nsubstring 101 has not been read.\\n• q101: M is in this state if the substring 101 has been read in the input\\nstring.\\n• q: In all other cases, M is in this state.\\nHere is the formal description of the ﬁnite automaton that accepts the\\nlanguage A:\\n• Q = {q, q1, q10, q101},\\n2.2.\\nDeterministic ﬁnite automata\\n29\\n• Σ = {0, 1},\\n• the start state is q,\\n• the set F of accept states is equal to F = {q101}, and\\n• the transition function δ is given by the following table:\\n0\\n1\\nq\\nq\\nq1\\nq1\\nq10\\nq1\\nq10\\nq\\nq101\\nq101\\nq101\\nq101\\nThe ﬁgure below gives the state diagram of the ﬁnite automaton M =\\n(Q, Σ, δ, q, F).\\nq\\nq1\\nq10\\nq101\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nThis ﬁnite automaton accepts the language A consisting of all binary\\nstrings that contain the substring 101. As an exercise, how would you obtain\\na ﬁnite automaton that accepts the complement of A, i.e., the language\\nconsisting of all binary strings that do not contain the substring 101?\\n2.2.3\\nA third example of a ﬁnite automaton\\nThe ﬁnite automata we have seen so far have exactly one accept state. In\\nthis section, we will see an example of a ﬁnite automaton having more accept\\nstates.\\n30\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right},\\nwhere {0, 1}∗is the set of all binary strings, including the empty string ϵ. We\\nclaim that A is a regular language. To prove this, we have to construct a ﬁnite\\nautomaton M such that A = L(M). At ﬁrst sight, it seems diﬃcult (or even\\nimpossible?) to construct such a ﬁnite automaton: How does the automaton\\n“know” that it has reached the third symbol from the right? It is, however,\\npossible to construct such an automaton. The main idea is to remember the\\nlast three symbols that have been read. Thus, the ﬁnite automaton has eight\\nstates qijk, where i, j, and k range over the two elements of {0, 1}. If the\\nautomaton is in state qijk, then the following hold:\\n• If M has read at least three symbols, then the three most recently read\\nsymbols are ijk.\\n• If M has read only two symbols, then these two symbols are jk; more-\\nover, i = 0.\\n• If M has read only one symbol, then this symbol is k; moreover, i =\\nj = 0.\\n• If M has not read any symbol, then i = j = k = 0.\\nThe start state is q000 and the set of accept states is {q100, q110, q101, q111}.\\nThe transition function of M is given by the following state diagram.\\nq000\\nq100\\nq010\\nq110\\nq001\\nq101\\nq011\\nq111\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n2.3.\\nRegular operations\\n31\\n2.3\\nRegular operations\\nIn this section, we deﬁne three operations on languages. Later, we will answer\\nthe question whether the set of all regular languages is closed under these\\noperations. Let A and B be two languages over the same alphabet.\\n1. The union of A and B is deﬁned as\\nA ∪B = {w : w ∈A or w ∈B}.\\n2. The concatenation of A and B is deﬁned as\\nAB = {ww′ : w ∈A and w′ ∈B}.\\nIn words, AB is the set of all strings obtained by taking an arbitrary\\nstring w in A and an arbitrary string w′ in B, and gluing them together\\n(such that w is to the left of w′).\\n3. The star of A is deﬁned as\\nA∗= {u1u2 . . . uk : k ≥0 and ui ∈A for all i = 1, 2, . . . , k}.\\nIn words, A∗is obtained by taking any ﬁnite number of strings in A, and\\ngluing them together. Observe that k = 0 is allowed; this corresponds\\nto the empty string ϵ. Thus, ϵ ∈A∗.\\nTo give an example, let A = {0, 01} and B = {1, 10}. Then\\nA ∪B = {0, 01, 1, 10},\\nAB = {01, 010, 011, 0110},\\nand\\nA∗= {ϵ, 0, 01, 00, 001, 010, 0101, 000, 0001, 00101, . . .}.\\nAs another example, if Σ = {0, 1}, then Σ∗is the set of all binary strings\\n(including the empty string). Observe that a string always has a ﬁnite length.\\nBefore we proceed, we give an alternative (and equivalent) deﬁnition of\\nthe star of the language A: Deﬁne\\nA0 = {ϵ}\\n32\\nChapter 2.\\nFinite Automata and Regular Languages\\nand, for k ≥1,\\nAk = AAk−1,\\ni.e., Ak is the concatenation of the two languages A and Ak−1. Then we have\\nA∗=\\n∞\\n[\\nk=0\\nAk.\\nTheorem 2.3.1 The set of regular languages is closed under the union op-\\neration, i.e., if A and B are regular languages over the same alphabet Σ, then\\nA ∪B is also a regular language.\\nProof.\\nSince A and B are regular languages, there are ﬁnite automata\\nM1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2) that accept A and B,\\nrespectively. In order to prove that A ∪B is regular, we have to construct a\\nﬁnite automaton M that accepts A ∪B. In other words, M must have the\\nproperty that for every string w ∈Σ∗,\\nM accepts w ⇔M1 accepts w or M2 accepts w.\\nAs a ﬁrst idea, we may think that M could do the following:\\n• Starting in the start state q1 of M1, M “runs” M1 on w.\\n• If, after having read w, M1 is in a state of F1, then w ∈A, thus\\nw ∈A ∪B and, therefore, M accepts w.\\n• On the other hand, if, after having read w, M1 is in a state that is not\\nin F1, then w ̸∈A and M “runs” M2 on w, starting in the start state\\nq2 of M2. If, after having read w, M2 is in a state of F2, then we know\\nthat w ∈B, thus w ∈A ∪B and, therefore, M accepts w. Otherwise,\\nwe know that w ̸∈A ∪B, and M rejects w.\\nThis idea does not work, because the ﬁnite automaton M can read the input\\nstring w only once. The correct approach is to run M1 and M2 simulta-\\nneously. We deﬁne the set Q of states of M to be the Cartesian product\\nQ1 × Q2. If M is in state (r1, r2), this means that\\n• if M1 would have read the input string up to this point, then it would\\nbe in state r1, and\\n2.3.\\nRegular operations\\n33\\n• if M2 would have read the input string up to this point, then it would\\nbe in state r2.\\nThis leads to the ﬁnite automaton M = (Q, Σ, δ, q, F), where\\n• Q = Q1 × Q2 = {(r1, r2) : r1 ∈Q1 and r2 ∈Q2}.\\nObserve that\\n|Q| = |Q1| × |Q2|, which is ﬁnite.\\n• Σ is the alphabet of A and B (recall that we assume that A and B are\\nlanguages over the same alphabet).\\n• The start state q of M is equal to q = (q1, q2).\\n• The set F of accept states of M is given by\\nF = {(r1, r2) : r1 ∈F1 or r2 ∈F2} = (F1 × Q2) ∪(Q1 × F2).\\n• The transition function δ : Q × Σ →Q is given by\\nδ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)),\\nfor all r1 ∈Q1, r2 ∈Q2, and a ∈Σ.\\nTo ﬁnish the proof, we have to show that this ﬁnite automaton M indeed\\naccepts the language A∪B. Intuitively, this should be clear from the discus-\\nsion above. The easiest way to give a formal proof is by using the extended\\ntransition functions δ1 and δ2. (The extended transition function has been\\ndeﬁned after Deﬁnition 2.2.4.) Here we go: Recall that we have to prove that\\nM accepts w ⇔M1 accepts w or M2 accepts w,\\ni.e,\\nM accepts w ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\nIn terms of the extended transition function δ of the transition function δ of\\nM, this becomes\\nδ((q1, q2), w) ∈F ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\n(2.1)\\nBy applying the deﬁnition of the extended transition function, as given after\\nDeﬁnition 2.2.4, to δ, it can be seen that\\nδ((q1, q2), w) = (δ1(q1, w), δ2(q2, w)).\\n34\\nChapter 2.\\nFinite Automata and Regular Languages\\nThe latter equality implies that (2.1) is true and, therefore, M indeed accepts\\nthe language A ∪B.\\nWhat about the closure of the regular languages under the concatenation\\nand star operations? It turns out that the regular languages are closed under\\nthese operations. But how do we prove this?\\nLet A and B be two regular languages, and let M1 and M2 be ﬁnite\\nautomata that accept A and B, respectively. How do we construct a ﬁnite\\nautomaton M that accepts the concatenation AB? Given an input string\\nu, M has to decide whether or not u can be broken into two strings w and\\nw′ (i.e., write u as u = ww′), such that w ∈A and w′ ∈B. In words, M\\nhas to decide whether or not u can be broken into two substrings, such that\\nthe ﬁrst substring is accepted by M1 and the second substring is accepted by\\nM2. The diﬃculty is caused by the fact that M has to make this decision by\\nscanning the string u only once. If u ∈AB, then M has to decide, during\\nthis single scan, where to break u into two substrings. Similarly, if u ̸∈AB,\\nthen M has to decide, during this single scan, that u cannot be broken into\\ntwo substrings such that the ﬁrst substring is in A and the second substring\\nis in B.\\nIt seems to be even more diﬃcult to prove that A∗is a regular language,\\nif A itself is regular. In order to prove this, we need a ﬁnite automaton that,\\nwhen given an arbitrary input string u, decides whether or not u can be\\nbroken into substrings such that each substring is in A. The problem is that,\\nif u ∈A∗, the ﬁnite automaton has to determine into how many substrings,\\nand where, the string u has to be broken; it has to do this during one single\\nscan of the string u.\\nAs we mentioned already, if A and B are regular languages, then both\\nAB and A∗are also regular. In order to prove these claims, we will introduce\\na more general type of ﬁnite automaton.\\nThe ﬁnite automata that we have seen so far are deterministic.\\nThis\\nmeans the following:\\n• If the ﬁnite automaton M is in state r and if it reads the symbol a,\\nthen M switches from state r to the uniquely deﬁned state δ(r, a).\\nFrom now on, we will call such a ﬁnite automaton a deterministic ﬁnite\\nautomaton (DFA). In the next section, we will deﬁne the notion of a nonde-\\nterministic ﬁnite automaton (NFA). For such an automaton, there are zero\\nor more possible states to switch to. At ﬁrst sight, nondeterministic ﬁnite\\n2.4.\\nNondeterministic ﬁnite automata\\n35\\nautomata seem to be more powerful than their deterministic counterparts.\\nWe will prove, however, that DFAs have the same power as NFAs. As we will\\nsee, using this fact, it will be easy to prove that the class of regular languages\\nis closed under the concatenation and star operations.\\n2.4\\nNondeterministic ﬁnite automata\\nWe start by giving three examples of nondeterministic ﬁnite automata. These\\nexamples will show the diﬀerence between this type of automata and the\\ndeterministic versions that we have considered in the previous sections. After\\nthese examples, we will give a formal deﬁnition of a nondeterministic ﬁnite\\nautomaton.\\n2.4.1\\nA ﬁrst example\\nConsider the following state diagram:\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,ε\\n1\\n0,1\\nYou will notice three diﬀerences with the ﬁnite automata that we have\\nseen until now. First, if the automaton is in state q1 and reads the symbol 1,\\nthen it has two options: Either it stays in state q1, or it switches to state q2.\\nSecond, if the automaton is in state q2, then it can switch to state q3 without\\nreading a symbol; this is indicated by the edge having the empty string ϵ as\\nlabel. Third, if the automaton is in state q3 and reads the symbol 0, then it\\ncannot continue.\\nLet us see what this automaton can do when it gets the string 010110 as\\ninput. Initially, the automaton is in the start state q1.\\n• Since the ﬁrst symbol in the input string is 0, the automaton stays in\\nstate q1 after having read this symbol.\\n• The second symbol is 1, and the automaton can either stay in state q1\\nor switch to state q2.\\n36\\nChapter 2.\\nFinite Automata and Regular Languages\\n– If the automaton stays in state q1, then it is still in this state after\\nhaving read the third symbol.\\n– If the automaton switches to state q2, then it again has two op-\\ntions:\\n∗Either read the third symbol in the input string, which is 0,\\nand switch to state q3,\\n∗or switch to state q3, without reading the third symbol.\\nIf we continue in this way, then we see that, for the input string 010110,\\nthere are seven possible computations. All these computations are given in\\nthe ﬁgure below.\\nq1\\nq1\\n0\\n1\\nq1\\nq1\\n0\\n1\\n1\\nq1\\nq2\\n1\\n1\\nq1\\nq2\\nq1\\n0\\n0\\nε\\nq3\\nq3\\nhang\\nhang\\nε\\nq3\\nq4\\n1\\n0\\nq4\\n1\\nq2\\n0\\nε\\nq3\\nq3\\nhang\\n1\\nq4\\n1\\nq4\\nq4\\n0\\nConsider the lowest path in the ﬁgure above:\\n• When reading the ﬁrst symbol, the automaton stays in state q1.\\n• When reading the second symbol, the automaton switches to state q2.\\n• The automaton does not read the third symbol (equivalently, it “reads”\\nthe empty string ϵ), and switches to state q3. At this moment, the\\n2.4.\\nNondeterministic ﬁnite automata\\n37\\nautomaton cannot continue: The third symbol is 0, but there is no\\nedge leaving q3 that is labeled 0, and there is no edge leaving q3 that\\nis labeled ϵ. Therefore, the computation hangs at this point.\\nFrom the ﬁgure, you can see that, out of the seven possible computations,\\nexactly two end in the accept state q4 (after the entire input string 010110 has\\nbeen read). We say that the automaton accepts the string 010110, because\\nthere is at least one computation that ends in the accept state.\\nNow consider the input string 010. In this case, there are three possible\\ncomputations:\\n1. q1\\n0→q1\\n1→q1\\n0→q1\\n2. q1\\n0→q1\\n1→q2\\n0→q3\\n3. q1\\n0→q1\\n1→q2\\nϵ→q3 →hang\\nNone of these computations ends in the accept state (after the entire input\\nstring 010 has been read). Therefore, we say that the automaton rejects the\\ninput string 010.\\nThe state diagram given above is an example of a nondeterministic ﬁnite\\nautomaton (NFA). Informally, an NFA accepts a string, if there exists at least\\none path in the state diagram that (i) starts in the start state, (ii) does not\\nhang before the entire string has been read, and (iii) ends in an accept state.\\nA string for which (i), (ii), and (iii) does not hold is rejected by the NFA.\\nThe NFA given above accepts all binary strings that contain 101 or 11 as\\na substring. All other binary strings are rejected.\\n2.4.2\\nA second example\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right}.\\nThe following state diagram deﬁnes an NFA that accepts all strings that are\\nin A, and rejects all strings that are not in A.\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,1\\n0,1\\n38\\nChapter 2.\\nFinite Automata and Regular Languages\\nThis NFA does the following. If it is in the start state q1 and reads the\\nsymbol 1, then it either stays in state q1 or it “guesses” that this symbol\\nis the third symbol from the right in the input string. In the latter case,\\nthe NFA switches to state q2, and then it “veriﬁes” that there are indeed\\nexactly two remaining symbols in the input string. If there are more than\\ntwo remaining symbols, then the NFA hangs (in state q4) after having read\\nthe next two symbols.\\nObserve how this guessing mechanism is used: The automaton can only\\nread the input string once, from left to right. Hence, it does not know when\\nit reaches the third symbol from the right. When the NFA reads a 1, it can\\nguess that this is the third symbol from the right; after having made this\\nguess, it veriﬁes whether or not the guess was correct.\\nIn Section 2.2.3, we have seen a DFA for the same language A. Observe\\nthat the NFA has a much simpler structure than the DFA.\\n2.4.3\\nA third example\\nConsider the following state diagram, which deﬁnes an NFA whose alphabet\\nis {0}.\\nε\\nε\\n0\\n0\\n0\\n0\\n0\\nThis NFA accepts the language\\nA = {0k : k ≡0 mod 2 or k ≡0 mod 3},\\nwhere 0k is the string consisting of k many 0s. (If k = 0, then 0k = ϵ.)\\nObserve that A is the union of the two languages\\nA1 = {0k : k ≡0 mod 2}\\n2.4.\\nNondeterministic ﬁnite automata\\n39\\nand\\nA2 = {0k : k ≡0 mod 3}.\\nThe NFA basically consists of two DFAs: one of these accepts A1, whereas the\\nother accepts A2. Given an input string w, the NFA has to decide whether\\nor not w ∈A, which is equivalent to deciding whether or not w ∈A1 or\\nw ∈A2. The NFA makes this decision in the following way: At the start, it\\n“guesses” whether (i) it is going to check whether or not w ∈A1 (i.e., the\\nlength of w is even), or (ii) it is going to check whether or not w ∈A2 (i.e.,\\nthe length of w is a multiple of 3). After having made the guess, it veriﬁes\\nwhether or not the guess was correct. If w ∈A, then there exists a way of\\nmaking the correct guess and verifying that w is indeed an element of A (by\\nending in an accept state). If w ̸∈A, then no matter which guess is made,\\nthe NFA will never end in an accept state.\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\nThe previous examples give you an idea what nondeterministic ﬁnite au-\\ntomata are and how they work. In this section, we give a formal deﬁnition\\nof these automata.\\nFor any alphabet Σ, we deﬁne Σϵ to be the set\\nΣϵ = Σ ∪{ϵ}.\\nRecall the notion of a power set: For any set Q, the power set of Q, denoted\\nby P(Q), is the set of all subsets of Q, i.e.,\\nP(Q) = {R : R ⊆Q}.\\nDeﬁnition 2.4.1 A nondeterministic ﬁnite automaton (NFA) is a 5-tuple\\nM = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σϵ →P(Q) is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\n40\\nChapter 2.\\nFinite Automata and Regular Languages\\nAs for DFAs, the transition function δ can be thought of as the “program”\\nof the ﬁnite automaton M = (Q, Σ, δ, q, F):\\n• Let r ∈Q, and let a ∈Σϵ. Then δ(r, a) is a (possibly empty) subset of\\nQ. If the NFA M is in state r, and if it reads a (where a may be the\\nempty string ϵ), then M can switch from state r to any state in δ(r, a).\\nIf δ(r, a) = ∅, then M cannot continue and the computation hangs.\\nThe example given in Section 2.4.1 is an NFA, where Q = {q1, q2, q3, q4},\\nΣ = {0, 1}, the start state is q1, the set of accept states is F = {q4}, and the\\ntransition function δ is given by the following table:\\n0\\n1\\nϵ\\nq1\\n{q1}\\n{q1, q2}\\n∅\\nq2\\n{q3}\\n∅\\n{q3}\\nq3\\n∅\\n{q4}\\n∅\\nq4\\n{q4}\\n{q4}\\n∅\\nDeﬁnition 2.4.2 Let M = (Q, Σ, δ, q, F) be an NFA, and let w ∈Σ∗. We\\nsay that M accepts w, if1\\n• w = ϵ and the start state q is an accept state, or\\n• there exists an integer m ≥1, such that w can be written as w =\\ny1y2 . . . ym, where yi ∈Σϵ for all i with 1 ≤i ≤m, and there exists a\\nsequence r0, r1, . . . , rm of states in Q, such that\\n– r0 = q,\\n– ri+1 ∈δ(ri, yi+1), for i = 0, 1, . . . , m −1, and\\n– rm ∈F.\\nOtherwise, we say that M rejects the string w.\\nThe NFA in the example in Section 2.4.1 accepts the string 01100. This\\ncan be seen by taking\\n• m = 6,\\n1Thanks to Antoine Vigneron for pointing out an error in a previous version of this\\ndeﬁnition.\\n2.5.\\nEquivalence of DFAs and NFAs\\n41\\n• w = 01ϵ100 = y1y2y3y4y5y6, and\\n• r0 = q1, r1 = q1, r2 = q2, r3 = q3, r4 = q4, r5 = q4, and r6 = q4.\\nDeﬁnition 2.4.3 Let M = (Q, Σ, δ, q, F) be an NFA. The language L(M)\\naccepted by M is deﬁned as\\nL(M) = {w ∈Σ∗: M accepts w }.\\n2.5\\nEquivalence of DFAs and NFAs\\nYou may have the impression that nondeterministic ﬁnite automata are more\\npowerful than deterministic ﬁnite automata. In this section, we will show\\nthat this is not the case.\\nThat is, we will prove that a language can be\\naccepted by a DFA if and only if it can be accepted by an NFA. In order\\nto prove this, we will show how to convert an arbitrary NFA to a DFA that\\naccepts the same language.\\nWhat about converting a DFA to an NFA? Well, there is (almost) nothing\\nto do, because a DFA is also an NFA. This is not quite true, because\\n• the transition function of a DFA maps a state and a symbol to a state,\\nwhereas\\n• the transition function of an NFA maps a state and a symbol to a set\\nof zero or more states.\\nThe formal conversion of a DFA to an NFA is done as follows: Let M =\\n(Q, Σ, δ, q, F) be a DFA. Recall that δ is a function δ : Q × Σ →Q. We\\ndeﬁne the function δ′ : Q × Σϵ →P(Q) as follows. For any r ∈Q and for\\nany a ∈Σϵ,\\nδ′(r, a) =\\n\\x1a {δ(r, a)}\\nif a ̸= ϵ,\\n∅\\nif a = ϵ.\\nThen N = (Q, Σ, δ′, q, F) is an NFA, whose behavior is exactly the same as\\nthat of the DFA M; the easiest way to see this is by observing that the state\\ndiagrams of M and N are equal. Therefore, we have L(M) = L(N).\\nIn the rest of this section, we will show how to convert an NFA to a DFA:\\nTheorem 2.5.1 Let N = (Q, Σ, δ, q, F) be a nondeterministic ﬁnite automa-\\nton. There exists a deterministic ﬁnite automaton M, such that L(M) =\\nL(N).\\n42\\nChapter 2.\\nFinite Automata and Regular Languages\\nProof.\\nRecall that the NFA N can (in general) perform more than one\\ncomputation on a given input string. The idea of the proof is to construct a\\nDFA M that runs all these diﬀerent computations simultaneously. (We have\\nseen this idea already in the proof of Theorem 2.3.1.) To be more precise,\\nthe DFA M will have the following property:\\n• the state that M is in after having read an initial part of the input\\nstring corresponds exactly to the set of all states that N can reach\\nafter having read the same part of the input string.\\nWe start by presenting the conversion for the case when N does not\\ncontain ϵ-transitions. In other words, the state diagram of N does not contain\\nany edge that has ϵ as a label. (Later, we will extend the conversion to the\\ngeneral case.) Let the DFA M be deﬁned as M = (Q′, Σ, δ′, q′, F ′), where\\n• the set Q′ of states is equal to Q′ = P(Q); observe that |Q′| = 2|Q|,\\n• the start state q′ is equal to q′ = {q}; so M has the “same” start state\\nas N,\\n• the set F ′ of accept states is equal to the set of all elements R of Q′\\nhaving the property that R contains at least one accept state of N, i.e.,\\nF ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• the transition function δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each\\nR ∈Q′ and for each a ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nδ(r, a).\\nLet us see what the transition function δ′ of M does. First observe that,\\nsince N is an NFA, δ(r, a) is a subset of Q. This implies that δ′(R, a) is the\\nunion of subsets of Q and, therefore, also a subset of Q. Hence, δ′(R, a) is\\nan element of Q′.\\nThe set δ(r, a) is equal to the set of all states of the NFA N that can be\\nreached from state r by reading the symbol a. We take the union of these\\nsets δ(r, a), where r ranges over all elements of R, to obtain the new set\\nδ′(R, a). This new set is the state that the DFA M reaches from state R, by\\nreading the symbol a.\\n2.5.\\nEquivalence of DFAs and NFAs\\n43\\nIn this way, we obtain the correspondence that was given in the beginning\\nof this proof.\\nAfter this warming-up, we can consider the general case. In other words,\\nfrom now on, we allow ϵ-transitions in the NFA N. The DFA M is deﬁned as\\nabove, except that the start state q′ and the transition function δ′ have to be\\nmodiﬁed. Recall that a computation of the NFA N consists of the following:\\n1. Start in the start state q and make zero or more ϵ-transitions.\\n2. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n3. Make zero or more ϵ-transitions.\\n4. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n5. Make zero or more ϵ-transitions.\\n6. Etc.\\nThe DFA M will simulate this computation in the following way:\\n• Simulate 1. in one single step. As we will see below, this simulation is\\nimplicitly encoded in the deﬁnition of the start state q′ of M.\\n• Simulate 2. and 3. in one single step.\\n• Simulate 4. and 5. in one single step.\\n• Etc.\\nThus, in one step, the DFA M simulates the reading of one “real” symbol of\\nΣ, followed by making zero or more ϵ-transitions.\\nTo formalize this, we need the notion of ϵ-closure. For any state r of the\\nNFA N, the ϵ-closure of r, denoted by Cϵ(r), is deﬁned to be the set of all\\nstates of N that can be reached from r, by making zero or more ϵ-transitions.\\nFor any state R of the DFA M (hence, R ⊆Q), we deﬁne\\nCϵ(R) =\\n[\\nr∈R\\nCϵ(r).\\n44\\nChapter 2.\\nFinite Automata and Regular Languages\\nHow do we deﬁne the start state q′ of the DFA M? Before the NFA N\\nreads its ﬁrst “real” symbol of Σ, it makes zero or more ϵ-transitions. In\\nother words, at the moment when N reads the ﬁrst symbol of Σ, it can be\\nin any state of Cϵ(q). Therefore, we deﬁne q′ to be\\nq′ = Cϵ(q) = Cϵ({q}).\\nHow do we deﬁne the transition function δ′ of the DFA M? Assume that\\nM is in state R, and reads the symbol a. At this moment, the NFA N would\\nhave been in any state r of R. By reading the symbol a, N can switch to\\nany state in δ(r, a), and then make zero or more ϵ-transitions. Hence, the\\nNFA can switch to any state in the set Cϵ(δ(r, a)). Based on this, we deﬁne\\nδ′(R, a) to be\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nTo summarize, the NFA N = (Q, Σ, δ, q, F) is converted to the DFA\\nM = (Q′, Σ, δ′, q′, F ′), where\\n• Q′ = P(Q),\\n• q′ = Cϵ({q}),\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nThe results proved until now can be summarized in the following theorem.\\nTheorem 2.5.2 Let A be a language. Then A is regular if and only if there\\nexists a nondeterministic ﬁnite automaton that accepts A.\\n2.5.1\\nAn example\\nConsider the NFA N = (Q, Σ, δ, q, F), where Q = {1, 2, 3}, Σ = {a, b}, q = 1,\\nF = {2}, and δ is given by the following table:\\n2.5.\\nEquivalence of DFAs and NFAs\\n45\\na\\nb\\nϵ\\n1\\n{3}\\n∅\\n{2}\\n2\\n{1}\\n∅\\n∅\\n3\\n{2}\\n{2, 3}\\n∅\\nThe state diagram of N is as follows:\\n1\\n2\\n3\\na\\na\\nǫ\\nb\\na, b\\nWe will show how to convert this NFA N to a DFA M that accepts the\\nsame language. Following the proof of Theorem 2.5.1, the DFA M is speciﬁed\\nby M = (Q′, Σ, δ′, q′, F ′), where each of the components is deﬁned below.\\n• Q′ = P(Q). Hence,\\nQ′ = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}.\\n• q′ = Cϵ({q}). Hence, the start state q′ of M is the set of all states of\\nN that can be reached from N’s start state q = 1, by making zero or\\nmore ϵ-transitions. We obtain\\nq′ = Cϵ({q}) = Cϵ({1}) = {1, 2}.\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅}. Hence, the accept states of M are those\\nstates that contain the accept state 2 of N. We obtain\\nF ′ = {{2}, {1, 2}, {2, 3}, {1, 2, 3}}.\\n46\\nChapter 2.\\nFinite Automata and Regular Languages\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nIn this example δ′ is given by\\nδ′(∅, a) = ∅\\nδ′(∅, b) = ∅\\nδ′({1}, a) = {3}\\nδ′({1}, b) = ∅\\nδ′({2}, a) = {1, 2}\\nδ′({2}, b) = ∅\\nδ′({3}, a) = {2}\\nδ′({3}, b) = {2, 3}\\nδ′({1, 2}, a) = {1, 2, 3}\\nδ′({1, 2}, b) = ∅\\nδ′({1, 3}, a) = {2, 3}\\nδ′({1, 3}, b) = {2, 3}\\nδ′({2, 3}, a) = {1, 2}\\nδ′({2, 3}, b) = {2, 3}\\nδ′({1, 2, 3}, a) = {1, 2, 3}\\nδ′({1, 2, 3}, b) = {2, 3}\\nThe state diagram of the DFA M is as follows:\\n2.5.\\nEquivalence of DFAs and NFAs\\n47\\n/0\\n{1}\\n{2}\\n{3}\\n{1,2}\\n{2,3}\\n{1,3}\\n{1,2,3}\\na,b\\nb\\na\\nb\\na\\na\\nb\\na,b\\na\\nb\\nb\\na\\nb\\na\\nWe make the following observations:\\n• The states {1} and {1, 3} do not have incoming edges. Therefore, these\\ntwo states cannot be reached from the start state {1, 2}.\\n• The state {3} has only one incoming edge; it comes from the state\\n{1}. Since {1} cannot be reached from the start state, {3} cannot be\\nreached from the start state.\\n• The state {2} has only one incoming edge; it comes from the state\\n{3}. Since {3} cannot be reached from the start state, {2} cannot be\\nreached from the start state.\\nHence, we can remove the four states {1}, {2}, {3}, and {1, 3}. The\\nresulting DFA accepts the same language as the DFA above.\\nThis leads\\nto the following state diagram, which depicts a DFA that accepts the same\\nlanguage as the NFA N:\\n48\\nChapter 2.\\nFinite Automata and Regular Languages\\n/0\\n{1,2}\\n{2,3}\\n{1,2,3}\\na,b\\na\\nb\\nb\\na\\nb\\na\\n2.6\\nClosure under the regular operations\\nIn Section 2.3, we have deﬁned the regular operations union, concatenation,\\nand star. We proved in Theorem 2.3.1 that the union of two regular lan-\\nguages is a regular language. We also explained why it is not clear that the\\nconcatenation of two regular languages is regular, and that the star of a reg-\\nular language is regular. In this section, we will see that the concept of NFA,\\ntogether with Theorem 2.5.2, can be used to give a simple proof of the fact\\nthat the regular languages are indeed closed under the regular operations.\\nWe start by giving an alternative proof of Theorem 2.3.1:\\nTheorem 2.6.1 The set of regular languages is closed under the union op-\\neration, i.e., if A1 and A2 are regular languages over the same alphabet Σ,\\nthen A1 ∪A2 is also a regular language.\\n2.6.\\nClosure under the regular operations\\n49\\nq1\\nM1\\nM2\\nq2\\nq0\\nq1\\nq2\\nε\\nε\\nM\\nFigure 2.1: The NFA M accepts L(M1) ∪L(M2).\\nProof.\\nSince A1 is regular, there is, by Theorem 2.5.2, an NFA M1 =\\n(Q1, Σ, δ1, q1, F1), such that A1 = L(M1). Similarly, there is an NFA M2 =\\n(Q2, Σ, δ2, q2, F2), such that A2 = L(M2). We may assume that Q1 ∩Q2 = ∅,\\nbecause otherwise, we can give new “names” to the states of Q1 and Q2.\\nFrom these two NFAs, we will construct an NFA M = (Q, Σ, δ, q0, F), such\\nthat L(M) = A1 ∪A2. The construction is illustrated in Figure 2.1. The\\nNFA M is deﬁned as follows:\\n1. Q = {q0} ∪Q1 ∪Q2, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = F1 ∪F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\n50\\nChapter 2.\\nFinite Automata and Regular Languages\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1,\\nδ2(r, a)\\nif r ∈Q2,\\n{q1, q2}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nTheorem 2.6.2 The set of regular languages is closed under the concatena-\\ntion operation, i.e., if A1 and A2 are regular languages over the same alphabet\\nΣ, then A1A2 is also a regular language.\\nProof.\\nLet M1 = (Q1, Σ, δ1, q1, F1) be an NFA, such that A1 = L(M1).\\nSimilarly, let M2 = (Q2, Σ, δ2, q2, F2) be an NFA, such that A2 = L(M2).\\nAs in the proof of Theorem 2.6.1, we may assume that Q1 ∩Q2 = ∅. We\\nwill construct an NFA M = (Q, Σ, δ, q0, F), such that L(M) = A1A2. The\\nconstruction is illustrated in Figure 2.2. The NFA M is deﬁned as follows:\\n1. Q = Q1 ∪Q2.\\n2. q0 = q1.\\n3. F = F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q2}\\nif r ∈F1 and a = ϵ,\\nδ2(r, a)\\nif r ∈Q2.\\nTheorem 2.6.3 The set of regular languages is closed under the star oper-\\nation, i.e., if A is a regular language, then A∗is also a regular language.\\n2.6.\\nClosure under the regular operations\\n51\\nq1\\nM1\\nM2\\nq2\\nq2\\nε\\nε\\nε\\nq0\\nM\\nFigure 2.2: The NFA M accepts L(M1)L(M2).\\nq1\\nN\\nq1\\nq0\\nε\\nε\\nε\\nε\\nM\\nFigure 2.3: The NFA M accepts (L(N))∗.\\nProof. Let Σ be the alphabet of A and let N = (Q1, Σ, δ1, q1, F1) be an\\nNFA, such that A = L(N). We will construct an NFA M = (Q, Σ, δ, q0, F),\\nsuch that L(M) = A∗. The construction is illustrated in Figure 2.3. The\\nNFA M is deﬁned as follows:\\n52\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. Q = {q0} ∪Q1, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = {q0} ∪F1. (Since ϵ ∈A∗, q0 has to be an accept state.)\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ,\\n{q1}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nIn the ﬁnal theorem of this section, we mention (without proof) two more\\nclosure properties of the regular languages:\\nTheorem 2.6.4 The set of regular languages is closed under the complement\\nand intersection operations:\\n1. If A is a regular language over the alphabet Σ, then the complement\\nA = {w ∈Σ∗: w ̸∈A}\\nis also a regular language.\\n2. If A1 and A2 are regular languages over the same alphabet Σ, then the\\nintersection\\nA1 ∩A2 = {w ∈Σ∗: w ∈A1 and w ∈A2}\\nis also a regular language.\\n2.7\\nRegular expressions\\nIn this section, we present regular expressions, which are a means to describe\\nlanguages. As we will see, the class of languages that can be described by\\nregular expressions coincides with the class of regular languages.\\n2.7.\\nRegular expressions\\n53\\nBefore formally deﬁning the notion of a regular expression, we give some\\nexamples. Consider the expression\\n(0 ∪1)01∗.\\nThe language described by this expression is the set of all binary strings\\n1. that start with either 0 or 1 (this is indicated by (0 ∪1)),\\n2. for which the second symbol is 0 (this is indicated by 0), and\\n3. that end with zero or more 1s (this is indicated by 1∗).\\nThat is, the language described by this expression is\\n{00, 001, 0011, 00111, . . . , 10, 101, 1011, 10111, . . .}.\\nHere are some more examples (in all cases, the alphabet is {0, 1}):\\n• The language {w : w contains exactly two 0s} is described by the ex-\\npression\\n1∗01∗01∗.\\n• The language {w : w contains at least two 0s} is described by the ex-\\npression\\n(0 ∪1)∗0(0 ∪1)∗0(0 ∪1)∗.\\n• The language {w : 1011 is a substring of w} is described by the ex-\\npression\\n(0 ∪1)∗1011(0 ∪1)∗.\\n• The language {w : the length of w is even} is described by the expres-\\nsion\\n((0 ∪1)(0 ∪1))∗.\\n• The language {w : the length of w is odd} is described by the expres-\\nsion\\n(0 ∪1) ((0 ∪1)(0 ∪1))∗.\\n• The language {1011, 0} is described by the expression\\n1011 ∪0.\\n54\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The language {w :\\nthe ﬁrst and last symbols of w are equal} is de-\\nscribed by the expression\\n0(0 ∪1)∗0 ∪1(0 ∪1)∗1 ∪0 ∪1.\\nAfter these examples, we give a formal (and inductive) deﬁnition of regular\\nexpressions:\\nDeﬁnition 2.7.1 Let Σ be a non-empty alphabet.\\n1. ϵ is a regular expression.\\n2. ∅is a regular expression.\\n3. For each a ∈Σ, a is a regular expression.\\n4. If R1 and R2 are regular expressions, then R1 ∪R2 is a regular expres-\\nsion.\\n5. If R1 and R2 are regular expressions, then R1R2 is a regular expression.\\n6. If R is a regular expression, then R∗is a regular expression.\\nYou can regard 1., 2., and 3. as being the “building blocks” of regular\\nexpressions.\\nItems 4., 5., and 6. give rules that can be used to combine\\nregular expressions into new (and “larger”) regular expressions. To give an\\nexample, we claim that\\n(0 ∪1)∗101(0 ∪1)∗\\nis a regular expression (where the alphabet Σ is equal to {0, 1}). In order\\nto prove this, we have to show that this expression can be “built” using the\\n“rules” given in Deﬁnition 2.7.1. Here we go:\\n• By 3., 0 is a regular expression.\\n• By 3., 1 is a regular expression.\\n• Since 0 and 1 are regular expressions, by 4., 0∪1 is a regular expression.\\n• Since 0∪1 is a regular expression, by 6., (0∪1)∗is a regular expression.\\n• Since 1 and 0 are regular expressions, by 5., 10 is a regular expression.\\n2.7.\\nRegular expressions\\n55\\n• Since 10 and 1 are regular expressions, by 5., 101 is a regular expression.\\n• Since (0 ∪1)∗and 101 are regular expressions, by 5., (0 ∪1)∗101 is a\\nregular expression.\\n• Since (0 ∪1)∗101 and (0 ∪1)∗are regular expressions, by 5., (0 ∪\\n1)∗101(0 ∪1)∗is a regular expression.\\nNext we deﬁne the language that is described by a regular expression:\\nDeﬁnition 2.7.2 Let Σ be a non-empty alphabet.\\n1. The regular expression ϵ describes the language {ϵ}.\\n2. The regular expression ∅describes the language ∅.\\n3. For each a ∈Σ, the regular expression a describes the language {a}.\\n4. Let R1 and R2 be regular expressions and let L1 and L2 be the lan-\\nguages described by them, respectively. The regular expression R1∪R2\\ndescribes the language L1 ∪L2.\\n5. Let R1 and R2 be regular expressions and let L1 and L2 be the languages\\ndescribed by them, respectively. The regular expression R1R2 describes\\nthe language L1L2.\\n6. Let R be a regular expression and let L be the language described by\\nit. The regular expression R∗describes the language L∗.\\nWe consider some examples:\\n• The regular expression (0∪ϵ)(1∪ϵ) describes the language {01, 0, 1, ϵ}.\\n• The regular expression 0 ∪ϵ describes the language {0, ϵ}, whereas the\\nregular expression 1∗describes the language {ϵ, 1, 11, 111, . . .}. There-\\nfore, the regular expression (0 ∪ϵ)1∗describes the language\\n{0, 01, 011, 0111, . . . , ϵ, 1, 11, 111, . . .}.\\nObserve that this language is also described by the regular expression\\n01∗∪1∗.\\n56\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The regular expression 1∗∅describes the empty language, i.e., the lan-\\nguage ∅. (You should convince yourself that this is correct.)\\n• The regular expression ∅∗describes the language {ϵ}.\\nDeﬁnition 2.7.3 Let R1 and R2 be regular expressions and let L1 and L2\\nbe the languages described by them, respectively. If L1 = L2 (i.e., R1 and\\nR2 describe the same language), then we will write R1 = R2.\\nHence, even though (0∪ϵ)1∗and 01∗∪1∗are diﬀerent regular expressions,\\nwe write\\n(0 ∪ϵ)1∗= 01∗∪1∗,\\nbecause they describe the same language.\\nIn Section 2.8.2, we will show that every regular language can be described\\nby a regular expression. The proof of this fact is purely algebraic and uses\\nthe following algebraic identities involving regular expressions.\\nTheorem 2.7.4 Let R1, R2, and R3 be regular expressions. The following\\nidentities hold:\\n1. R1∅= ∅R1 = ∅.\\n2. R1ϵ = ϵR1 = R1.\\n3. R1 ∪∅= ∅∪R1 = R1.\\n4. R1 ∪R1 = R1.\\n5. R1 ∪R2 = R2 ∪R1.\\n6. R1(R2 ∪R3) = R1R2 ∪R1R3.\\n7. (R1 ∪R2)R3 = R1R3 ∪R2R3.\\n8. R1(R2R3) = (R1R2)R3.\\n9. ∅∗= ϵ.\\n10. ϵ∗= ϵ.\\n11. (ϵ ∪R1)∗= R∗\\n1.\\n2.8.\\nEquivalence of regular expressions and regular languages 57\\n12. (ϵ ∪R1)(ϵ ∪R1)∗= R∗\\n1.\\n13. R∗\\n1(ϵ ∪R1) = (ϵ ∪R1)R∗\\n1 = R∗\\n1.\\n14. R∗\\n1R2 ∪R2 = R∗\\n1R2.\\n15. R1(R2R1)∗= (R1R2)∗R1.\\n16. (R1 ∪R2)∗= (R∗\\n1R2)∗R∗\\n1 = (R∗\\n2R1)∗R∗\\n2.\\nWe will not present the (boring) proofs of these identities, but urge you\\nto convince yourself informally that they make perfect sense. To give an\\nexample, we mentioned above that\\n(0 ∪ϵ)1∗= 01∗∪1∗.\\nWe can verify this identity in the following way:\\n(0 ∪ϵ)1∗\\n=\\n01∗∪ϵ1∗\\n(by identity 7)\\n=\\n01∗∪1∗\\n(by identity 2)\\n2.8\\nEquivalence of regular expressions and reg-\\nular languages\\nIn the beginning of Section 2.7, we mentioned the following result:\\nTheorem 2.8.1 Let L be a language. Then L is regular if and only if there\\nexists a regular expression that describes L.\\nThe proof of this theorem consists of two parts:\\n• In Section 2.8.1, we will prove that every regular expression describes\\na regular language.\\n• In Section 2.8.2, we will prove that every DFA M can be converted to\\na regular expression that describes the language L(M).\\nThese two results will prove Theorem 2.8.1.\\n58\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.8.1\\nEvery regular expression describes a regular lan-\\nguage\\nLet R be an arbitrary regular expression over the alphabet Σ. We will prove\\nthat the language described by R is a regular language. The proof is by\\ninduction on the structure of R (i.e., by induction on the way R is “built”\\nusing the “rules” given in Deﬁnition 2.7.1).\\nThe ﬁrst base case: Assume that R = ϵ.\\nThen R describes the lan-\\nguage {ϵ}. In order to prove that this language is regular, it suﬃces, by\\nTheorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q, F) that accepts this\\nlanguage. This NFA is obtained by deﬁning Q = {q}, q is the start state,\\nF = {q}, and δ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state\\ndiagram of M:\\nq\\nThe second base case: Assume that R = ∅. Then R describes the language\\n∅. In order to prove that this language is regular, it suﬃces, by Theorem 2.5.2,\\nto construct an NFA M = (Q, Σ, δ, q, F) that accepts this language. This\\nNFA is obtained by deﬁning Q = {q}, q is the start state, F = ∅, and\\nδ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state diagram of M:\\nq\\nThe third base case: Let a ∈Σ and assume that R = a. Then R describes\\nthe language {a}. In order to prove that this language is regular, it suﬃces,\\nby Theorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q1, F) that accepts\\nthis language. This NFA is obtained by deﬁning Q = {q1, q2}, q1 is the start\\nstate, F = {q2}, and\\nδ(q1, a)\\n=\\n{q2},\\nδ(q1, b)\\n=\\n∅for all b ∈Σϵ \\\\ {a},\\nδ(q2, b)\\n=\\n∅for all b ∈Σϵ.\\nThe ﬁgure below gives the state diagram of M:\\n2.8.\\nEquivalence of regular expressions and regular languages 59\\nq1\\nq2\\na\\nThe ﬁrst case of the induction step: Assume that R = R1 ∪R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1 ∪L2, which, by Theorem 2.6.1, is regular.\\nThe second case of the induction step: Assume that R = R1R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1L2, which, by Theorem 2.6.2, is regular.\\nThe third case of the induction step: Assume that R = (R1)∗, where\\nR1 is a regular expression.\\nLet L1 be the language described by R1 and\\nassume that L1 is regular. Then R describes the language (L1)∗, which, by\\nTheorem 2.6.3, is regular.\\nThis concludes the proof of the claim that every regular expression de-\\nscribes a regular language.\\nTo give an example, consider the regular expression\\n(ab ∪a)∗,\\nwhere the alphabet is {a, b}. We will prove that this regular expression de-\\nscribes a regular language, by constructing an NFA that accepts the language\\ndescribed by this regular expression. Observe how the regular expression is\\n“built”:\\n• Take the regular expressions a and b, and combine them into the regular\\nexpression ab.\\n• Take the regular expressions ab and a, and combine them into the\\nregular expression ab ∪a.\\n• Take the regular expression ab ∪a, and transform it into the regular\\nexpression (ab ∪a)∗.\\nFirst, we construct an NFA M1 that accepts the language described by\\nthe regular expression a:\\n60\\nChapter 2.\\nFinite Automata and Regular Languages\\na\\nM1\\nNext, we construct an NFA M2 that accepts the language described by\\nthe regular expression b:\\nM2\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.2 to\\nM1 and M2. This gives an NFA M3 that accepts the language described by\\nthe regular expression ab:\\nM3\\na\\nε\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.1 to\\nM3 and M1. This gives an NFA M4 that accepts the language described by\\nthe regular expression ab ∪a:\\na\\nε\\nb\\na\\nε\\nε\\nM4\\nFinally, we apply the construction given in the proof of Theorem 2.6.3\\nto M4. This gives an NFA M5 that accepts the language described by the\\nregular expression (ab ∪a)∗:\\n2.8.\\nEquivalence of regular expressions and regular languages 61\\na\\nε\\nb\\na\\nε\\nε\\nε\\nε\\nε\\nM5\\n2.8.2\\nConverting a DFA to a regular expression\\nIn this section, we will prove that every DFA M can be converted to a regular\\nexpression that describes the language L(M). In order to prove this result,\\nwe need to solve recurrence relations involving languages.\\nSolving recurrence relations\\nLet Σ be an alphabet, let B and C be “known” languages in Σ∗such that\\nϵ ̸∈B, and let L be an “unknown” language such that\\nL = BL ∪C.\\nCan we “solve” this equation for L? That is, can we express L in terms of\\nB and C?\\nConsider an arbitrary string u in L. We are going to determine how u\\nlooks like. Since u ∈L and L = BL ∪C, we know that u is a string in\\nBL ∪C. Hence, there are two possibilities for u.\\n1. u is an element of C.\\n2. u is an element of BL. In this case, there are strings b ∈B and v ∈L\\nsuch that u = bv. Since ϵ ̸∈B, we have b ̸= ϵ and, therefore, |v| < |u|.\\n(Recall that |v| denotes the length, i.e., the number of symbols, of the\\nstring v.) Since v is a string in L, which is equal to BL ∪C, v is a\\nstring in BL ∪C. Hence, there are two possibilities for v.\\n62\\nChapter 2.\\nFinite Automata and Regular Languages\\n(a) v is an element of C. In this case,\\nu = bv, where b ∈B and v ∈C; thus, u ∈BC.\\n(b) v is an element of BL. In this case, there are strings b′ ∈B and\\nw ∈L such that v = b′w. Since ϵ ̸∈B, we have b′ ̸= ϵ and,\\ntherefore, |w| < |v|. Since w is a string in L, which is equal to\\nBL∪C, w is a string in BL∪C. Hence, there are two possibilities\\nfor w.\\ni. w is an element of C. In this case,\\nu = bb′w, where b, b′ ∈B and w ∈C; thus, u ∈BBC.\\nii. w is an element of BL. In this case, there are strings b′′ ∈B\\nand x ∈L such that w = b′′x. Since ϵ ̸∈B, we have b′′ ̸= ϵ\\nand, therefore, |x| < |w|. Since x is a string in L, which is\\nequal to BL ∪C, x is a string in BL ∪C. Hence, there are\\ntwo possibilities for x.\\nA. x is an element of C. In this case,\\nu = bb′b′′x, where b, b′, b′′ ∈B and x ∈C; thus, u ∈BBBC.\\nB. x is an element of BL. Etc., etc.\\nThis process hopefully convinces you that any string u in L can be written\\nas the concatenation of zero or more strings in B, followed by one string in\\nC. In fact, L consists of exactly those strings having this property:\\nLemma 2.8.2 Let Σ be an alphabet, and let B, C, and L be languages in\\nΣ∗such that ϵ ̸∈B and\\nL = BL ∪C.\\nThen\\nL = B∗C.\\nProof. First, we show that B∗C ⊆L. Let u be an arbitrary string in B∗C.\\nThen u is the concatenation of k strings of B, for some k ≥0, followed by\\none string of C. We proceed by induction on k.\\nThe base case is when k = 0. In this case, u is a string in C. Hence, u is\\na string in BL ∪C. Since BL ∪C = L, it follows that u is a string in L.\\n2.8.\\nEquivalence of regular expressions and regular languages 63\\nNow let k ≥1. Then we can write u = vwc, where v is a string in B,\\nw is the concatenation of k −1 strings of B, and c is a string of C. Deﬁne\\ny = wc. Observe that y is the concatenation of k −1 strings of B followed\\nby one string of C. Therefore, by induction, the string y is an element of L.\\nHence, u = vy, where v is a string in B and y is a string in L. This shows\\nthat u is a string in BL. Hence, u is a string in BL ∪C. Since BL ∪C = L,\\nit follows that u is a string in L. This completes the proof that B∗C ⊆L.\\nIt remains to show that L ⊆B∗C. Let u be an arbitrary string in L,\\nand let ℓbe its length (i.e., ℓis the number of symbols in u). We prove by\\ninduction on ℓthat u is a string in B∗C.\\nThe base case is when ℓ= 0. Then u = ϵ. Since u ∈L and L = BL ∪C,\\nu is a string in BL ∪C. Since ϵ ̸∈B, u cannot be a string in BL. Hence, u\\nmust be a string in C. Since C ⊆B∗C, it follows that u is a string in B∗C.\\nLet ℓ≥1. If u is a string in C, then u is a string in B∗C and we are done.\\nSo assume that u is not a string in C. Since u ∈L and L = BL ∪C, u is a\\nstring in BL. Hence, there are strings b ∈B and v ∈L such that u = bv.\\nSince ϵ ̸∈B, the length of b is at least one; hence, the length of v is less than\\nthe length of u. By induction, v is a string in B∗C. Hence, u = bv, where\\nb ∈B and v ∈B∗C. This shows that u ∈B(B∗C). Since B(B∗C) ⊆B∗C,\\nit follows that u ∈B∗C.\\nNote that Lemma 2.8.2 holds for any language B that does not contain\\nthe empty string ϵ. As an example, assume that B = ∅. Then the language\\nL satisﬁes the equation\\nL = BL ∪C = ∅L ∪C.\\nUsing Theorem 2.7.4, this equation becomes\\nL = ∅∪C = C.\\nWe now show that Lemma 2.8.2 also implies that L = C: Since ϵ ̸∈B,\\nLemma 2.8.2 implies that L = B∗C, which, using Theorem 2.7.4, becomes\\nL = B∗C = ∅∗C = ϵC = C.\\nThe conversion\\nWe will now use Lemma 2.8.2 to prove that every DFA can be converted to\\na regular expression.\\n64\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet M = (Q, Σ, δ, q, F) be an arbitrary deterministic ﬁnite automaton.\\nWe will show that there exists a regular expression that describes the lan-\\nguage L(M).\\nFor each state r ∈Q, we deﬁne\\nLr = {w ∈Σ∗:\\nthe path in the state diagram of M that starts\\nin state r and that corresponds to w ends in a\\nstate of F }.\\nIn words, Lr is the language accepted by M, if r were the start state.\\nWe will show that each such language Lr can be described by a regular\\nexpression. Since L(M) = Lq, this will prove that L(M) can be described by\\na regular expression.\\nThe basic idea is to set up equations for the languages Lr, which we then\\nsolve using Lemma 2.8.2. We claim that\\nLr =\\n[\\na∈Σ\\na · Lδ(r,a)\\nif r ̸∈F.\\n(2.2)\\nWhy is this true? Let w be a string in Lr. Then the path P in the state\\ndiagram of M that starts in state r and that corresponds to w ends in a\\nstate of F. Since r ̸∈F, this path contains at least one edge. Let r′ be the\\nstate that follows the ﬁrst state (i.e., r) of P. Then r′ = δ(r, b) for some\\nsymbol b ∈Σ. Hence, b is the ﬁrst symbol of w. Write w = bv, where v is\\nthe remaining part of w. Then the path P ′ = P \\\\ {r} in the state diagram\\nof M that starts in state r′ and that corresponds to v ends in a state of F.\\nTherefore, v ∈Lr′ = Lδ(r,b). Hence,\\nw ∈b · Lδ(r,b) ⊆\\n[\\na∈Σ\\na · Lδ(r,a).\\nConversely, let w be a string in S\\na∈Σ a · Lδ(r,a). Then there is a symbol b ∈Σ\\nand a string v ∈Lδ(r,b) such that w = bv. Let P ′ be the path in the state\\ndiagram of M that starts in state δ(r, b) and that corresponds to v. Since\\nv ∈Lδ(r,b), this path ends in a state of F. Let P be the path in the state\\ndiagram of M that starts in r, follows the edge to δ(r, b), and then follows P ′.\\nThis path P corresponds to w and ends in a state of F. Therefore, w ∈Lr.\\nThis proves the correctness of (2.2).\\n2.8.\\nEquivalence of regular expressions and regular languages 65\\nSimilarly, we can prove that\\nLr = ϵ ∪\\n [\\na∈Σ\\na · Lδ(r,a)\\n!\\nif r ∈F.\\n(2.3)\\nSo we now have a set of equations in the “unknowns” Lr, for r ∈Q. The\\nnumber of equations is equal to the size of Q. In other words, the number\\nof equations is equal to the number of unknowns. The regular expression for\\nL(M) = Lq is obtained by solving these equations using Lemma 2.8.2.\\nOf course, we have to convince ourselves that these equations have a so-\\nlution for any given DFA. Before we deal with this issue, we give an example.\\nAn example\\nConsider the deterministic ﬁnite automaton M = (Q, Σ, δ, q0, F), where Q =\\n{q0, q1, q2}, Σ = {a, b}, q0 is the start state, F = {q2}, and δ is given in the\\nstate diagram below. We show how to obtain the regular expression that\\ndescribes the language accepted by M.\\nq0\\nq1\\nq2\\na\\na\\na\\nb\\nb\\nb\\nFor this case, (2.2) and (2.3) give the following equations:\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nLq0\\n=\\na · Lq0 ∪b · Lq2\\nLq1\\n=\\na · Lq0 ∪b · Lq1\\nLq2\\n=\\nϵ ∪a · Lq1 ∪b · Lq0\\n66\\nChapter 2.\\nFinite Automata and Regular Languages\\nIn the third equation, Lq2 is expressed in terms of Lq0 and Lq1. Hence, if we\\nsubstitute the third equation into the ﬁrst one, and use Theorem 2.7.4, then\\nwe get\\nLq0\\n=\\na · Lq0 ∪b · (ϵ ∪a · Lq1 ∪b · Lq0)\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b.\\nWe obtain the following set of equations.\\n\\x1a Lq0\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b\\nLq1\\n=\\nb · Lq1 ∪a · Lq0\\nLet L = Lq1, B = b, and C = a · Lq0. Then ϵ ̸∈B and the second equation\\nreads L = BL ∪C. Hence, by Lemma 2.8.2,\\nLq1 = L = B∗C = b∗a · Lq0.\\nIf we substitute Lq1 into the ﬁrst equation, then we get (again using Theo-\\nrem 2.7.4)\\nLq0\\n=\\n(a ∪bb) · Lq0 ∪ba · b∗a · Lq0 ∪b\\n=\\n(a ∪bb ∪bab∗a)Lq0 ∪b.\\nAgain applying Lemma 2.8.2, this time with L = Lq0, B = a∪bb∪bab∗a and\\nC = b, gives\\nLq0 = (a ∪bb ∪bab∗a)∗b.\\nThus, the regular expression that describes the language accepted by M is\\n(a ∪bb ∪bab∗a)∗b.\\nCompleting the correctness of the conversion\\nIt remains to prove that, for any DFA, the system of equations (2.2) and (2.3)\\ncan be solved. This will follow from the following (more general) lemma.\\n(You should verify that the equations (2.2) and (2.3) are in the form as\\nspeciﬁed in this lemma.)\\n2.8.\\nEquivalence of regular expressions and regular languages 67\\nLemma 2.8.3 Let n ≥1 be an integer and, for 1 ≤i ≤n and 1 ≤j ≤n,\\nlet Bij and Ci be regular expressions such that ϵ ̸∈Bij. Let L1, L2, . . . , Ln be\\nlanguages that satisfy\\nLi =\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci for 1 ≤i ≤n.\\nThen L1 can be expressed as a regular expression only involving the regular\\nexpressions Bij and Ci.\\nProof. The proof is by induction on n. The base case is when n = 1. In\\nthis case, we have\\nL1 = B11L1 ∪C1.\\nSince ϵ ̸∈B11, it follows from Lemma 2.8.2 that L1 = B∗\\n11C1. This proves\\nthe base case.\\nLet n ≥2 and assume the lemma is true for n −1. We have\\nLn\\n=\\n n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n=\\nBnnLn ∪\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn.\\nSince ϵ ̸∈Bnn, it follows from Lemma 2.8.2 that\\nLn\\n=\\nB∗\\nnn\\n  n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n!\\n=\\nB∗\\nnn\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪B∗\\nnnCn\\n=\\n n−1\\n[\\nj=1\\nB∗\\nnnBnjLj\\n!\\n∪B∗\\nnnCn\\nBy substituting this equation for Ln into the equations for Li, 1 ≤i ≤n −1,\\n68\\nChapter 2.\\nFinite Automata and Regular Languages\\nwe obtain\\nLi\\n=\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\nBinLn ∪\\n n−1\\n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\n n−1\\n[\\nj=1\\n(BinB∗\\nnnBnj ∪Bij) Lj\\n!\\n∪BinB∗\\nnnCn ∪Ci.\\nThus, we have obtained n −1 equations in L1, L2, . . . , Ln−1.\\nSince ϵ ̸∈\\nBinB∗\\nnnBnj ∪Bij, it follows from the induction hypothesis that L1 can be\\nexpressed as a regular expression only involving the regular expressions Bij\\nand Ci.\\n2.9\\nThe pumping lemma and nonregular lan-\\nguages\\nIn the previous sections, we have seen that the class of regular languages is\\nclosed under various operations, and that these languages can be described by\\n(deterministic or nondeterministic) ﬁnite automata and regular expressions.\\nThese properties helped in developing techniques for showing that a language\\nis regular. In this section, we will present a tool that can be used to prove\\nthat certain languages are not regular. Observe that for a regular language,\\n1. the amount of memory that is needed to determine whether or not a\\ngiven string is in the language is ﬁnite and independent of the length\\nof the string, and\\n2. if the language consists of an inﬁnite number of strings, then this lan-\\nguage should contain inﬁnite subsets having a fairly repetitive struc-\\nture.\\nIntuitively, languages that do not follow 1. or 2. should be nonregular. For\\nexample, consider the language\\n{0n1n : n ≥0}.\\n2.9.\\nThe pumping lemma and nonregular languages\\n69\\nThis language should be nonregular, because it seems unlikely that a DFA can\\nremember how many 0s it has seen when it has reached the border between\\nthe 0s and the 1s. Similarly the language\\n{0n : n is a prime number}\\nshould be nonregular, because the prime numbers do not seem to have any\\nrepetitive structure that can be used by a DFA. To be more rigorous about\\nthis, we will establish a property that all regular languages must possess.\\nThis property is called the pumping lemma. If a language does not have this\\nproperty, then it must be nonregular.\\nThe pumping lemma states that any suﬃciently long string in a regular\\nlanguage can be pumped, i.e., there is a section in that string that can be\\nrepeated any number of times, so that the resulting strings are all in the\\nlanguage.\\nTheorem 2.9.1 (Pumping Lemma for Regular Languages) Let A be\\na regular language. Then there exists an integer p ≥1, called the pumping\\nlength, such that the following holds: Every string s in A, with |s| ≥p, can\\nbe written as s = xyz, such that\\n1. y ̸= ϵ (i.e., |y| ≥1),\\n2. |xy| ≤p, and\\n3. for all i ≥0, xyiz ∈A.\\nIn words, the pumping lemma states that by replacing the portion y in s\\nby zero or more copies of it, the resulting string is still in the language A.\\nProof. Let Σ be the alphabet of A. Since A is a regular language, there\\nexists a DFA M = (Q, Σ, δ, q, F) that accepts A. We deﬁne p to be the\\nnumber of states in Q.\\nLet s = s1s2 . . . sn be an arbitrary string in A such that n ≥p. Deﬁne\\nr1 = q, r2 = δ(r1, s1), r3 = δ(r2, s2), . . ., rn+1 = δ(rn, sn). Thus, when the\\nDFA M reads the string s from left to right, it visits the states r1, r2, . . . , rn+1.\\nSince s is a string in A, we know that rn+1 belongs to F.\\nConsider the ﬁrst p + 1 states r1, r2, . . . , rp+1 in this sequence. Since the\\nnumber of states of M is equal to p, the pigeonhole principle implies that\\nthere must be a state that occurs twice in this sequence. That is, there are\\nindices j and ℓsuch that 1 ≤j < ℓ≤p + 1 and rj = rℓ.\\n70\\nChapter 2.\\nFinite Automata and Regular Languages\\nq = r1\\nrn+1\\nr j = rℓ\\nread x\\nread y\\nread z\\nWe deﬁne x = s1s2 . . . sj−1, y = sj . . . sℓ−1, and z = sℓ. . . sn. Since j < ℓ,\\nwe have y ̸= ϵ, proving the ﬁrst claim in the theorem. Since ℓ≤p + 1, we\\nhave |xy| = ℓ−1 ≤p, proving the second claim in the theorem. To see that\\nthe third claim also holds, recall that the string s = xyz is accepted by M.\\nWhile reading x, M moves from the start state q to state rj. While reading\\ny, it moves from state rj to state rℓ= rj, i.e., after having read y, M is again\\nin state rj. While reading z, M moves from state rj to the accept state rn+1.\\nTherefore, the substring y can be repeated any number i ≥0 of times, and\\nthe corresponding string xyiz will still be accepted by M. It follows that\\nxyiz ∈A for all i ≥0.\\n2.9.1\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {0n1n : n ≥0}.\\nWe will prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. It is clear\\nthat s ∈A and |s| = 2p ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that, since |xy| ≤p, the string y contains only 0s. Moreover,\\nsince y ̸= ϵ, y contains at least one 0. But now we are in trouble: None of\\nthe strings xy0z = xz, xy2z = xyyz, xy3z = xyyyz, . . . , is contained in A.\\nHowever, by the pumping lemma, all these strings must be in A. Hence, we\\nhave a contradiction and we conclude that A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n71\\nSecond example\\nConsider the language\\nA = {w ∈{0, 1}∗: the number of 0s in w equals the number of 1s in w}.\\nAgain, we prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. Then s ∈A\\nand |s| = 2p ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz contains more 0s than 1s,\\nwhich implies that this string is not contained in A. But, by the pumping\\nlemma, this string is contained in A. This is a contradiction and, therefore,\\nA is not a regular language.\\nThird example\\nConsider the language\\nA = {ww : w ∈{0, 1}∗}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p10p1. Then s ∈A\\nand |s| = 2p + 2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz is not contained in A. But,\\nby the pumping lemma, this string is contained in A. This is a contradiction\\nand, therefore, A is not a regular language.\\nYou should convince yourself that by choosing s = 02p (which is a string\\nin A whose length is at least p), we do not obtain a contradiction. The reason\\nis that the string y may have an even length. Thus, 02p is the “wrong” string\\nfor showing that A is not regular. By choosing s = 0p10p1, we do obtain\\na contradiction; thus, this is the “correct” string for showing that A is not\\nregular.\\n72\\nChapter 2.\\nFinite Automata and Regular Languages\\nFourth example\\nConsider the language\\nA = {0m1n : m > n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p+11p. Then s ∈A\\nand |s| = 2p + 1 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Consider the string xy0z = xz. The number of 1s in this string\\nis equal to p, whereas the number of 0s is at most equal to p. Therefore, the\\nstring xy0z is not contained in A. But, by the pumping lemma, this string\\nis contained in A. This is a contradiction and, therefore, A is not a regular\\nlanguage.\\nFifth example\\nConsider the language\\nA = {1n2 : n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 1p2. Then s ∈A\\nand |s| = p2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that\\n|s| = |xyz| = p2\\nand\\n|xy2z| = |xyyz| = |xyz| + |y| = p2 + |y|.\\nSince |xy| ≤p, we have |y| ≤p. Since y ̸= ϵ, we have |y| ≥1. It follows that\\np2 < |xy2z| ≤p2 + p < (p + 1)2.\\nHence, the length of the string xy2z is strictly between two consecutive\\nsquares.\\nIt follows that this length is not a square and, therefore, xy2z\\nis not contained in A. But, by the pumping lemma, this string is contained\\nin A. This is a contradiction and, therefore, A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n73\\nSixth example\\nConsider the language\\nA = {1n : n is a prime number}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Let n ≥p be a prime number, and consider\\nthe string s = 1n. Then s ∈A and |s| = n ≥p. By the pumping lemma, s\\ncan be written as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nLet k be the integer such that y = 1k. Since y ̸= ϵ, we have k ≥1. For\\neach i ≥0, n + (i −1)k is a prime number, because xyiz = 1n+(i−1)k ∈A.\\nFor i = n + 1, however, we have\\nn + (i −1)k = n + nk = n(1 + k),\\nwhich is not a prime number, because n ≥2 and 1 + k ≥2.\\nThis is a\\ncontradiction and, therefore, A is not a regular language.\\nSeventh example\\nConsider the language\\nA = {w ∈{0, 1}∗:\\nthe number of occurrences of 01 in w is equal to\\nthe number of occurrences of 10 in w }.\\nSince this language has the same ﬂavor as the one in the second example,\\nwe may suspect that A is not a regular language. This is, however, not true:\\nAs we will show, A is a regular language.\\nThe key property is the following one: Let w be an arbitrary string in\\n{0, 1}∗. Then\\nthe absolute value of the number of occurrences of 01 in w minus\\nthe number of occurrences of 10 in w is at most one.\\nThis property holds, because between any two consecutive occurrences of\\n01, there must be exactly one occurrence of 10. Similarly, between any two\\nconsecutive occurrences of 10, there must be exactly one occurrence of 01.\\nWe will construct a DFA that accepts A. This DFA uses the following\\nﬁve states:\\n74\\nChapter 2.\\nFinite Automata and Regular Languages\\n• q: start state; no symbol has been read.\\n• q01: the last symbol read was 1; in the part of the string read so far, the\\nnumber of occurrences of 01 is one more than the number of occurrences\\nof 10.\\n• q10: the last symbol read was 0; in the part of the string read so far, the\\nnumber of occurrences of 10 is one more than the number of occurrences\\nof 01.\\n• q0\\nequal: the last symbol read was 0; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\n• q1\\nequal: the last symbol read was 1; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\nThe set of accept states is equal to {q, q0\\nequal, q1\\nequal}. The state diagram of\\nthe DFA is given below.\\nq0\\nequal\\nq1\\nequal\\nq01\\nq10\\nq\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\nIn fact, the key property mentioned above implies that the language A\\nconsists of the empty string ϵ and all non-empty binary strings that start\\n2.9.\\nThe pumping lemma and nonregular languages\\n75\\nand end with the same symbol. As a result, A is the language described by\\nthe regular expression\\nϵ ∪0 ∪1 ∪0(0 ∪1)∗0 ∪1(0 ∪1)∗1.\\nThis gives an alternative proof for the fact that A is a regular language.\\nEighth example\\nConsider the language\\nL = {w ∈{0, 1}∗: w is the binary representation of a prime number}.\\nWe assume that for any positive integer, the leftmost bit in its binary repre-\\nsentation is 1. In other words, we assume that there are no 0’s added to the\\nleft of such a binary representation. Thus,\\nL = {10, 11, 101, 111, 1011, 1101, 10001, . . .}.\\nWe will prove that L is not a regular language.\\nAssume that L is a regular language. Let p ≥1 be the pumping length.\\nLet N > 2p be a prime number and let s ∈{0, 1}∗be the binary representa-\\ntion of N. Observe that |s| ≥p + 1. Also, the leftmost and rightmost bits of\\ns are 1.\\nSince s ∈L and |s| ≥p + 1 ≥p, the Pumping Lemma implies that we\\ncan write s = xyz, such that\\n1. |y| ≥1,\\n2. |xy| ≤p (and, thus, |z| ≥1), and\\n3. for all i ≥0, xyiz ∈L, i.e., xyiz is the binary representation of a prime\\nnumber.\\nDeﬁne A, B, and C to be the integers whose binary representations are\\nx, y, and z, respectively. Note that both y and z may have leading 0’s. In\\nfact, y may be a string consisting of 0’s only, in which case B = 0. However,\\nsince the rightmost bit of z is 1, we have C ≥1. Observe that\\nN = C + B · 2|z| + A · 2|z|+|y|.\\n(2.4)\\n76\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet i = N, consider the bitstring xyiz = xyNz, and let M be the prime\\nnumber whose binary representation is given by this bitstring. Then,\\nM\\n=\\nC +\\nN−1\\nX\\nk=0\\nB · 2|z|+k|y| + A · 2|z|+N|y|\\n=\\nC + B · 2|z|\\nN−1\\nX\\nk=0\\n2k|y| + A · 2|z|+N|y|.\\nLet\\nT =\\nN−1\\nX\\nk=0\\n2k|y|.\\nThen\\n\\x002|y| −1\\n\\x01\\nT = 2N|y| −1.\\n(2.5)\\nBy Fermat’s Little Theorem, we have\\n2N ≡2\\n(mod N),\\nimplying that\\n2N|y| −1 =\\n\\x002N\\x01|y| −1 ≡2|y| −1\\n(mod N).\\nThus, (2.5) implies that\\n\\x002|y| −1\\n\\x01\\nT ≡2|y| −1\\n(mod N).\\n(2.6)\\nObserve that 2|y| ≤2p < N, because |y| ≤|xy| ≤p. Also, 2|y| ≥2, because\\ny ̸= ϵ. It follows that\\n1 ≤2|y| −1 < N,\\nimplying that\\n2|y| −1 ̸≡0\\n(mod N).\\nThis, together with (2.6), implies that\\nT ≡1\\n(mod N).\\nSince\\nM = C + B · 2|z| · T + A · 2|z|+N|y|,\\n2.10.\\nHigman’s Theorem\\n77\\nit follows that\\nM ≡C + B · 2|z| + A · 2|z|+|y|\\n(mod N).\\nThis, together with (2.4), implies that\\nM ≡0\\n(mod N),\\ni.e., N divides M. Since M > N, we conclude that M is not a prime number,\\nwhich is a contradiction. Thus, the language L is not regular.\\n2.10\\nHigman’s Theorem\\nLet Σ be a ﬁnite alphabet. For any two strings x and y in Σ∗, we say that x\\nis a subsequence of y, if x can be obtained by deleting zero or more symbols\\nfrom y. For example, 10110 is a subsequence of 0010010101010001. For any\\nlanguage L ⊆Σ∗, we deﬁne\\nSUBSEQ(L) := {x : there exists a y ∈L such that x is a subsequence of y}.\\nThat is, SUBSEQ(L) is the language consisting of the subsequences of all\\nstrings in L. In 1952, Higman proved the following result:\\nTheorem 2.10.1 (Higman) For any ﬁnite alphabet Σ and for any lan-\\nguage L ⊆Σ∗, the language SUBSEQ(L) is regular.\\n2.10.1\\nDickson’s Theorem\\nOur proof of Higman’s Theorem will use a theorem that was proved in 1913\\nby Dickson.\\nRecall that N denotes the set of positive integers. Let n ∈N. For any\\ntwo points p = (p1, p2, . . . , pn) and q = (q1, q2, . . . , qn) in Nn, we say that p is\\ndominated by q, if pi ≤qi for all i with 1 ≤i ≤n.\\nTheorem 2.10.2 (Dickson) Let S ⊆Nn, and let M be the set consisting of\\nall elements of S that are minimal in the relation “is dominated by”. Thus,\\nM = {q ∈S : there is no p in S \\\\ {q} such that p is dominated by q}.\\nThen, the set M is ﬁnite.\\n78\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe will prove this theorem by induction on the dimension n. If n = 1,\\nthen either M = ∅(if S = ∅) or M consists of exactly one element (if S ̸= ∅).\\nTherefore, the theorem holds if n = 1. Let n ≥2 and assume the theorem\\nholds for all subsets of Nn−1. Let S be a subset of Nn and consider the set\\nM of minimal elements in S. If S = ∅, then M = ∅and, thus, M is ﬁnite.\\nAssume that S ̸= ∅. We ﬁx an arbitrary element q in M. If p ∈M \\\\ {q},\\nthen q is not dominated by p. Therefore, there exists an index i such that\\npi ≤qi −1. It follows that\\nM \\\\ {q} ⊆\\nn[\\ni=1\\n\\x00Ni−1 × [1, qi −1] × Nn−i\\x01\\n.\\nFor all i and k with 1 ≤i ≤n and 1 ≤k ≤qi −1, we deﬁne\\nSik = {p ∈S : pi = k}\\nand\\nMik = {p ∈M : pi = k}.\\nThen,\\nM \\\\ {q} =\\nn[\\ni=1\\nqi−1\\n[\\nk=1\\nMik.\\n(2.7)\\nLemma 2.10.3 Mik is a subset of the set of all elements of Sik that are\\nminimal in the relation “is dominated by”.\\nProof. Let p be an element of Mik, and assume that p is not minimal in\\nSik. Then there is an element r in Sik, such that r ̸= p and r is dominated\\nby p. Since p and r are both elements of S, it follows that p ̸∈M. This is a\\ncontradiction.\\nSince the set Sik is basically a subset of Nn−1, it follows from the induction\\nhypothesis that Sik contains ﬁnitely many minimal elements. This, combined\\nwith Lemma 2.10.3, implies that Mik is a ﬁnite set. Thus, by (2.7), M \\\\ {q}\\nis the union of ﬁnitely many ﬁnite sets. Therefore, the set M is ﬁnite.\\n2.10.2\\nProof of Higman’s Theorem\\nWe give the proof of Theorem 2.10.1 for the case when Σ = {0, 1}. If L = ∅\\nor SUBSEQ(L) = {0, 1}∗, then SUBSEQ(L) is obviously a regular language.\\n2.10.\\nHigman’s Theorem\\n79\\nHence, we may assume that L is non-empty and SUBSEQ(L) is a proper\\nsubset of {0, 1}∗.\\nWe ﬁx a string z of length at least two in the complement SUBSEQ(L) of\\nthe language SUBSEQ(L). Observe that this is possible, because SUBSEQ(L)\\nis an inﬁnite language. We insert 0s and 1s into z, such that, in the result-\\ning string z′, 0s and 1s alternate. For example, if z = 0011101011, then\\nz′ = 01010101010101. Let n = |z′| −1, where |z′| denotes the length of z′.\\nThen, n ≥|z| −1 ≥1.\\nA (0, 1)-alternation in a binary string x is any occurrence of 01 or 10 in x.\\nFor example, the string 1101001 contains four (0, 1)-alternations. We deﬁne\\nA = {x ∈{0, 1}∗: x has at most n many (0, 1)-alternations}.\\nLemma 2.10.4 SUBSEQ(L) ⊆A.\\nProof. Let x ∈SUBSEQ(L) and assume that x ̸∈A. Then, x has at least\\nn + 1 = |z′| many (0, 1)-alternations and, therefore, z′ is a subsequence of x.\\nIn particular, z is a subsequence of x. Since x ∈SUBSEQ(L), it follows that\\nz ∈SUBSEQ(L), which is a contradiction.\\nLemma 2.10.5 SUBSEQ(L) =\\n\\x10\\nA ∩SUBSEQ(L)\\n\\x11\\n∪A.\\nProof. Follows from Lemma 2.10.4.\\nLemma 2.10.6 The language A is regular.\\nProof.\\nThe complement A of A is the language consisting of all binary\\nstrings with at least n + 1 many (0, 1)-alternations. If, for example, n = 3,\\nthen A is described by the regular expression\\n(00∗11∗00∗11∗0(0 ∪1)∗) ∪(11∗00∗11∗00∗1(0 ∪1)∗) .\\nThis should convince you that the claim is true for any value of n.\\nFor any b ∈{0, 1} and for any k ≥0, we deﬁne Abk to be the language\\nconsisting of all binary strings that start with a b and have exactly k many\\n(0, 1)-alternations. Then, we have\\nA = {ϵ} ∪\\n 1[\\nb=0\\nn[\\nk=0\\nAbk\\n!\\n.\\n80\\nChapter 2.\\nFinite Automata and Regular Languages\\nThus, if we deﬁne\\nFbk = Abk ∩SUBSEQ(L),\\nand use the fact that ϵ ∈SUBSEQ(L) (which is true because L ̸= ∅), then\\nA ∩SUBSEQ(L) =\\n1[\\nb=0\\nn[\\nk=0\\nFbk.\\n(2.8)\\nFor any b ∈{0, 1} and for any k ≥0, consider the relation “is a subse-\\nquence of” on the language Fbk. We deﬁne Mbk to be the language consisting\\nof all strings in Fbk that are minimal in this relation. Thus,\\nMbk = {x ∈Fbk : there is no x′ in Fbk \\\\ {x} such that x′ is a subsequence of x}.\\nIt is clear that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Fbk : x is a subsequence of y}.\\nIf x ∈Mbk, y ∈Abk, and x is a subsequence of y, then y must be in\\nSUBSEQ(L) and, therefore, y must be in Fbk. To prove this, assume that\\ny ∈SUBSEQ(L).\\nThen, x ∈SUBSEQ(L), contradicting the fact that\\nx ∈Mbk ⊆Fbk ⊆SUBSEQ(L). It follows that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Abk : x is a subsequence of y}.\\n(2.9)\\nLemma 2.10.7 Let b ∈{0, 1} and 0 ≤k ≤n, and let x be an element of\\nMbk. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis regular.\\nProof. We will prove the claim by means of an example. Assume that b = 1,\\nk = 3, and x = 11110001000. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis described by the regular expression\\n11111∗0000∗11∗0000∗.\\nThis should convince you that the claim is true in general.\\nExercises\\n81\\nLemma 2.10.8 For each b ∈{0, 1} and each 0 ≤k ≤n, the set Mbk is\\nﬁnite.\\nProof. Again, we will prove the claim by means of an example. Assume\\nthat b = 1 and k = 3. Any string in Fbk can be written as 1a0b1c0d, for some\\nintegers a, b, c, d ≥1. Consider the function ϕ : Fbk →N4 that is deﬁned by\\nϕ(1a0b1c0d) = (a, b, c, d). Then, ϕ is an injective function, and the following\\nis true, for any two strings x and x′ in Fbk:\\nx is a subsequence of x′ if and only if ϕ(x) is dominated by ϕ(x′).\\nIt follows that the elements of Mbk are in one-to-one correspondence with\\nthose elements of ϕ(Fbk) that are minimal in the relation “is dominated by”.\\nThe lemma thus follows from Dickson’s Theorem.\\nNow we can complete the proof of Higman’s Theorem:\\n• It follows from (2.9) and Lemmas 2.10.7 and 2.10.8, that Fbk is the\\nunion of ﬁnitely many regular languages. Therefore, by Theorem 2.3.1,\\nFbk is a regular language.\\n• It follows from (2.8) that A∩SUBSEQ(L) is the union of ﬁnitely many\\nregular languages. Therefore, again by Theorem 2.3.1, A∩SUBSEQ(L)\\nis a regular language.\\n• Since A ∩SUBSEQ(L) is regular and, by Lemma 2.10.6, A is regular,\\nit follows from Lemma 2.10.5 that SUBSEQ(L) is the union of two reg-\\nular languages. Therefore, by Theorem 2.3.1, SUBSEQ(L) is a regular\\nlanguage.\\n• Since SUBSEQ(L) is regular, it follows from Theorem 2.6.4 that the\\nlanguage SUBSEQ(L) is regular as well.\\nExercises\\n2.1 For each of the following languages, construct a DFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : the length of w is divisible by three}\\n82\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. {w : 110 is not a substring of w}\\n3. {w : w contains at least ﬁve 1s}\\n4. {w : w contains the substring 1011}\\n5. {w : w contains at least two 1s and at most two 0s}\\n6. {w : w contains an odd number of 1s or exactly two 0s}\\n7. {w : w begins with 1 and ends with 0}\\n8. {w : every odd position in w is 1}\\n9. {w : w has length at least 3 and its third symbol is 0}\\n10. {ϵ, 0}\\n2.2 For each of the following languages, construct an NFA, with the speciﬁed\\nnumber of states, that accepts the language. In all cases, the alphabet is\\n{0, 1}.\\n1. The language {w : w ends with 10} with three states.\\n2. The language {w : w contains the substring 1011} with ﬁve states.\\n3. The language {w : w contains an odd number of 1s or exactly two 0s}\\nwith six states.\\n2.3 For each of the following languages, construct an NFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : w contains the substring 11001}\\n2. {w : w has length at least 2 and does not end with 10}\\n3. {w : w begins with 1 or ends with 0}\\n2.4 Convert the following NFA to an equivalent DFA.\\nExercises\\n83\\n1\\n2\\na\\nb\\na, b\\n2.5 Convert the following NFA to an equivalent DFA.\\n1\\n3\\n2\\na\\na\\nb\\na\\nε,b\\n2.6 Convert the following NFA to an equivalent DFA.\\n0\\n1\\n2\\n3\\na, ǫ\\nb\\na\\nǫ\\nb\\n2.7 In the proof of Theorem 2.6.3, we introduced a new start state q0, which\\nis also an accept state. Explain why the following is not a valid proof of\\nTheorem 2.6.3:\\nLet N = (Q1, Σ, δ1, q1, F1) be an NFA, such that A = L(N). Deﬁne the\\nNFA M = (Q1, Σ, δ, q1, F), where\\n84\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. F = {q1} ∪F1.\\n2. δ : Q1 × Σϵ →P(Q1) is deﬁned as follows: For any r ∈Q1 and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ.\\nThen L(M) = A∗.\\n2.8 Prove Theorem 2.6.4.\\n2.9 Let A be a language over the alphabet Σ = {0, 1} and let A be the\\ncomplement of A. Thus, A is the language consisting of all binary strings\\nthat are not in A.\\nAssume that A is a regular language. Let M = (Q, Σ, δ, q, F) be a non-\\ndeterministic ﬁnite automaton (NFA) that accepts A.\\nConsider the NFA N = (Q, Σ, δ, q, F), where F = Q\\\\F is the complement\\nof F. Thus, N is obtained from M by turning all accept states into nonaccept\\nstates, and turning all nonaccept states into accept states.\\n1. Is it true that the language accepted by N is equal to A?\\n2. Assume now that M is a deterministic ﬁnite automaton (DFA) that\\naccepts A. Deﬁne N as above; thus, turn all accept states into nonac-\\ncept states, and turn all nonaccept states into accept states. Is it true\\nthat the language accepted by N is equal to A?\\n2.10 Recall the alternative deﬁnition for the star of a language A that we\\ngave just before Theorem 2.3.1.\\nIn Theorems 2.3.1 and 2.6.2, we have shown that the class of regular\\nlanguages is closed under the union and concatenation operations.\\nSince\\nA∗= S∞\\nk=0 Ak, why doesn’t this imply that the class of regular languages is\\nclosed under the star operation?\\n2.11 Let A and B be two regular languages over the same alphabet Σ. Prove\\nthat the diﬀerence of A and B, i.e., the language\\nA \\\\ B = {w : w ∈A and w ̸∈B}\\nis a regular language.\\nExercises\\n85\\n2.12 For each of the following regular expressions, give two strings that are\\nmembers and two strings that are not members of the language described by\\nthe expression. The alphabet is Σ = {a, b}.\\n1. a(ba)∗b.\\n2. (a ∪b)∗a(a ∪b)∗b(a ∪b)∗a(a ∪b)∗.\\n3. (a ∪ba ∪bb)(a ∪b)∗.\\n2.13 Give regular expressions describing the following languages.\\nIn all\\ncases, the alphabet is {0, 1}.\\n1. {w : w contains at least three 1s}.\\n2. {w : w contains at least two 1s and at most one 0},\\n3. {w : w contains an even number of 0s and exactly two 1s}.\\n4. {w : w contains exactly two 0s and at least two 1s}.\\n5. {w : w contains an even number of 0s and each 0 is followed by at least one 1}.\\n6. {w : every odd position in w is 1}.\\n2.14 Convert each of the following regular expressions to an NFA.\\n1. (0 ∪1)∗000(0 ∪1)∗\\n2. (((10)∗(00)) ∪10)∗\\n3. ((0 ∪1)(11)∗∪0)∗\\n2.15 Convert the following DFA to a regular expression.\\n86\\nChapter 2.\\nFinite Automata and Regular Languages\\n1\\n2\\n3\\na\\na\\nb\\nb\\na\\nb\\n2.16 Convert the following DFA to a regular expression.\\n1\\n2\\n3\\na, b\\na\\na\\nb\\nb\\n2.17 Convert the following DFA to a regular expression.\\na, b\\n2.18\\n1. Let A be a non-empty regular language. Prove that there exists\\nan NFA that accepts A and that has exactly one accept state.\\nExercises\\n87\\n2. For any string w = w1w2 . . . wn, we denote by wR the string obtained\\nby reading w backwards, i.e., wR = wnwn−1 . . . w2w1. For any language\\nA, we deﬁne AR to be the language obtained by reading all strings in\\nA backwards, i.e.,\\nAR = {wR : w ∈A}.\\nLet A be a non-empty regular language. Prove that the language AR\\nis also regular.\\n2.19 If n ≥1 is an integer and w = a1a2 . . . an is a string, then for any i\\nwith 0 ≤i < n, the string a1a2 . . . ai is called a proper preﬁx of w. (If i = 0,\\nthen a1a2 . . . ai = ϵ.)\\nFor any language L, we deﬁne MIN(L) to be the language\\nMIN(L) = {w ∈L : no proper preﬁx of w belongs to L}.\\nProve the following claim: If L is a regular language, then MIN(L) is regular\\nas well.\\n2.20 Use the pumping lemma to prove that the following languages are not\\nregular.\\n1. {anbmcn+m : n ≥0, m ≥0}.\\n2. {anbnc2n : n ≥0}.\\n3. {anbman : n ≥0, m ≥0}.\\n4. {a2n : n ≥0}. (Remark: a2n is the string consisting of 2n many a’s.)\\n5. {anbmck : n ≥0, m ≥0, k ≥0, n2 + m2 = k2}.\\n6. {uvu : u ∈{a, b}∗, u ̸= ϵ, v ∈{a, b}∗}.\\n2.21 Prove that the language\\n{ambn : m ≥0, n ≥0, m ̸= n}\\nis not regular. (Using the pumping lemma for this one is a bit tricky. You\\ncan avoid using the pumping lemma by combining results about the closure\\nunder regular operations.)\\n88\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.22\\n1. Give an example of a regular language A and a non-regular lan-\\nguage B for which A ⊆B.\\n2. Give an example of a non-regular language A and a regular language\\nB for which A ⊆B.\\n2.23 Let A be a language consisting of ﬁnitely many strings.\\n1. Prove that A is a regular language.\\n2. Let n be the maximum length of any string in A. Prove that every\\ndeterministic ﬁnite automaton (DFA) that accepts A has at least n+1\\nstates. (Hint: How is the pumping length chosen in the proof of the\\npumping lemma?)\\n2.24 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L ̸= ∅if and only\\nif L contains a string of length less than p.\\n2.25 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L is an inﬁnite\\nlanguage if and only if L contains a string w with p ≤|w| ≤2p −1.\\n2.26 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. We deﬁne a binary relation RL on Σ∗× Σ∗, in the following way:\\nFor any two strings u and u′ in Σ∗,\\nuRLu′ if and only if (∀v ∈Σ∗: uv ∈L ⇔u′v ∈L) .\\nProve that RL is an equivalence relation.\\n2.27 Let Σ = {0, 1}, let\\nL = {w ∈Σ∗: |w| is odd},\\nand consider the relation RL deﬁned in Exercise 2.26.\\n1. Prove that for any two strings u and u′ in Σ∗,\\nuRLu′ ⇔|u| −|u′| is even.\\nExercises\\n89\\n2. Determine all equivalence classes of the relation RL.\\n2.28 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. Recall the equivalence relation RL that was deﬁned in Exercise 2.26.\\n1. Assume that L is a regular language, and let M = (Q, Σ, δ, q0, F) be\\na DFA that accepts L. Let u and u′ be strings in Σ∗. Let q be the\\nstate reached, when following the path in the state diagram of M, that\\nstarts in q0 and that is obtained by reading the string u. Similarly, let\\nq′ be the state reached, when following the path in the state diagram\\nof M, that starts in q0 and that is obtained by reading the string u′.\\nProve the following: If q = q′, then uRLu′.\\n2. Prove the following claim: If L is a regular language, then the equiva-\\nlence relation RL has a ﬁnite number of equivalence classes.\\n2.29 Let L be the language deﬁned by\\nL = {uuR : u ∈{0, 1}∗}.\\nIn words, a string is in L if and only if its length is even, and the second half\\nis the reverse of the ﬁrst half. Consider the equivalence relation RL that was\\ndeﬁned in Exercise 2.26.\\n1. Let m and n be two distinct positive integers and consider the two\\nstrings u = 0m1 and u′ = 0n1. Prove that ¬(uRLu′).\\n2. Prove that L is not a regular language, without using the pumping\\nlemma.\\n3. Use the pumping lemma to prove that L is not a regular language.\\n2.30 In this exercise, we will show that the converse of the pumping lemma\\ndoes, in general, not hold. Consider the language\\nA = {ambncn : m ≥1, n ≥0} ∪{bnck : n ≥0, k ≥0}.\\n1. Show that A satisﬁes the conclusion of the pumping lemma for p = 1.\\nThus, show that every string s in A whose length is at least p can be\\nwritten as s = xyz, such that y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all\\ni ≥0.\\n90\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. Consider the equivalence relation RA that was deﬁned in Exercise 2.26.\\nLet n and n′ be two distinct non-negative integers and consider the two\\nstrings u = abn and u′ = abn′. Prove that ¬(uRAu′).\\n3. Prove that A is not a regular language.\\nChapter 3\\nContext-Free Languages\\nIn this chapter, we introduce the class of context-free languages.\\nAs we\\nwill see, this class contains all regular languages, as well as some nonregular\\nlanguages such as {0n1n : n ≥0}.\\nThe class of context-free languages consists of languages that have some\\nsort of recursive structure. We will see two equivalent methods to obtain this\\nclass. We start with context-free grammars, which are used for deﬁning the\\nsyntax of programming languages and their compilation. Then we introduce\\nthe notion of (nondeterministic) pushdown automata, and show that these\\nautomata have the same power as context-free grammars.\\n3.1\\nContext-free grammars\\nWe start with an example. Consider the following ﬁve (substitution) rules:\\nS\\n→\\nAB\\nA\\n→\\na\\nA\\n→\\naA\\nB\\n→\\nb\\nB\\n→\\nbB\\nHere, S, A, and B are variables, S is the start variable, and a and b are\\nterminals. We use these rules to derive strings consisting of terminals (i.e.,\\nelements of {a, b}∗), in the following manner:\\n1. Initialize the current string to be the string consisting of the start\\nvariable S.\\n92\\nChapter 3.\\nContext-Free Languages\\n2. Take any variable in the current string and take any rule that has this\\nvariable on the left-hand side. Then, in the current string, replace this\\nvariable by the right-hand side of the rule.\\n3. Repeat 2. until the current string only contains terminals.\\nFor example, the string aaaabb can be derived in the following way:\\nS\\n⇒\\nAB\\n⇒\\naAB\\n⇒\\naAbB\\n⇒\\naaAbB\\n⇒\\naaaAbB\\n⇒\\naaaabB\\n⇒\\naaaabb\\nThis derivation can also be represented using a parse tree, as in the ﬁgure\\nbelow:\\nS\\nA\\nA\\nA\\nA\\na\\na\\na\\na\\nb\\nb\\nB\\nB\\nThe ﬁve rules in this example constitute a context-free grammar. The\\nlanguage of this grammar is the set of all strings that\\n3.1.\\nContext-free grammars\\n93\\n• can be derived from the start variable and\\n• only contain terminals.\\nFor this example, the language is\\n{ambn : m ≥1, n ≥1},\\nbecause every string of the form ambn, for some m ≥1 and n ≥1, can be\\nderived from the start variable, whereas no other string over the alphabet\\n{a, b} can be derived from the start variable.\\nDeﬁnition 3.1.1 A context-free grammar is a 4-tuple G = (V, Σ, R, S),\\nwhere\\n1. V is a ﬁnite set, whose elements are called variables,\\n2. Σ is a ﬁnite set, whose elements are called terminals,\\n3. V ∩Σ = ∅,\\n4. S is an element of V ; it is called the start variable,\\n5. R is a ﬁnite set, whose elements are called rules. Each rule has the\\nform A →w, where A ∈V and w ∈(V ∪Σ)∗.\\nIn our example, we have V = {S, A, B}, Σ = {a, b}, and\\nR = {S →AB, A →a, A →aA, B →b, B →bB}.\\nDeﬁnition 3.1.2 Let G = (V, Σ, R, S) be a context-free grammar. Let A be\\nan element in V and let u, v, and w be strings in (V ∪Σ)∗such that A →w\\nis a rule in R. We say that the string uwv can be derived in one step from\\nthe string uAv, and write this as\\nuAv ⇒uwv.\\nIn other words, by applying the rule A →w to the string uAv, we obtain\\nthe string uwv. In our example, we see that aaAbb ⇒aaaAbb.\\nDeﬁnition 3.1.3 Let G = (V, Σ, R, S) be a context-free grammar. Let u\\nand v be strings in (V ∪Σ)∗. We say that v can be derived from u, and write\\nthis as u\\n∗⇒v, if one of the following two conditions holds:\\n94\\nChapter 3.\\nContext-Free Languages\\n1. u = v or\\n2. there exist an integer k ≥2 and a sequence u1, u2, . . . , uk of strings in\\n(V ∪Σ)∗, such that\\n(a) u = u1,\\n(b) v = uk, and\\n(c) u1 ⇒u2 ⇒. . . ⇒uk.\\nIn other words, by starting with the string u and applying rules zero or\\nmore times, we obtain the string v. In our example, we see that aaAbB\\n∗⇒\\naaaabbbB.\\nDeﬁnition 3.1.4 Let G = (V, Σ, R, S) be a context-free grammar.\\nThe\\nlanguage of G is deﬁned to be the set of all strings in Σ∗that can be derived\\nfrom the start variable S:\\nL(G) = {w ∈Σ∗: S\\n∗⇒w}.\\nDeﬁnition 3.1.5 A language L is called context-free, if there exists a context-\\nfree grammar G such that L(G) = L.\\n3.2\\nExamples of context-free grammars\\n3.2.1\\nProperly nested parentheses\\nConsider the context-free grammar G = (V, Σ, R, S), where V = {S}, Σ =\\n{a, b}, and\\nR = {S →ϵ, S →aSb, S →SS}.\\nWe write the three rules in R as\\nS →ϵ|aSb|SS,\\nwhere you can think of “|” as being a short-hand for “or”.\\n3.2.\\nExamples of context-free grammars\\n95\\nBy applying the rules in R, starting with the start variable S, we obtain,\\nfor example,\\nS\\n⇒\\nSS\\n⇒\\naSbS\\n⇒\\naSbSS\\n⇒\\naSSbSS\\n⇒\\naaSbSbSS\\n⇒\\naabSbSS\\n⇒\\naabbSS\\n⇒\\naabbaSbS\\n⇒\\naabbabS\\n⇒\\naabbabaSb\\n⇒\\naabbabab\\nWhat is the language L(G) of this context-free grammar G? If we think\\nof a as being a left-parenthesis “(”, and of b as being a right-parenthesis “)”,\\nthen L(G) is the language consisting of all strings of properly nested paren-\\ntheses. Here is the explanation: Any string of properly nested parentheses is\\neither\\n• empty (which we derive from S by the rule S →ϵ),\\n• consists of a left-parenthesis, followed by an arbitrary string of properly\\nnested parentheses, followed by a right-parenthesis (these are derived\\nfrom S by ﬁrst applying the rule S →aSb), or\\n• consists of an arbitrary string of properly nested parentheses, followed\\nby an arbitrary string of properly nested parentheses (these are derived\\nfrom S by ﬁrst applying the rule S →SS).\\n3.2.2\\nA context-free grammar for a nonregular lan-\\nguage\\nConsider the language L1 = {0n1n : n ≥0}. We have seen in Section 2.9.1\\nthat L1 is not a regular language. We claim that L1 is a context-free language.\\n96\\nChapter 3.\\nContext-Free Languages\\nIn order to prove this claim, we have to construct a context-free grammar\\nG1 such that L(G1) = L1.\\nObserve that any string in L1 is either\\n• empty or\\n• consists of a 0, followed by an arbitrary string in L1, followed by a 1.\\nThis leads to the context-free grammar G1 = (V1, Σ, R1, S1), where V1 =\\n{S1}, Σ = {0, 1}, and R1 consists of the rules\\nS1 →ϵ|0S11.\\nHence, R1 = {S1 →ϵ, S1 →0S11}.\\nTo derive the string 0n1n from the start variable S1, we do the following:\\n• Starting with S1, apply the rule S1 →0S11 exactly n times. This gives\\nthe string 0nS11n.\\n• Apply the rule S1 →ϵ. This gives the string 0n1n.\\nIt is not diﬃcult to see that these are the only strings that can be derived\\nfrom the start variable S1. Thus, L(G1) = L1.\\nIn a symmetric way, we see that the context-free grammar G2 = (V2, Σ, R2, S2),\\nwhere V2 = {S2}, Σ = {0, 1}, and R2 consists of the rules\\nS2 →ϵ|1S20,\\nhas the property that L(G2) = L2, where L2 = {1n0n : n ≥0}. Thus, L2 is\\na context-free language.\\nDeﬁne L = L1 ∪L2, i.e.,\\nL = {0n1n : n ≥0} ∪{1n0n : n ≥0}.\\nThe context-free grammar G = (V, Σ, R, S), where V = {S, S1, S2}, Σ =\\n{0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2\\nS1\\n→\\nϵ|0S11\\nS2\\n→\\nϵ|1S20,\\nhas the property that L(G) = L. Hence, L is a context-free language.\\n3.2.\\nExamples of context-free grammars\\n97\\n3.2.3\\nA context-free grammar for the complement of\\na nonregular language\\nLet L be the (nonregular) language L = {0n1n : n ≥0}. We want to prove\\nthat the complement L of L is a context-free language. Hence, we want to\\nconstruct a context-free grammar G whose language is equal to L. Observe\\nthat a binary string w is in L if and only if\\n1. w = 0m1n, for some integers m and n with 0 ≤m < n, or\\n2. w = 0m1n, for some integers m and n with 0 ≤n < m, or\\n3. w contains 10 as a substring.\\nThus, we can write L as the union of the languages of all strings of type 1.,\\ntype 2., and type 3.\\nAny string of type 1. is either\\n• the string 1,\\n• consists of a string of type 1., followed by one 1, or\\n• consists of one 0, followed by an arbitrary string of type 1., followed by\\none 1.\\nThus, using the rules\\nS1 →1|S11|0S11,\\nwe can derive, from S1, all strings of type 1.\\nSimilarly, using the rules\\nS2 →0|0S2|0S21,\\nwe can derive, from S2, all strings of type 2.\\nAny string of type 3.\\n• consists of an arbitrary binary string, followed by the string 10, followed\\nby an arbitrary binary string.\\nUsing the rules\\nX →ϵ|0X|1X,\\n98\\nChapter 3.\\nContext-Free Languages\\nwe can derive, from X, all binary strings. Thus, by combining these with\\nthe rule\\nS3 →X10X,\\nwe can derive, from S3, all strings of type 3.\\nWe arrive at the context-free grammar G = (V, Σ, R, S), where V =\\n{S, S1, S2, S3, X}, Σ = {0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2|S3\\nS1\\n→\\n1|S11|0S11\\nS2\\n→\\n0|0S2|0S21\\nS3\\n→\\nX10X\\nX\\n→\\nϵ|0X|1X\\nTo summarize, we have\\nS1\\n∗⇒0m1n, for all integers m and n with 0 ≤m < n,\\nS2\\n∗⇒0m1n, for all integers m and n with 0 ≤n < m,\\nX\\n∗⇒u, for each string u in {0, 1}∗,\\nand\\nS3\\n∗⇒w, for every binary string w that contains 10 as a substring.\\nFrom these observations, it follows that that L(G) = L.\\n3.2.4\\nA context-free grammar that veriﬁes addition\\nConsider the language\\nL = {anbmcn+m : n ≥0, m ≥0}.\\nUsing the pumping lemma for regular languages (Theorem 2.9.1), it can\\nbe shown that L is not a regular language. We will construct a context-\\nfree grammar G whose language is equal to L, thereby proving that L is a\\ncontext-free language.\\nFirst observe that ϵ ∈L. Therefore, we will take S →ϵ to be one of the\\nrules in the grammar.\\nLet us see how we can derive all strings in L from the start variable S:\\n3.3.\\nRegular languages are context-free\\n99\\n1. Every time we add an a, we also add a c. In this way, we obtain all\\nstrings of the form ancn, where n ≥0.\\n2. Given a string of the form ancn, we start adding bs. Every time we add\\na b, we also add a c. Observe that every b has to be added between\\nthe as and the cs. Therefore, we use a variable B as a “pointer” to\\nthe position in the current string where a b can be added: Instead of\\nderiving ancn from S, we derive the string anBcn. Then, from B, we\\nderive all strings of the form bmcm, where m ≥0.\\nWe obtain the context-free grammar G = (V, Σ, R, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R consists of the rules\\nS\\n→\\nϵ|A\\nA\\n→\\nϵ|aAc|B\\nB\\n→\\nϵ|bBc\\nThe facts that\\n• A\\n∗⇒anBcn, for every n ≥0,\\n• B\\n∗⇒bmcm, for every m ≥0,\\nimply that the following strings can be derived from the start variable S:\\n• S\\n∗⇒anBcn\\n∗⇒anbmcmcn = anbmcn+m, for all n ≥0 and m ≥0.\\nIn fact, no other strings in {a, b, c}∗can be derived from S. Therefore, we\\nhave L(G) = L. Since\\nS ⇒A ⇒B ⇒ϵ,\\nwe can simplify this grammar G, by eliminating the rules S →ϵ and A →ϵ.\\nThis gives the context-free grammar G′ = (V, Σ, R′, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R′ consists of the rules\\nS\\n→\\nA\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\nFinally, observe that we do not need S; instead, we can use A as start\\nvariable. This gives our ﬁnal context-free grammar G′′ = (V, Σ, R′′, A), where\\nV = {A, B}, Σ = {a, b, c}, and R′′ consists of the rules\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\n100\\nChapter 3.\\nContext-Free Languages\\n3.3\\nRegular languages are context-free\\nWe mentioned already that the class of context-free languages includes the\\nclass of regular languages. In this section, we will prove this claim.\\nTheorem 3.3.1 Let Σ be an alphabet and let L ⊆Σ∗be a regular language.\\nThen L is a context-free language.\\nProof.\\nSince L is a regular language, there exists a deterministic ﬁnite\\nautomaton M = (Q, Σ, δ, q, F) that accepts L.\\nTo prove that L is context-free, we have to deﬁne a context-free grammar\\nG = (V, Σ, R, S), such that L = L(M) = L(G). Thus, G must have the\\nfollowing property: For every string w ∈Σ∗,\\nw ∈L(M) if and only if w ∈L(G),\\nwhich can be reformulated as\\nM accepts w if and only if S\\n∗⇒w.\\nWe will deﬁne the context-free grammar G in such a way that the following\\ncorrespondence holds for any string w = w1w2 . . . wn:\\n• Assume that M is in state A just after it has read the substring\\nw1w2 . . . wi.\\n• Then in the context-free grammar G, we have S\\n∗⇒w1w2 . . . wiA.\\nIn the next step, M reads the symbol wi+1 and switches from state A to,\\nsay, state B; thus, δ(A, wi+1) = B. In order to guarantee that the above\\ncorrespondence still holds, we have to add the rule A →wi+1B to G.\\nConsider the moment when M has read the entire string w. Let A be the\\nstate M is in at that moment. By the above correspondence, we have\\nS\\n∗⇒w1w2 . . . wnA = wA.\\nRecall that G must have the property that\\nM accepts w if and only if S\\n∗⇒w,\\nwhich is equivalent to\\nA ∈F if and only if S\\n∗⇒w.\\n3.3.\\nRegular languages are context-free\\n101\\nWe guarantee this property by adding to G the rule A →ϵ for every accept\\nstate A of M.\\nWe are now ready to give the formal deﬁnition of the context-free gram-\\nmar G = (V, Σ, R, S):\\n• V = Q, i.e., the variables of G are the states of M.\\n• S = q, i.e., the start variable of G is the start state of M.\\n• R consists of the rules\\nA →aB, where A ∈Q, a ∈Σ, B ∈Q, and δ(A, a) = B,\\nand\\nA →ϵ, where A ∈F.\\nIn words,\\n• every transition δ(A, a) = B of M (i.e., when M is in the state A and\\nreads the symbol a, it switches to the state B) corresponds to a rule\\nA →aB in the grammar G,\\n• every accept state A of M corresponds to a rule A →ϵ in the grammar\\nG.\\nWe claim that L(G) = L. In order to prove this, we have to show that\\nL(G) ⊆L and L ⊆L(G).\\nWe prove that L ⊆L(G). Let w = w1w2 . . . wn be an arbitrary string\\nin L. When the ﬁnite automaton M reads the string w, it visits the states\\nr0, r1, . . . , rn, where\\n• r0 = q, and\\n• ri+1 = δ(ri, wi+1) for i = 0, 1, . . . , n −1.\\nSince w ∈L = L(M), we know that rn ∈F.\\nIt follows from the way we deﬁned the grammar G that\\n• for each i = 0, 1, . . . , n −1, ri →wi+1ri+1 is a rule in R, and\\n• rn →ϵ is a rule in R.\\n102\\nChapter 3.\\nContext-Free Languages\\nTherefore, we have\\nS = q = r0 ⇒w1r1 ⇒w1w2r2 ⇒. . . ⇒w1w2 . . . wnrn ⇒w1w2 . . . wn = w.\\nThis proves that w ∈L(G).\\nThe proof of the claim that L(G) ⊆L is left as an exercise.\\nIn Sections 2.9.1 and 3.2.2, we have seen that the language {0n1n : n ≥\\n0} is not regular, but context-free. Therefore, the class of all context-free\\nlanguages properly contains the class of regular languages.\\n3.3.1\\nAn example\\nLet L be the language deﬁned as\\nL = {w ∈{0, 1}∗: 101 is a substring of w}.\\nIn Section 2.2.2, we have seen that L is a regular language. In that section,\\nwe constructed the following deterministic ﬁnite automaton M that accepts\\nL (we have renamed the states):\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nS\\nA\\nB\\nC\\nWe apply the construction given in the proof of Theorem 3.3.1 to convert\\nM to a context-free grammar G whose language is equal to L. According\\nto this construction, we have G = (V, Σ, R, S), where V = {S, A, B, C},\\nΣ = {0, 1}, the start variable S is the start state of M, and R consists of the\\nrules\\nS\\n→\\n0S|1A\\nA\\n→\\n0B|1A\\nB\\n→\\n0S|1C\\nC\\n→\\n0C|1C|ϵ\\n3.4.\\nChomsky normal form\\n103\\nConsider the string 010011011, which is an element of L. When the ﬁnite\\nautomaton M reads this string, it visits the states\\nS, S, A, B, S, A, A, B, C, C.\\nIn the grammar G, this corresponds to the derivation\\nS\\n⇒\\n0S\\n⇒\\n01A\\n⇒\\n010B\\n⇒\\n0100S\\n⇒\\n01001A\\n⇒\\n010011A\\n⇒\\n0100110B\\n⇒\\n01001101C\\n⇒\\n010011011C\\n⇒\\n010011011.\\nHence,\\nS\\n∗⇒010011011,\\nimplying that the string 010011011 is in the language L(G) of the context-free\\ngrammar G.\\nThe string 10011 is not in the language L. When the ﬁnite automaton\\nM reads this string, it visits the states\\nS, A, B, S, A, A,\\ni.e., after the string has been read, M is in the non-accept state A. In the\\ngrammar G, reading the string 10011 corresponds to the derivation\\nS\\n⇒\\n1A\\n⇒\\n10B\\n⇒\\n100S\\n⇒\\n1001A\\n⇒\\n10011A.\\nSince A is not an accept state in M, the grammar G does not contain the\\nrule A →ϵ. This implies that the string 10011 cannot be derived from the\\nstart variable S. Thus, 10011 is not in the language L(G) of G.\\n104\\nChapter 3.\\nContext-Free Languages\\n3.4\\nChomsky normal form\\nThe rules in a context-free grammar G = (V, Σ, R, S) are of the form\\nA →w,\\nwhere A is a variable and w is a string over the alphabet V ∪Σ. In this\\nsection, we show that every context-free grammar G can be converted to a\\ncontext-free grammar G′, such that L(G) = L(G′), and the rules of G′ are of\\na restricted form, as speciﬁed in the following deﬁnition:\\nDeﬁnition 3.4.1 A context-free grammar G = (V, Σ, R, S) is said to be in\\nChomsky normal form, if every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are elements of V , B ̸= S, and C ̸= S.\\n2. A →a, where A is an element of V and a is an element of Σ.\\n3. S →ϵ, where S is the start variable.\\nYou should convince yourself that, for such a grammar, R contains the\\nrule S →ϵ if and only if ϵ ∈L(G).\\nTheorem 3.4.2 Let Σ be an alphabet and let L ⊆Σ∗be a context-free lan-\\nguage. There exists a context-free grammar in Chomsky normal form, whose\\nlanguage is L.\\nProof. Since L is a context-free language, there exists a context-free gram-\\nmar G = (V, Σ, R, S), such that L(G) = L. We will transform G into a\\ngrammar that is in Chomsky normal form and whose language is equal to\\nL(G). The transformation consists of ﬁve steps.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe deﬁne G1 = (V1, Σ, R1, S1), where S1 is the start variable (which is a\\nnew variable), V1 = V ∪{S1}, and R1 = R ∪{S1 →S}. This grammar has\\nthe property that\\n• the start variable S1 does not occur on the right-hand side of any rule\\nin R1, and\\n• L(G1) = L(G).\\n3.4.\\nChomsky normal form\\n105\\nStep 2: An ϵ-rule is a rule that is of the form A →ϵ, where A is a variable\\nthat is not equal to the start variable. In the second step, we eliminate all\\nϵ-rules from G1.\\nWe consider all ϵ-rules, one after another. Let A →ϵ be one such rule,\\nwhere A ∈V1 and A ̸= S1. We modify G1 as follows:\\n1. Remove the rule A →ϵ from the current set R1.\\n2. For each rule in the current set R1 that is of the form\\n(a) B →A, add the rule B →ϵ to R1, unless this rule has already\\nbeen deleted from R1; observe that in this way, we replace the two-\\nstep derivation B ⇒A ⇒ϵ by the one-step derivation B ⇒ϵ;\\n(b) B →uAv (where u and v are strings that are not both empty),\\nadd the rule B →uv to R1; observe that in this way, we replace\\nthe two-step derivation B ⇒uAv ⇒uv by the one-step derivation\\nB ⇒uv;\\n(c) B →uAvAw (where u, v, and w are strings), add the rules B →\\nuvw, B →uAvw, and B →uvAw to R1; if u = v = w = ϵ and\\nthe rule B →ϵ has already been deleted from R1, then we do not\\nadd the rule B →ϵ;\\n(d) treat rules in which A occurs more than twice on the right-hand\\nside in a similar fashion.\\nWe repeat this process until all ϵ-rules have been eliminated.\\nLet R2\\nbe the set of rules, after all ϵ-rules have been eliminated. We deﬁne G2 =\\n(V2, Σ, R2, S2), where V2 = V1 and S2 = S1. This grammar has the property\\nthat\\n• the start variable S2 does not occur on the right-hand side of any rule\\nin R2,\\n• R2 does not contain any ϵ-rule (it may contain the rule S2 →ϵ), and\\n• L(G2) = L(G1) = L(G).\\nStep 3: A unit-rule is a rule that is of the form A →B, where A and B are\\nvariables. In the third step, we eliminate all unit-rules from G2.\\n106\\nChapter 3.\\nContext-Free Languages\\nWe consider all unit-rules, one after another. Let A →B be one such\\nrule, where A and B are elements of V2. We know that B ̸= S2. We modify\\nG2 as follows:\\n1. Remove the rule A →B from the current set R2.\\n2. For each rule in the current set R2 that is of the form B →u, where\\nu ∈(V2 ∪Σ)∗, add the rule A →u to the current set R2, unless this is\\na unit-rule that has already been eliminated.\\nObserve that in this way, we replace the two-step derivation A ⇒B ⇒\\nu by the one-step derivation A ⇒u.\\nWe repeat this process until all unit-rules have been eliminated.\\nLet\\nR3 be the set of rules, after all unit-rules have been eliminated. We deﬁne\\nG3 = (V3, Σ, R3, S3), where V3 = V2 and S3 = S2. This grammar has the\\nproperty that\\n• the start variable S3 does not occur on the right-hand side of any rule\\nin R3,\\n• R3 does not contain any ϵ-rule (it may contain the rule S3 →ϵ),\\n• R3 does not contain any unit-rule, and\\n• L(G3) = L(G2) = L(G1) = L(G).\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside.\\nFor each rule in the current set R3 that is of the form A →u1u2 . . . uk,\\nwhere k ≥3 and each ui is an element of V3 ∪Σ, we modify G3 as follows:\\n1. Remove the rule A →u1u2 . . . uk from the current set R3.\\n2. Add the following rules to the current set R3:\\nA\\n→\\nu1A1\\nA1\\n→\\nu2A2\\nA2\\n→\\nu3A3\\n...\\nAk−3\\n→\\nuk−2Ak−2\\nAk−2\\n→\\nuk−1uk\\n3.4.\\nChomsky normal form\\n107\\nwhere A1, A2, . . . , Ak−2 are new variables that are added to the current\\nset V3.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 . . . uk by the (k −1)-step derivation\\nA ⇒u1A1 ⇒u1u2A2 ⇒. . . ⇒u1u2 . . . uk−2Ak−2 ⇒u1u2 . . . uk.\\nLet R4 be the set of rules, and let V4 be the set of variables, after all rules\\nwith more than two symbols on the right-hand side have been eliminated. We\\ndeﬁne G4 = (V4, Σ, R4, S4), where S4 = S3. This grammar has the property\\nthat\\n• the start variable S4 does not occur on the right-hand side of any rule\\nin R4,\\n• R4 does not contain any ϵ-rule (it may contain the rule S4 →ϵ),\\n• R4 does not contain any unit-rule,\\n• R4 does not contain any rule with more than two symbols on the right-\\nhand side, and\\n• L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nStep 5: Eliminate all rules of the form A →u1u2, where u1 and u2 are not\\nboth variables.\\nFor each rule in the current set R4 that is of the form A →u1u2, where\\nu1 and u2 are elements of V4 ∪Σ, but u1 and u2 are not both contained in\\nV4, we modify G3 as follows:\\n1. If u1 ∈Σ and u2 ∈V4, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →U1u2 and U1 →u1, where U1 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒U1u2 ⇒u1u2.\\n2. If u1 ∈V4 and u2 ∈Σ, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →u1U2 and U2 →u2, where U2 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒u1U2 ⇒u1u2.\\n108\\nChapter 3.\\nContext-Free Languages\\n3. If u1 ∈Σ, u2 ∈Σ, and u1 ̸= u2, then replace the rule A →u1u2 in the\\ncurrent set R4 by the three rules A →U1U2, U1 →u1, and U2 →u2,\\nwhere U1 and U2 are new variables that are added to the current set\\nV4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the three-step derivation A ⇒U1U2 ⇒u1U2 ⇒u1u2.\\n4. If u1 ∈Σ, u2 ∈Σ, and u1 = u2, then replace the rule A →u1u2 = u1u1\\nin the current set R4 by the two rules A →U1U1 and U1 →u1, where\\nU1 is a new variable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 = u1u1 by the three-step derivation A ⇒U1U1 ⇒u1U1 ⇒u1u1.\\nLet R5 be the set of rules, and let V5 be the set of variables, after Step 5\\nhas been completed. We deﬁne G5 = (V5, Σ, R5, S5), where S5 = S4. This\\ngrammar has the property that\\n• the start variable S5 does not occur on the right-hand side of any rule\\nin R5,\\n• R5 does not contain any ϵ-rule (it may contain the rule S5 →ϵ),\\n• R5 does not contain any unit-rule,\\n• R5 does not contain any rule with more than two symbols on the right-\\nhand side,\\n• R5 does not contain any rule of the form A →u1u2, where u1 and u2\\nare not both variables of V5, and\\n• L(G5) = L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nSince the grammar G5 is in Chomsky normal form, the proof is complete.\\n3.4.\\nChomsky normal form\\n109\\n3.4.1\\nAn example\\nConsider the context-free grammar G = (V, Σ, R, A), where V = {A, B},\\nΣ = {0, 1}, A is the start variable, and R consists of the rules\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nWe apply the construction given in the proof of Theorem 3.4.2 to convert\\nthis grammar to a context-free grammar in Chomsky normal form whose\\nlanguage is the same as that of G. Throughout the construction, upper case\\nletters will denote variables.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe introduce a new start variable S, and add the rule S →A. This gives\\nthe following grammar:\\nS\\n→\\nA\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nStep 2: Eliminate all ϵ-rules.\\nWe take the ϵ-rule A →ϵ, and remove it. Then we consider all rules that\\ncontain A on the right-hand side. There are two such rules:\\n• S →A; we add the rule S →ϵ;\\n• A →BAB; we add the rule A →BB.\\nThis gives the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB\\nB\\n→\\n00|ϵ\\nWe take the ϵ-rule B →ϵ, and remove it. Then we consider all rules that\\ncontain B on the right-hand side. There are three such rules:\\n• A →BAB; we add the rules A →AB, A →BA, and A →A;\\n• A →B; we do not add the rule A →ϵ, because it has already been\\nremoved;\\n110\\nChapter 3.\\nContext-Free Languages\\n• A →BB; we add the rule A →B, but not the rule A →ϵ (because it\\nhas already been removed).\\nAt this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA|A\\nB\\n→\\n00\\nSince all ϵ-rules have been eliminated, this completes Step 2. (Observe that\\nthe rule S →ϵ is allowed, because S is the start variable.)\\nStep 3: Eliminate all unit-rules.\\nWe take the unit-rule A →A. We can remove this rule, without adding\\nany new rule. At this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →A, remove it, and add the rules\\nS →BAB|B|BB|AB|BA.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BAB|B|BB|AB|BA\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →B, remove it, and add the rule S →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule A →B, remove it, and add the rule A →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|BB|AB|BA|00\\nB\\n→\\n00\\n3.5.\\nPushdown automata\\n111\\nSince all unit-rules have been eliminated, this concludes Step 3.\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside. There are two such rules:\\n• We take the rule S →BAB, remove it, and add the rules S →BA1\\nand A1 →AB.\\n• We take the rule A →BAB, remove it, and add the rules A →BA2\\nand A2 →AB.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BB|AB|BA|00|BA1\\nA\\n→\\nBB|AB|BA|00|BA2\\nB\\n→\\n00\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nStep 4 is now completed.\\nStep 5: Eliminate all rules, whose right-hand side contains exactly two\\nsymbols, which are not both variables. There are three such rules:\\n• We replace the rule S →00 by the rules S →A3A3 and A3 →0.\\n• We replace the rule A →00 by the rules A →A4A4 and A4 →0.\\n• We replace the rule B →00 by the rules B →A5A5 and A5 →0.\\nThis gives the following grammar, which is in Chomsky normal form:\\nS\\n→\\nϵ|BB|AB|BA|BA1|A3A3\\nA\\n→\\nBB|AB|BA|BA2|A4A4\\nB\\n→\\nA5A5\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nA3\\n→\\n0\\nA4\\n→\\n0\\nA5\\n→\\n0\\n112\\nChapter 3.\\nContext-Free Languages\\n3.5\\nPushdown automata\\nIn this section, we introduce nondeterministic pushdown automata. As we\\nwill see, the class of languages that can be accepted by these automata is\\nexactly the class of context-free languages.\\nWe start with an informal description of a deterministic pushdown au-\\ntomaton. Such an automaton consists of the following, see also Figure 3.1.\\n1. There is a tape which is divided into cells. Each cell stores a symbol\\nbelonging to a ﬁnite set Σ, called the tape alphabet. There is a special\\nsymbol 2 that is not contained in Σ; this symbol is called the blank\\nsymbol. If a cell contains 2, then this means that the cell is actually\\nempty.\\n2. There is a tape head which can move along the tape, one cell to the\\nright per move. This tape head can also read the cell it currently scans.\\n3. There is a stack containing symbols from a ﬁnite set Γ, called the stack\\nalphabet. This set contains a special symbol $.\\n4. There is a stack head which can read the top symbol of the stack. This\\nhead can also pop the top symbol, and it can push symbols of Γ onto\\nthe stack.\\n5. There is a state control, which can be in any one of a ﬁnite number\\nof states. The set of states is denoted by Q. The set Q contains one\\nspecial state q, called the start state.\\nThe input for a pushdown automaton is a string in Σ∗. This input string\\nis stored on the tape of the pushdown automaton and, initially, the tape head\\nis on the leftmost symbol of the input string. Initially, the stack only contains\\nthe special symbol $, and the pushdown automaton is in the start state q.\\nIn one computation step, the pushdown automaton does the following:\\n1. Assume that the pushdown automaton is currently in state r. Let a be\\nthe symbol of Σ that is read by the tape head, and let A be the symbol\\nof Γ that is on top of the stack.\\n2. Depending on the current state r, the tape symbol a, and the stack\\nsymbol A,\\n3.5.\\nPushdown automata\\n113\\nstate control\\na a b a b b a b a b 2\\ntape\\n6\\n$\\nA\\nA\\nB\\nA\\nstack\\n-\\nFigure 3.1: A pushdown automaton.\\n(a) the pushdown automaton switches to a state r′ of Q (which may\\nbe equal to r),\\n(b) the tape head either moves one cell to the right or stays at the\\ncurrent cell, and\\n(c) the top symbol A is replaced by a string w that belongs to Γ∗. To\\nbe more precise,\\ni. if w = ϵ, then A is popped from the stack, whereas\\nii. if w = B1B2 . . . Bk, with k ≥1 and B1, B2, . . . , Bk ∈Γ, then\\nA is replaced by w, and Bk becomes the new top symbol of\\nthe stack.\\nLater, we will specify when the pushdown automaton accepts the input\\nstring.\\nWe now give a formal deﬁnition of a deterministic pushdown automaton.\\nDeﬁnition 3.5.1 A deterministic pushdown automaton is a 5-tuple M =\\n(Σ, Γ, Q, δ, q), where\\n114\\nChapter 3.\\nContext-Free Languages\\n1. Σ is a ﬁnite set, called the tape alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the stack alphabet; this alphabet contains the\\nspecial symbol $,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. δ is called the transition function, which is a function\\nδ : Q × (Σ ∪{2}) × Γ →Q × {N, R} × Γ∗.\\nThe transition function δ can be thought of as being the “program” of the\\npushdown automaton. This function tells us what the automaton can do in\\none “computation step”: Let r ∈Q, a ∈Σ ∪{2}, and A ∈Γ. Furthermore,\\nlet r′ ∈Q, σ ∈{R, N}, and w ∈Γ∗be such that\\nδ(r, a, A) = (r′, σ, w).\\n(3.1)\\nThis transition means that if\\n• the pushdown automaton is in state r,\\n• the tape head reads the symbol a, and\\n• the top symbol on the stack is A,\\nthen\\n• the pushdown automaton switches to state r′,\\n• the tape head moves according to σ: if σ = R, then it moves one cell\\nto the right; if σ = N, then it does not move, and\\n• the top symbol A on the stack is replaced by the string w.\\nWe will write the computation step (3.1) in the form of the instruction\\nraA →r′σw.\\nWe now specify the computation of the pushdown automaton M = (Σ, Γ, Q, δ, q).\\n3.6.\\nExamples of pushdown automata\\n115\\nStart conﬁguration: Initially, the pushdown automaton is in the start state\\nq, the tape head is on the leftmost symbol of the input string a1a2 . . . an, and\\nthe stack only contains the special symbol $.\\nComputation and termination: Starting in the start conﬁguration, the\\npushdown automaton performs a sequence of computation steps as described\\nabove. It terminates at the moment when the stack becomes empty. (Hence,\\nif the stack never gets empty, the pushdown automaton does not terminate.)\\nAcceptance: The pushdown automaton accepts the input string a1a2 . . . an ∈\\nΣ∗, if\\n1. the automaton terminates on this input, and\\n2. at the time of termination (i.e., at the moment when the stack gets\\nempty), the tape head is on the cell immediately to the right of the cell\\ncontaining the symbol an (this cell must contain the blank symbol 2).\\nIn all other cases, the pushdown automaton rejects the input string. Thus,\\nthe pushdown automaton rejects this string if\\n1. the automaton does not terminate on this input (i.e., the computation\\n“loops forever”) or\\n2. at the time of termination, the tape head is not on the cell immediately\\nto the right of the cell containing the symbol an.\\nWe denote by L(M) the language accepted by the pushdown automaton\\nM. Thus,\\nL(M) = {w ∈Σ∗: M accepts w}.\\nThe pushdown automaton described above is deterministic. For a non-\\ndeterministic pushdown automata, the current computation step may not\\nbe uniquely deﬁned, but the automaton can make a choice out of a ﬁnite\\nnumber of possibilities. In this case, the transition function δ is a function\\nδ : Q × (Σ ∪{2}) × Γ →Pf(Q × {N, R} × Γ∗),\\nwhere Pf(K) is the set of all ﬁnite subsets of the set K.\\nWe say that a nondeterministic pushdown automaton M accepts an input\\nstring, if there exists an accepting computation, in the sense as described for\\ndeterministic pushdown automata. We say that M rejects an input string, if\\nevery computation on this string is rejecting. As before, we denote by L(M)\\nthe set of all strings in Σ∗that are accepted by M.\\n116\\nChapter 3.\\nContext-Free Languages\\n3.6\\nExamples of pushdown automata\\n3.6.1\\nProperly nested parentheses\\nWe will show how to construct a deterministic pushdown automaton, that\\naccepts the set of all strings of properly nested parentheses. Observe that a\\nstring w in {(, )}∗is properly nested if and only if\\n• in every preﬁx of w, the number of “(” is greater than or equal to the\\nnumber of “)”, and\\n• in the complete string w, the number of “(” is equal to the number of\\n“)”.\\nWe will use the tape symbol a for “(”, and the tape symbol b for “)”.\\nThe idea is as follows. Recall that initially, the stack only contains the\\nspecial symbol $. The pushdown automaton reads the input string from left\\nto right. For every a it reads, it pushes the symbol S onto the stack, and\\nfor every b it reads, it pops the top symbol from the stack. In this way, the\\nnumber of symbols S on the stack will always be equal to the number of as\\nthat have been read minus the number of bs that have been read; additionally,\\nthe bottom of the stack will contain the special symbol $. The input string\\nis properly nested if and only if (i) this diﬀerence is always non-negative and\\n(ii) this diﬀerence is zero once the entire input string has been read. Hence,\\nthe input string is accepted if and only if during this process, (i) the stack\\nalways contains at least the special symbol $ and (ii) at the end, the stack\\nonly contains the special symbol $ (which will then be popped in the ﬁnal\\nstep).\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q), where Σ = {a, b}, Γ = {$, S}, Q = {q}, and the\\ntransition function δ is speciﬁed by the following instructions:\\n3.6.\\nExamples of pushdown automata\\n117\\nqa$ →qR$S\\nbecause of the a, S is pushed onto the stack\\nqaS →qRSS\\nbecause of the a, S is pushed onto the stack\\nqbS →qRϵ\\nbecause of the b, the top element is popped\\nfrom the stack\\nqb$ →qNϵ\\nthe number of bs read is larger than the number\\nof as read; the stack is made empty (hence,\\nthe computation terminates before the entire\\nstring has been read), and the input string is rejected\\nq2$ →qNϵ\\nthe entire input string has been read; the stack is\\nmade empty, and the input string is accepted\\nq2S →qNS\\nthe entire input string has been read, it contains\\nmore as than bs; no changes are made (thus, the\\nautomaton does not terminate), and the input string\\nis rejected\\n3.6.2\\nStrings of the form 0n1n\\nWe construct a deterministic pushdown automata that accepts the language\\n{0n1n : n ≥0}.\\nThe automaton uses two states q0 and q1, where q0 is the start state.\\nInitially, the automaton is in state q0.\\n• For each 0 that it reads, the automaton pushes one symbol S onto the\\nstack and stays in state q0.\\n• When the ﬁrst 1 is read, the automaton switches to state q1. From that\\nmoment,\\n– for each 1 that is read, the automaton pops the top symbol from\\nthe stack and stays in state q1;\\n– if a 0 is read, the automaton does not make any change and,\\ntherefore, does not terminate.\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q0), where Σ = {0, 1}, Γ = {$, S}, Q = {q0, q1}, q0 is\\nthe start state, and the transition function δ is speciﬁed by the following\\ninstructions:\\n118\\nChapter 3.\\nContext-Free Languages\\nq00$ →q0R$S\\npush S onto the stack\\nq00S →q0RSS\\npush S onto the stack\\nq01$ →q0N$\\nﬁrst symbol in the input is 1; loop forever\\nq01S →q1Rϵ\\nﬁrst 1 is encountered\\nq02$ →q0Nϵ\\ninput string is empty; accept\\nq02S →q0NS\\ninput only consists of 0s; loop forever\\nq10$ →q1N$\\n0 to the right of 1; loop forever\\nq10S →q1NS\\n0 to the right of 1; loop forever\\nq11$ →q1N$\\ntoo many 1s; loop forever\\nq11S →q1Rϵ\\npop top symbol from the stack\\nq12$ →q1Nϵ\\naccept\\nq12S →q1NS\\ntoo many 0s; loop forever\\n3.6.3\\nStrings with b in the middle\\nWe will construct a nondeterministic pushdown automaton that accepts the\\nset L of all strings in {a, b}∗having an odd length and whose middle symbol\\nis b, i.e.,\\nL = {vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|}.\\nThe idea is as follows. The automaton uses two states q and q′, where q\\nis the start state. These states have the following meaning:\\n• If the automaton is in state q, then it has not reached the middle symbol\\nb of the input string.\\n• If the automaton is in state q′, then it has read the middle symbol b.\\nObserve that since the automaton can only make one single pass over the\\ninput string, it has to “guess” (i.e., use nondeterminism) when it reaches the\\nmiddle of the string.\\n• If the automaton is in state q, then, when reading the current tape\\nsymbol,\\n– it either pushes one symbol S onto the stack and stays in state q\\n– or, in case the current tape symbol is b, it “guesses” that it has\\nreached the middle of the input string, by switching to state q′.\\n• If the automaton is in state q′, then, when reading the current tape\\nsymbol, it pops the top symbol S from the stack and stays in state q′.\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n119\\nIn this way, the number of symbols S on the stack will always be equal to the\\ndiﬀerence of (i) the number of symbols in the part to the left of the middle\\nsymbol b that have been read and (ii) the number of symbols in the part\\nto the right of the middle symbol b that have been read; additionally, the\\nbottom of the stack will contain the special symbol $.\\nThe input string is accepted if and only if, at the moment when the blank\\nsymbol 2 is read, the automaton is in state q′ and the top symbol on the\\nstack is $. In this case, the stack is made empty and, thus, the computation\\nterminates.\\nWe obtain the nondeterministic pushdown automaton M = (Σ, Γ, Q, δ, q),\\nwhere Σ = {a, b}, Γ = {$, S}, Q = {q, q′}, q is the start state, and the\\ntransition function δ is speciﬁed by the following instructions:\\nqa$ →qR$S\\npush S onto the stack\\nqaS →qRSS\\npush S onto the stack\\nqb$ →q′R$\\nreached the middle\\nqb$ →qR$S\\ndid not reach the middle; push S onto the stack\\nqbS →q′RS\\nreached the middle\\nqbS →qRSS\\ndid not reach the middle; push S onto the stack\\nq2$ →qN$\\ninput string is empty; loop forever\\nq2S →qNS\\nloop forever\\nq′a$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′aS →q′Rϵ\\npop top symbol from stack\\nq′b$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′bS →q′Rϵ\\npop top symbol from stack\\nq′2$ →q′Nϵ\\naccept\\nq′2S →q′NS\\nloop forever\\nRemark 3.6.1 It can be shown that there is no deterministic pushdown\\nautomaton that accepts the language L. The reason is that a deterministic\\npushdown automaton cannot determine when it reaches the middle of the\\ninput string. Thus, unlike as for ﬁnite automata, nondeterministic pushdown\\nautomata are more powerful than their deterministic counterparts.\\n120\\nChapter 3.\\nContext-Free Languages\\n3.7\\nEquivalence of pushdown automata and\\ncontext-free grammars\\nThe main result of this section is that nondeterministic pushdown automata\\nand context-free grammars are equivalent in power:\\nTheorem 3.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then\\nA is context-free if and only if there exists a nondeterministic pushdown\\nautomaton that accepts A.\\nWe will only prove one direction of this theorem. That is, we will show\\nhow to convert an arbitrary context-free grammar to a nondeterministic push-\\ndown automaton.\\nLet G = (V, Σ, R, $) be a context-free grammar, where V is the set of\\nvariables, Σ is the set of terminals, R is the set of rules, and $ is the start\\nvariable. By Theorem 3.4.2, we may assume that G is in Chomsky normal\\nform. Hence, every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are variables, B ̸= $, and C ̸= $.\\n2. A →a, where A is a variable and a is a terminal.\\n3. $ →ϵ.\\nWe will construct a nondeterministic pushdown automaton M that ac-\\ncepts the language L(G) of this grammar G. Observe that M must have the\\nfollowing property: For every string w = a1a2 . . . an ∈Σ∗,\\nw ∈L(G) if and only if M accepts w.\\nThis can be reformulated as follows:\\n$\\n∗⇒a1a2 . . . an\\nif and only if there exists a computation of M that starts in the initial\\nconﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n121\\nand ends in the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nwhere ∅indicates that the stack is empty.\\nAssume that $\\n∗⇒a1a2 . . . an. Then there exists a derivation (using the\\nrules of R) of the string a1a2 . . . an from the start variable $. We may assume\\nthat in each step in this derivation, a rule is applied to the leftmost variable\\nin the current string. Hence, because the grammar G is in Chomsky normal\\nform, at any moment during the derivation, the current string has the form\\na1a2 . . . ai−1AkAk−1 . . . A1,\\n(3.2)\\nfor some integers i and k with 1 ≤i ≤n + 1 and k ≥0, and variables\\nA1, A2, . . . , Ak. (In particular, at the start of the derivation, we have i = 1\\nand k = 1, and the current string is Ak = $. At the end of the derivation,\\nwe have i = n + 1 and k = 0, and the current string is a1a2 . . . an.)\\nWe will deﬁne the pushdown automaton M in such a way that the current\\nstring (3.2) corresponds to the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nBased on this discussion, we obtain the nondeterministic pushdown au-\\ntomaton M = (Σ, V, {q}, δ, q), where\\n• the tape alphabet is the set Σ of terminals of G,\\n• the stack alphabet is the set V of variables of G,\\n• the set of states consists of one state q, which is the start state, and\\n• the transition function δ is obtained from the rules in R, in the following\\nway:\\n122\\nChapter 3.\\nContext-Free Languages\\n– For each rule in R that is of the form A →BC, with A, B, C ∈V ,\\nthe pushdown automaton M has the instructions\\nqaA →qNCB, for all a ∈Σ.\\n– For each rule in R that is of the form A →a, with A ∈V and\\na ∈Σ, the pushdown automaton M has the instruction\\nqaA →qRϵ.\\n– If R contains the rule $ →ϵ, then the pushdown automaton M\\nhas the instruction\\nq2$ →qNϵ.\\nThis concludes the deﬁnition of M. It remains to prove that L(M) =\\nL(G), i.e., the language of the nondeterministic pushdown automaton M is\\nequal to the language of the context-free grammar G. Hence, we have to\\nshow that for every string w ∈Σ∗,\\nw ∈L(G) if and only if w ∈L(M),\\nwhich can be rewritten as\\n$\\n∗⇒w if and only if M accepts w.\\nClaim 3.7.2 Let a1a2 . . . an be a string in Σ∗, let A1, A2, . . . , Ak be variables\\nin V , and let i and k be integers with 1 ≤i ≤n + 1 and k ≥0. Then the\\nfollowing holds:\\n$\\n∗⇒a1a2 . . . ai−1AkAk−1 . . . A1\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n123\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nProof. The claim can be proved by induction. Let\\nw = a1a2 . . . ai−1AkAk−1 . . . A1.\\nAssume that k ≥1 and assume that the claim is true for the string w. Then\\nwe have to show that the claim is still true after applying a rule in R to the\\nleftmost variable Ak in w. Since the grammar is in Chomsky normal form,\\nthe rule to be applied is either of the form Ak →BC or of the form Ak →ai.\\nIn both cases, the property mentioned in the claim is maintained.\\nWe now use Claim 3.7.2 to prove that L(M) = L(G). Let w = a1a2 . . . an\\nbe an arbitrary string in Σ∗. By applying Claim 3.7.2, with i = n + 1 and\\nk = 0, we see that w ∈L(G), i.e.,\\n$\\n∗⇒a1a2 . . . an,\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nBut this means that w ∈L(G) if and only if the automaton M accepts the\\nstring w.\\nThis concludes the proof of the fact that every context-free grammar can\\nbe converted to a nondeterministic pushdown automaton.\\nAs mentioned\\nalready, we will not give the conversion in the other direction. We ﬁnish this\\nsection with the following observation:\\n124\\nChapter 3.\\nContext-Free Languages\\nTheorem 3.7.3 Let Σ be an alphabet and let A ⊆Σ∗be a context-free lan-\\nguage. Then there exists a nondeterministic pushdown automaton that ac-\\ncepts A and has only one state.\\nProof. Since A is context-free, there exists a context-free grammar G0 such\\nthat L(G0) = A. By Theorem 3.4.2, there exists a context-free grammar G\\nthat is in Chomsky normal form and for which L(G) = L(G0). The construc-\\ntion given above converts G to a nondeterministic pushdown automaton M\\nthat has only one state and for which L(M) = L(G).\\n3.8\\nThe pumping lemma for context-free lan-\\nguages\\nIn Section 2.9, we proved the pumping lemma for regular languages and\\nused it to prove that certain languages are not regular. In this section, we\\ngeneralize the pumping lemma to context-free languages.\\nThe idea is to\\nconsider the parse tree (see Section 3.1) that describes the derivation of a\\nsuﬃciently long string in the context-free language L. Since the number of\\nvariables in the corresponding context-free grammar G is ﬁnite, there is at\\nleast one variable, say Aj, that occurs more than once on the longest root-\\nto-leaf path in the parse tree. The subtree which is sandwiched between two\\noccurrences of Aj on this path can be copied any number of times. This will\\nresult in a legal parse tree and, hence, in a “pumped” string that is in the\\nlanguage L.\\nTheorem 3.8.1 (Pumping Lemma for Context-Free Languages) Let\\nL be a context-free language. Then there exists an integer p ≥1, called the\\npumping length, such that the following holds: Every string s in L, with\\n|s| ≥p, can be written as s = uvxyz, such that\\n1. |vy| ≥1 (i.e., v and y are not both empty),\\n2. |vxy| ≤p, and\\n3. uvixyiz ∈L, for all i ≥0.\\n3.8.\\nThe pumping lemma for context-free languages\\n125\\n3.8.1\\nProof of the pumping lemma\\nThe proof of the pumping lemma will use the following result about parse\\ntrees:\\nLemma 3.8.2 Let G be a context-free grammar in Chomsky normal form,\\nlet s be a non-empty string in L(G), and let T be a parse tree for s. Let ℓbe\\nthe height of T, i.e., ℓis the number of edges on a longest root-to-leaf path\\nin T. Then\\n|s| ≤2ℓ−1.\\nProof. The claim can be proved by induction on ℓ. By looking at some\\nsmall values of ℓand using the fact that G is in Chomsky normal form, you\\nshould be able to verify the claim.\\nNow we can start with the proof of the pumping lemma. Let L be a\\ncontext-free language and let Σ be the alphabet of L. By Theorem 3.4.2, there\\nexists a context-free grammar in Chomsky normal form, G = (V, Σ, R, S),\\nsuch that L = L(G).\\nDeﬁne r to be the number of variables of G and deﬁne p = 2r. We will\\nprove that the value of p can be used as the pumping length. Consider an\\narbitrary string s in L such that |s| ≥p, and let T be a parse tree for s. Let\\nℓbe the height of T. Then, by Lemma 3.8.2, we have\\n|s| ≤2ℓ−1.\\nOn the other hand, we have\\n|s| ≥p = 2r.\\nBy combining these inequalities, we see that 2r ≤2ℓ−1, which can be rewrit-\\nten as\\nℓ≥r + 1.\\nConsider the nodes on a longest root-to-leaf path in T.\\nSince this path\\nconsists of ℓedges, it consists of ℓ+ 1 nodes. The ﬁrst ℓof these nodes store\\nvariables, which we denote by A0, A1, . . . , Aℓ−1 (where A0 = S), and the last\\nnode (which is a leaf) stores a terminal, which we denote by a.\\nSince ℓ−1 −r ≥0, the sequence\\nAℓ−1−r, Aℓ−r, . . . , Aℓ−1\\n126\\nChapter 3.\\nContext-Free Languages\\nof variables is well-deﬁned.\\nObserve that this sequence consists of r + 1\\nvariables. Since the number of variables in the grammar G is equal to r,\\nthe pigeonhole principle implies that there is a variable that occurs at least\\ntwice in this sequence. In other words, there are indices j and k, such that\\nℓ−1 −r ≤j < k ≤ℓ−1 and Aj = Ak. Refer to the ﬁgure below for an\\nillustration.\\nS\\nA j\\nAk\\nu\\nv\\nx\\ny\\nz\\ns\\nA0 = S\\nA1\\nAℓ−1−r\\nAℓ−r\\nAℓ−2\\nAℓ−1\\na\\nr +1\\nvariables\\nRecall that T is a parse tree for the string s. Therefore, the terminals\\nstored at the leaves of T, in the order from left to right, form s. As indicated\\nin the ﬁgure above, the nodes storing the variables Aj and Ak partition s\\ninto ﬁve substrings u, v, x, y, and z, such that s = uvxyz.\\nIt remains to prove that the three properties stated in the pumping lemma\\n3.8.\\nThe pumping lemma for context-free languages\\n127\\nhold. We start with the third property, i.e., we prove that\\nuvixyiz ∈L, for all i ≥0.\\nIn the grammar G, we have\\nS\\n∗⇒uAjz.\\n(3.3)\\nSince Aj\\n∗⇒vAky and Ak = Aj, we have\\nAj\\n∗⇒vAjy.\\n(3.4)\\nFinally, since Ak\\n∗⇒x and Ak = Aj, we have\\nAj\\n∗⇒x.\\n(3.5)\\nFrom (3.3) and (3.5), it follows that\\nS\\n∗⇒uAjz\\n∗⇒uxz,\\nwhich implies that the string uxz is in the language L. Similarly, it follows\\nfrom (3.3), (3.4), and (3.5) that\\nS\\n∗⇒uAjz\\n∗⇒uvAjyz\\n∗⇒uvvAjyyz\\n∗⇒uvvxyyz.\\nHence, the string uv2xy2z is in the language L. In general, for each i ≥0,\\nthe string uvixyiz is in the language L, because\\nS\\n∗⇒uAjz\\n∗⇒uviAjyiz\\n∗⇒uvixyiz.\\nThis proves that the third property in the pumping lemma holds.\\nNext we show that the second property holds. That is, we prove that\\n|vxy| ≤p.\\nConsider the subtree rooted at the node storing the variable\\nAj.\\nThe path from the node storing Aj to the leaf storing the terminal\\na is a longest path in this subtree. (Convince yourself that this is true.)\\nMoreover, this path consists of ℓ−j edges. Since Aj\\n∗⇒vxy, this subtree\\nis a parse tree for the string vxy (where Aj is used as the start variable).\\nTherefore, by Lemma 3.8.2, we can conclude that |vxy| ≤2ℓ−j−1. We know\\nthat ℓ−1 −r ≤j, which is equivalent to ℓ−j −1 ≤r. It follows that\\n|vxy| ≤2ℓ−j−1 ≤2r = p.\\n128\\nChapter 3.\\nContext-Free Languages\\nFinally, we show that the ﬁrst property in the pumping lemma holds.\\nThat is, we prove that |vy| ≥1. Recall that\\nAj\\n∗⇒vAky.\\nLet the ﬁrst rule used in this derivation be Aj →BC. (Since the variables\\nAj and Ak, even though they are equal, are stored at diﬀerent nodes of the\\nparse tree, and since the grammar G is in Chomsky normal form, this ﬁrst\\nrule exists.) Then\\nAj ⇒BC\\n∗⇒vAky.\\nObserve that the string BC has length two. Moreover, by applying rules of\\na grammar in Chomsky normal form, strings cannot become shorter. (Here,\\nwe use the fact that the start variable does not occur on the right-hand side\\nof any rule.) Therefore, we have |vAky| ≥2. But this implies that |vy| ≥1.\\nThis completes the proof of the pumping lemma.\\n3.8.2\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {anbncn : n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. Consider the string s = apbpcp.\\nObserve that s ∈A and |s| = 3p ≥p. Hence, by the pumping lemma, s can\\nbe written as s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all\\ni ≥0.\\nObserve that the pumping lemma does not tell us the location of the\\nsubstring vxy in the string s, it only gives us an upper bound on the length\\nof this substring. Therefore, we have to consider three cases, depending on\\nthe location of vxy in s.\\nCase 1: The substring vxy does not contain any c.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many as or more than p many bs. Since it contains\\nexactly p many cs, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\n3.8.\\nThe pumping lemma for context-free languages\\n129\\nCase 2: The substring vxy does not contain any a.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many bs or more than p many cs. Since it contains\\nexactly p many as, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\nCase 3: The substring vxy contains at least one a and at least one c.\\nSince s = apbpcp, this implies that |vxy| > p, which again contradicts the\\npumping lemma.\\nThus, in all of the three cases, we have obtained a contradiction. There-\\nfore, we have shown that the language A is not context-free.\\nSecond example\\nConsider the languages\\nA = {wwR : w ∈{a, b}∗},\\nwhere wR is the string obtained by writing w backwards, and\\nB = {ww : w ∈{a, b}∗}.\\nEven though these languages look similar, we will show that A is context-free\\nand B is not context-free.\\nConsider the following context-free grammar, in which S is the start vari-\\nable:\\nS →ϵ|aSa|bSb.\\nIt is easy to see that the language of this grammar is exactly the language A.\\nTherefore, A is context-free. Alternatively, we can show that A is context-\\nfree, by constructing a (nondeterministic) pushdown automaton that accepts\\nA. This automaton has two states q and q′, where q is the start state. If the\\nautomaton is in state q, then it did not yet ﬁnish reading the leftmost half of\\nthe input string; it pushes all symbols read onto the stack. If the automaton\\nis in state q′, then it is reading the rightmost half of the input string; for each\\nsymbol read, it checks whether it is equal to the symbol on top of the stack\\nand, if so, pops the top symbol from the stack. The pushdown automaton\\nuses nondeterminism to “guess” when to switch from state q to state q′ (i.e.,\\nwhen it has completed reading the leftmost half of the input string).\\n130\\nChapter 3.\\nContext-Free Languages\\nAt this point, you should convince yourself that the two approaches above,\\nwhich showed that A is context-free, do not work for B. The reason why\\nthey do not work is that the language B is not context-free, as we will prove\\nnow.\\nAssume that B is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. At this point, we must choose a\\nstring s in B, whose length is at least p, and that does not satisfy the three\\nproperties stated in the pumping lemma. Let us try the string s = apbapb.\\nThen s ∈B and |s| = 2p + 2 ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = uvxyz, where (i) |vy| ≥1, (ii) |vxy| ≤p, and (iii) uvixyiz ∈B\\nfor all i ≥0. It may happen that p ≥3, u = ap−1, v = a, x = b, y = a,\\nand z = ap−1b. If this is the case, then properties (i), (ii), and (iii) hold,\\nand, thus, we do not get a contradiction. In other words, we have chosen\\nthe “wrong” string s. This string is “wrong”, because there is only one b\\nbetween the as. Because of this, v can be in the leftmost block of as, and\\ny can be in the rightmost block of as. Observe that if there were at least p\\nmany bs between the as, then this would not happen, because |vxy| ≤p.\\nBased on the discussion above, we choose s = apbpapbp. Observe that\\ns ∈B and |s| = 4p ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈B for all i ≥0. Based\\non the location of vxy in the string s, we distinguish three cases:\\nCase 1: The substring vxy overlaps both the leftmost half and the rightmost\\nhalf of s.\\nSince |vxy| ≤p, the substring vxy is contained in the “middle” part of s,\\ni.e., vxy is contained in the block bpap. Consider the string uv0xy0z = uxz.\\nSince |vy| ≥1, we know that at least one of v and y is non-empty.\\n• If v ̸= ϵ, then v contains at least one b from the leftmost block of bs in\\ns, whereas y does not contain any b from the rightmost block of bs in s.\\nTherefore, in the string uxz, the leftmost block of bs contains fewer bs\\nthan the rightmost block of bs. Hence, the string uxz is not contained\\nin B.\\n• If y ̸= ϵ, then y contains at least one a from the rightmost block of\\nas in s, whereas v does not contain any a from the leftmost block of\\nas in s. Therefore, in the string uxz, the leftmost block of as contains\\nmore as than the rightmost block of as. Hence, the string uxz is not\\ncontained in B.\\n3.8.\\nThe pumping lemma for context-free languages\\n131\\nIn both cases, we conclude that the string uxz is not an element of the\\nlanguage B. But, by the pumping lemma, this string is contained in B.\\nCase 2: The substring vxy is in the leftmost half of s.\\nIn this case, none of the strings uxz, uv2xy2z, uv3xy3z, uv4xy4z, etc.,\\nis contained in B.\\nBut, by the pumping lemma, each of these strings is\\ncontained in B.\\nCase 3: The substring vxy is in the rightmost half of s.\\nThis case is symmetric to Case 2: None of the strings uxz, uv2xy2z,\\nuv3xy3z, uv4xy4z, etc., is contained in B. But, by the pumping lemma, each\\nof these strings is contained in B.\\nTo summarize, in each of the three cases, we have obtained a contradic-\\ntion. Therefore, the language B is not context-free.\\nThird example\\nWe have seen in Section 3.2.4 that the language\\n{ambncm+n : m ≥0, n ≥0}\\nis context-free. Using the pumping lemma for regular languages, it is easy to\\nprove that this language is not regular. In other words, context-free gram-\\nmars can verify addition, whereas ﬁnite automata are not powerful enough\\nfor this. We now consider the problem of verifying multiplication: Let A be\\nthe language deﬁned as\\nA = {ambncmn : m ≥0, n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is context-free. Let p ≥1 be the pumping length, as\\ngiven by the pumping lemma. Consider the string s = apbpcp2. Then, s ∈A\\nand |s| = 2p + p2 ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all i ≥0.\\nThere are three possible cases, depending on the locations of v and y in\\nthe string s.\\nCase 1: The substring v does not contain any a and does not contain any\\nb, and the substring y does not contain any a and does not contain any b.\\n132\\nChapter 3.\\nContext-Free Languages\\nConsider the string uv2xy2z. Since |vy| ≥1, this string consists of p\\nmany as, p many bs, but more than p2 many cs. Therefore, this string is not\\ncontained in A. But, by the pumping lemma, it is contained in A.\\nCase 2: The substring v does not contain any c and the substring y does\\nnot contain any c.\\nConsider again the string uv2xy2z. This string consists of p2 many cs.\\nSince |vy| ≥1, in this string,\\n• the number of as is at least p + 1 and the number of bs is at least p, or\\n• the number of as is at least p and the number of bs is at least p + 1.\\nTherefore, the number of as multiplied by the number of bs is at least p(p+1),\\nwhich is larger than p2. Therefore, uv2xy2z is not contained in A. But, by\\nthe pumping lemma, this string is contained in A.\\nCase 3: The substring v contains at least one b and the substring y contains\\nat least one c.\\nSince |vxy| ≤p, the substring vy does not contain any a. Thus, we can\\nwrite vy = bjck, where j ≥1 and k ≥1. Consider the string uxz. We can\\nwrite this string as uxz = apbp−jcp2−k. Since, by the pumping lemma, this\\nstring is contained in A, we have p(p−j) = p2−k, which implies that jp = k.\\nThus,\\n|vxy| ≥|vy| = j + k = j + jp ≥1 + p.\\nBut, by the pumping lemma, we have |vxy| ≤p.\\nObserve that, since |vxy| ≤p, the above three cases cover all possibilities\\nfor the locations of v and y in the string s. In each of the three cases, we\\nhave obtained a contradiction. Therefore, the language A is not context-free.\\nExercises\\n3.1 Construct context-free grammars that generate the following languages.\\nIn all cases, Σ = {0, 1}.\\n• {02n1n : n ≥0}\\n• {w : w contains at least three 1s}\\n• {w : the length of w is odd and its middle symbol is 0}\\nExercises\\n133\\n• {w : w is a palindrome}.\\nA palindrome is a string w having the property that w = wR, i.e.,\\nreading w from left to right gives the same result as reading w from\\nright to left.\\n• {w : w starts and ends with the same symbol}\\n• {w : w starts and ends with diﬀerent symbols}\\n3.2 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {0, 1}, S is the start variable, and R consists of the rules\\nS\\n→\\n0S|1A|ϵ\\nA\\n→\\n0B|1S\\nB\\n→\\n0A|1B\\nDeﬁne the following language L:\\nL = {w ∈{0, 1}∗:\\nw is the binary representation of a non-negative\\ninteger that is divisible by three } ∪{ϵ}.\\nProve that L = L(G). (Hint: The variables S, A, and B are used to\\nremember the remainder after division by three.)\\n3.3 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {a, b}, S is the start variable, and R consists of the rules\\nS\\n→\\naB|bA\\nA\\n→\\na|aS|BAA\\nB\\n→\\nb|bS|ABB\\n• Prove that ababba ∈L(G).\\n• Prove that L(G) is the set of all non-empty strings w over the alphabet\\n{a, b} such that the number of as in w is equal to the number of bs in\\nw.\\n3.4 Let A and B be context-free languages over the same alphabet Σ.\\n• Prove that the union A ∪B of A and B is also context-free.\\n• Prove that the concatenation AB of A and B is also context-free.\\n134\\nChapter 3.\\nContext-Free Languages\\n• Prove that the star A∗of A is also context-free.\\n3.5 Deﬁne the following two languages A and B:\\nA = {ambncn : m ≥0, n ≥0}\\nand\\nB = {ambmcn : m ≥0, n ≥0}.\\n• Prove that both A and B are context-free, by constructing two context-\\nfree grammars, one that generates A and one that generates B.\\n• We have seen in Section 3.8.2 that the language\\n{anbncn : n ≥0}\\nis not context-free. Explain why this implies that the intersection of\\ntwo context-free languages is not necessarily context-free.\\n• Use De Morgan’s Law to conclude that the complement of a context-\\nfree language is not necessarily context-free.\\n3.6 Let A be a context-free language and let B be a regular language.\\n• Prove that the intersection A ∩B of A and B is context-free.\\n• Prove that the set-diﬀerence\\nA \\\\ B = {w : w ∈A, w ̸∈B}\\nof A and B is context-free.\\n• Is the set-diﬀerence of two context-free languages necessarily context-\\nfree?\\n3.7 Let L be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that\\n• the number of as in w is equal to the number of bs in w,\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\nExercises\\n135\\nIn this exercise, you will prove that L is context-free.\\nLet A be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that the number of as in w is equal to the number of bs\\nin w. In Exercise 3.3, you have shown that A is context-free.\\nLet B be the language consisting of all strings w over the alphabet {a, b}\\nsuch that\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\n1. Give a regular expression that describes the complement of B.\\n2. Argue that B is a regular language.\\n3. Use Exercise 3.6 to argue that L is a context-free language.\\n3.8 Construct (deterministic or nondeterministic) pushdown automata that\\naccept the following languages.\\n1. {02n1n : n ≥0}.\\n2. {0n1m0n : n ≥1, m ≥1}.\\n3. {w ∈{0, 1}∗: w contains more 1s than 0s}.\\n4. {wwR : w ∈{0, 1}∗}.\\n(If w = w1 . . . wn, then wR = wn . . . w1.)\\n5. {w ∈{0, 1}∗: w is a palindrome}.\\n3.9 Let L be the language\\nL = {ambn : 0 ≤m ≤n ≤2m}.\\n1. Prove that L is context-free, by constructing a context-free grammar\\nwhose language is equal to L.\\n2. Prove that L is context-free, by constructing a nondeterministic push-\\ndown automaton that accepts L.\\n3.10 Prove that the following languages are not context-free.\\n136\\nChapter 3.\\nContext-Free Languages\\n• {an b a2n b a3n : n ≥0}.\\n• {anbnanbn : n ≥0}.\\n• {ambnck : m ≥0, n ≥0, k = max(m, n)}.\\n• {w#x : w is a substring of x, and w, x ∈{a, b}∗}.\\nFor example, the string aba#abbababbb is in the language, whereas the\\nstring aba#baabbaabb is not in the language. The alphabet is {a, b, #}.\\n•\\n{ w ∈{a, b, c}∗\\n:\\nw contains more b’s than a’s and\\nw contains more c’s than a’s }.\\n• {1n : n is a prime number}.\\n• {(abn)n : n ≥0}. (The parentheses are not part of the alphabet; thus,\\nthe alphabet is {a, b, }.)\\n3.11 Let L be a language consisting of ﬁnitely many strings. Show that L\\nis regular and, therefore, context-free. Let k be the maximum length of any\\nstring in L.\\n• Prove that every context-free grammar in Chomsky normal form that\\ngenerates L has more than log k variables. (The logarithm is in base\\n2.)\\n• Prove that there is a context-free grammar that generates L and that\\nhas only one variable.\\n3.12 Let L be a context-free language. Prove that there exists an integer\\np ≥1, such that the following is true: For every string s in L with |s| ≥p,\\nthere exists a string s′ in L such that |s| < |s′| ≤|s| + p.\\nChapter 4\\nTuring Machines and the\\nChurch-Turing Thesis\\nIn the previous chapters, we have seen several computational devices that\\ncan be used to accept or generate regular and context-free languages. Even\\nthough these two classes of languages are fairly large, we have seen in Sec-\\ntion 3.8.2 that these devices are not powerful enough to accept simple lan-\\nguages such as A = {ambncmn : m ≥0, n ≥0}. In this chapter, we introduce\\nthe Turing machine, which is a simple model of a real computer. Turing ma-\\nchines can be used to accept all context-free languages, but also languages\\nsuch as A. We will argue that every problem that can be solved on a real\\ncomputer can also be solved by a Turing machine (this statement is known\\nas the Church-Turing Thesis). In Chapter 5, we will consider the limitations\\nof Turing machines and, hence, of real computers.\\n4.1\\nDeﬁnition of a Turing machine\\nWe start with an informal description of a Turing machine. Such a machine\\nconsists of the following, see also Figure 4.1.\\n1. There are k tapes, for some ﬁxed k ≥1. Each tape is divided into\\ncells, and is inﬁnite both to the left and to the right. Each cell stores\\na symbol belonging to a ﬁnite set Γ, which is called the tape alphabet.\\nThe tape alphabet contains the blank symbol 2. If a cell contains 2,\\nthen this means that the cell is actually empty.\\n138\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nstate control\\n. . . 2 2 2 a a b a b b a b a b 2 2 2\\n. . .\\n?\\n. . . 2 2 2 b a a b 2 a b 2 2 2\\n. . .\\n?\\nFigure 4.1: A Turing machine with k = 2 tapes.\\n2. Each tape has a tape head which can move along the tape, one cell\\nper move. It can also read the cell it currently scans and replace the\\nsymbol in this cell by another symbol.\\n3. There is a state control, which can be in any one of a ﬁnite number of\\nstates. The ﬁnite set of states is denoted by Q. The set Q contains\\nthree special states: a start state, an accept state, and a reject state.\\nThe Turing machine performs a sequence of computation steps. In one\\nsuch step, it does the following:\\n1. Immediately before the computation step, the Turing machine is in a\\nstate r of Q, and each of the k tape heads is on a certain cell.\\n2. Depending on the current state r and the k symbols that are read by\\nthe tape heads,\\n(a) the Turing machine switches to a state r′ of Q (which may be\\nequal to r),\\n(b) each tape head writes a symbol of Γ in the cell it is currently\\nscanning (this symbol may be equal to the symbol currently stored\\nin the cell), and\\n4.1.\\nDeﬁnition of a Turing machine\\n139\\n(c) each tape head either moves one cell to the left, moves one cell to\\nthe right, or stays at the current cell.\\nWe now give a formal deﬁnition of a deterministic Turing machine.\\nDeﬁnition 4.1.1 A deterministic Turing machine is a 7-tuple\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject),\\nwhere\\n1. Σ is a ﬁnite set, called the input alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the tape alphabet; this alphabet contains the\\nblank symbol 2, and Σ ⊆Γ,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. qaccept is an element of Q; it is called the accept state,\\n6. qreject is an element of Q; it is called the reject state,\\n7. δ is called the transition function, which is a function\\nδ : Q × Γk →Q × Γk × {L, R, N}k.\\nThe transition function δ is basically the “program” of the Turing ma-\\nchine. This function tells us what the machine can do in “one computation\\nstep”: Let r ∈Q, and let a1, a2, . . . , ak ∈Γ.\\nFurthermore, let r′ ∈Q,\\na′\\n1, a′\\n2, . . . , a′\\nk ∈Γ, and σ1, σ2, . . . , σk ∈{L, R, N} be such that\\nδ(r, a1, a2, . . . , ak) = (r′, a′\\n1, a′\\n2, . . . , a′\\nk, σ1, σ2, . . . , σk).\\n(4.1)\\nThis transition means that if\\n• the Turing machine is in state r, and\\n• the head of the i-th tape reads the symbol ai, 1 ≤i ≤k,\\nthen\\n140\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• the Turing machine switches to state r′,\\n• the head of the i-th tape replaces the scanned symbol ai by the symbol\\na′\\ni, 1 ≤i ≤k, and\\n• the head of the i-th tape moves according to σi, 1 ≤i ≤k: if σi = L,\\nthen the tape head moves one cell to the left; if σi = R, then it moves\\none cell to the right; if σi = N, then the tape head does not move.\\nWe will write the computation step (4.1) in the form of the instruction\\nra1a2 . . . ak →r′a′\\n1a′\\n2 . . . a′\\nkσ1σ2 . . . σk.\\nWe now specify the computation of the Turing machine\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject).\\nStart conﬁguration: The input is a string over the input alphabet Σ.\\nInitially, this input string is stored on the ﬁrst tape, and the head of this\\ntape is on the leftmost symbol of the input string. Initially, all other k −1\\ntapes are empty, i.e., only contain blank symbols, and the Turing machine is\\nin the start state q.\\nComputation and termination: Starting in the start conﬁguration, the\\nTuring machine performs a sequence of computation steps as described above.\\nThe computation terminates at the moment when the Turing machine en-\\nters the accept state qaccept or the reject state qreject. (Hence, if the Turing\\nmachine never enters the states qaccept and qreject, the computation does not\\nterminate.)\\nAcceptance: The Turing machine M accepts the input string w ∈Σ∗, if the\\ncomputation on this input terminates in the state qaccept. If the computation\\non this input terminates in the state qreject, then M rejects the input string\\nw.\\nWe denote by L(M) the language accepted by the Turing machine M.\\nThus, L(M) is the set of all strings in Σ∗that are accepted by M.\\nObserve that a string w ∈Σ∗does not belong to L(M) if and only if on\\ninput w,\\n• the computation of M terminates in the state qreject or\\n• the computation of M does not terminate.\\n4.2.\\nExamples of Turing machines\\n141\\n4.2\\nExamples of Turing machines\\n4.2.1\\nAccepting palindromes using one tape\\nWe will show how to construct a Turing machine with one tape, that decides\\nwhether or not any input string w ∈{a, b}∗is a palindrome. Recall that the\\nstring w is called a palindrome, if reading w from left to right gives the same\\nresult as reading w from right to left. Examples of palindromes are abba,\\nbaabbbbaab, and the empty string ϵ.\\nStart of the computation: The tape contains the input string w, the tape\\nhead is on the leftmost symbol of w, and the Turing machine is in the start\\nstate q0.\\nIdea: The tape head reads the leftmost symbol of w, deletes this symbol\\nand “remembers” it by means of a state.\\nThen the tape head moves to\\nthe rightmost symbol and tests whether it is equal to the (already deleted)\\nleftmost symbol.\\n• If they are equal, then the rightmost symbol is deleted, the tape head\\nmoves to the new leftmost symbol, and the whole process is repeated.\\n• If they are not equal, the Turing machine enters the reject state, and\\nthe computation terminates.\\nThe Turing machine enters the accept state as soon as the string currently\\nstored on the tape is empty.\\nWe will use the input alphabet Σ = {a, b} and the tape alphabet Γ =\\n{a, b, 2}. The set Q of states consists of the following eight states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost symbol was a; tape head is moving to the right\\nqb :\\nleftmost symbol was b; tape head is moving to the right\\nq′\\na :\\nreached rightmost symbol; test whether it is equal to a, and delete it\\nq′\\nb :\\nreached rightmost symbol; test whether it is equal to b, and delete it\\nqL :\\ntest was positive; tape head is moving to the left\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n142\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nqba →qbaR\\nq0b →qb2R\\nqab →qabR\\nqbb →qbbR\\nq02 →qaccept\\nqa2 →q′\\na2L\\nqb2 →q′\\nb2L\\nq′\\naa →qL2L\\nq′\\nba →qreject\\nqLa →qLaL\\nq′\\nab →qreject\\nq′\\nbb →qL2L\\nqLb →qLbL\\nq′\\na2 →qaccept\\nq′\\nb2 →qaccept\\nqL2 →q02R\\nYou should go through the computation of this Turing machine for some\\nsample inputs, for example abba, b, abb and the empty string (which is a\\npalindrome).\\n4.2.2\\nAccepting palindromes using two tapes\\nWe again consider the palindrome problem, but now we use a Turing machine\\nwith two tapes.\\nStart of the computation: The ﬁrst tape contains the input string w and\\nthe head of the ﬁrst tape is on the leftmost symbol of w. The second tape is\\nempty and its tape head is at an arbitrary position. The Turing machine is\\nin the start state q0.\\nIdea: First, the input string w is copied to the second tape. Then the head\\nof the ﬁrst tape moves back to the leftmost symbol of w, while the head of\\nthe second tape stays at the rightmost symbol of w. Finally, the actual test\\nstarts: The head of the ﬁrst tape moves to the right and, at the same time,\\nthe head of the second tape moves to the left. While moving, the Turing\\nmachine tests whether the two tape heads read the same symbol in each\\nstep.\\nThe input alphabet is Σ = {a, b} and the tape alphabet is Γ = {a, b, 2}.\\nThe set Q of states consists of the following ﬁve states:\\nq0 :\\nstart state; copy w to the second tape\\nq1 :\\nw has been copied; head of ﬁrst tape moves to the left\\nq2 :\\nhead of ﬁrst tape moves to the right; head of second tape moves\\nto the left; until now, all tests were positive\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n4.2.\\nExamples of Turing machines\\n143\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a2 →q0aaRR\\nq1aa →q1aaLN\\nq0b2 →q0bbRR\\nq1ab →q1abLN\\nq022 →q122LL\\nq1ba →q1baLN\\nq1bb →q1bbLN\\nq12a →q22aRN\\nq12b →q22bRN\\nq122 →qaccept\\nq2aa →q2aaRL\\nq2ab →qreject\\nq2ba →qreject\\nq2bb →q2bbRL\\nq222 →qaccept\\nAgain, you should run this Turing machine for some sample inputs.\\n4.2.3\\nAccepting anbncn using one tape\\nWe will construct1 a Turing machine with one tape that accepts the language\\n{anbncn : n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: In the previous examples, the tape alphabet Γ was equal to the union\\nof the input alphabet Σ and {2}. In this example, we will add one symbol\\nd to the tape alphabet. As we will see, this simpliﬁes the construction of\\nthe Turing machine. Thus, the input alphabet is Σ = {a, b, c} and the tape\\nalphabet is Γ = {a, b, c, d, 2}. Recall that the input string w belongs to Σ∗.\\nThe general approach is to split the computation into two stages.\\n1Thanks to Michael Fleming for pointing out an error in a previous version of this\\nconstruction.\\n144\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nStage 1: In this stage, we check if the string w is in the language described\\nby the regular expression a∗b∗c∗. If this is the case, then we walk back to\\nthe leftmost symbol. For this stage, we use the following states, besides the\\nstates qaccept and qreject:\\nqa :\\nstart state; we are reading the block of a’s\\nqb :\\nwe are reading the block of b’s\\nqc :\\nwe are reading the block of c’s\\nqL :\\nwalk to the leftmost symbol\\nStage 2: In this stage, we repeat the following: Walk along the string from\\nleft to right, replace the leftmost a by d, replace the leftmost b by d, replace\\nthe leftmost c by d, and walk back to the leftmost symbol.\\nFor this stage, we use the following states:\\nq′\\na :\\nstart state of Stage 2; search for the leftmost a\\nq′\\nb :\\nleftmost a has been replaced by d;\\nsearch for the leftmost b\\nq′\\nc :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nsearch for the leftmost c\\nq′\\nL :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nleftmost c has been replaced by d;\\nwalk to the leftmost symbol\\nThe transition function δ is speciﬁed by the following instructions:\\nqaa →qaaR\\nqba →qreject\\nqab →qbbR\\nqbb →qbbR\\nqac →qccR\\nqbc →qccR\\nqad →cannot happen\\nqbd →cannot happen\\nqa2 →qL2L\\nqb2 →qL2L\\nqca →qreject\\nqLa →qLaL\\nqcb →qreject\\nqLb →qLbL\\nqcc →qccR\\nqLc →qLcL\\nqcd →cannot happen\\nqLd →cannot happen\\nqc2 →qL2L\\nqL2 →q′\\na2R\\n4.2.\\nExamples of Turing machines\\n145\\nq′\\naa →q′\\nbdR\\nq′\\nba →q′\\nbaR\\nq′\\nab →qreject\\nq′\\nbb →q′\\ncdR\\nq′\\nac →qreject\\nq′\\nbc →qreject\\nq′\\nad →q′\\nadR\\nq′\\nbd →q′\\nbdR\\nq′\\na2 →qaccept\\nq′\\nb2 →qreject\\nq′\\nca →qreject\\nq′\\nLa →q′\\nLaL\\nq′\\ncb →q′\\ncbR\\nq′\\nLb →q′\\nLbL\\nq′\\ncc →q′\\nLdL\\nq′\\nLc →q′\\nLcL\\nq′\\ncd →q′\\ncdR\\nq′\\nLd →q′\\nLdL\\nq′\\nc2 →qreject\\nq′\\nL2 →q′\\na2R\\nWe remark that Stage 1 is really necessary for this Turing machine: If we\\nomit this stage, and use only Stage 2, then the string aabcbc will be accepted.\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2}\\nWe consider again the language {anbncn : n ≥0}. In the previous section,\\nwe presented a Turing machine that uses an extra symbol d. The reader may\\nwonder if we can construct a Turing machine for this language that does not\\nuse any extra symbols. We will show below that this is indeed possible.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate q0.\\nIdea: Repeat the following Stages 1 and 2, until the string is empty.\\nStage 1. Walk along the string from left to right, delete the leftmost a,\\ndelete the leftmost b, and delete the rightmost c.\\nStage 2. Shift the substring of bs and cs one position to the left; then walk\\nback to the leftmost symbol.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, 2}.\\n146\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nFor Stage 1, we use the following states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost a has been deleted; have not read b\\nqb :\\nleftmost b has been deleted; have not read c\\nqc :\\nleftmost c has been read; tape head moves to the right\\nq′\\nc :\\ntape head is on the rightmost c\\nq1 :\\nrightmost c has been deleted; tape head is on the rightmost\\nsymbol or 2\\nqaccept :\\naccept state\\nqreject :\\nreject state\\nThe transitions for Stage 1 are speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nq0b →qreject\\nqab →qb2R\\nq0c →qreject\\nqac →qreject\\nq02 →qaccept\\nqa2 →qreject\\nqba →qreject\\nqca →qreject\\nqbb →qbbR\\nqcb →qreject\\nqbc →qccR\\nqcc →qccR\\nqb2 →qreject\\nqc2 →q′\\nc2L\\nq′\\ncc →q12L\\nFor Stage 2, we use the following states:\\nq1 :\\nas above; tape head is on the rightmost symbol or on 2\\nqc :\\ncopy c one cell to the left\\nqb :\\ncopy b one cell to the left\\nq2 :\\ndone with shifting; head moves to the left\\nAdditionally, we use a state q′\\n1 which has the following meaning: If the input\\nstring is of the form aibc, for some i ≥1, then after Stage 1, the tape contains\\nthe string ai−122, the tape head is on the 2 immediately to the right of the\\nas, and the Turing machine is in state q1. In this case, we move one cell to\\nthe left; if we then read 2, then i = 1, and we accept; otherwise, we read a,\\nand we reject.\\n4.2.\\nExamples of Turing machines\\n147\\nThe transitions for Stage 2 are speciﬁed by the following instructions:\\nq1a →cannot happen\\nq′\\n1a →qreject\\nq1b →qreject\\nq′\\n1b →cannot happen\\nq1c →qc2L\\nq′\\n1c →cannot happen\\nq12 →q′\\n12L\\nq′\\n12 →qaccept\\nqca →cannot happen\\nqba →cannot happen\\nqcb →qbcL\\nqbb →qbbL\\nqcc →qccL\\nqbc →cannot happen\\nqc2 →qreject\\nqb2 →q2bL\\nq2a →q2aL\\nq2b →cannot happen\\nq2c →cannot happen\\nq22 →q02R\\n4.2.5\\nAccepting ambncmn using one tape\\nWe will sketch how to construct a Turing machine with one tape that accepts\\nthe language\\n{ambncmn : m ≥0, n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, $, 2},\\nwhere the purpose of the symbol $ will become clear below.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: Observe that a string ambnck is in the language if and only if for every\\na, the string contains n many cs. Based on this, the computation consists of\\nthe following stages:\\nStage 1. Walk along the input string w from left to right and check whether\\nw is an element of the language described by the regular expression a∗b∗c∗.\\nIf this is not the case, then reject the input string. Otherwise, go to Stage 2.\\nStage 2. Walk back to the leftmost symbol of w. Go to Stage 3.\\nStage 3. In this stage, the Turing machine does the following:\\n148\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• Replace the leftmost a by the blank symbol 2.\\n• Walk to the leftmost b.\\n• Zigzag between the bs and cs; each time, replace the leftmost b by the\\nsymbol $, and replace the rightmost c by the blank symbol 2. If, for\\nsome b, there is no c left, the Turing machine rejects the input string.\\n• Continue zigzagging until there are no bs left. Then go to Stage 4.\\nObserve that in this third stage, the string ambnck is transformed to the\\nstring am−1$nck−n.\\nStage 4. In this stage, the Turing machine does the following:\\n• Replace each $ by b.\\n• Walk to the leftmost a.\\nHence, in this fourth stage, the string am−1$nck−n is transformed to the string\\nam−1bnck−n.\\nObserve that the input string ambnck is in the language if and only if the\\nstring am−1bnck−n is in the language. Therefore, the Turing machine repeats\\nStages 3 and 4, until there are no as left. At that moment, it checks whether\\nthere are any cs left; if so, it rejects the input string; otherwise, it accepts\\nthe input string.\\nWe hope that you believe that this description of the algorithm can be\\nturned into a formal description of a Turing machine.\\n4.3\\nMulti-tape Turing machines\\nIn Section 4.2, we have seen two Turing machines that accept palindromes;\\nthe ﬁrst Turing machine has one tape, whereas the second one has two tapes.\\nYou will have noticed that the two-tape Turing machine was easier to obtain\\nthan the one-tape Turing machine. This leads to the question whether multi-\\ntape Turing machines are more powerful than their one-tape counterparts.\\nThe answer is “no”:\\nTheorem 4.3.1 Let k ≥1 be an integer. Any k-tape Turing machine can\\nbe converted to an equivalent one-tape Turing machine.\\n4.3.\\nMulti-tape Turing machines\\n149\\nProof.2\\nWe will sketch the proof for the case when k = 2.\\nLet M =\\n(Σ, Γ, Q, δ, q, qaccept, qreject) be a two-tape Turing machine.\\nOur goal is to\\nconvert M to an equivalent one-tape Turing machine N. That is, N should\\nhave the property that for all strings w ∈Σ∗,\\n• M accepts w if and only if N accepts w,\\n• M rejects w if and only if N rejects w,\\n• M does not terminate on input w if and only if N does not terminate\\non input w.\\nThe tape alphabet of the one-tape Turing machine N is\\nΓ ∪{ ˙x : x ∈Γ} ∪{#}.\\nIn words, we take the tape alphabet Γ of M, and add, for each x ∈Γ, the\\nsymbol ˙x. Moreover, we add a special symbol #.\\nThe Turing machine N will be deﬁned in such a way that any conﬁgura-\\ntion of the two-tape Turing machine M, for example\\n. . . 2 1 0 0 1 2 . . .\\n6\\n. . . 2 a a b a 2 . . .\\n6\\ncorresponds to the following conﬁguration of the one-tape Turing machine\\nN:\\n. . .\\n2\\n#\\n1\\n0\\n˙0\\n1\\n#\\na\\n˙a\\nb\\na\\n#\\n2 . . .\\n6\\n2Thanks to Sergio Cabello for pointing out an error in a previous version of this proof.\\n150\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThus, the contents of the two tapes of M are encoded on the single tape of\\nN. The dotted symbols are used to indicate the positions of the two tape\\nheads of M, whereas the three occurrences of the special symbol # are used\\nto mark the boundaries of the strings on the two tapes of M.\\nThe Turing machine N simulates one computation step of M, in the\\nfollowing way:\\n• Throughout the simulation of this step, N “remembers” the current\\nstate of M.\\n• At the start of the simulation, the tape head of N is on the leftmost\\nsymbol #.\\n• N walks along the string to the right until it ﬁnds the ﬁrst dotted\\nsymbol. (This symbol indicates the location of the head on the ﬁrst tape\\nof M.) N remembers this ﬁrst dotted symbol and continues walking\\nto the right until it ﬁnds the second dotted symbol.\\n(This symbol\\nindicates the location of the head on the second tape of M.) Again, N\\nremembers this second dotted symbol.\\n• At this moment, N is still at the second dotted symbol. N updates\\nthis part of the tape, by making the change that M would make on its\\nsecond tape. (This change is given by the transition function of M; it\\ndepends on the current state of M and the two symbols that M reads\\non its two tapes.)\\n• N walks to the left until it ﬁnds the ﬁrst dotted symbol.\\nThen, it\\nupdates this part of the tape, by making the change that M would\\nmake on its ﬁrst tape.\\n• In the previous two steps, in which the tape is updated, it may be\\nnecessary to shift a part of the tape.\\n• Finally, N remembers the new state of M and walks back to the left-\\nmost symbol #.\\nIt should be clear that the Turing machine N can be constructed by\\nintroducing appropriate states.\\n4.4.\\nThe Church-Turing Thesis\\n151\\n4.4\\nThe Church-Turing Thesis\\nWe all have some intuitive notion of what an algorithm is. This notion will\\nprobably be something like “an algorithm is a procedure consisting of com-\\nputation steps that can be speciﬁed in a ﬁnite amount of text”. For example,\\nany “computational process” that can be speciﬁed by a Java program, should\\nbe considered an algorithm. Similarly, a Turing machine speciﬁes a “com-\\nputational process” and, therefore, should be considered an algorithm. This\\nleads to the question of whether it is possible to give a mathematical deﬁni-\\ntion of an “algorithm”. We just saw that every Java program represents an\\nalgorithm and that every Turing machine also represents an algorithm. Are\\nthese two notions of an algorithm equivalent? The answer is “yes”. In fact,\\nthe following theorem states that many diﬀerent notions of “computational\\nprocess” are equivalent. (We hope that you have gained suﬃcient intuition,\\nso that none of the claims in this theorem comes as a surprise to you.)\\nTheorem 4.4.1 The following computation models are equivalent, i.e., any\\none of them can be converted to any other one:\\n1. One-tape Turing machines.\\n2. k-tape Turing machines, for any k ≥1.\\n3. Non-deterministic Turing machines.\\n4. Java programs.\\n5. C++ programs.\\n6. Lisp programs.\\nIn other words, if we deﬁne the notion of an algorithm using any of the\\nmodels in this theorem, then it does not matter which model we take: All\\nthese models give the same notion of an algorithm.\\nThe problem of deﬁning the notion of an algorithm goes back to David\\nHilbert. On August 8, 1900, at the Second International Congress of Math-\\nematicians in Paris, Hilbert presented a list of problems that he considered\\ncrucial for the further development of mathematics. Hilbert’s 10th problem\\nis the following:\\n152\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nDoes there exist a ﬁnite process that decides whether or not any\\ngiven polynomial with integer coeﬃcients has integral roots?\\nOf course, in our language, Hilbert asked whether or not there exists an\\nalgorithm that decides, when given an arbitrary polynomial equation (with\\ninteger coeﬃcients) such as\\n12x3y7z5 + 7x2y4z −x4 + y2z7 −z3 + 10 = 0,\\nwhether or not this equation has a solution in integers. In 1970, Matiyasevich\\nproved that such an algorithm does not exist. Of course, in order to prove\\nthis claim, we ﬁrst have to agree on what an algorithm is. In the beginning\\nof the twentieth century, mathematicians gave several deﬁnitions, such as\\nTuring machines (1936) and the λ-calculus (1936), and they proved that all\\nthese are equivalent. Later, after programming languages were invented, it\\nwas shown that these older notions of an algorithm are equivalent to notions\\nof an algorithm that are based on C programs, Java programs, Lisp programs,\\nPascal programs, etc.\\nIn other words, all attempts to give a rigorous deﬁnition of the notion of\\nan algorithm led to the same concept. Because of this, computer scientists\\nnowadays agree on what is called the Church-Turing Thesis:\\nChurch-Turing Thesis:\\nEvery computational process that is intuitively\\nconsidered to be an algorithm can be converted to a Turing machine.\\nIn other words, this basically states that we deﬁne an algorithm to be a\\nTuring machine. At this point, you should ask yourself, whether the Church-\\nTuring Thesis can be proved. Alternatively, what has to be done in order to\\ndisprove this thesis?\\nExercises\\n4.1 Construct a Turing machine with one tape, that accepts the language\\n{02n1n : n ≥0}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\nExercises\\n153\\n4.2 Construct a Turing machine with one tape, that accepts the language\\n{w : w contains twice as many 0s as 1s}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\n4.3 Let A be the language\\nA\\n=\\n{ w ∈{a, b, c}∗\\n:\\nw contains more bs than as and\\nw contains more cs than as }.\\nGive an informal description (in plain English) of a Turing machine with one\\ntape, that accepts the language A.\\n4.4 Construct a Turing machine with one tape that receives as input a non-\\nnegative integer x and returns as output the integer x + 1.\\nIntegers are\\nrepresented as binary strings.\\nStart of the computation: The tape contains the binary representation\\nof the input x. The tape head is on the leftmost symbol and the Turing\\nmachine is in the start state q0. For example, if x = 431, the tape looks as\\nfollows:\\n. . . 2 2 2 1 1 0 1 0 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x + 1. The tape head is on the leftmost symbol and the Turing\\nmachine is in the ﬁnal state q1. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 0 1 1 0 0 0 0 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state q1. As soon as state q1 is entered,\\nthe Turing machine terminates. At termination, the contents of the tape is\\nthe output of the Turing machine.\\n154\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n4.5 Construct a Turing machine with two tapes that receives as input two\\nnon-negative integers x and y, and returns as output the integer x + y.\\nIntegers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost bit\\nof y. At the start, the Turing machine is in the start state q0.\\nEnd of the computation: The ﬁrst tape contains the binary representation\\nof x and its head is on the rightmost symbol of x. The second tape contains\\nthe binary representation of the integer x + y (thus, the integer y is “gone”).\\nThe head of the second tape is on the rightmost bit of x + y. The Turing\\nmachine is in the ﬁnal state q1.\\n4.6 Give an informal description (in plain English) of a Turing machine with\\none tape that receives as input two non-negative integers x and y, and returns\\nas output the integer x+y. Integers are represented as binary strings. If you\\nare an adventurous student, you may give a formal deﬁnition of your Turing\\nmachine.\\n4.7 Construct a Turing machine with one tape that receives as input an\\ninteger x ≥1 and returns as output the integer x−1. Integers are represented\\nin binary.\\nStart of the computation: The tape contains the binary representation of\\nthe input x. The tape head is on the rightmost symbol of x and the Turing\\nmachine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x −1. The tape head is on the rightmost bit of x −1 and the\\nTuring machine is in the ﬁnal state q1.\\n4.8 Give an informal description (in plain English) of a Turing machine with\\nthree tapes that receives as input two non-negative integers x and y, and\\nreturns as output the integer xy. Integers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost sym-\\nbol of y. The third tape is empty and its head is at an arbitrary location.\\nThe Turing machine is in the start state q0.\\nExercises\\n155\\nEnd of the computation: The ﬁrst and second tapes are empty. The third\\ntape contains the binary representation of the product xy and its head is on\\nthe rightmost bit of xy. The Turing machine is in the ﬁnal state q1.\\nHint: Use the Turing machines of Exercises 4.5 and 4.7.\\n4.9 Construct a Turing machine with one tape that receives as input a string\\nof the form 1n for some integer n ≥0; thus, the input is a string of n many\\n1s. The output of the Turing machine is the string 1n21n. Thus, this Turing\\nmachine makes a copy of its input.\\nThe input alphabet is Σ = {1} and the tape alphabet is Γ = {1, 2}.\\nStart of the computation: The tape contains a string of the form 1n, for\\nsome integer n ≥0, the tape head is on the leftmost symbol, and the Turing\\nmachine is in the start state. For example, if n = 4, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the string 1n21n, the tape\\nhead is on the 2 in the middle of this string, and the Turing machine is in\\nthe ﬁnal state. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 1 1 1 1 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this state is entered, the\\nTuring machine terminates. At termination, the contents of the tape is the\\noutput of the Turing machine.\\n156\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nChapter 5\\nDecidable and Undecidable\\nLanguages\\nWe have seen in Chapter 4 that Turing machines form a model for “everything\\nthat is intuitively computable”. In this chapter, we consider the limitations\\nof Turing machines. That is, we ask ourselves the question whether or not\\n“everything” is computable. As we will see, the answer is “no”. In fact, we\\nwill even see that “most” problems are not solvable by Turing machines and,\\ntherefore, not solvable by computers.\\n5.1\\nDecidability\\nIn Chapter 4, we have deﬁned when a Turing machine accepts an input string\\nand when it rejects an input string. Based on this, we deﬁne the following\\nclass of languages.\\nDeﬁnition 5.1.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is decidable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the reject state.\\n158\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is decidable, if there exists an algorithm\\nthat (i) terminates on every input string w, and (ii) correctly tells us whether\\nw ∈A or w ̸∈A.\\nA language A that is not decidable is called undecidable.\\nFor such a\\nlanguage, there does not exist an algorithm that satisﬁes (i) and (ii) above.\\nIn Section 4.2, we have seen several examples of languages that are de-\\ncidable.\\nIn the following subsections, we will give some examples of decidable and\\nundecidable languages. These examples involve languages A whose elements\\nare pairs of the form (C, w), where C is some computation model (for ex-\\nample, a deterministic ﬁnite automaton) and w is a string over the alphabet\\nΣ. The pair (C, w) is in the language A if and only if the string w is in the\\nlanguage of the computation model C. For diﬀerent computation models C,\\nwe will ask the question whether A is decidable, i.e., whether an algorithm\\nexists that decides, for any input (C, w), whether or not this input belongs\\nto the language A. Since the input to any algorithm is a string over some\\nalphabet, we must encode the pair (C, w) as a string. In all cases that we\\nconsider, such a pair can be described using a ﬁnite amount of text. There-\\nfore, we assume, without loss of generality, that binary strings are used for\\nthese encodings. Throughout the rest of this chapter, we will denote the\\nbinary encoding of a pair (C, w) by\\n⟨C, w⟩.\\n5.1.1\\nThe language ADFA\\nWe deﬁne the following language:\\nADFA = {⟨M, w⟩:\\nM is a deterministic ﬁnite automaton that\\naccepts the string w}.\\nKeep in mind that ⟨M, w⟩denotes the binary string that forms an en-\\ncoding of the ﬁnite automaton M and the string w that is given as input to\\nM.\\nWe claim that the language ADFA is decidable. In order to prove this,\\nwe have to construct an algorithm with the following property, for any given\\ninput string u:\\n• If u is the encoding of a deterministic ﬁnite automaton M and a string\\nw (i.e., u is in the correct format ⟨M, w⟩), and if M accepts w, then\\n5.1.\\nDecidability\\n159\\nthe algorithm terminates in its accept state.\\n• In all other cases, the algorithm terminates in its reject state.\\nAn algorithm that exactly does this, is easy to obtain: On input u, the algo-\\nrithm ﬁrst checks whether or not u encodes a deterministic ﬁnite automaton\\nM and a string w. If this is not the case, then it terminates and rejects\\nthe input string u. Otherwise, the algorithm “constructs” M and w, and\\nthen simulates the computation of M on the input string w. If M accepts\\nw, then the algorithm terminates and accepts the input string u. If M does\\nnot accept w, then the algorithm terminates and rejects the input string u.\\nThus, we have proved the following result:\\nTheorem 5.1.2 The language ADFA is decidable.\\n5.1.2\\nThe language ANFA\\nWe deﬁne the following language:\\nANFA = {⟨M, w⟩:\\nM is a nondeterministic ﬁnite automaton that\\naccepts the string w}.\\nTo prove that this language is decidable, consider the algorithm that\\ndoes the following: On input u, the algorithm ﬁrst checks whether or not\\nu encodes a nondeterministic ﬁnite automaton M and a string w. If this is\\nnot the case, then it terminates and rejects the input string u. Otherwise,\\nthe algorithm constructs M and w. Since a computation of M (on input w)\\nis not unique, the algorithm ﬁrst converts M to an equivalent deterministic\\nﬁnite automaton N. Then, it proceeds as in Section 5.1.1.\\nObserve that the construction for converting a nondeterministic ﬁnite au-\\ntomaton to a deterministic ﬁnite automaton (see Section 2.5) is algorithmic,\\nin the sense that it can be described by an algorithm. Because of this, the\\nalgorithm described above is a valid algorithm; it accepts all strings u that\\nare in ANFA, and it rejects all strings u that are not in ANFA. Thus, we have\\nproved the following result:\\nTheorem 5.1.3 The language ANFA is decidable.\\n160\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.1.3\\nThe language ACFG\\nWe deﬁne the following language:\\nACFG = {⟨G, w⟩: G is a context-free grammar such that w ∈L(G)}.\\nWe claim that this language is decidable. In order to prove this claim, con-\\nsider a string u that encodes a context-free grammar G = (V, Σ, S, R) and a\\nstring w ∈Σ∗. Deciding whether or not w ∈L(G) is equivalent to deciding\\nwhether or not S\\n∗⇒w. A ﬁrst idea to decide this is by trying all possible\\nderivations that start with the start variable S and that use rules of R. The\\nproblem is that, in case w ̸∈L(G), it is not clear how many such derivations\\nhave to be checked before we can be sure that w is not in the language of\\nG: If w ∈L(G), then it may be that w can be derived from S, only by ﬁrst\\nderiving a very long string, say v, and then use rules to shorten it so as to\\nobtain the string w. Since there is no obvious upper bound on the length of\\nthe string v, we have to be careful.\\nThe trick is to do the following. First, convert the grammar G to an\\nequivalent grammar G′ in Chomsky normal form. (The construction given\\nin Section 3.4 can be described by an algorithm.) Let n be the length of the\\nstring w. Then, if w ∈L(G) = L(G′), any derivation of w in G′, from the\\nstart variable of G′, consists of exactly 2n−1 steps (where a “step” is deﬁned\\nas applying one rule of G′). Hence, we can decide whether or not w ∈L(G),\\nby trying all possible derivations, in G′, consisting of 2n −1 steps. If one of\\nthese (ﬁnite number of) derivations leads to the string w, then w ∈L(G).\\nOtherwise, w ̸∈L(G). Thus, we have proved the following result:\\nTheorem 5.1.4 The language ACFG is decidable.\\nIn fact, the arguments above imply the following result:\\nTheorem 5.1.5 Every context-free language is decidable.\\nProof. Let Σ be an alphabet and let A ⊆Σ∗be an arbitrary context-free\\nlanguage. There exists a context-free grammar in Chomsky normal form,\\nwhose language is equal to A. Given an arbitrary string w ∈Σ∗, we have\\nseen above how we can decide whether or not w can be derived from the\\nstart variable of this grammar.\\n5.1.\\nDecidability\\n161\\n5.1.4\\nThe language ATM\\nAfter having seen the languages ADFA, ANFA, and ACFG, it is natural to\\nconsider the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nWe will prove that this language is undecidable. Before we give the proof,\\nlet us mention what this means:\\nThere is no algorithm that, when given an arbitrary algorithm M\\nand an arbitrary input string w for M, decides in a ﬁnite amount\\nof time, whether or not M accepts w.\\nThe proof of the claim that ATM is undecidable is by contradiction. Thus,\\nwe assume that ATM is decidable. Then there exists a Turing machine H\\nthat has the following property. For every input string ⟨M, w⟩for H:\\n• If ⟨M, w⟩∈ATM (i.e., M accepts w), then H terminates in its accept\\nstate.\\n• If ⟨M, w⟩̸∈ATM (i.e., M rejects w or M does not terminate on input\\nw), then H terminates in its reject state.\\n• In particular, H terminates on any input ⟨M, w⟩.\\nWe construct a new Turing machine D, that does the following: On input\\n⟨M⟩, the Turing machine D uses H as a subroutine to determine what M\\ndoes when it is given its own description as input. Once D has determined\\nthis information, it does the opposite of what H does.\\nTuring machine D: On input ⟨M⟩, where M is a Turing machine,\\nthe new Turing machine D does the following:\\nStep 1: Run the Turing machine H on the input ⟨M, ⟨M⟩⟩.\\nStep 2:\\n• If H terminates in its accept state, then D terminates in its\\nreject state.\\n• If H terminates in its reject state, then D terminates in its\\naccept state.\\n162\\nChapter 5.\\nDecidable and Undecidable Languages\\nFirst observe that this new Turing machine D terminates on any input\\nstring ⟨M⟩, because H terminates on every input. Next observe that, for any\\ninput string ⟨M⟩for D:\\n• If ⟨M, ⟨M⟩⟩∈ATM (i.e., M accepts ⟨M⟩), then D terminates in its\\nreject state.\\n• If ⟨M, ⟨M⟩⟩̸∈ATM (i.e., M rejects ⟨M⟩or M does not terminate on\\ninput ⟨M⟩), then D terminates in its accept state.\\nThis means that for any string ⟨M⟩:\\n• If M accepts ⟨M⟩, then D rejects ⟨M⟩.\\n• If M rejects ⟨M⟩or M does not terminate on input ⟨M⟩, then D\\naccepts ⟨M⟩.\\nWe now consider what happens if we give the Turing machine D the string\\n⟨D⟩as input, i.e., we take M = D:\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩or D does not terminate on input ⟨D⟩, then D accepts\\n⟨D⟩.\\nSince D terminates on every input string, this means that\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩, then D accepts ⟨D⟩.\\nThis is clearly a contradiction. Therefore, the Turing machine H that decides\\nthe language ATM cannot exist and, thus, ATM is undecidable. We have\\nproved the following result:\\nTheorem 5.1.6 The language ATM is undecidable.\\n5.1.\\nDecidability\\n163\\n5.1.5\\nThe Halting Problem\\nWe deﬁne the following language:\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}.\\nTheorem 5.1.7 The language Halt is undecidable.\\nProof. The proof is by contradiction. Thus, we assume that the language\\nHalt is decidable. Then there exists a Java program H that takes as input a\\nstring of the form ⟨P, w⟩, where P is an arbitrary Java program and w is an\\narbitrary input for P. The program H has the following property:\\n• If ⟨P, w⟩∈Halt (i.e., program P terminates on input w), then H\\noutputs true.\\n• If ⟨P, w⟩̸∈Halt (i.e., program P does not terminate on input w), then\\nH outputs false.\\n• In particular, H terminates on any input ⟨P, w⟩.\\nWe will write the output of H as H(P, w). Moreover, we will denote by P(w)\\nthe computation obtained by running the program P on the input w. Hence,\\nH(P, w) =\\n\\x1a true\\nif P(w) terminates,\\nfalse\\nif P(w) does not terminate.\\nConsider the following algorithm Q, which takes as input the encoding\\n⟨P⟩of an arbitrary Java program P:\\nAlgorithm Q(⟨P⟩):\\nwhile H(P, ⟨P⟩) = true\\ndo have a beer\\nendwhile\\nSince H is a Java program, this new algorithm Q can also be written as\\na Java program. Observe that\\nQ(⟨P⟩) terminates if and only if H(P, ⟨P⟩) = false.\\n164\\nChapter 5.\\nDecidable and Undecidable Languages\\nThis means that for every Java program P,\\nQ(⟨P⟩) terminates if and only if P(⟨P⟩) does not terminate.\\n(5.1)\\nWhat happens if we run the Java program Q on the input string ⟨Q⟩?\\nIn other words, what happens if we run Q(⟨Q⟩)? Then, in (5.1), we have to\\nreplace all occurrences of P by Q. Hence,\\nQ(⟨Q⟩) terminates if and only if Q(⟨Q⟩) does not terminate.\\nThis is obviously a contradiction, and we can conclude that the Java program\\nH does not exist. Therefore, the language Halt is undecidable.\\nRemark 5.1.8 In this proof, we run the Java program Q on the input ⟨Q⟩.\\nThis means that the input to Q is a description of itself. In other words, we\\ngive Q itself as input. This is an example of what is called self-reference. An-\\nother example of self-reference can be found in Remark 5.1.8 of the textbook\\nIntroduction to Theory of Computation by A. Maheshwari and M. Smid.\\n5.2\\nCountable sets\\nThe proofs that we gave in Sections 5.1.4 and 5.1.5 seem to be bizarre. In\\nthis section, we will convince you that these proofs in fact use a technique\\nthat you have seen in the course COMP 1805: Cantor’s Diagonalization.\\nLet A and B be two sets and let f : A →B be a function. Recall that f\\nis called a bijection, if\\n• f is one-to-one (or injective), i.e., for any two distinct elements a and\\na′ in A, we have f(a) ̸= f(a′), and\\n• f is onto (or surjective), i.e., for each element b ∈B, there exists an\\nelement a ∈A, such that f(a) = b.\\nThe set of natural numbers is denoted by N. That is, N = {1, 2, 3, . . .}.\\nDeﬁnition 5.2.1 Let A and B be two sets. We say that A and B have the\\nsame size, if there exists a bijection f : A →B.\\nDeﬁnition 5.2.2 Let A be a set. We say that A is countable, if A is ﬁnite,\\nor A and N have the same size.\\n5.2.\\nCountable sets\\n165\\nIn other words, if A is an inﬁnite and countable set, then there exists a\\nbijection f : N →A, and we can write A as\\nA = {f(1), f(2), f(3), f(4), . . .}.\\nSince f is a bijection, every element of A occurs exactly once in the set on\\nthe right-hand side. This means that we can number the elements of A using\\nthe positive integers: Every element of A receives a unique number.\\nTheorem 5.2.3 The following sets are countable:\\n1. The set Z of integers:\\nZ = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n2. The Cartesian product N × N:\\nN × N = {(m, n) : m ∈N, n ∈N}.\\n3. The set Q of rational numbers:\\nQ = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\nProof. To prove that the set Z is countable, we have to give each element of\\nZ a unique number in N. We obtain this numbering, by listing the elements\\nof Z in the following order:\\n0, 1, −1, 2, −2, 3, −3, 4, −4, . . .\\nIn this (inﬁnite) list, every element of Z occurs exactly once. The number of\\nan element of Z is given by its position in this list.\\nFormally, deﬁne the function f : N →Z by\\nf(n) =\\n\\x1a n/2\\nif n is even,\\n−(n −1)/2\\nif n is odd.\\nThis function f is a bijection and, therefore, the sets N and Z have the same\\nsize. Hence, the set Z is countable.\\nFor the proofs of the other two claims, we refer to the course COMP 1805.\\nWe now use Cantor’s Diagonalization principle to prove that the set of\\nreal numbers is not countable:\\n166\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.2.4 The set R of real numbers is not countable.\\nProof. Deﬁne\\nA = {x ∈R : 0 ≤x < 1}.\\nWe will prove that the set A is not countable. This will imply that the set\\nR is not countable, because A ⊆R.\\nThe proof that A is not countable is by contradiction. So we assume that\\nA is countable. Then there exists a bijection f : N →A. Thus, for each\\nn ∈N, f(n) is a real number between zero and one. We can write\\nA = {f(1), f(2), f(3), . . .},\\n(5.2)\\nwhere every element of A occurs exactly once in the set on the right-hand\\nside.\\nConsider the real number f(1). We can write this number in decimal\\nnotation as\\nf(1) = 0.d11d12d13 . . . ,\\nwhere each d1i is a digit in the set {0, 1, 2, . . . , 9}. In general, for every n ∈N,\\nwe can write the real number f(n) as\\nf(n) = 0.dn1dn2dn3 . . . ,\\nwhere, again, each dni is a digit in {0, 1, 2, . . . , 9}.\\nWe deﬁne the real number\\nx = 0.d1d2d3 . . . ,\\nwhere, for each integer n ≥1,\\ndn =\\n\\x1a 4\\nif dnn ̸= 4,\\n5\\nif dnn = 4.\\nObserve that x is a real number between zero and one, i.e., x ∈A. Therefore,\\nby (5.2), there is an element n ∈N, such that f(n) = x. We compare the\\nn-th digits of f(n) and x:\\n• The n-th digit of f(n) is equal to dnn.\\n• The n-th digit of x is equal to dn.\\n5.2.\\nCountable sets\\n167\\nSince f(n) and x are equal, their n-th digits must be equal, i.e., dnn = dn.\\nBut, by the deﬁnition of dn, we have dnn ̸= dn. This is a contradiction and,\\ntherefore, the set A is not countable.\\nNotice how we deﬁned the real number x: For each n ≥1, the n-th digit\\nof x is not equal to the n-th digit of f(n). Therefore, for each n ≥1, x ̸= f(n)\\nand, thus, x ̸∈A.\\nThe ﬁnal result of this section is the fact that for every set A, its power\\nset\\nP(A) = {B : B ⊆A}\\nis “strictly larger” than A. Deﬁne the function f : A →P(A) by\\nf(a) = {a},\\nfor any a in A. Since f is one-to-one, we can say that P(A) is “at least as\\nlarge as” A.\\nTheorem 5.2.5 Let A be an arbitrary set. Then A and P(A) do not have\\nthe same size.\\nProof. The proof is by contradiction. Thus, we assume that there exists a\\nbijection g : A →P(A). Deﬁne the set B as\\nB = {a ∈A : a ̸∈g(a)}.\\nSince B ∈P(A) and g is a bijection, there exists an element a in A such that\\ng(a) = B.\\nFirst assume that a ∈B. Since g(a) = B, we have a ∈g(a). But then,\\nfrom the deﬁnition of the set B, we have a ̸∈B, which is a contradiction.\\nNext assume that a ̸∈B.\\nSince g(a) = B, we have a ̸∈g(a).\\nBut\\nthen, from the deﬁnition of the set B, we have a ∈B, which is again a\\ncontradiction.\\nWe conclude that the bijection g does not exist. Therefore, A and P(A)\\ndo not have the same size.\\n168\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.2.1\\nThe Halting Problem revisited\\nNow that we know about countability, we give a diﬀerent way to look at the\\nproof in Section 5.1.5 of the fact that the language\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}\\nis undecidable.\\nYou should convince yourself that the proof given below\\nfollows the same reasoning as the one used in the proof of Theorem 5.2.4.\\nWe ﬁrst argue that the set of all Java programs is countable. Indeed,\\nevery Java program P can be described by a ﬁnite amount of text. In fact,\\nwe have been using ⟨P⟩to denote such a description by a binary string. For\\nany integer n ≥0, there are at most 2n (i.e., ﬁnitely many) Java programs\\nP whose description ⟨P⟩has length n. Therefore, to obtain a list of all Java\\nprograms, we do the following:\\n• List all Java programs P whose description ⟨P⟩has length 0. (Well,\\nthe empty string does not describe any Java program, so in this step,\\nnothing happens.)\\n• List all Java programs P whose description ⟨P⟩has length 1.\\n• List all Java programs P whose description ⟨P⟩has length 2.\\n• List all Java programs P whose description ⟨P⟩has length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every Java program occurs exactly once. Therefore, the\\nset of all Java programs is countable.\\nConsider an inﬁnite list\\nP1, P2, P3, . . .\\nin which every Java program occurs exactly once.\\nAssume that the language Halt is decidable. Then there exists a Java\\nprogram H that decides this language. We may assume that, on input ⟨P, w⟩,\\nH returns true if P terminates on input w, and false if P does not terminate\\non input w.\\nWe construct a new Java program D that does the following:\\n5.3.\\nRice’s Theorem\\n169\\nAlgorithm D:\\nOn input ⟨Pn⟩, where n is a positive integer, the\\nnew Java program D does the following:\\nStep 1: Run the Java program H on the input ⟨Pn, ⟨Pn⟩⟩.\\nStep 2:\\n• If H returns true, then D goes into an inﬁnite loop.\\n• If H returns false, then D returns true and terminates its com-\\nputation.\\nObserve that D can be written as a Java program. Therefore, there exists\\nan integer n ≥1 such that D = Pn. The next two observations follow from\\nthe pseudocode:\\n• If D terminates on input ⟨Pn⟩, then H returns false on input ⟨Pn, ⟨Pn⟩⟩,\\ni.e., Pn does not terminate on input ⟨Pn⟩.\\n• If D does not terminate on input ⟨Pn⟩, then H returns true on input\\n⟨Pn, ⟨Pn⟩⟩, i.e., Pn terminates on input ⟨Pn⟩.\\nThus,\\n• D terminates on input ⟨Pn⟩if and only if Pn does not terminate on\\ninput ⟨Pn⟩.\\nSince D = Pn, this becomes\\n• D terminates on input ⟨D⟩if and only if D does not terminate on input\\n⟨D⟩.\\nThus, we have obtained a contradiction.\\nRemark 5.2.6 We deﬁned the Java program D in such a way that, for each\\nn ≥1, the computation of D on input ⟨Pn⟩diﬀers from the computation of\\nPn on input ⟨Pn⟩. Hence, for each n ≥1, D ̸= Pn. However, since D is a\\nJava program, there must be an integer n ≥1 such that D = Pn.\\n170\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.3\\nRice’s Theorem\\nWe have seen two examples of undecidable languages: ATM and Halt. In this\\nsection, we prove that many languages involving Turing machines (or Java\\nprograms) are undecidable.\\nDeﬁne T to be the set of binary encodings of all Turing machines, i.e.,\\nT = {⟨M⟩: M is a Turing machine with input alphabet {0,1}}.\\nTheorem 5.3.1 (Rice) Let P be a subset of T such that\\n1. P ̸= ∅, i.e., there exists a Turing machine M such that ⟨M⟩∈P,\\n2. P is a proper subset of T , i.e., there exists a Turing machine N such\\nthat ⟨N⟩̸∈P, and\\n3. for any two Turing machines M1 and M2 with L(M1) = L(M2),\\n(a) either both ⟨M1⟩and ⟨M2⟩are in P or\\n(b) none of ⟨M1⟩and ⟨M2⟩is in P.\\nThen the language P is undecidable.\\nYou can think of P as the set of all Turing machines that satisfy a certain\\nproperty. The ﬁrst two conditions state that at least one Turing machine\\nsatisﬁes this property and not all Turing machines satisfy this property. The\\nthird condition states that, for any Turing machine M, whether or not M\\nsatisﬁes this property only depends on the language L(M) of M.\\nHere are some examples of languages that satisfy the conditions in Rice’s\\nTheorem:\\nP1 = {⟨M⟩: M is a Turing machine and ϵ ∈L(M)},\\nP2 = {⟨M⟩: M is a Turing machine and L(M) = {1011, 001100}},\\nP3 = {⟨M⟩: M is a Turing machine and L(M) is a regular language}.\\nYou are encouraged to verify that Rice’s Theorem indeed implies that each\\nof P1, P2, and P3 is undecidable.\\n5.3.\\nRice’s Theorem\\n171\\n5.3.1\\nProof of Rice’s Theorem\\nThe strategy of the proof is as follows: Assuming that the language P is\\ndecidable, we show that the language\\nHalt = {⟨M, w⟩:\\nM is a Turing machine that terminates on\\nthe input string w}\\nis decidable. This will contradict Theorem 5.1.7.\\nThe assumption that P is decidable implies the existence of a Turing\\nmachine H that decides P. Observe that H takes as input a binary string\\n⟨M⟩encoding a Turing machine M. In order to show that Halt is decidable,\\nwe need a Turing machine that takes as input a binary string ⟨M, w⟩encoding\\na Turing machine M and a binary string w. In the rest of this section, we\\nwill explain how this Turing machine can be obtained.\\nLet M1 be a Turing machine that, for any input string, switches in its\\nﬁrst computation step from its start state to its reject state. In other words,\\nM1 is a Turing machine with L(M1) = ∅. We assume that\\n⟨M1⟩̸∈P.\\n(At the end of the proof, we will consider the case when ⟨M1⟩∈P.) We also\\nchoose a Turing machine M2 such that\\n⟨M2⟩∈P.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nif M terminates\\nthen run M2 on input x;\\nif M2 terminates in the accept state\\nthen terminate in the accept state\\nelse if M2 terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\n172\\nChapter 5.\\nDecidable and Undecidable Languages\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that for any string x,\\nx is accepted by TMw if and only if x is accepted by M2.\\nThus, L(TMw) = L(M2).\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩̸∈Halt.\\nThen it follows from the pseudocode that for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅. In particular, L(TMw) =\\nL(M1).\\nRecall that ⟨M1⟩̸∈P, whereas ⟨M2⟩∈P. Then the following follows from\\nthe third condition in Rice’s Theorem:\\n• If ⟨M, w⟩∈Halt, then ⟨TMw⟩∈P.\\n• If ⟨M, w⟩̸∈Halt, then ⟨TMw⟩̸∈P.\\nThus, we have obtained a connection between the languages P and Halt.\\nThis suggests that we proceed as follows.\\nAssume that the language P is decidable. Let H be a Turing machine\\nthat decides P. Then, for any Turing machine M,\\n• if ⟨M⟩∈P, then H accepts the string ⟨M⟩,\\n• if ⟨M⟩̸∈P, then H rejects the string ⟨M⟩, and\\n• H terminates on any input string.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\n5.4.\\nEnumerability\\n173\\nIt follows from the pseudocode that H′ terminates on any input. We\\nobserve the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that ⟨TMw⟩∈P.\\nSince H decides the language P, it follows that H accepts the string\\n⟨TMw⟩. Therefore, from the pseudocode, H′ accepts the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then we have seen before that ⟨TMw⟩̸∈\\nP. Since H decides the language P, it follows that H rejects (and\\nterminates on) the string ⟨TMw⟩. Therefore, from the pseudocode, H′\\nrejects (and terminates on) the string ⟨M, w⟩.\\nWe have shown that the Turing machine H′ decides the language Halt.\\nThis is a contradiction and, therefore, we conclude that the language P is\\nundecidable.\\nUntil now, we assumed that ⟨M1⟩̸∈P. If ⟨M1⟩∈P, then we repeat the\\nproof with P replaced by its complement P. This revised proof then shows\\nthat P is undecidable. Since for every language L,\\nL is decidable if and only if L is decidable,\\nwe again conclude that P is undecidable.\\n5.4\\nEnumerability\\nWe now come to the last class of languages in this chapter:\\nDeﬁnition 5.4.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is enumerable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, does not terminate in the accept state. That is, either the\\ncomputation terminates in the reject state or the computation does not\\nterminate.\\n174\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is enumerable, if there exists an algorithm\\nhaving the following property. If w ∈A, then the algorithm terminates on\\nthe input string w and tells us that w ∈A. On the other hand, if w ̸∈A,\\nthen either (i) the algorithm terminates on the input string w and tells us\\nthat w ̸∈A or (ii) the algorithm does not terminate on the input string w,\\nin which case it does not tell us that w ̸∈A.\\nIn Section 5.5, we will show where the term “enumerable” comes from.\\nThe following theorem follows immediately from Deﬁnitions 5.1.1 and 5.4.1.\\nTheorem 5.4.2 Every decidable language is enumerable.\\nIn the following subsections, we will give some examples of enumerable\\nlanguages.\\n5.4.1\\nHilbert’s problem\\nWe have seen Hilbert’s problem in Section 4.4: Is there an algorithm that\\ndecides, for any given polynomial p with integer coeﬃcients, whether or not\\np has integral roots? If we formulate this problem in terms of languages,\\nthen Hilbert asked whether or not the language\\nHilbert = {⟨p⟩:\\np is a polynomial with integer coeﬃcients\\nthat has an integral root}\\nis decidable. As usual, ⟨p⟩denotes the binary string that forms an encoding\\nof the polynomial p.\\nAs we mentioned in Section 4.4, it was proven by Matiyasevich in 1970\\nthat the language Hilbert is not decidable. We claim, that this language\\nis enumerable.\\nIn order to prove this claim, we have to construct an al-\\ngorithm Hilbert with the following property: For any input polynomial p\\nwith integer coeﬃcients,\\n• if p has an integral root, then algorithm Hilbert will ﬁnd one in a\\nﬁnite amount of time,\\n• if p does not have an integral root, then either algorithm Hilbert ter-\\nminates and tells us that p does not have an integral root, or algorithm\\nHilbert does not terminate.\\n5.4.\\nEnumerability\\n175\\nRecall that Z denotes the set of integers. Algorithm Hilbert does the\\nfollowing, on any input polynomial p with integer coeﬃcients.\\nLet n de-\\nnote the number of variables in p. Algorithm Hilbert tries all elements\\n(x1, x2, . . . , xn) ∈Zn, in a systematic way, and for each such element, it\\ncomputes p(x1, x2, . . . , xn). If this value is zero, then algorithm Hilbert\\nterminates and accepts the input.\\nWe observe the following:\\n• If ⟨p⟩∈Hilbert, then algorithm Hilbert terminates and accepts p,\\nprovided we are able to visit all elements (x1, x2, . . . , xn) ∈Zn in a\\n“systematic way”.\\n• If ⟨p⟩̸∈Hilbert, then p(x1, x2, . . . , xn) ̸= 0 for all (x1, x2, . . . , xn) ∈Zn\\nand, therefore, algorithm Hilbert does not terminate.\\nThese are exactly the requirements for the language Hilbert to be enumerable.\\nIt remains to explain how we visit all elements (x1, x2, . . . , xn) ∈Zn in a\\nsystematic way. For any integer d ≥0, let Hd denote the hypercube in Zn\\nwith sides of length 2d that is centered at the origin. That is, Hd consists\\nof the set of all points (x1, x2, . . . , xn) in Zn, such that −d ≤xi ≤d for all\\n1 ≤i ≤n and there exists at least one index j for which xj = d or xj = −d.\\nWe observe that Hd contains a ﬁnite number of elements. In fact, if d ≥1,\\nthen this number is equal to (2d + 1)n −(2d −1)n. The algorithm will visit\\nall elements (x1, x2, . . . , xn) ∈Zn, in the following order: First, it visits the\\norigin, which is the only element of H0. Then, it visits all elements of H1,\\nfollowed by all elements of H2, etc., etc.\\nTo summarize, we obtain the following algorithm, proving that the lan-\\nguage Hilbert is enumerable:\\n176\\nChapter 5.\\nDecidable and Undecidable Languages\\nAlgorithm Hilbert(⟨p⟩):\\nn := the number of variables in p;\\nd := 0;\\nwhile d ≥0\\ndo for each (x1, x2, . . . , xn) ∈Hd\\ndo R := p(x1, x2, . . . , xn);\\nif R = 0\\nthen terminate and accept\\nendif\\nendfor;\\nd := d + 1\\nendwhile\\nTheorem 5.4.3 The language Hilbert is enumerable.\\n5.4.2\\nThe language ATM\\nWe have shown in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nis undecidable. In this section, we will prove that this language is enumerable.\\nThus, we have to construct an algorithm P having the following property,\\nfor any given input string u:\\n• If\\n– u encodes a Turing machine M and an input string w for M (i.e.,\\nu is in the correct format ⟨M, w⟩) and\\n– ⟨M, w⟩∈ATM (i.e., M accepts w),\\nthen algorithm P terminates in its accept state.\\n• In all other cases, either algorithm P terminates in its reject state, or\\nalgorithm P does not terminate.\\nOn input string u = ⟨M, w⟩, which is in the correct format, algorithm P does\\nthe following:\\n5.5.\\nWhere does the term “enumerable” come from?\\n177\\n1. It simulates the computation of M on input w.\\n2. If M terminates in its accept state, then P terminates in its accept\\nstate.\\n3. If M terminates in its reject state, then P terminates in its reject state.\\n4. If M does not terminate, then P does not terminate.\\nHence, if u = ⟨M, w⟩∈ATM, then M accepts w and, therefore, P accepts\\nu. On the other hand, if u = ⟨M, w⟩̸∈ATM, then M does not accept w. This\\nmeans that, on input w, M either terminates in its reject state or does not\\nterminate. But this implies that, on input u, P either terminates in its reject\\nstate or does not terminate. This proves that algorithm P has the properties\\nthat are needed in order to show that the language ATM is enumerable. We\\nhave proved the following result:\\nTheorem 5.4.4 The language ATM is enumerable.\\n5.5\\nWhere does the term “enumerable” come\\nfrom?\\nIn Deﬁnition 5.4.1, we have deﬁned what it means for a language to be\\nenumerable. In this section, we will see where this term comes from.\\nDeﬁnition 5.5.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. An\\nenumerator for A is a Turing machine E having the following properties:\\n1. Besides the standard features as in Section 4.1, E has a print tape and\\na print state. During its computation, E writes symbols of Σ on the\\nprint tape. Each time, E enters the print state, the current string on\\nthe print tape is sent to the printer and the print tape is made empty.\\n2. At the start of the computation, all tapes are empty and E is in the\\nstart state.\\n3. Every string w in A is sent to the printer at least once.\\n4. Every string w that is not in A is never sent to the printer.\\n178\\nChapter 5.\\nDecidable and Undecidable Languages\\nThus, an enumerator E for A really enumerates all strings in the language\\nA. There is no particular order in which the strings of A are sent to the\\nprinter. Moreover, a string in A may be sent to the printer multiple times.\\nIf the language A is inﬁnite, then the Turing machine E obviously does not\\nterminate; however, every string in A (and only strings in A) will be sent to\\nthe printer at some time during the computation.\\nTo give an example, let A = {0n : n ≥0}. The following Turing machine\\nis an enumerator for A.\\nTuring machine StringsOfZeros:\\nn := 0;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo write 0 on the print tape\\nendfor;\\nenter the print state;\\nn := n + 1\\nendwhile\\nIn the rest of this section, we will prove the following result.\\nTheorem 5.5.2 A language is enumerable if and only if it has an enumer-\\nator.\\nFor the ﬁrst part of the proof, assume that the language A has an enu-\\nmerator E. We construct the following Turing machine M, which takes an\\narbitrary string w as input:\\nTuring machine M(w):\\nrun E; every time E enters the print state:\\nlet v be the string on the print tape;\\nif w = v\\nthen terminate in the accept state\\nendif\\nThe Turing machine M has the following properties:\\n• If w ∈A, then w will be sent to the printer at some time during the\\n5.5.\\nWhere does the term “enumerable” come from?\\n179\\ncomputation of E. It follows from the pseudocode that, on input w,\\nM terminates in the accept state.\\n• If w ̸∈A, then E will never sent w to the printer. It follows from the\\npseudocode that, on input w, M does not terminate.\\nThus, M satisﬁes the conditions in Deﬁnition 5.4.1. We conclude that the\\nlanguage A is enumerable.\\nTo prove the converse, we now assume that A is enumerable. Let M be\\na Turing machine that satisﬁes the conditions in Deﬁnition 5.4.1.\\nWe ﬁx an inﬁnite list\\ns1, s2, s3, . . .\\nof all strings in Σ∗. For example, if Σ = {0, 1}, then we can take this list to\\nbe\\nϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .\\nWe construct the following Turing machine E, which takes the empty\\nstring as input:\\nTuring machine E:\\nn := 1;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo run M for n steps on the input string si;\\nif M accepts si within n steps\\nthen write si on the print tape;\\nenter the print state\\nendif\\nendfor;\\nn := n + 1\\nendwhile\\nWe claim that E is an enumerator for the language A. To prove this, it\\nis obvious that any string that is sent to the printer by E belongs to A.\\nIt remains to prove that every string in A will be sent to the printer by E.\\nLet w be a string in A. Then, on input w, the Turing machine M terminates\\nin the accept state. Let m be the number of steps made by M on input w.\\nLet i be the index such that w = si. Deﬁne n = max(m, i). Consider the\\n180\\nChapter 5.\\nDecidable and Undecidable Languages\\nn-th iteration of the while-loop and the i-th iteration of the for-loop. In this\\niteration, M accepts si = w in m ≤n steps and, therefore, w is sent to the\\nprinter.\\n5.6\\nMost languages are not enumerable\\nIn this section, we will prove that most languages are not enumerable. The\\nproof is based on the following two facts:\\n• The set consisting of all enumerable languages is countable; we will\\nprove this in Section 5.6.1.\\n• The set consisting of all languages is not countable; we will prove this\\nin Section 5.6.2.\\n5.6.1\\nThe set of enumerable languages is countable\\nWe deﬁne the set E as\\nE = {A : A ⊆{0, 1}∗is an enumerable language}.\\nIn words, E is the set whose elements are the enumerable languages. Every\\nelement of E is an enumerable language. Hence, every element of the set E\\nis itself a set consisting of strings.\\nLemma 5.6.1 The set E is countable.\\nProof. Let A ⊆{0, 1}∗be an enumerable language. There exists a Turing\\nmachine TA that satisﬁes the conditions in Deﬁnition 5.4.1.\\nThis Turing\\nmachine TA can be uniquely speciﬁed by a string in English. This string can\\nbe converted to a binary string sA. Hence, the binary string sA is a unique\\nencoding of the Turing machine TA.\\nConsider the set\\nS = {sA : A ⊆{0, 1}∗is an enumerable language}.\\nObserve that the function f : E →S, deﬁned by f(A) = sA for each A ∈E,\\nis a bijection. Therefore, the sets E and S have the same size. Hence, in\\norder to prove that the set E is countable, it is suﬃcient to prove that the\\nset S is countable.\\n5.6.\\nMost languages are not enumerable\\n181\\nWhy is the set S countable? For each integer n ≥0, there are exactly 2n\\nbinary strings of length n. Since there are binary strings that are not encod-\\nings of Turing machines, the set S contains at most 2n strings of length n.\\nIn particular, the number of strings in S having length n is ﬁnite. Therefore,\\nwe obtain an inﬁnite list of the elements of S in the following way:\\n• List all strings in S having length 0. (Well, the empty string is not in\\nS, so in this step, nothing happens.)\\n• List all strings in S having length 1.\\n• List all strings in S having length 2.\\n• List all strings in S having length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every element of S occurs exactly once. Therefore, S is\\ncountable.\\n5.6.2\\nThe set of all languages is not countable\\nWe deﬁne the set L as\\nL = {A : A ⊆{0, 1}∗is a language}.\\nIn words, L is the set consisting of all languages. Every element of the set L\\nis a set consisting of strings.\\nLemma 5.6.2 The set L is not countable.\\nProof. We deﬁne the set B as\\nB = {w : w is an inﬁnite binary sequence}.\\nWe claim that this set is not countable. The proof of this claim is almost\\nidentical to the proof of Theorem 5.2.4. We assume that the set B is count-\\nable. Then there exists a bijection f : N →B. Thus, for each n ∈N, f(n) is\\nan inﬁnite binary sequence. We can write\\nB = {f(1), f(2), f(3), . . .},\\n(5.3)\\n182\\nChapter 5.\\nDecidable and Undecidable Languages\\nwhere every element of B occurs exactly once in the set on the right-hand\\nside.\\nWe deﬁne the inﬁnite binary sequence w = w1w2w3 . . ., where, for each\\ninteger n ≥1,\\nwn =\\n\\x1a 1\\nif the n-th bit of f(n) is 0,\\n0\\nif the n-th bit of f(n) is 1.\\nSince w ∈B, it follows from (5.3) that there is an element n ∈N, such that\\nf(n) = w. Hence, the n-th bits of f(n) and w are equal. But, by deﬁnition,\\nthese n-th bits are not equal. This is a contradiction and, therefore, the set\\nB is not countable.\\nIn the rest of the proof, we will show that the sets L and B have the same\\nsize. Since B is not countable, this will imply that L is not countable.\\nIn order to prove that L and B have the same size, we have to show that\\nthere exists a bijection\\ng : L →B.\\nWe ﬁrst observe that the set {0, 1}∗is countable, because for each integer\\nn ≥0, there are only ﬁnitely many (to be precise, exactly 2n) binary strings\\nof length n. In fact, we can write\\n{0, 1}∗= {ϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .}.\\nFor each integer n ≥1, we denote by sn the n-th string in this list. Hence,\\n{0, 1}∗= {s1, s2, s3, . . .}.\\n(5.4)\\nNow we are ready to deﬁne the bijection g : L →B: Let A ∈L, i.e.,\\nA ⊆{0, 1}∗is a language. We deﬁne the inﬁnite binary sequence g(A) as\\nfollows: For each integer n ≥1, the n-th bit of g(A) is equal to\\n\\x1a 1\\nif sn ∈A,\\n0\\nif sn ̸∈A.\\nIn words, the inﬁnite binary sequence g(A) contains a 1 exactly in those\\npositions n for which the string sn in (5.4) is in the language A.\\nTo give an example, assume that A is the language consisting of all binary\\nstrings that start with 0. The following table gives the corresponding inﬁnite\\nbinary sequence g(A) (this sequence is obtained by reading the rightmost\\ncolumn from top to bottom):\\n5.6.\\nMost languages are not enumerable\\n183\\n{0, 1}∗\\nA\\ng(A)\\nϵ\\nnot in A\\n0\\n0\\nin A\\n1\\n1\\nnot in A\\n0\\n00\\nin A\\n1\\n01\\nin A\\n1\\n10\\nnot in A\\n0\\n11\\nnot in A\\n0\\n000\\nin A\\n1\\n001\\nin A\\n1\\n010\\nin A\\n1\\n100\\nnot in A\\n0\\n011\\nin A\\n1\\n101\\nnot in A\\n0\\n110\\nnot in A\\n0\\n111\\nnot in A\\n0\\n...\\n...\\n...\\nThe function g deﬁned above has the following properties:\\n• If A and A′ are two diﬀerent languages in L, then g(A) ̸= g(A′).\\n• For every inﬁnite binary sequence w in B, there exists a language A in\\nL, such that g(A) = w.\\nThis means that the function g is a bijection from L to B.\\n5.6.3\\nThere are languages that are not enumerable\\nWe have proved that the set\\nE = {A : A ⊆{0, 1}∗is an enumerable language}\\nis countable, whereas the set\\nL = {A : A ⊆{0, 1}∗is a language}\\nis not countable. This means that there are “more” languages in L than\\nthere are in E, proving the following result:\\n184\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.6.3 There exist languages that are not enumerable.\\nThe proof given above shows the existence of languages that are not\\nenumerable. However, the proof does not give us a speciﬁc example of a\\nlanguage that is not enumerable. In the next sections, we will see examples\\nof such languages. Before we move on to these examples, we mention the\\ndiﬀerence between being countable and being enumerable:\\n• Any language A is countable, i.e., we can number the elements of A\\nand, thus, write\\nA = {s1, s2, s3, s4, . . .}.\\n• If the language A is enumerable, then, by Theorem 5.5.2, there is an\\nalgorithm that produces this numbering.\\n• If the language A is not enumerable, then, again by Theorem 5.5.2,\\nthere does not exist an algorithm that produces this numbering.\\n5.7\\nThe relationship between decidable and\\nenumerable languages\\nWe know from Theorem 5.4.2 that every decidable language is enumerable.\\nOn the other hand, we know from Theorems 5.1.6 and 5.4.4 that the converse\\nis not true. The following result should not come as a surprise:\\nTheorem 5.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then,\\nA is decidable if and only if both A and its complement A are enumerable.\\nProof. We ﬁrst assume that A is decidable. Then, by Theorem 5.4.2, A\\nis enumerable. Since A is decidable, it is not diﬃcult to see that A is also\\ndecidable. Then, again by Theorem 5.4.2, A is enumerable.\\nTo prove the converse, we assume that both A and A are enumerable.\\nSince A is enumerable, there exists a Turing machine M1, such that for any\\nstring w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M1, on the input string w, terminates\\nin the accept state of M1.\\n5.7.\\nDecidable versus enumerable languages\\n185\\n• If w ̸∈A, then the computation of M1, on the input string w, terminates\\nin the reject state of M1 or does not terminate.\\nSimilarly, since A is enumerable, there exists a Turing machine M2, such that\\nfor any string w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M2, on the input string w, terminates\\nin the accept state of M2.\\n• If w ̸∈A, then the computation of M2, on the input string w, terminates\\nin the reject state of M2 or does not terminate.\\nWe construct a two-tape Turing machine M:\\nTwo-tape Turing machine M: For any input string w ∈Σ∗, M\\ndoes the following:\\n• M simulates the computation of M1, on input w, on the ﬁrst\\ntape, and, simultaneously, it simulates the computation of M2,\\non input w, on the second tape.\\n• If the simulation of M1 terminates in the accept state of M1,\\nthen M terminates in its accept state.\\n• If the simulation of M2 terminates in the accept state of M2,\\nthen M terminates in its reject state.\\nObserve the following:\\n• If w ∈A, then M1 terminates in its accept state and, therefore, M\\nterminates in its accept state.\\n• If w ̸∈A, then M2 terminates in its accept state and, therefore, M\\nterminates in its reject state.\\nWe conclude that the Turing machine M accepts all strings in A, and rejects\\nall strings that are not in A. This proves that the language A is decidable.\\nWe now use Theorem 5.7.1 to give examples of languages that are not\\nenumerable:\\n186\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.7.2 The language ATM is not enumerable.\\nProof. We know from Theorems 5.4.4 and 5.1.6 that the language ATM is\\nenumerable but not decidable. Combining these facts with Theorem 5.7.1\\nimplies that the language ATM is not enumerable.\\nThe following result can be proved in exactly the same way:\\nTheorem 5.7.3 The language Halt is not enumerable.\\n5.8\\nA language A such that both A and A are\\nnot enumerable\\nIn Theorem 5.7.2, we have seen that the complement of the language ATM\\nis not enumerable.\\nIn Theorem 5.4.4, however, we have shown that the\\nlanguage ATM itself is enumerable. In this section, we consider the language\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\nWe will show the following result:\\nTheorem 5.8.1 Both EQTM and its complement EQTM are not enumer-\\nable.\\n5.8.1\\nEQTM is not enumerable\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We construct\\na new Turing machine TMw that takes as input an arbitrary binary string x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nterminate in the accept state\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that every string x is accepted by TMw.\\nThus, L(TMw) = {0, 1}∗.\\n5.8.\\nBoth A and A not enumerable\\n187\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩∈Halt.\\nThen it follows from the pseudocode that, for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that rejects every input string;\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. By our choice of M1, we have L(M1) = ∅as well. Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt, i.e., ⟨M, w⟩∈Halt. Then we have seen\\nbefore that L(TMw) ̸= ∅= L(M1). Therefore, on input ⟨M1, TMw⟩, H\\neither terminates in the reject state or does not terminate. It follows\\n188\\nChapter 5.\\nDecidable and Undecidable Languages\\nfrom the pseudocode that, on input ⟨M, w⟩, H′ either terminates in the\\nreject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\n5.8.2\\nEQTM is not enumerable\\nThis proof is symmetric to the one in Section 5.8.1.\\nFor a ﬁxed Turing\\nmachine M and a ﬁxed binary string w, we will use the same Turing machine\\nTMw as in Section 5.8.1.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that accepts every input string;\\nconstruct the Turing machine TMw of Section 5.8.1;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\nExercises\\n189\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. Thus, by our choice of M1, we have L(TMw) ̸= L(M1). Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then L(TMw) = {0, 1}∗= L(M1) and, on\\ninput ⟨M1, TMw⟩, H either terminates in the reject state or does not\\nterminate. It follows from the pseudocode that, on input ⟨M, w⟩, H′\\neither terminates in the reject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\nExercises\\n5.1 Prove that the language\\n{w ∈{0, 1}∗: w is the binary representation of 2n for some n ≥0}\\nis decidable. In other words, construct a Turing machine that gets as input\\nan arbitrary number x ∈N, represented in binary as a string w, and that\\ndecides whether or not x is a power of two.\\n5.2 Let F be the set of all functions f : N →N.\\nProve that F is not\\ncountable.\\n5.3 A function f : N →N is called computable, if there exists a Turing\\nmachine, that gets as input an arbitrary positive integer n, written in binary,\\nand gives as output the value of f(n), again written in binary. This Turing\\nmachine has a ﬁnal state. As soon as the Turing machine enters this ﬁnal\\nstate, the computation terminates, and the output is the binary string that\\nis written on its tape.\\nProve that there exist functions f : N →N that are not computable.\\n5.4 Let n be a ﬁxed positive integer, and let k be the number of bits in the\\nbinary representation of n. (Hence, k = 1 + ⌊log n⌋.) Construct a Turing\\nmachine with one tape, tape alphabet {0, 1, 2}, and exactly k + 1 states\\nq0, q1, . . . , qk, that does the following:\\n190\\nChapter 5.\\nDecidable and Undecidable Languages\\nStart of the computation: The tape is empty, i.e., every cell of the tape\\ncontains 2, and the Turing machine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer n, the tape head is on the rightmost bit of the binary represen-\\ntation of n, and the Turing machine is in the ﬁnal state qk.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state qk. As soon as state qk is entered,\\nthe Turing machine terminates.\\n5.5 Give an informal description (in plain English) of a Turing machine\\nwith three tapes, that gets as input the binary representation of an arbitrary\\ninteger m ≥1, and returns as output the unary representation of m.\\nStart of the computation: The ﬁrst tape contains the binary representa-\\ntion of the input m. The other two tapes are empty (i.e., contain only 2s).\\nThe Turing machine is in the start state.\\nEnd of the computation: The third tape contains the unary representation\\nof m, i.e., a string consisting of m many ones. The Turing machine is in the\\nﬁnal state.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this ﬁnal state is entered,\\nthe Turing machine terminates.\\nHint: Use the second tape to maintain a string of ones, whose length is\\na power of two.\\n5.6 In this exercise, you are asked to prove that the busy beaver function\\nBB : N →N is not computable.\\nFor any integer n ≥1, we deﬁne TM n to be the set of all Turing machines\\nM, such that\\n• M has one tape,\\n• M has exactly n states,\\n• the tape alphabet of M is {0, 1, 2}, and\\n• M terminates, when given the empty string ϵ as input.\\nExercises\\n191\\nFor every Turing machine M ∈TM n, we deﬁne f(M) to be the number of\\nones on the tape, after the computation of M, on the empty input string,\\nhas terminated.\\nThe busy beaver function BB : N →N is deﬁned as\\nBB(n) := max{f(M) : M ∈TM n}, for every n ≥1.\\nIn words, BB(n) is the maximum number of ones that any Turing machine\\nwith n states can produce, when given the empty string as input, and as-\\nsuming the Turing machine terminates on this input.\\nProve that the function BB is not computable.\\nHint: Assume that BB is computable. Then there exists a Turing ma-\\nchine M that, for any given n ≥1, computes the value of BB(n). Fix a large\\ninteger n ≥1. Deﬁne (in plain English) a Turing machine that, when given\\nthe empty string as input, terminates and outputs a string consisting of more\\nthan BB(n) many ones. Use Exercises 5.4 and 5.5 to argue that there exists\\nsuch a Turing machine having O(log n) states. Then, if you assume that n\\nis large enough, the number of states is at most n.\\n5.7 Since the set\\nT = {M : M is a Turing machine}\\nis countable, there is an inﬁnite list\\nM1, M2, M3, M4, . . . ,\\nsuch that every Turing machine occurs exactly once in this list.\\nFor any positive integer n, let ⟨n⟩denote the binary representation of n;\\nobserve that ⟨n⟩is a binary string.\\nLet A be the language deﬁned as\\nA = {⟨n⟩:\\nthe Turing machine Mn terminates on the input string ⟨n⟩,\\nand it rejects this string}.\\nProve that the language A is undecidable.\\n5.8 Consider the three languages\\nEmpty = {⟨M⟩: M is a Turing machine for which L(M) = ∅},\\n192\\nChapter 5.\\nDecidable and Undecidable Languages\\nUselessState = {⟨M, q⟩:\\nM is a Turing machine, q is a state of M,\\nfor every input string w, the computation of M on\\ninput w never visits state q},\\nand\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\n• Use Rice’s Theorem to show that Empty is undecidable.\\n• Use the ﬁrst part to show that UselessState is undecidable.\\n• Use the ﬁrst part to show that EQTM is undecidable.\\n5.9 Consider the language\\nREGTM = {⟨M⟩: M is a Turing machine whose language L(M) is regular}.\\nUse Rice’s Theorem to prove that REGTM is undecidable.\\n5.10 We have seen in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts w}\\nis undecidable. Consider the language REGTM of Exercise 5.9. The questions\\nbelow will lead you through a proof of the claim that the language REGTM\\nis undecidable.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nif x = 0n1n for some n ≥0\\nthen terminate in the accept state\\nelse run M on the input string w;\\nif M terminates in the accept state\\nthen terminate in the accept state\\nelse if M terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\nExercises\\n193\\nAnswer the following two questions:\\n• Assume that M accepts the string w. What is the language L(TMw) of\\nthe new Turing machine TMw?\\n• Assume that M does not accept the string w. What is the language\\nL(TMw) of the new Turing machine TMw?\\nThe goal is to prove that the language REGTM is undecidable. We will\\nprove this by contradiction. Thus, we assume that R is a Turing machine\\nthat decides REGTM. Recall what this means:\\n• If M is a Turing machine whose language is regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the accept state.\\n• If M is a Turing machine whose language is not regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the reject state.\\nWe construct a new Turing machine R′ which takes as input an arbitrary\\nTuring machine M and an arbitrary binary string w:\\nTuring machine R′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun R on the input ⟨TMw⟩;\\nif R terminates in the accept state\\nthen terminate in the accept state\\nelse if R terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nProve that M accepts w if and only if R′ (when given ⟨M, w⟩as input),\\nterminates in the accept state.\\nNow ﬁnish the proof by arguing that the language REGTM is undecidable.\\n5.11 A Java program P is called a Hello-World-program, if the following is\\ntrue: When given the empty string ϵ as input, P outputs the string Hello\\nWorld and then terminates. (We do not care what P does when the input\\nstring is non-empty.)\\n194\\nChapter 5.\\nDecidable and Undecidable Languages\\nConsider the language\\nHW = {⟨P⟩: P is a Hello-World-program}.\\nThe questions below will lead you through a proof of the claim that the\\nlanguage HW is undecidable.\\nConsider a ﬁxed Java program P and a ﬁxed binary string w. We write\\na new Java program JPw which takes as input an arbitrary binary string x:\\nJava program JPw(x):\\nrun P on the input w;\\nprint Hello World\\n• Argue that P terminates on input w if and only if ⟨JPw⟩∈HW .\\nThe goal is to prove that the language HW is undecidable. We will prove this\\nby contradiction. Thus, we assume that H is a Java program that decides\\nHW . Recall what this means:\\n• If P is a Hello-World-program, then H, when given ⟨P⟩as input, will\\nterminate in the accept state.\\n• If P is not a Hello-World-program, then H, when given ⟨P⟩as input,\\nwill terminate in the reject state.\\nWe write a new Java program H′ which takes as input the binary encoding\\n⟨P, w⟩of an arbitrary Java program P and an arbitrary binary string w:\\nJava program H′(⟨P, w⟩):\\nconstruct the Java program JPw described above;\\nrun H on the input ⟨JPw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\nArgue that the following are true:\\nExercises\\n195\\n• For any input ⟨P, w⟩, H′ terminates.\\n• If P terminates on input w, then H′ (when given ⟨P, w⟩as input),\\nterminates in the accept state.\\n• If P does not terminate on input w, then H′ (when given ⟨P, w⟩as\\ninput), terminates in the reject state.\\nNow ﬁnish the proof by arguing that the language HW is undecidable.\\n5.12 Prove that the language Halt, see Section 5.1.5, is enumerable.\\n5.13 We deﬁne the following language:\\nL = {u\\n:\\nu = ⟨0, M, w⟩for some ⟨M, w⟩∈ATM,\\nor u = ⟨1, M, w⟩for some ⟨M, w⟩̸∈ATM } .\\nProve that neither L nor its complement L is enumerable.\\nHint: There are two ways to solve this exercise. In the ﬁrst solution, (i)\\nyou assume that L is enumerable, and then prove that ATM is decidable, and\\n(ii) you assume that L is enumerable, and then prove that ATM is decidable.\\nIn the second solution, (i) you assume that L is enumerable, and then prove\\nthat ATM is enumerable, and (ii) you assume that L is enumerable, and then\\nprove that ATM is enumerable.\\n196\\nChapter 5.\\nDecidable and Undecidable Languages\\nChapter 6\\nComplexity Theory\\nIn the previous chapters, we have considered the problem of what can be\\ncomputed by Turing machines (i.e., computers) and what cannot be com-\\nputed. We did not, however, take the eﬃciency of the computations into\\naccount. In this chapter, we introduce a classiﬁcation of decidable languages\\nA, based on the running time of the “best” algorithm that decides A. That\\nis, given a decidable language A, we are interested in the “fastest” algorithm\\nthat, for any given string w, decides whether or not w ∈A.\\n6.1\\nThe running time of algorithms\\nLet M be a Turing machine, and let w be an input string for M. We deﬁne\\nthe running time tM(w) of M on input w as\\ntM(w) := the number of computation steps made by M on input w.\\nAs usual, we denote by |w|, the number of symbols in the string w. We\\ndenote the set of non-negative integers by N0.\\nDeﬁnition 6.1.1 Let Σ be an alphabet, let T : N0 →N0 be a function, let\\nA ⊆Σ∗be a decidable language, and let F : Σ∗→Σ∗be a computable\\nfunction.\\n• We say that the Turing machine M decides the language A in time T,\\nif\\ntM(w) ≤T(|w|)\\nfor all strings w in Σ∗.\\n198\\nChapter 6.\\nComplexity Theory\\n• We say that the Turing machine M computes the function F in time\\nT, if\\ntM(w) ≤T(|w|)\\nfor all strings w ∈Σ∗.\\nIn other words, the “running time function” T is a function of the length\\nof the input, which we usually denote by n. For any n, the value of T(n) is\\nan upper bound on the running time of the Turing machine M, on any input\\nstring of length n.\\nTo give an example, consider the Turing machine of Section 4.2.1 that\\ndecides, using one tape, the language consisting of all palindromes. The tape\\nhead of this Turing machine moves from the left to the right, then back to\\nthe left, then to the right again, back to the left, etc. Each time it reaches\\nthe leftmost or rightmost symbol, it deletes this symbol. The running time\\nof this Turing machine, on any input string of length n, is\\nO(1 + 2 + 3 + . . . + n) = O(n2).\\nOn the other hand, the running time of the Turing machine of Section 4.2.2,\\nwhich also decides the palindromes, but using two tapes instead of just one,\\nis O(n).\\nIn Section 4.4, we mentioned that all computation models listed there are\\nequivalent, in the sense that if a language can be decided in one model, it\\ncan be decided in any of the other models. We just saw, however, that the\\nlanguage consisting of all palindromes allows a faster algorithm on a two-\\ntape Turing machine than on one-tape Turing machines. (Even though we\\ndid not prove this, it is true that Ω(n2) is a lower bound on the running\\ntime to decide palindromes on a one-tape Turing machine.) The following\\ntheorem can be proved.\\nTheorem 6.1.2 Let A be a language (resp. let F be a function) that can be\\ndecided (resp. computed) in time T by an algorithm of type M. Then there is\\nan algorithm of type N that decides A (resp. computes F) in time T ′, where\\nM\\nN\\nT ′\\nk-tape Turing machine\\none-tape Turing machine\\nO(T 2)\\none-tape Turing machine\\nJava program\\nO(T 2)\\nJava program\\nk-tape Turing machine\\nO(T 4)\\n6.2.\\nThe complexity class P\\n199\\n6.2\\nThe complexity class P\\nDeﬁnition 6.2.1 We say that algorithm M decides the language A (resp.\\ncomputes the function F) in polynomial time, if there exists an integer k ≥1,\\nsuch that the running time of M is O(nk), for any input string of length n.\\nIt follows from Theorem 6.1.2 that this notion of “polynomial time” does\\nnot depend on the model of computation:\\nTheorem 6.2.2 Consider the models of computation “Java program”, “k-\\ntape Turing machine”, and “one-tape Turing machine”. If a language can\\nbe decided (resp. a function can be computed) in polynomial time in one of\\nthese models, then it can be decided (resp. computed) in polynomial time in\\nall of these models.\\nBecause of this theorem, we can deﬁne the following two complexity\\nclasses:\\nP := {A : the language A is decidable in polynomial time},\\nand\\nFP := {F : the function F is computable in polynomial time}.\\n6.2.1\\nSome examples\\nPalindromes\\nLet Pal be the language\\nPal := {w ∈{a, b}∗: w is a palindrome}.\\nWe have seen that there exists a one-tape Turing machine that decides Pal\\nin O(n2) time. Therefore, Pal ∈P.\\nSome functions in FP\\nThe following functions are in the class FP:\\n• F1 : N0 →N0 deﬁned by F1(x) := x + 1,\\n• F2 : N2\\n0 →N0 deﬁned by F2(x, y) := x + y,\\n• F3 : N2\\n0 →N0 deﬁned by F3(x, y) := xy.\\n200\\nChapter 6.\\nComplexity Theory\\nr\\nb\\nb\\nb\\nr\\nr\\nb\\nG1\\nG2\\nFigure 6.1: The graph G1 is 2-colorable; r stands for red; b stands for blue.\\nThe graph G2 is not 2-colorable.\\nContext-free languages\\nWe have shown in Section 5.1.3 that every context-free language is decid-\\nable. The algorithm presented there, however, does not run in polynomial\\ntime. Using a technique called dynamic programming (which you will learn\\nin COMP 3804), the following result can be shown:\\nTheorem 6.2.3 Let Σ be an alphabet, and let A ⊆Σ∗be a context-free\\nlanguage. Then A ∈P.\\nObserve that, obviously, every language in P is decidable.\\nThe 2-coloring problem\\nLet G be a graph with vertex set V and edge set E.\\nWe say that G is\\n2-colorable, if it is possible to give each vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. only two colors are used to color all vertices.\\nSee Figure 6.1 for two examples. We deﬁne the following language:\\n2Color := {⟨G⟩: the graph G is 2-colorable},\\nwhere ⟨G⟩denotes the binary string that encodes the graph G.\\n6.2.\\nThe complexity class P\\n201\\nWe claim that 2Color ∈P. In order to show this, we have to construct an\\nalgorithm that decides in polynomial time, whether or not any given graph\\nis 2-colorable.\\nLet G be an arbitrary graph with vertex set V = {1, 2, . . . , m}. The edge\\nset of G is given by an adjacency matrix. This matrix, which we denote by\\nE, is a two-dimensional array with m rows and m columns. For all i and j\\nwith 1 ≤i ≤m and 1 ≤j ≤m, we have\\nE(i, j) =\\n\\x1a 1\\nif (i, j) is an edge of G,\\n0\\notherwise.\\nThe length of the input G, i.e., the number of bits needed to specify G, is\\nequal to m2 =: n. We will present an algorithm that decides, in O(n) time,\\nwhether or not the graph G is 2-colorable.\\nThe algorithm uses the colors red and blue. It gives the ﬁrst vertex the\\ncolor red. Then, the algorithm considers all vertices that are connected by\\nan edge to the ﬁrst vertex, and colors them blue. Now the algorithm is done\\nwith the ﬁrst vertex; it marks this ﬁrst vertex.\\nNext, the algorithm chooses a vertex i that already has a color, but that\\nhas not been marked. Then it considers all vertices j that are connected by\\nan edge to i. If j has the same color as i, then the input graph G is not\\n2-colorable. Otherwise, if vertex j does not have a color yet, the algorithm\\ngives j the color that is diﬀerent from i’s color. After having done this for\\nall neighbors j of i, the algorithm is done with vertex i, so it marks i.\\nIt may happen that there is no vertex i that already has a color but that\\nhas not been marked. (In other words, each vertex i that is not marked does\\nnot have a color yet.) In this case, the algorithm chooses an arbitrary vertex\\ni having this property, and colors it red. (This vertex i is the ﬁrst vertex in\\nits connected component that gets a color.)\\nThis procedure is repeated until all vertices of G have been marked.\\nWe now give a formal description of this algorithm. Vertex i has been\\nmarked, if\\n1. i has a color,\\n2. all vertices that are connected by an edge to i have a color, and\\n3. the algorithm has veriﬁed that each vertex that is connected by an edge\\nto i has a color diﬀerent from i’s color.\\n202\\nChapter 6.\\nComplexity Theory\\nThe algorithm uses two arrays f(1 . . . m) and a(1 . . . m), and a variable\\nM. The value of f(i) is equal to the color (red or blue) of vertex i; if i does\\nnot have a color yet, then f(i) = 0. The value of a(i) is equal to\\na(i) =\\n\\x1a 1\\nif vertex i has been marked,\\n0\\notherwise.\\nThe value of M is equal to the number of marked vertices. The algorithm\\nis presented in Figure 6.2. You are encouraged to convince yourself of the\\ncorrectness of this algorithm. That is, you should convince yourself that this\\nalgorithm returns YES if the graph G is 2-colorable, whereas it returns NO\\notherwise.\\nWhat is the running time of this algorithm? First we count the number\\nof iterations of the outer while-loop. In one iteration, either M increases by\\none, or a vertex i, for which a(i) = 0, gets the color red. In the latter case,\\nthe variable M is increased during the next iteration of the outer while-loop.\\nSince, during the entire outer while-loop, the value of M is increased from\\nzero to m, it follows that there are at most 2m iterations of the outer while-\\nloop. (In fact, the number of iterations is equal to m plus the number of\\nconnected components of G minus one.)\\nOne iteration of the outer while-loop takes O(m) time. Hence, the total\\nrunning time of the algorithm is O(m2), which is O(n). Therefore, we have\\nshown that 2Color ∈P.\\n6.3\\nThe complexity class NP\\nBefore we deﬁne the class NP, we consider some examples.\\nExample 6.3.1 Let G be a graph with vertex set V and edge set E, and\\nlet k ≥1 be an integer. We say that G is k-colorable, if it is possible to give\\neach vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. at most k diﬀerent colors are used to color all vertices.\\nWe deﬁne the following language:\\nkColor := {⟨G⟩: the graph G is k-colorable}.\\n6.3.\\nThe complexity class NP\\n203\\nAlgorithm 2Color\\nfor i := 1 to m do f(i) := 0; a(i) := 0 endfor;\\nf(1) := red; M := 0;\\nwhile M ̸= m\\ndo (∗Find the minimum index i for which vertex i has not\\nbeen marked, but has a color already ∗)\\nbool := false; i := 1;\\nwhile bool = false and i ≤m\\ndo if a(i) = 0 and f(i) ̸= 0 then bool := true else i := i + 1 endif;\\nendwhile;\\n(∗If bool = true, then i is the smallest index such that\\na(i) = 0 and f(i) ̸= 0.\\nIf bool = false, then for all i, the following holds: if a(i) = 0, then\\nf(i) = 0; because M < m, there is at least one such i. ∗)\\nif bool = true\\nthen for j := 1 to m\\ndo if E(i, j) = 1\\nthen if f(i) = f(j)\\nthen return NO and terminate\\nelse if f(j) = 0\\nthen if f(i) = red\\nthen f(j) := blue\\nelse f(j) := red\\nendif\\nendif\\nendif\\nendif\\nendfor;\\na(i) := 1; M := M + 1;\\nelse i := 1;\\nwhile a(i) ̸= 0 do i := i + 1 endwhile;\\n(∗an unvisited connected component starts at vertex i ∗)\\nf(i) := red\\nendif\\nendwhile;\\nreturn YES\\nFigure 6.2:\\nAn algorithm that decides whether or not a graph G is 2-\\ncolorable.\\nWe have seen that for k = 2, this problem is in the class P. For k ≥3, it\\nis not known whether there exists an algorithm that decides, in polynomial\\ntime, whether or not any given graph is k-colorable. In other words, for\\n204\\nChapter 6.\\nComplexity Theory\\nk ≥3, it is not known whether or not kColor is in the class P.\\nExample 6.3.2 Let G be a graph with vertex set V = {1, 2, . . . , m} and\\nedge set E. A Hamilton cycle is a cycle in G that visits each vertex exactly\\nonce. Formally, it is a sequence v1, v2, . . . , vm of vertices such that\\n1. {v1, v2, . . . , vm} = V , and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nWe deﬁne the following language:\\nHC := {⟨G⟩: the graph G contains a Hamilton cycle}.\\nIt is not known whether or not HC is in the class P.\\nExample 6.3.3 The sum of subset language is deﬁned as follows:\\nSOS := {⟨a1, a2, . . . , am, b⟩:\\nm, a1, a2, . . . , am, b ∈N0 and\\n∃I ⊆{1, 2, . . . , m}, P\\ni∈I ai = b}.\\nAlso in this case, no polynomial-time algorithm is known that decides the\\nlanguage SOS. That is, it is not known whether or not SOS is in the class\\nP.\\nExample 6.3.4 An integer x ≥2 is a prime number, if there are no a, b ∈N\\nsuch that a ̸= x, b ̸= x, and x = ab. Hence, the language of all non-primes\\nthat are greater than or equal to two, is\\nNPrim := {⟨x⟩: x ≥2 and x is not a prime number}.\\nIt is not obvious at all, whether or not NPrim is in the class P. In fact, it\\nwas shown only in 2002 that NPrim is in the class P.\\nObservation 6.3.5 The four languages above have the following in com-\\nmon: If someone gives us a “solution” for any given input, then we can\\neasily, i.e., in polynomial time, verify whether or not this “solution” is a cor-\\nrect solution. Moreover, for any input to each of these four problems, there\\nexists a “solution” whose length is polynomial in the length of the input.\\n6.3.\\nThe complexity class NP\\n205\\nLet us again consider the language kColor. Let G be a graph with vertex\\nset V = {1, 2, . . . , m} and edge set E, and let k be a positive integer. We\\nwant to decide whether or not G is k-colorable. A “solution” is a coloring of\\nthe nodes using at most k diﬀerent colors. That is, a solution is a sequence\\nf1, f2, . . . , fm. (Interpret this as: vertex i receives color fi, 1 ≤i ≤m). This\\nsequence is a correct solution if and only if\\n1. fi ∈{1, 2, . . . , k}, for all i with 1 ≤i ≤m, and\\n2. for all i with 1 ≤i ≤m, and for all j with 1 ≤j ≤m, if (i, j) ∈E,\\nthen fi ̸= fj.\\nIf someone gives us this solution (i.e., the sequence f1, f2, . . . , fm), then\\nwe can verify in polynomial time whether or not these two conditions are\\nsatisﬁed. The length of this solution is O(m log k): for each i, we need about\\nlog k bits to represent fi. Hence, the length of the solution is polynomial in\\nthe length of the input, i.e., it is polynomial in the number of bits needed to\\nrepresent the graph G and the number k.\\nFor the Hamilton cycle problem, a solution consists of a sequence v1,\\nv2, . . . , vm of vertices. This sequence is a correct solution if and only if\\n1. {v1, v2, . . . , vm} = {1, 2, . . . , m} and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nThese two conditions can be veriﬁed in polynomial time.\\nMoreover, the\\nlength of the solution is polynomial in the length of the input graph.\\nConsider the sum of subset problem. A solution is a sequence c1, c2, . . . , cm.\\nIt is a correct solution if and only if\\n1. ci ∈{0, 1}, for all i with 1 ≤i ≤m, and\\n2. Pm\\ni=1 ciai = b.\\nHence, the set I ⊆{1, 2, . . . , m} in the deﬁnition of SOS is the set of indices\\ni for which ci = 1. Again, these two conditions can be veriﬁed in polynomial\\ntime, and the length of the solution is polynomial in the length of the input.\\nFinally, let us consider the language NPrim. Let x ≥2 be an integer.\\nThe integers a and b form a “solution” for x if and only if\\n206\\nChapter 6.\\nComplexity Theory\\n1. 2 ≤a < x,\\n2. 2 ≤b < x, and\\n3. x = ab.\\nClearly, these three conditions can be veriﬁed in polynomial time. Moreover,\\nthe length of this solution, i.e., the total number of bits in the binary rep-\\nresentations of a and b, is polynomial in the number of bits in the binary\\nrepresentation of x.\\nLanguages having the property that the correctness of a proposed “solu-\\ntion” can be veriﬁed in polynomial time, form the class NP:\\nDeﬁnition 6.3.6 A language A belongs to the class NP, if there exist a\\npolynomial p and a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\nIn words, a language A is in the class NP, if for every string w, w ∈A if\\nand only if the following two conditions are satisﬁed:\\n1. There is a “solution” s, whose length |s| is polynomial in the length of\\nw (i.e., |s| ≤p(|w|), where p is a polynomial).\\n2. In polynomial time, we can verify whether or not s is a correct “solu-\\ntion” for w (i.e., ⟨w, s⟩∈B and B ∈P).\\nHence, the language B can be regarded to be the “veriﬁcation language”:\\nB = {⟨w, s⟩: s is a correct “solution” for w}.\\nWe have given already informal proofs of the fact that the languages\\nkColor, HC, SOS, and NPrim are all contained in the class NP. Below, we\\nformally prove that NPrim ∈NP. To prove this claim, we have to specify\\nthe polynomial p and the language B ∈P. First, we observe that\\nNPrim = {⟨x⟩:\\nthere exist a and b in N such that\\n2 ≤a < x, 2 ≤b < x and x = ab }.\\n(6.1)\\nWe deﬁne the polynomial p by p(n) := n + 2, and the language B as\\nB := {⟨x, a, b⟩: x ≥2, 2 ≤a < x, 2 ≤b < x and x = ab}.\\n6.3.\\nThe complexity class NP\\n207\\nIt is obvious that B ∈P: For any three positive integers x, a, and b, we\\ncan verify in polynomial time whether or not ⟨x, a, b⟩∈B. In order to do\\nthis, we only have to verify whether or not x ≥2, 2 ≤a < x, 2 ≤b < x,\\nand x = ab. If all these four conditions are satisﬁed, then ⟨x, a, b⟩∈B. If at\\nleast one of them is not satisﬁed, then ⟨x, a, b⟩̸∈B.\\nIt remains to show that for all x ∈N:\\n⟨x⟩∈NPrim ⇐⇒∃a, b : |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B.\\n(6.2)\\n(Remember that |⟨x⟩| denotes the number of bits in the binary representation\\nof x; |⟨a, b⟩| denotes the total number of bits of a and b, i.e., |⟨a, b⟩| =\\n|⟨a⟩| + |⟨b⟩|.)\\nLet x ∈NPrim. It follows from (6.1) that there exist a and b in N, such\\nthat 2 ≤a < x, 2 ≤b < x, and x = ab. Since x = ab ≥2 · 2 = 4 ≥2, it\\nfollows that ⟨x, a, b⟩∈B. Hence, it remains to show that\\n|⟨a, b⟩| ≤|⟨x⟩| + 2.\\nThe binary representation of x contains ⌊log x⌋+1 bits, i.e., |⟨x⟩| = ⌊log x⌋+1.\\nWe have\\n|⟨a, b⟩|\\n=\\n|⟨a⟩| + |⟨b⟩|\\n=\\n(⌊log a⌋+ 1) + (⌊log b⌋+ 1)\\n≤\\nlog a + log b + 2\\n=\\nlog ab + 2\\n=\\nlog x + 2\\n≤\\n⌊log x⌋+ 3\\n=\\n|⟨x⟩| + 2.\\nThis proves one direction of (6.2).\\nTo prove the other direction, we assume that there are positive integers\\na and b, such that |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B. Then it follows\\nimmediately from (6.1) and the deﬁnition of the language B, that x ∈NPrim.\\nHence, we have proved the other direction of (6.2). This completes the proof\\nof the claim that\\nNPrim ∈NP.\\n208\\nChapter 6.\\nComplexity Theory\\n6.3.1\\nP is contained in NP\\nIntuitively, it is clear that P ⊆NP, because a language is\\n• in P, if for every string w, it is possible to compute the “solution” s in\\npolynomial time,\\n• in NP, if for every string w and for any given “solution” s, it is possible\\nto verify in polynomial time whether or not s is a correct solution for\\nw (hence, we do not need to compute the solution s ourselves, we only\\nhave to verify it).\\nWe give a formal proof of this:\\nTheorem 6.3.7 P ⊆NP.\\nProof. Let A ∈P. We will prove that A ∈NP. Deﬁne the polynomial p\\nby p(n) := 0 for all n ∈N0, and deﬁne\\nB := {⟨w, ϵ⟩: w ∈A}.\\nSince A ∈P, the language B is also contained in P. It is easy to see that\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) = 0 and ⟨w, s⟩∈B.\\nThis completes the proof.\\n6.3.2\\nDeciding NP-languages in exponential time\\nLet us look again at the deﬁnition of the class NP. Let A be a language in\\nthis class. Then there exist a polynomial p and a language B ∈P, such that\\nfor all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.3)\\nHow do we decide whether or not any given string w belongs to the language\\nA? If we can ﬁnd a string s that satisﬁes the right-hand side in (6.3), then\\nwe know that w ∈A. On the other hand, if there is no such string s, then\\nw ̸∈A. How much time do we need to decide whether or not such a string s\\nexists?\\n6.3.\\nThe complexity class NP\\n209\\nAlgorithm NonPrime\\n(∗decides whether or not ⟨x⟩∈NPrim ∗)\\nif x = 0 or x = 1 or x = 2\\nthen return NO and terminate\\nelse a := 2;\\nwhile a < x\\ndo if x mod a = 0\\nthen return YES and terminate\\nelse a := a + 1\\nendif\\nendwhile;\\nreturn NO\\nendif\\nFigure 6.3: An algorithm that decides whether or not a number x is contained\\nin the language NPrim.\\nFor example, let A be the language\\nNPrim = {⟨x⟩: x ≥2 and x is not a prime number},\\nand let x ∈N. The algorithm in Figure 6.3 decides whether or not ⟨x⟩∈\\nNPrim.\\nIt is clear that this algorithm is correct. Let n be the length of the binary\\nrepresentation of x, i.e., n = ⌊log x⌋+ 1. If x > 2 and x is a prime number,\\nthen the while-loop makes x−2 iterations. Therefore, since n−1 = ⌊log x⌋≤\\nlog x, the running time of this algorithm is at least\\nx −2 ≥2n−1 −2,\\ni.e., it is at least exponential in the length of the input.\\nWe now prove that every language in NP can be decided in exponential\\ntime. Let A be an arbitrary language in NP. Let p be the polynomial, and\\nlet B ∈P be the language such that for all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.4)\\nThe following algorithm decides, for any given string w, whether or not\\nw ∈A. It does so by looking at all possible strings s for which |s| ≤p(|w|):\\n210\\nChapter 6.\\nComplexity Theory\\nfor all s with |s| ≤p(|w|)\\ndo if ⟨w, s⟩∈B\\nthen return YES and terminate\\nendif\\nendfor;\\nreturn NO\\nThe correctness of the algorithm follows from (6.4). What is the running\\ntime? We assume that w and s are represented as binary strings. Let n be\\nthe length of the input, i.e., n = |w|.\\nHow many binary strings s are there whose length is at most p(|w|)? Any\\nsuch s can be described by a sequence of length p(|w|) = p(n), consisting of\\nthe symbols “0”, “1”, and the blank symbol. Hence, there are at most 3p(n)\\nmany binary strings s with |s| ≤p(n). Therefore, the for-loop makes at most\\n3p(n) iterations.\\nSince B ∈P, there is an algorithm and a polynomial q, such that this\\nalgorithm, when given any input string z, decides in q(|z|) time, whether or\\nnot z ∈B. This input z has the form ⟨w, s⟩, and we have\\n|z| = |w| + |s| ≤|w| + p(|w|) = n + p(n).\\nIt follows that the total running time of our algorithm that decides whether\\nor not w ∈A, is bounded from above by\\n3p(n) · q(n + p(n))\\n≤\\n22p(n) · q(n + p(n))\\n≤\\n22p(n) · 2q(n+p(n))\\n=\\n2p′(n),\\nwhere p′ is the polynomial that is deﬁned by p′(n) := 2p(n) + q(n + p(n)).\\nIf we deﬁne the class EXP as\\nEXP :=\\n{A :\\nthere exists a polynomial p, such that A can be\\ndecided in time 2p(n) } ,\\nthen we have proved the following theorem.\\nTheorem 6.3.8 NP ⊆EXP.\\n6.4.\\nNon-deterministic algorithms\\n211\\n6.3.3\\nSummary\\n• P ⊆NP. It is not known whether P is a proper subclass of NP, or\\nwhether P = NP. This is one of the most important open problems in\\ncomputer science. If you can solve this problem, then you will get one\\nmillion dollars; not from us, but from the Clay Mathematics Institute,\\nsee\\nhttp://www.claymath.org/prizeproblems/index.htm\\nMost people believe that P is a proper subclass of NP.\\n• NP ⊆EXP, i.e., each language in NP can be decided in exponential\\ntime. It is not known whether NP is a proper subclass of EXP, or\\nwhether NP = EXP.\\n• It follows from P ⊆NP and NP ⊆EXP, that P ⊆EXP. It can\\nbe shown that P is a proper subset of EXP, i.e., there exist languages\\nthat can be decided in exponential time, but that cannot be decided in\\npolynomial time.\\n• P is the class of those languages that can be decided eﬃciently, i.e., in\\npolynomial time. Sets that are not in P, are not eﬃciently decidable.\\n6.4\\nNon-deterministic algorithms\\nThe abbreviation NP stands for Non-deterministic Polynomial time. The al-\\ngorithms that we have considered so far are deterministic, which means that\\nat any time during the computation, the next computation step is uniquely\\ndetermined. In a non-deterministic algorithm, there are one or more possi-\\nbilities for being the next computation step, and the algorithm chooses one\\nof them.\\nTo give an example, we consider the language SOS, see Example 6.3.3.\\nLet m, a1, a2, . . . , am, and b be elements of N0. Then\\n⟨a1, a2, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, c2, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciai = b.\\nThe following non-deterministic algorithm decides the language SOS:\\n212\\nChapter 6.\\nComplexity Theory\\nAlgorithm SOS(m, a1, a2, . . . , am, b):\\ns := 0;\\nfor i := 1 to m\\ndo s := s | s := s + ai\\nendfor;\\nif s = b\\nthen return YES\\nelse return NO\\nendif\\nThe line\\ns := s | s := s + ai\\nmeans that either the instruction “s := s” or the instruction “s := s + ai” is\\nexecuted.\\nLet us assume that ⟨a1, a2, . . . , am, b⟩∈SOS. Then there are c1, c2, . . . , cm ∈\\n{0, 1} such that Pm\\ni=1 ciai = b. Assume our algorithm does the following, for\\neach i with 1 ≤i ≤m: In the i-th iteration,\\n• if ci = 0, then it executes the instruction “s := s”,\\n• if ci = 1, then it executes the instruction “s := s + ai”.\\nThen after the for-loop, we have s = b, and the algorithm returns YES;\\nhence, the algorithm has correctly found out that ⟨a1, a2, . . . , am, b⟩∈SOS.\\nIn other words, in this case, there exists at least one accepting computation.\\nOn the other hand, if ⟨a1, a2, . . . , am, b⟩̸∈SOS, then the algorithm always\\nreturns NO, no matter which of the two instructions is executed in each\\niteration of the for-loop. In this case, there is no accepting computation.\\nDeﬁnition 6.4.1 Let M be a non-deterministic algorithm. We say that M\\naccepts a string w, if there exists at least one computation that, on input w,\\nreturns YES.\\nDeﬁnition 6.4.2 We say that a non-deterministic algorithm M decides a\\nlanguage A in time T, if for every string w, the following holds: w ∈A if\\nand only if there exists at least one computation that, on input w, returns\\nYES and that takes at most T(|w|) time.\\n6.5.\\nNP-complete languages\\n213\\nThe non-deterministic algorithm that we have seen above decides the\\nlanguage SOS in linear time: Let ⟨a1, a2, . . . , am, b⟩∈SOS, and let n be the\\nlength of this input. Then\\nn = |⟨a1⟩| + |⟨a2⟩| + . . . + |⟨am⟩| + |⟨b⟩| ≥m.\\nFor this input, there is a computation that returns YES and that takes\\nO(m) = O(n) time.\\nAs in Section 6.2, we deﬁne the notion of “polynomial time” for non-\\ndeterministic algorithms. The following theorem relates this notion to the\\nclass NP that we deﬁned in Deﬁnition 6.3.6.\\nTheorem 6.4.3 A language A is in the class NP if and only if there exists\\na non-deterministic Turing machine (or Java program) that decides A in\\npolynomial time.\\n6.5\\nNP-complete languages\\nLanguages in the class P are considered easy, i.e., they can be decided in\\npolynomial time. People believe (but cannot prove) that P is a proper sub-\\nclass of NP. If this is true, then there are languages in NP that are hard,\\ni.e., cannot be decided in polynomial time.\\nIntuition tells us that if P ̸= NP, then the hardest languages in NP are\\nnot contained in P. These languages are called NP-complete. In this section,\\nwe will give a formal deﬁnition of this concept.\\nIf we want to talk about the “hardest” languages in NP, then we have to\\nbe able to compare two languages according to their “diﬃculty”. The idea is\\nas follows: We say that a language B is “at least as hard” as a language A,\\nif the following holds: If B can be decided in polynomial time, then A can\\nalso be decided in polynomial time.\\nDeﬁnition 6.5.1 Let A ⊆{0, 1}∗and B ⊆{0, 1}∗be languages. We say\\nthat A ≤P B, if there exists a function\\nf : {0, 1}∗→{0, 1}∗\\nsuch that\\n1. f ∈FP and\\n214\\nChapter 6.\\nComplexity Theory\\n2. for all strings w in {0, 1}∗,\\nw ∈A ⇐⇒f(w) ∈B.\\nIf A ≤P B, then we also say that “B is at least as hard as A”, or “A is\\npolynomial-time reducible to B”.\\nWe ﬁrst show that this formal deﬁnition is in accordance with the intuitive\\ndeﬁnition given above.\\nTheorem 6.5.2 Let A and B be languages such that B ∈P and A ≤P B.\\nThen A ∈P.\\nProof. Let f : {0, 1}∗→{0, 1}∗be the function in FP for which\\nw ∈A ⇐⇒f(w) ∈B.\\n(6.5)\\nThe following algorithm decides whether or not any given binary string w is\\nin A:\\nu := f(w);\\nif u ∈B\\nthen return YES\\nelse return NO\\nendif\\nThe correctness of this algorithm follows immediately from (6.5). So it\\nremains to show that the running time is polynomial in the length of the\\ninput string w.\\nSince f ∈FP, there exists a polynomial p such that the function f can\\nbe computed in time p. Similarly, since B ∈P, there exists a polynomial q,\\nsuch that the language B can be decided in time q.\\nLet n be the length of the input string w, i.e., n = |w|. Then the length\\nof the string u is less than or equal to p(|w|) = p(n). (Why?) Therefore, the\\nrunning time of our algorithm is bounded from above by\\np(|w|) + q(|u|) ≤p(n) + q(p(n)).\\nSince the function p′, deﬁned by p′(n) := p(n)+q(p(n)), is a polynomial, this\\nproves that A ∈P.\\nThe following theorem states that the relation ≤P is reﬂexive and tran-\\nsitive. We leave the proof as an exercise.\\n6.5.\\nNP-complete languages\\n215\\nTheorem 6.5.3 Let A, B, and C be languages. Then\\n1. A ≤P A, and\\n2. if A ≤P B and B ≤P C, then A ≤P C.\\nWe next show that the languages in P are the easiest languages in NP:\\nTheorem 6.5.4 Let A be a language in P, and let B be an arbitrary lan-\\nguage such that B ̸= ∅and B ̸= {0, 1}∗. Then A ≤P B.\\nProof. We choose two strings u and v in {0, 1}∗, such that u ∈B and v ̸∈B.\\n(Observe that this is possible.) Deﬁne the function f : {0, 1}∗→{0, 1}∗by\\nf(w) :=\\n\\x1a u\\nif w ∈A,\\nv\\nif w ̸∈A.\\nThen it is clear that for any binary string w,\\nw ∈A ⇐⇒f(w) ∈B.\\nSince A ∈P, the function f can be computed in polynomial time, i.e.,\\nf ∈FP.\\n6.5.1\\nTwo examples of reductions\\nSum of subsets and knapsacks\\nWe start with a simple reduction. Consider the two languages\\nSOS := {⟨a1, . . . , am, b⟩:\\nm, a1, . . . , am, b ∈N0 and there exist\\nc1, . . . , cm ∈{0, 1}, such that Pm\\ni=1 ciai = b}\\nand\\nKS\\n:=\\n{⟨w1, . . . , wm, k1, . . . , km, W, K⟩:\\nm, w1, . . . , wm, k1, . . . , km, W, K ∈N0\\nand there exist c1, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciwi ≤W and Pm\\ni=1 ciki ≥K}.\\nThe notation KS stands for knapsack: We have m pieces of food. The\\ni-th piece has weight wi and contains ki calories. We want to decide whether\\nor not we can ﬁll our knapsack with a subset of the pieces of food such that\\nthe total weight is at most W, and the total amount of calories is at least K.\\n216\\nChapter 6.\\nComplexity Theory\\nTheorem 6.5.5 SOS ≤P KS.\\nProof. Let us ﬁrst see what we have to show. According to Deﬁnition 6.5.1,\\nwe need a function f ∈FP, that maps input strings for SOS to input strings\\nfor KS, in such a way that\\n⟨a1, . . . , am, b⟩∈SOS ⇐⇒f(⟨a1, . . . , am, b⟩) ∈KS.\\nIn order for f(⟨a1, . . . , am, b⟩) to be an input string for KS, this function\\nvalue has to be of the form\\nf(⟨a1, . . . , am, b⟩) = ⟨w1, . . . , wm, k1, . . . , km, W, K⟩.\\nWe deﬁne\\nf(⟨a1, . . . , am, b⟩) := ⟨a1, . . . , am, a1, . . . , am, b, b⟩.\\nIt is clear that f ∈FP. We have\\n⟨a1, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai = b\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai ≤b and Pm\\ni=1 ciai ≥b\\n⇐⇒\\n⟨a1, . . . , am, a1, . . . , am, b, b⟩∈KS\\n⇐⇒\\nf(⟨a1, . . . , am, b⟩) ∈KS.\\nCliques and Boolean formulas\\nWe will deﬁne two languages A = 3SAT and B = Clique that have, at\\nﬁrst sight, nothing to do with each other. Then we show that, nevertheless,\\nA ≤P B.\\nLet G be a graph with vertex set V and edge set E. A subset V ′ of V is\\ncalled a clique, if each pair of distinct vertices in V ′ is connected by an edge\\nin E. We deﬁne the following language:\\nClique := {⟨G, k⟩: k ∈N and G has a clique with k vertices}.\\nWe encourage you to prove the following claim:\\n6.5.\\nNP-complete languages\\n217\\nTheorem 6.5.6 Clique ∈NP.\\nNext we consider Boolean formulas ϕ, with variables x1, x2, . . . , xm, hav-\\ning the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.6)\\nwhere each Ci, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nEach ℓi\\na is either a variable or the negation of a variable. An example of such\\na formula is\\nϕ = (x1 ∨¬x1 ∨¬x2) ∧(x3 ∨x2 ∨x4) ∧(¬x1 ∨¬x3 ∨¬x4).\\nA formula ϕ of the form (6.6) is said to be satisﬁable, if there exists a truth-\\nvalue in {0, 1} for each of the variables x1, x2, . . . , xm, such that the entire\\nformula ϕ is true. Our example formula is satisﬁable: If we take x1 = 0 and\\nx2 = 1, and give x3 and x4 an arbitrary value, then\\nϕ = (0 ∨1 ∨0) ∧(x3 ∨1 ∨x4) ∧(1 ∨¬x3 ∨¬x4) = 1.\\nWe deﬁne the following language:\\n3SAT := {⟨ϕ⟩: ϕ is of the form (6.6) and is satisﬁable}.\\nAgain, we encourage you to prove the following claim:\\nTheorem 6.5.7 3SAT ∈NP.\\nObserve that the elements of Clique (which are pairs consisting of a graph\\nand a positive integer) are completely diﬀerent from the elements of 3SAT\\n(which are Boolean formulas). We will show that 3SAT ≤P Clique. Recall\\nthat this means the following: If the language Clique can be decided in\\npolynomial time, then the language 3SAT can also be decided in polynomial\\ntime. In other words, any polynomial-time algorithm that decides Clique can\\nbe converted to a polynomial-time algorithm that decides 3SAT.\\nTheorem 6.5.8 3SAT ≤P Clique.\\n218\\nChapter 6.\\nComplexity Theory\\nProof. We have to show that there exists a function f ∈FP, that maps\\ninput strings for 3SAT to input strings for Clique, such that for each Boolean\\nformula ϕ that is of the form (6.6),\\n⟨ϕ⟩∈3SAT ⇐⇒f(⟨ϕ⟩) ∈Clique.\\nThe function f maps the binary string encoding an arbitrary Boolean formula\\nϕ to a binary string encoding a pair (G, k), where G is a graph and k is a\\npositive integer. We have to deﬁne this function f in such a way that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\nLet\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an arbitrary Boolean formula in the variables x1, x2, . . . , xm, where each\\nCi, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nRemember that each ℓi\\na is either a variable or the negation of a variable.\\nThe formula ϕ is mapped to the pair (G, k), where the vertex set V and\\nthe edge set E of the graph G are deﬁned as follows:\\n• V = {v1\\n1, v1\\n2, v1\\n3, . . . , vk\\n1, vk\\n2, vk\\n3}. The idea is that each vertex vi\\na corre-\\nsponds to one term ℓi\\na.\\n• The pair (vi\\na, vj\\nb) of vertices form an edge in E if and only if\\n– i ̸= j and\\n– ℓi\\na is not the negation of ℓj\\nb.\\nTo give an example, let ϕ be the Boolean formula\\nϕ = (x1 ∨¬x2 ∨¬x3) ∧(¬x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3),\\n(6.7)\\ni.e., k = 3, C1 = x1 ∨¬x2 ∨¬x3, C2 = ¬x1 ∨x2 ∨x3, and C3 = x1 ∨x2 ∨x3.\\nThe graph G that corresponds to this formula is given in Figure 6.4.\\nIt is not diﬃcult to see that the function f can be computed in polynomial\\ntime. So it remains to prove that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\n(6.8)\\n6.5.\\nNP-complete languages\\n219\\n¬x2\\n¬x3\\nx1\\n¬x1\\nx2\\nx3\\nx1\\nx2\\nx3\\nFigure 6.4: The formula ϕ in (6.7) is mapped to this graph. The vertices on\\nthe top represent C1; the vertices on the left represent C2; the vertices on\\nthe right represent C3.\\nTo prove this, we ﬁrst assume that the formula\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nis satisﬁable. Then there exists a truth-value in {0, 1} for each of the variables\\nx1, x2, . . . , xm, such that the entire formula ϕ is true. Hence, for each i with\\n1 ≤i ≤k, there is at least one term ℓi\\na in\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3\\nthat is true (i.e., has value 1).\\nLet V ′ be the set of vertices obtained by choosing for each i, 1 ≤i ≤k,\\nexactly one vertex vi\\na such that ℓi\\na has value 1.\\nIt is clear that V ′ contains exactly k vertices. We claim that this set is\\na clique in G. To prove this claim, let vi\\na and vj\\nb be two distinct vertices in\\nV ′. It follows from the deﬁnition of V ′ that i ̸= j and ℓi\\na = ℓj\\nb = 1. Hence,\\nℓi\\na is not the negation of ℓj\\nb. But this means that the vertices vi\\na and vj\\nb are\\nconnected by an edge in G.\\nThis proves one direction of (6.8). To prove the other direction, we assume\\nthat the graph G contains a clique V ′ with k vertices.\\n220\\nChapter 6.\\nComplexity Theory\\nThe vertices of G consist of k groups, where each group contains exactly\\nthree vertices. Since vertices within the same group are not connected by\\nedges, the clique V ′ contains exactly one vertex from each group. Hence, for\\neach i with 1 ≤i ≤k, there is exactly one a, such that vi\\na ∈V ′. Consider\\nthe corresponding term ℓi\\na. We know that this term is either a variable or\\nthe negation of a variable, i.e., ℓi\\na is either of the form xj or of the form ¬xj.\\nIf ℓi\\na = xj, then we give xj the truth-value 1. Otherwise, we have ℓi\\na = ¬xj,\\nin which case we give xj the truth-value 0. Since V ′ is a clique, each variable\\ngets at most one truth-value. If a variable has no truth-value yet, then we\\ngive it an arbitrary truth-value.\\nIf we substitute these truth-values into ϕ, then the entire formula has\\nvalue 1. Hence, ϕ is satisﬁable.\\nIn order to get a better understanding of this proof, you should verify the\\nproof for the formula ϕ in (6.7) and the graph G in Figure 6.4.\\n6.5.2\\nDeﬁnition of NP-completeness\\nReductions, as deﬁned in Deﬁnition 6.5.1, allow us to compare two language\\naccording to their diﬃculty. A language B in NP is called NP-complete,\\nif B belongs to the most diﬃcult languages in NP; in other words, B is at\\nleast as hard as any other language in NP.\\nDeﬁnition 6.5.9 Let B ⊆{0, 1}∗be a language. We say that B is NP-\\ncomplete, if\\n1. B ∈NP and\\n2. A ≤P B, for every language A in NP.\\nTheorem 6.5.10 Let B be an NP-complete language. Then\\nB ∈P ⇐⇒P = NP.\\nProof. Intuitively, this theorem should be true: If the language B is in P,\\nthen B is an easy language. On the other hand, since B is NP-complete,\\nit belongs to the most diﬃcult languages in NP. Hence, the most diﬃcult\\nlanguage in NP is easy. But then all languages in NP must be easy, i.e.,\\nP = NP.\\n6.5.\\nNP-complete languages\\n221\\nWe give a formal proof. Let us ﬁrst assume that B ∈P. We already\\nknow that P ⊆NP. Hence, it remains to show that NP ⊆P. Let A be an\\narbitrary language in NP. Since B is NP-complete, we have A ≤P B. Then,\\nby Theorem 6.5.2, we have A ∈P.\\nTo prove the converse, assume that P = NP. Since B ∈NP, it follows\\nimmediately that B ∈P.\\nTheorem 6.5.11 Let B and C be languages, such that C ∈NP and B ≤P\\nC. If B is NP-complete, then C is also NP-complete.\\nProof. First, we give an intuitive explanation of the claim: By assumption,\\nB belongs to the most diﬃcult languages in NP, and C is at least as hard as\\nB. Since C ∈NP, it follows that C belongs to the most diﬃcult languages\\nin NP. Hence, C is NP-complete.\\nTo give a formal proof, we have to show that A ≤P C, for all languages A\\nin NP. Let A be an arbitrary language in NP. Since B is NP-complete, we\\nhave A ≤P B. Since B ≤P C, it follows from Theorem 6.5.3, that A ≤P C.\\nTherefore, C is NP-complete.\\nTheorem 6.5.11 can be used to prove the NP-completeness of languages:\\nLet C be a language, and assume that we want to prove that C is NP-\\ncomplete. We can do this in the following way:\\n1. We ﬁrst prove that C ∈NP.\\n2. Then we ﬁnd a language B that looks “similar” to C, and for which\\nwe already know that it is NP-complete.\\n3. Finally, we prove that B ≤P C.\\n4. Then, Theorem 6.5.11 tells us that C is NP-complete.\\nOf course, this leads to the question “How do we know that the language\\nB is NP-complete?” In order to apply Theorem 6.5.11, we need a “ﬁrst” NP-\\ncomplete language; the NP-completeness of this language must be proven\\nusing Deﬁnition 6.5.9.\\nObserve that it is not clear at all that there exist NP-complete languages!\\nFor example, consider the language 3SAT. If we want to use Deﬁnition 6.5.9\\nto show that this language is NP-complete, then we have to show that\\n222\\nChapter 6.\\nComplexity Theory\\n• 3SAT ∈NP. We know from Theorem 6.5.7 that this is true.\\n• A ≤P 3SAT, for every language A ∈NP. Hence, we have to show this\\nfor languages A such as kColor, HC, SOS, NPrim, KS, Clique, and\\nfor inﬁnitely many other languages.\\nIn 1971, Cook has exactly done this: He showed that the language 3SAT\\nis NP-complete. Since his proof is rather technical, we will prove the NP-\\ncompleteness of another language.\\n6.5.3\\nAn NP-complete domino game\\nWe are given a ﬁnite collection of tile types. For each such type, there are\\narbitrarily many tiles of this type. A tile is a square that is partitioned into\\nfour triangles. Each of these triangles contains a symbol that belongs to a\\nﬁnite alphabet Σ. Hence, a tile looks as follows:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\na\\nb\\nc\\nd\\nWe are also given a square frame, consisting of cells. Each cell has the same\\nsize as a tile, and contains a symbol of Σ.\\nThe problem is to decide whether or not this domino game has a solution.\\nThat is, can we completely ﬁll the frame with tiles such that\\n• for any two neighboring tiles s and s′, the two triangles of s and s′ that\\ntouch each other contain the same symbol, and\\n• each triangle that touches the frame contains the same symbol as the\\ncell of the frame that is touched by this triangle.\\nThere is one ﬁnal restriction: The orientation of the tiles is ﬁxed, they cannot\\nbe rotated.\\nLet us give a formal deﬁnition of this problem. We assume that the sym-\\nbols belong to the ﬁnite alphabet Σ = {0, 1}m, i.e., each symbol is encoded\\nas a bit-string of length m. Then, a tile type can be encoded as a tuple of\\nfour bit-strings, i.e., as an element of Σ4. A frame consisting of t rows and t\\ncolumns can be encoded as a string in Σ4t.\\n6.5.\\nNP-complete languages\\n223\\nWe denote the language of all solvable domino games by Domino:\\nDomino\\n:=\\n{⟨m, k, t, R, T1, . . . , Tk⟩:\\nm ≥1, k ≥1, t ≥1, R ∈Σ4t, Ti ∈Σ4, 1 ≤i ≤k,\\nframe R can be ﬁlled using tiles of types\\nT1, . . . , Tk.}\\nWe will prove the following theorem.\\nTheorem 6.5.12 The language Domino is NP-complete.\\nProof. It is clear that Domino ∈NP: A solution consists of a t × t matrix,\\nin which the (i, j)-entry indicates the type of the tile that occupies position\\n(i, j) in the frame. The number of bits needed to specify such a solution is\\npolynomial in the length of the input. Moreover, we can verify in polynomial\\ntime whether or not any given “solution” is correct.\\nIt remains to show that\\nA ≤P Domino, for every language A in NP.\\nLet A be an arbitrary language in NP. Then there exist a polynomial p and\\na non-deterministic Turing machine M, that decides the language A in time\\np. We may assume that this Turing machine has only one tape.\\nOn input w = a1a2 . . . an, the Turing machine M starts in the start state\\nz0, with its tape head on the cell containing the symbol a1. We may assume\\nthat during the entire computation, the tape head never moves to the left of\\nthis initial cell. Hence, the entire computation “takes place” in and to the\\nright of the initial cell. We know that\\nw ∈A\\n⇐⇒\\non input w, there exists an accepting computation\\nthat makes at most p(n) computation steps.\\nAt the end of such an accepting computation, the tape only contains the\\nsymbol 1, which we may assume to be in the initial cell, and M is in the ﬁnal\\nstate z1. In this case, we may assume that the accepting computation makes\\nexactly p(n) computation steps. (If this is not the case, then we extend the\\ncomputation using the instruction z11 →z11N.)\\nWe need one more technical detail: We may assume that za →z′bR and\\nza′ →z′′b′L are not both instructions of M. Hence, the state of the Turing\\nmachine uniquely determines the direction in which the tape head moves.\\n224\\nChapter 6.\\nComplexity Theory\\nWe have to deﬁne a domino game, that depends on the input string w\\nand the Turing machine M, such that\\nw ∈A ⇐⇒this domino game is solvable.\\nThe idea is to encode an accepting computation of the Turing machine M as\\na solution of the domino game. In order to do this, we use a frame in which\\neach row corresponds to one computation step. This frame consists of p(n)\\nrows. Since an accepting computation makes exactly p(n) computation steps,\\nand since the tape head never moves to the left of the initial cell, this tape\\nhead can visit only p(n) cells. Therefore, our frame will have p(n) columns.\\nThe domino game will use the following tile types:\\n1. For each symbol a in the alphabet of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\n#\\na\\nIntuition: Before and after the computation step, the tape head is not\\non this cell.\\n2. For each instruction za →z′bR of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\nz′\\nb\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the right.\\n3. For each instruction za →z′bL of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz′\\n(z, a)\\n#\\nb\\n6.5.\\nNP-complete languages\\n225\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the left.\\n4. For each instruction za →z′bN of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\n#\\n(z′, b)\\nIntuition: Before and after the computation step, the tape head is on\\nthis cell.\\n5. For each state z and for each symbol a in the alphabet of the Turing\\nmachine M, there are two tile types:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz\\na\\n#\\n(z, a)\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\nz\\n(z, a)\\nIntuition: The leftmost tile indicates that the tape head enters this cell\\nfrom the left; the righmost tile indicates that the tape head enters this\\ncell from the right.\\nThis speciﬁes all tile types. The p(n) × p(n) frame is given in Figure 6.5.\\nThe top row corresponds to the start of the computation, whereas the bottom\\nrow corresponds to the end of the computation. The left and right columns\\ncorrespond to the part of the tape in which the tape head can move.\\nThe encodings of these tile types and the frame can be computed in\\npolynomial time.\\nIt can be shown that, for any input string w, any accepting computation\\nof length p(n) of the Turing machine M can be encoded as a solution of\\nthis domino game. Conversely, any solution of this domino game can be\\n“translated” to an accepting computation of length p(n) of M, on input\\nstring w. Hence, the following holds.\\nw ∈A\\n⇐⇒\\nthere exists an accepting computation that makes\\np(n) computation steps\\n⇐⇒\\nthe domino game is solvable.\\n226\\nChapter 6.\\nComplexity Theory\\n(z0, a1)\\na2\\n. . .\\nan\\n✷\\n. . .\\n✷\\n#\\n#\\n#\\n#\\n#\\n...\\n#\\n...\\n✷\\n✷\\n✷\\n✷\\n✷\\n. . .\\n(z1, 1)\\np(n)\\np(n)\\nFigure 6.5: The p(n) × p(n) frame for the domino game.\\nTherefore, we have A ≤P Domino. Hence, the language Domino is NP-\\ncomplete.\\nAn example of a domino game\\nWe have deﬁned the domino game corresponding to a Turing machine that\\nsolves a decision problem. Of course, we can also do this for Turing machines\\nthat compute functions. In this section, we will exactly do this for a Turing\\nmachine that computes the successor function x →x + 1.\\nWe will design a Turing machine with one tape, that gets as input the\\nbinary representation of a natural number x, and that computes the binary\\nrepresentation of x + 1.\\nStart of the computation: The tape contains a 0 followed by the binary\\nrepresentation of the integer x ∈N0. The tape head is on the leftmost bit\\n(which is 0), and the Turing machine is in the start state z0. Here is an\\nexample, where x = 431:\\n6.5.\\nNP-complete languages\\n227\\n0 1 1 0 1 0 1 1 1 1 2\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe number x + 1. The tape head is on the rightmost 1, and the Turing\\nmachine is in the ﬁnal state z1. For our example, the tape looks as follows:\\n0 1 1 0 1 1 0 0 0 0 2\\n6\\nOur Turing machine will use the following states:\\nz0 :\\nstart state; tape head moves to the right\\nz1 :\\nﬁnal state\\nz2 :\\ntape head moves to the left; on its way to the left, it has not read 0\\nThe Turing machine has the following instructions:\\nz00 →z00R\\nz21 →z20L\\nz01 →z01R\\nz20 →z11N\\nz02 →z22L\\nIn Figure 6.6, you can see the sequence of states and tape contents of this\\nTuring machine on input x = 11.\\nWe now construct the domino game that corresponds to the computation\\nof this Turing machine on input x = 11. Following the general construction\\nin Section 6.5.3, we obtain the following tile types:\\n1. The three symbols of the alphabet yield three tile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n2\\n2\\n2. The ﬁve instructions of the Turing machine yield ﬁve tile types:\\n228\\nChapter 6.\\nComplexity Theory\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n0\\n(z0, 1)\\n0\\n1\\n1\\n2\\n0\\n1\\n(z0, 0)\\n1\\n1\\n2\\n0\\n1\\n0\\n(z0, 1)\\n1\\n2\\n0\\n1\\n0\\n1\\n(z0, 1)\\n2\\n0\\n1\\n0\\n1\\n1\\n(z0, 2)\\n0\\n1\\n0\\n1\\n(z2, 1)\\n2\\n0\\n1\\n0\\n(z2, 1)\\n0\\n2\\n0\\n1\\n(z2, 0)\\n0\\n0\\n2\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\nFigure 6.6: The computation of the Turing machine on input x = 11. The\\npair (state,symbol) indicates the position of the tape head.\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n3. The states z0 and z2, and the three symbols of the alphabet yield twelve\\ntile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 2)\\n2\\nThe computation of the Turing machine on input x = 11 consists of nine\\ncomputation steps. During this computation, the tape head visits exactly\\nsix cells. Therefore, the frame for the domino game has nine rows and six\\ncolumns.\\nThis frame is given in Figure 6.7.\\nIn Figure 6.8, you ﬁnd the\\nsolution of the domino game.\\nObserve that this solution is nothing but\\nan equivalent way of writing the computation of Figure 6.6.\\nHence, the\\ncomputation of the Turing machine corresponds to a solution of the domino\\ngame; in fact, the converse also holds.\\n6.5.\\nNP-complete languages\\n229\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\nFigure 6.7: The frame for the domino game for input x = 11.\\n230\\nChapter 6.\\nComplexity Theory\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\nFigure 6.8: The solution for the domino game for input x = 11.\\n6.5.\\nNP-complete languages\\n231\\n6.5.4\\nExamples of NP-complete languages\\nIn Section 6.5.3, we have shown that Domino is NP-complete. Using this\\nresult, we will apply Theorem 6.5.11 to prove the NP-completeness of some\\nother languages.\\nSatisﬁability\\nWe consider Boolean formulas ϕ, in the variables x1, x2, . . . , xm, having the\\nform\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.9)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nEach ℓi\\nj is either a variable or the negation of a variable. Such a formula ϕ\\nis said to be satisﬁable, if there exists a truth-value in {0, 1} for each of the\\nvariables x1, x2, . . . , xm, such that the entire formula ϕ is true. We deﬁne the\\nfollowing language:\\nSAT := {⟨ϕ⟩: ϕ is of the form (6.9) and is satisﬁable}.\\nWe will prove that SAT is NP-complete.\\nIt is clear that SAT ∈NP. If we can show that\\nDomino ≤P SAT,\\nthen it follows from Theorem 6.5.11 that SAT is NP-complete. (In Theo-\\nrem 6.5.11, take B := Domino and C := SAT.)\\nHence, we need a function f ∈FP, that maps input strings for Domino\\nto input strings for SAT, in such a way that for every domino game D, the\\nfollowing holds:\\ndomino game D is solvable ⇐⇒the formula encoded by the\\nstring f(⟨D⟩) is satisﬁable.\\n(6.10)\\nLet us consider an arbitrary domino game D. Let k be the number of\\ntile types, and let the frame have t rows and t columns. We denote the tile\\ntypes by T1, T2, . . . , Tk.\\n232\\nChapter 6.\\nComplexity Theory\\nWe map this domino game D to a Boolean formula ϕ, such that (6.10)\\nholds. The formula ϕ will have variables\\nxijℓ, 1 ≤i ≤t, 1 ≤j ≤t, 1 ≤ℓ≤k.\\nThese variables can be interpretated as follows:\\nxijℓ= 1 ⇐⇒there is a tile of type Tℓat position (i, j) of the frame.\\nWe deﬁne:\\n• For all i and j with 1 ≤i ≤t and 1 ≤j ≤t:\\nC1\\nij := xij1 ∨xij2 ∨. . . ∨xijk.\\nThis formula expresses the condition that there is at least one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j ≤t, and 1 ≤ℓ< ℓ′ ≤k:\\nC2\\nijℓℓ′ := ¬xijℓ∨¬xijℓ′.\\nThis formula expresses the condition that there is at most one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j < t, 1 ≤ℓ≤k and 1 ≤ℓ′ ≤k,\\nsuch that i < t and the right symbol on a tile of type Tℓis not equal\\nto the left symbol on a tile of type Tℓ′:\\nC3\\nijℓℓ′ := ¬xijℓ∨¬xi,j+1,ℓ′.\\nThis formula expresses the condition that neighboring tiles in the same\\nrow “ﬁt” together. There are symmetric formulas for neighboring tiles\\nin the same column.\\n• For all j and ℓwith 1 ≤j ≤t and 1 ≤ℓ≤k, such that the top symbol\\non a tile of type Tℓis not equal to the symbol at position j of the upper\\nboundary of the frame:\\nC4\\njℓ:= ¬x1jℓ.\\nThis formula expresses the condition that tiles that touch the upper\\nboundary of the frame “ﬁt” there. There are symmetric formulas for\\nthe lower, left, and right boundaries of the frame.\\n6.5.\\nNP-complete languages\\n233\\nThe formula ϕ is the conjunction of all these formulas C1\\nij, C2\\nijℓℓ′, C3\\nijℓℓ′, and\\nC4\\njℓ. The complete formula ϕ consists of\\nO(t2k + t2k2 + t2k2 + tk) = O(t2k2)\\nterms, i.e., its length is polynomial in the length of the domino game. This\\nimplies that ϕ can be constructed in polynomial time. Hence, the function\\nf that maps the domino game D to the Boolean formula ϕ, is in the class\\nFP. It is not diﬃcult to see that (6.10) holds for this function f. Therefore,\\nwe have proved the following result.\\nTheorem 6.5.13 The language SAT is NP-complete.\\nIn Section 6.5.1, we have deﬁned the language 3SAT.\\nTheorem 6.5.14 The language 3SAT is NP-complete.\\nProof. It is clear that 3SAT ∈NP. If we can show that\\nSAT ≤P 3SAT,\\nthen the claim follows from Theorem 6.5.11. Let\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an input for SAT, in the variables x1, x2, . . . , xm. We map ϕ, in polynomial\\ntime, to an input ϕ′ for 3SAT, such that\\nϕ is satisﬁable ⇐⇒ϕ′ is satisﬁable.\\n(6.11)\\nFor each i with 1 ≤i ≤k, we do the following. Consider\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\n• If ki = 1, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n1 ∨ℓi\\n1.\\n• If ki = 2, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n2.\\n234\\nChapter 6.\\nComplexity Theory\\n• If ki = 3, then we deﬁne\\nC′\\ni := Ci.\\n• If ki ≥4, then we deﬁne\\nC′\\ni\\n:=\\n(ℓi\\n1 ∨ℓi\\n2 ∨zi\\n1) ∧(¬zi\\n1 ∨ℓi\\n3 ∨zi\\n2) ∧(¬zi\\n2 ∨ℓi\\n4 ∨zi\\n3) ∧. . .\\n∧(¬zi\\nki−3 ∨ℓi\\nki−1 ∨ℓi\\nki),\\nwhere zi\\n1, . . . , zi\\nki−3 are new variables.\\nLet\\nϕ′ := C′\\n1 ∧C′\\n2 ∧. . . ∧C′\\nk.\\nThen ϕ′ is an input for 3SAT, and (6.11) holds.\\nTheorems 6.5.6, 6.5.8, 6.5.11, and 6.5.14 imply:\\nTheorem 6.5.15 The language Clique is NP-complete.\\nThe traveling salesperson problem\\nWe are given two positive integers k and m, a set of m cities, and an integer\\nm × m matrix M, where\\nM(i, j) = the cost of driving from city i to city j,\\nfor all i, j ∈{1, 2, . . . , m}. We want to decide whether or not there is a tour\\nthrough all cities whose total cost is less than or equal to k. This problem is\\nNP-complete.\\nBin packing\\nWe are given three positive integers m, k, and ℓ, a set of m objects having\\nvolumes a1, a2, . . . , am, and k bins. Each bin has volume ℓ. We want to\\ndecide whether or not all objects ﬁt within these bins. This problem is NP-\\ncomplete.\\nHere is another interpretation of this problem: We are given m jobs that\\nneed time a1, a2, . . . , am to complete. We are also given k processors, and an\\ninteger ℓ. We want to decide whether or not it is possible to divide the jobs\\nover the k processors, such that no processor needs more than ℓtime.\\nExercises\\n235\\nTime tables\\nWe are given a set of courses, class rooms, and professors.\\nWe want to\\ndecide whether or not there exists a time table such that all courses are\\nbeing taught, no two courses are taught at the same time in the same class\\nroom, no professor teaches two courses at the same time, and conditions such\\nas “Prof. L. Azy does not teach before 1pm” are satisﬁed. This problem is\\nNP-complete.\\nMotion planning\\nWe are given two positive integers k and ℓ, a set of k polyhedra, and two\\npoints s and t in Q3. We want to decide whether or not there exists a path\\nbetween s and t, that does not intersect any of the polyhedra, and whose\\nlength is less than or equal to ℓ. This problem is NP-complete.\\nMap labeling\\nWe are given a map with m cities, where each city is represented by a point.\\nFor each city, we are given a rectangle that is large enough to contain the\\nname of the city. We want to decide whether or not these rectangles can be\\nplaced on the map, such that\\n• no two rectangles overlap,\\n• For each i with 1 ≤i ≤m, the point that represents city i is a corner\\nof its rectangle.\\nThis problem is NP-complete.\\nThis list of NP-complete problems can be extended almost arbitrarily:\\nFor thousands of problems, it is known that they are NP-complete. For all\\nof these, it is not known, whether or not they can be solved eﬃciently (i.e.,\\nin polynomial time). Collections of NP-complete problems can be found in\\nthe book\\n• M.R. Garey and D.S. Johnson. Computers and Intractability: A Guide\\nto the Theory of NP-Completeness. W.H. Freeman, New York, 1979,\\nand on the web page\\nhttp://www.nada.kth.se/~viggo/wwwcompendium/\\n236\\nChapter 6.\\nComplexity Theory\\nExercises\\n6.1 Prove that the function F : N →N, deﬁned by F(x) := 2x, is not in FP.\\n6.2 Prove Theorem 6.5.3.\\n6.3 Prove that the language Clique is in the class NP.\\n6.4 Prove that the language 3SAT is in the class NP.\\n6.5 We deﬁne the following languages:\\n• Sum of subset:\\nSOS := {⟨a1, a2, . . . , am, b⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai = b}.\\n• Set partition:\\nSP := {⟨a1, a2, . . . , am⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai =\\nX\\ni̸∈I\\nai}.\\n• Bin packing: BP is the set of all strings ⟨s1, s2, . . . , sm, B⟩for which\\n1. 0 < si < 1, for all i,\\n2. B ∈N,\\n3. the numbers s1, s2, . . . , sm ﬁt into B bins, where each bin has size\\none, i.e., there exists a partition of {1, 2, . . . , m} into subsets Ik,\\n1 ≤k ≤B, such that P\\ni∈Ik si ≤1 for all k, 1 ≤k ≤B.\\nFor example, ⟨1/6, 1/2, 1/5, 1/9, 3/5, 1/5, 1/2, 11/18, 3⟩∈BP, because\\nthe eight fractions ﬁt into three bins:\\n1/6 + 1/9 + 11/18 ≤1, 1/2 + 1/2 = 1, and 1/5 + 3/5 + 1/5 = 1.\\n1. Prove that SOS ≤P SP.\\n2. Prove that the language SOS is NP-complete. You may use the fact\\nthat the language SP is NP-complete.\\nExercises\\n237\\n3. Prove that the language BP is NP-complete. Again, you may use the\\nfact that the language SP is NP-complete.\\n6.6 Prove that 3Color ≤P 3SAT.\\nHint: For each vertex i, and for each of the three colors k, introduce a\\nBoolean variable xik.\\n6.7 The (0, 1)-integer programming language IP is deﬁned as follows:\\nIP := {⟨A, c⟩:\\nA is an integer m × n matrix for some m, n ∈N,\\nc is an integer vector of length m, and\\n∃x ∈{0, 1}n such that Ax ≤c (componentwise) }.\\nProve that the language IP is NP-complete. You may use the fact that\\nthe language SOS is NP-complete.\\n6.8 Let ϕ be a Boolean formula in the variables x1, x2, . . . , xm.\\nWe say that ϕ is in disjunctive normal form (DNF) if it is of the form\\nϕ = C1 ∨C2 ∨. . . ∨Ck,\\n(6.12)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∧ℓi\\n2 ∧. . . ∧ℓi\\nki.\\nEach ℓi\\nj is a literal, which is either a variable or the negation of a variable.\\nWe say that ϕ is in conjunctive normal form (CNF) if it is of the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.13)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nAgain, each ℓi\\nj is a literal.\\nWe deﬁne the following two languages:\\nDNFSAT := {⟨ϕ⟩: ϕ is in DNF-form and is satisﬁable},\\nand\\nCNFSAT := {⟨ϕ⟩: ϕ is in CNF-form and is satisﬁable}.\\n238\\nChapter 6.\\nComplexity Theory\\n1. Prove that the language DNFSAT is in P.\\n2. What is wrong with the following argument: Since we can rewrite\\nany Boolean formula in DNF-form, we have CNFSAT ≤P DNFSAT.\\nHence, since CNFSAT is NP-complete and since DNFSAT ∈P, we\\nhave P = NP.\\n3. Prove directly that for every language A in P, A ≤P CNFSAT. “Di-\\nrectly” means that you should not use the fact that CNFSAT is NP-\\ncomplete.\\n6.9 1 Prove that the polynomial upper bound on the length of the string y\\nin the deﬁnition of NP is necessary, in the sense that if it is left out, then\\nany enumerable language would satisfy the condition.\\nMore precisely, we say that the language A belongs to the class E, if there\\nexists a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃y : ⟨w, y⟩∈B.\\nProve that E is equal to the class of all enumerable languages.\\n1Thanks to Antoine Vigneron for poining out an error in a previous version of this\\nexercise.\\nChapter 7\\nSummary\\nWe have seen several diﬀerent models for “processing” languages, i.e., pro-\\ncessing sets of strings over some ﬁnite alphabet. For each of these models,\\nwe have asked the question which types of languages can be processed, and\\nwhich types of languages cannot be processed. In this ﬁnal chapter, we give\\na brief summary of these results.\\nRegular languages:\\nThis class of languages was considered in Chapter 2.\\nThe following statements are equivalent:\\n1. The language A is regular, i.e., there exists a deterministic ﬁnite au-\\ntomaton that accepts A.\\n2. There exists a nondeterministic ﬁnite automaton that accepts A.\\n3. There exists a regular expression that describes A.\\nThis claim was proved by the following conversions:\\n1. Every nondeterministic ﬁnite automaton can be converted to an equiv-\\nalent deterministic ﬁnite automaton.\\n2. Every deterministic ﬁnite automaton can be converted to an equivalent\\nregular expression.\\n3. Every regular expression can be converted to an equivalent nondeter-\\nministic ﬁnite automaton.\\n240\\nChapter 7.\\nSummary\\nWe have seen that the class of regular languages is closed under the regular\\noperations: If A and B are regular languages, then\\n1. A ∪B is regular,\\n2. AB is regular,\\n3. A∗is regular,\\n4. A is regular, and\\n5. A ∩B is regular.\\nFinally, the pumping lemma for regular languages gives a property that\\nevery regular language possesses. We have used this to prove that languages\\nsuch as {anbn : n ≥0} are not regular.\\nContext-free languages:\\nThis class of languages was considered in Chap-\\nter 3. We have seen that every regular language is context-free. Moreover,\\nthere exist languages, for example {anbn : n ≥0}, that are context-free, but\\nnot regular. The following statements are equivalent:\\n1. The language A is context-free, i.e., there exists a context-free grammar\\nwhose language is A.\\n2. There exists a context-free grammar in Chomsky normal form whose\\nlanguage is A.\\n3. There exists a nondeterministic pushdown automaton that accepts A.\\nThis claim was proved by the following conversions:\\n1. Every context-free grammar can be converted to an equivalent context-\\nfree grammar in Chomsky normal form.\\n2. Every context-free grammar in Chomsky normal form can be converted\\nto an equivalent nondeterministic pushdown automaton.\\n3. Every nondeterministic pushdown automaton can be converted to an\\nequivalent context-free grammar. (This conversion was not covered in\\nthis book.)\\nChapter 7.\\nSummary\\n241\\nNondeterministic pushdown automata are more powerful than determin-\\nistic pushdown automata: There exists a nondeterministic pushdown au-\\ntomaton that accepts the language\\n{vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|},\\nbut there is no deterministic pushdown automaton that accepts this language.\\n(We did not prove this in this book.)\\nWe have seen that the class of context-free languages is closed under\\nthe union, concatenation, and star operations: If A and B are context-free\\nlanguages, then\\n1. A ∪B is context-free,\\n2. AB is context-free, and\\n3. A∗is context-free.\\nHowever,\\n1. the intersection of two context-free languages is not necessarily context-\\nfree, and\\n2. the complement of a context-free language is not necessarily context-\\nfree.\\nFinally, the pumping lemma for context-free languages gives a property\\nthat every context-free language possesses. We have used this to prove that\\nlanguages such as {anbncn : n ≥0} are not context-free.\\nThe Church-Turing Thesis:\\nIn Chapter 4, we considered “reasonable”\\ncomputational devices that model real computers. Examples of such devices\\nare Turing machines (with one or more tapes) and Java programs. It turns\\nout that all known “reasonable” devices are equivalent, i.e., can be converted\\nto each other. This led to the Church-Turing Thesis:\\n• Every computational process that is intuitively considered to be an\\nalgorithm can be converted to a Turing machine.\\n242\\nChapter 7.\\nSummary\\nDecidable and enumerable languages:\\nThese classes of languages were\\nconsidered in Chapter 5. They are deﬁned based on “reasonable” computa-\\ntional devices, such as Turing machines and Java programs. We have seen\\nthat\\n1. every context-free language is decidable, and\\n2. every decidable language is enumerable.\\nMoreover,\\n1. there exist languages, for example {anbncn : n ≥0}, that are decidable,\\nbut not context-free,\\n2. there exist languages, for example the Halting Problem, that are enu-\\nmerable, but not decidable,\\n3. there exist languages, for example the complement of the Halting Prob-\\nlem, that are not enumerable.\\nIn fact,\\n1. the class of all languages is not countable, whereas\\n2. the class of all enumerable languages is countable.\\nThe following statements are equivalent:\\n1. The language A is decidable.\\n2. Both A and its complement A are enumerable.\\nComplexity classes:\\nThese classes of languages were considered in Chap-\\nter 6.\\n1. The class P consists of all languages that can be decided in polynomial\\ntime by a deterministic Turing machine.\\n2. The class NP consists of all languages that can be decided in poly-\\nnomial time by a nondeterministic Turing machine. Equivalently, a\\nlanguage A is in the class NP, if for every string w ∈A, there exists a\\n“solution” s, such that (i) the length of s is polynomial in the length\\nof w, and (ii) the correctness of s can be veriﬁed in polynomial time.\\nChapter 7.\\nSummary\\n243\\nThe following properties hold:\\n1. Every context-free language is in P. (We did not prove this).\\n2. Every language in P is also in NP.\\n3. It is not known if there exist languages that are in NP, but not in P.\\n4. Every language in NP is decidable.\\nWe have introduced reductions to deﬁne the notion of a language B to be\\n“at least as hard” as a language A. A language B is called NP-complete, if\\n1. B belongs to the class NP, and\\n2. B is at least as hard as every language in the class NP.\\nWe have seen that NP-complete exist.\\nThe ﬁgure below summarizes the relationships among the various classes\\nof languages.\\n244\\nChapter 7.\\nSummary\\nregular\\ncontext-free\\nP\\nNP\\ndecidable\\nenumerable\\nall languages\\n', 'Introduction to Theory of Computation\\nAnil Maheshwari\\nMichiel Smid\\nSchool of Computer Science\\nCarleton University\\nOttawa\\nCanada\\n{anil,michiel}@scs.carleton.ca\\nAugust 29, 2024\\nii\\nContents\\nContents\\nPreface\\nvi\\n1\\nIntroduction\\n1\\n1.1\\nPurpose and motivation\\n. . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1.1\\nComplexity theory\\n. . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.2\\nComputability theory . . . . . . . . . . . . . . . . . . .\\n2\\n1.1.3\\nAutomata theory . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.1.4\\nThis course\\n. . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n1.2\\nMathematical preliminaries\\n. . . . . . . . . . . . . . . . . . .\\n4\\n1.3\\nProof techniques\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n1.3.1\\nDirect proofs\\n. . . . . . . . . . . . . . . . . . . . . . .\\n8\\n1.3.2\\nConstructive proofs . . . . . . . . . . . . . . . . . . . .\\n9\\n1.3.3\\nNonconstructive proofs . . . . . . . . . . . . . . . . . .\\n10\\n1.3.4\\nProofs by contradiction . . . . . . . . . . . . . . . . . .\\n11\\n1.3.5\\nThe pigeon hole principle . . . . . . . . . . . . . . . . .\\n12\\n1.3.6\\nProofs by induction . . . . . . . . . . . . . . . . . . . .\\n13\\n1.3.7\\nMore examples of proofs . . . . . . . . . . . . . . . . .\\n15\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2\\nFinite Automata and Regular Languages\\n21\\n2.1\\nAn example: Controling a toll gate . . . . . . . . . . . . . . .\\n21\\n2.2\\nDeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . . . .\\n23\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton . . . . . . . . . .\\n26\\n2.2.2\\nA second example of a ﬁnite automaton\\n. . . . . . . .\\n28\\n2.2.3\\nA third example of a ﬁnite automaton\\n. . . . . . . . .\\n29\\n2.3\\nRegular operations . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n2.4\\nNondeterministic ﬁnite automata\\n. . . . . . . . . . . . . . . .\\n35\\n2.4.1\\nA ﬁrst example . . . . . . . . . . . . . . . . . . . . . .\\n35\\niv\\nContents\\n2.4.2\\nA second example . . . . . . . . . . . . . . . . . . . . .\\n37\\n2.4.3\\nA third example . . . . . . . . . . . . . . . . . . . . . .\\n38\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\n. . . .\\n39\\n2.5\\nEquivalence of DFAs and NFAs . . . . . . . . . . . . . . . . .\\n41\\n2.5.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n2.6\\nClosure under the regular operations\\n. . . . . . . . . . . . . .\\n48\\n2.7\\nRegular expressions . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n2.8\\nEquivalence of regular expressions and regular languages . . .\\n57\\n2.8.1\\nEvery regular expression describes a regular language .\\n58\\n2.8.2\\nConverting a DFA to a regular expression\\n. . . . . . .\\n61\\n2.9\\nThe pumping lemma and nonregular languages . . . . . . . . .\\n68\\n2.9.1\\nApplications of the pumping lemma . . . . . . . . . . .\\n70\\n2.10 Higman’s Theorem . . . . . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.1 Dickson’s Theorem . . . . . . . . . . . . . . . . . . . .\\n77\\n2.10.2 Proof of Higman’s Theorem . . . . . . . . . . . . . . .\\n78\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n81\\n3\\nContext-Free Languages\\n91\\n3.1\\nContext-free grammars . . . . . . . . . . . . . . . . . . . . . .\\n91\\n3.2\\nExamples of context-free grammars . . . . . . . . . . . . . . .\\n94\\n3.2.1\\nProperly nested parentheses . . . . . . . . . . . . . . .\\n94\\n3.2.2\\nA context-free grammar for a nonregular language . . .\\n95\\n3.2.3\\nA context-free grammar for the complement of a non-\\nregular language\\n. . . . . . . . . . . . . . . . . . . . .\\n97\\n3.2.4\\nA context-free grammar that veriﬁes addition\\n. . . . .\\n98\\n3.3\\nRegular languages are context-free . . . . . . . . . . . . . . . . 100\\n3.3.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 102\\n3.4\\nChomsky normal form\\n. . . . . . . . . . . . . . . . . . . . . . 104\\n3.4.1\\nAn example . . . . . . . . . . . . . . . . . . . . . . . . 109\\n3.5\\nPushdown automata\\n. . . . . . . . . . . . . . . . . . . . . . . 112\\n3.6\\nExamples of pushdown automata\\n. . . . . . . . . . . . . . . . 116\\n3.6.1\\nProperly nested parentheses . . . . . . . . . . . . . . . 116\\n3.6.2\\nStrings of the form 0n1n\\n. . . . . . . . . . . . . . . . . 117\\n3.6.3\\nStrings with b in the middle . . . . . . . . . . . . . . . 118\\n3.7\\nEquivalence of pushdown automata and context-free grammars 120\\n3.8\\nThe pumping lemma for context-free languages\\n. . . . . . . . 124\\n3.8.1\\nProof of the pumping lemma . . . . . . . . . . . . . . . 125\\n3.8.2\\nApplications of the pumping lemma . . . . . . . . . . . 128\\nContents\\nv\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\\n4\\nTuring Machines and the Church-Turing Thesis\\n137\\n4.1\\nDeﬁnition of a Turing machine . . . . . . . . . . . . . . . . . . 137\\n4.2\\nExamples of Turing machines\\n. . . . . . . . . . . . . . . . . . 141\\n4.2.1\\nAccepting palindromes using one tape\\n. . . . . . . . . 141\\n4.2.2\\nAccepting palindromes using two tapes . . . . . . . . . 142\\n4.2.3\\nAccepting anbncn using one tape . . . . . . . . . . . . . 143\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2} . . . . 145\\n4.2.5\\nAccepting ambncmn using one tape . . . . . . . . . . . . 147\\n4.3\\nMulti-tape Turing machines . . . . . . . . . . . . . . . . . . . 148\\n4.4\\nThe Church-Turing Thesis . . . . . . . . . . . . . . . . . . . . 151\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n5\\nDecidable and Undecidable Languages\\n157\\n5.1\\nDecidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n5.1.1\\nThe language ADFA . . . . . . . . . . . . . . . . . . . . 158\\n5.1.2\\nThe language ANFA . . . . . . . . . . . . . . . . . . . . 159\\n5.1.3\\nThe language ACFG . . . . . . . . . . . . . . . . . . . . 160\\n5.1.4\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 161\\n5.1.5\\nThe Halting Problem . . . . . . . . . . . . . . . . . . . 163\\n5.2\\nCountable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\\n5.2.1\\nThe Halting Problem revisited . . . . . . . . . . . . . . 168\\n5.3\\nRice’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 170\\n5.3.1\\nProof of Rice’s Theorem . . . . . . . . . . . . . . . . . 171\\n5.4\\nEnumerability . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\\n5.4.1\\nHilbert’s problem . . . . . . . . . . . . . . . . . . . . . 174\\n5.4.2\\nThe language ATM\\n. . . . . . . . . . . . . . . . . . . . 176\\n5.5\\nWhere does the term “enumerable” come from? . . . . . . . . 177\\n5.6\\nMost languages are not enumerable . . . . . . . . . . . . . . . 180\\n5.6.1\\nThe set of enumerable languages is countable\\n. . . . . 180\\n5.6.2\\nThe set of all languages is not countable . . . . . . . . 181\\n5.6.3\\nThere are languages that are not enumerable . . . . . . 183\\n5.7\\nThe relationship between decidable and enumerable languages 184\\n5.8\\nA language A such that both A and A are not enumerable . . 186\\n5.8.1\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 186\\n5.8.2\\nEQTM is not enumerable . . . . . . . . . . . . . . . . . 188\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\\nvi\\nContents\\n6\\nComplexity Theory\\n197\\n6.1\\nThe running time of algorithms . . . . . . . . . . . . . . . . . 197\\n6.2\\nThe complexity class P . . . . . . . . . . . . . . . . . . . . . . 199\\n6.2.1\\nSome examples . . . . . . . . . . . . . . . . . . . . . . 199\\n6.3\\nThe complexity class NP . . . . . . . . . . . . . . . . . . . . . 202\\n6.3.1\\nP is contained in NP . . . . . . . . . . . . . . . . . . . 208\\n6.3.2\\nDeciding NP-languages in exponential time\\n. . . . . . 208\\n6.3.3\\nSummary\\n. . . . . . . . . . . . . . . . . . . . . . . . . 211\\n6.4\\nNon-deterministic algorithms\\n. . . . . . . . . . . . . . . . . . 211\\n6.5\\nNP-complete languages\\n. . . . . . . . . . . . . . . . . . . . . 213\\n6.5.1\\nTwo examples of reductions . . . . . . . . . . . . . . . 215\\n6.5.2\\nDeﬁnition of NP-completeness . . . . . . . . . . . . . . 220\\n6.5.3\\nAn NP-complete domino game\\n. . . . . . . . . . . . . 222\\n6.5.4\\nExamples of NP-complete languages . . . . . . . . . . 231\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\\n7\\nSummary\\n239\\nPreface\\nThis is a free textbook for an undergraduate course on the Theory of Com-\\nputation, which we have been teaching at Carleton University since 2002.\\nUntil the 2011/2012 academic year, this course was oﬀered as a second-year\\ncourse (COMP 2805) and was compulsory for all Computer Science students.\\nStarting with the 2012/2013 academic year, the course has been downgraded\\nto a third-year optional course (COMP 3803).\\nWe have been developing this book since we started teaching this course.\\nCurrently, we cover most of the material from Chapters 2–5 during a 12-week\\nterm with three hours of classes per week.\\nThe material from Chapter 6, on Complexity Theory, is taught in the\\nthird-year course COMP 3804 (Design and Analysis of Algorithms). In the\\nearly years of COMP 2805, we gave a two-lecture overview of Complexity\\nTheory at the end of the term. Even though this overview has disappeared\\nfrom the course, we decided to keep Chapter 6. This chapter has not been\\nrevised/modiﬁed for a long time.\\nThe course as we teach it today has been inﬂuenced by the following two\\ntextbooks:\\n• Introduction to the Theory of Computation (second edition), by Michael\\nSipser, Thomson Course Technnology, Boston, 2006.\\n• Einf¨uhrung in die Theoretische Informatik, by Klaus Wagner, Springer-\\nVerlag, Berlin, 1994.\\nBesides reading this text, we recommend that you also take a look at\\nthese excellent textbooks, as well as one or more of the following ones:\\n• Elements of the Theory of Computation (second edition), by Harry\\nLewis and Christos Papadimitriou, Prentice-Hall, 1998.\\nviii\\n• Introduction to Languages and the Theory of Computation (third edi-\\ntion), by John Martin, McGraw-Hill, 2003.\\n• Introduction to Automata Theory, Languages, and Computation (third\\nedition), by John Hopcroft, Rajeev Motwani, Jeﬀrey Ullman, Addison\\nWesley, 2007.\\nPlease let us know if you ﬁnd errors, typos, simpler proofs, comments,\\nomissions, or if you think that some parts of the book “need improvement”.\\nChapter 1\\nIntroduction\\n1.1\\nPurpose and motivation\\nThis course is on the Theory of Computation, which tries to answer the\\nfollowing questions:\\n• What are the mathematical properties of computer hardware and soft-\\nware?\\n• What is a computation and what is an algorithm? Can we give rigorous\\nmathematical deﬁnitions of these notions?\\n• What are the limitations of computers?\\nCan “everything” be com-\\nputed? (As we will see, the answer to this question is “no”.)\\nPurpose of the Theory of Computation: Develop formal math-\\nematical models of computation that reﬂect real-world computers.\\nThis ﬁeld of research was started by mathematicians and logicians in the\\n1930’s, when they were trying to understand the meaning of a “computation”.\\nA central question asked was whether all mathematical problems can be\\nsolved in a systematic way. The research that started in those days led to\\ncomputers as we know them today.\\nNowadays, the Theory of Computation can be divided into the follow-\\ning three areas: Complexity Theory, Computability Theory, and Automata\\nTheory.\\n2\\nChapter 1.\\nIntroduction\\n1.1.1\\nComplexity theory\\nThe main question asked in this area is “What makes some problems com-\\nputationally hard and other problems easy?”\\nInformally, a problem is called “easy”, if it is eﬃciently solvable. Exam-\\nples of “easy” problems are (i) sorting a sequence of, say, 1,000,000 numbers,\\n(ii) searching for a name in a telephone directory, and (iii) computing the\\nfastest way to drive from Ottawa to Miami. On the other hand, a problem is\\ncalled “hard”, if it cannot be solved eﬃciently, or if we don’t know whether\\nit can be solved eﬃciently. Examples of “hard” problems are (i) time table\\nscheduling for all courses at Carleton, (ii) factoring a 300-digit integer into\\nits prime factors, and (iii) computing a layout for chips in VLSI.\\nCentral Question in Complexity Theory: Classify problems ac-\\ncording to their degree of “diﬃculty”. Give a rigorous proof that\\nproblems that seem to be “hard” are really “hard”.\\n1.1.2\\nComputability theory\\nIn the 1930’s, G¨odel, Turing, and Church discovered that some of the fun-\\ndamental mathematical problems cannot be solved by a “computer”. (This\\nmay sound strange, because computers were invented only in the 1940’s).\\nAn example of such a problem is “Is an arbitrary mathematical statement\\ntrue or false?” To attack such a problem, we need formal deﬁnitions of the\\nnotions of\\n• computer,\\n• algorithm, and\\n• computation.\\nThe theoretical models that were proposed in order to understand solvable\\nand unsolvable problems led to the development of real computers.\\nCentral Question in Computability Theory: Classify problems\\nas being solvable or unsolvable.\\n1.1.\\nPurpose and motivation\\n3\\n1.1.3\\nAutomata theory\\nAutomata Theory deals with deﬁnitions and properties of diﬀerent types of\\n“computation models”. Examples of such models are:\\n• Finite Automata. These are used in text processing, compilers, and\\nhardware design.\\n• Context-Free Grammars. These are used to deﬁne programming lan-\\nguages and in Artiﬁcial Intelligence.\\n• Turing Machines.\\nThese form a simple abstract model of a “real”\\ncomputer, such as your PC at home.\\nCentral Question in Automata Theory: Do these models have\\nthe same power, or can one model solve more problems than the\\nother?\\n1.1.4\\nThis course\\nIn this course, we will study the last two areas in reverse order: We will start\\nwith Automata Theory, followed by Computability Theory. The ﬁrst area,\\nComplexity Theory, will be covered in COMP 3804.\\nActually, before we start, we will review some mathematical proof tech-\\nniques. As you may guess, this is a fairly theoretical course, with lots of\\ndeﬁnitions, theorems, and proofs. You may guess this course is fun stuﬀfor\\nmath lovers, but boring and irrelevant for others. You guessed it wrong, and\\nhere are the reasons:\\n1. This course is about the fundamental capabilities and limitations of\\ncomputers. These topics form the core of computer science.\\n2. It is about mathematical properties of computer hardware and software.\\n3. This theory is very much relevant to practice, for example, in the design\\nof new programming languages, compilers, string searching, pattern\\nmatching, computer security, artiﬁcial intelligence, etc., etc.\\n4. This course helps you to learn problem solving skills. Theory teaches\\nyou how to think, prove, argue, solve problems, express, and abstract.\\n4\\nChapter 1.\\nIntroduction\\n5. This theory simpliﬁes the complex computers to an abstract and simple\\nmathematical model, and helps you to understand them better.\\n6. This course is about rigorously analyzing capabilities and limitations\\nof systems.\\nWhere does this course ﬁt in the Computer Science Curriculum at Car-\\nleton University? It is a theory course that is the third part in the series\\nCOMP 1805, COMP 2804, COMP 3803, COMP 3804, and COMP 4804.\\nThis course also widens your understanding of computers and will inﬂuence\\nother courses including Compilers, Programming Languages, and Artiﬁcial\\nIntelligence.\\n1.2\\nMathematical preliminaries\\nThroughout this course, we will assume that you know the following mathe-\\nmatical concepts:\\n1. A set is a collection of well-deﬁned objects. Examples are (i) the set of\\nall Dutch Olympic Gold Medallists, (ii) the set of all pubs in Ottawa,\\nand (iii) the set of all even natural numbers.\\n2. The set of natural numbers is N = {1, 2, 3, . . .}.\\n3. The set of integers is Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n4. The set of rational numbers is Q = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\n5. The set of real numbers is denoted by R.\\n6. If A and B are sets, then A is a subset of B, written as A ⊆B, if every\\nelement of A is also an element of B. For example, the set of even\\nnatural numbers is a subset of the set of all natural numbers. Every\\nset A is a subset of itself, i.e., A ⊆A. The empty set is a subset of\\nevery set A, i.e., ∅⊆A.\\n7. If B is a set, then the power set P(B) of B is deﬁned to be the set of\\nall subsets of B:\\nP(B) = {A : A ⊆B}.\\nObserve that ∅∈P(B) and B ∈P(B).\\n1.2.\\nMathematical preliminaries\\n5\\n8. If A and B are two sets, then\\n(a) their union is deﬁned as\\nA ∪B = {x : x ∈A or x ∈B},\\n(b) their intersection is deﬁned as\\nA ∩B = {x : x ∈A and x ∈B},\\n(c) their diﬀerence is deﬁned as\\nA \\\\ B = {x : x ∈A and x ̸∈B},\\n(d) the Cartesian product of A and B is deﬁned as\\nA × B = {(x, y) : x ∈A and y ∈B},\\n(e) the complement of A is deﬁned as\\nA = {x : x ̸∈A}.\\n9. A binary relation on two sets A and B is a subset of A × B.\\n10. A function f from A to B, denoted by f : A →B, is a binary relation\\nR, having the property that for each element a ∈A, there is exactly\\none ordered pair in R, whose ﬁrst component is a. We will also say\\nthat f(a) = b, or f maps a to b, or the image of a under f is b. The\\nset A is called the domain of f, and the set\\n{b ∈B : there is an a ∈A with f(a) = b}\\nis called the range of f.\\n11. A function f : A →B is one-to-one (or injective), if for any two distinct\\nelements a and a′ in A, we have f(a) ̸= f(a′). The function f is onto\\n(or surjective), if for each element b ∈B, there exists an element a ∈A,\\nsuch that f(a) = b; in other words, the range of f is equal to the set\\nB. A function f is a bijection, if f is both injective and surjective.\\n12. A binary relation R ⊆A × A is an equivalence relation, if it satisﬁes\\nthe following three conditions:\\n6\\nChapter 1.\\nIntroduction\\n(a) R is reﬂexive: For every element in a ∈A, we have (a, a) ∈R.\\n(b) R is symmetric: For all a and b in A, if (a, b) ∈R, then also\\n(b, a) ∈R.\\n(c) R is transitive: For all a, b, and c in A, if (a, b) ∈R and (b, c) ∈R,\\nthen also (a, c) ∈R.\\n13. A graph G = (V, E) is a pair consisting of a set V , whose elements are\\ncalled vertices, and a set E, where each element of E is a pair of distinct\\nvertices. The elements of E are called edges. The ﬁgure below shows\\nsome well-known graphs: K5 (the complete graph on ﬁve vertices), K3,3\\n(the complete bipartite graph on 2 × 3 = 6 vertices), and the Peterson\\ngraph.\\nK5\\nK3,3\\nPeterson graph\\nThe degree of a vertex v, denoted by deg(v), is deﬁned to be the number\\nof edges that are incident on v.\\nA path in a graph is a sequence of vertices that are connected by edges.\\nA path is a cycle, if it starts and ends at the same vertex. A simple\\npath is a path without any repeated vertices. A graph is connected, if\\nthere is a path between every pair of vertices.\\n14. In the context of strings, an alphabet is a ﬁnite set, whose elements\\nare called symbols. Examples of alphabets are Σ = {0, 1} and Σ =\\n{a, b, c, . . . , z}.\\n15. A string over an alphabet Σ is a ﬁnite sequence of symbols, where each\\nsymbol is an element of Σ. The length of a string w, denoted by |w|, is\\nthe number of symbols contained in w. The empty string, denoted by\\n1.3.\\nProof techniques\\n7\\nϵ, is the string having length zero. For example, if the alphabet Σ is\\nequal to {0, 1}, then 10, 1000, 0, 101, and ϵ are strings over Σ, having\\nlengths 2, 4, 1, 3, and 0, respectively.\\n16. A language is a set of strings.\\n17. The Boolean values are 1 and 0, that represent true and false, respec-\\ntively. The basic Boolean operations include\\n(a) negation (or NOT), represented by ¬,\\n(b) conjunction (or AND), represented by ∧,\\n(c) disjunction (or OR), represented by ∨,\\n(d) exclusive-or (or XOR), represented by ⊕,\\n(e) equivalence, represented by ↔or ⇔,\\n(f) implication, represented by →or ⇒.\\nThe following table explains the meanings of these operations.\\nNOT\\nAND\\nOR\\nXOR\\nequivalence\\nimplication\\n¬0 = 1\\n0 ∧0 = 0\\n0 ∨0 = 0\\n0 ⊕0 = 0\\n0 ↔0 = 1\\n0 →0 = 1\\n¬1 = 0\\n0 ∧1 = 0\\n0 ∨1 = 1\\n0 ⊕1 = 1\\n0 ↔1 = 0\\n0 →1 = 1\\n1 ∧0 = 0\\n1 ∨0 = 1\\n1 ⊕0 = 1\\n1 ↔0 = 0\\n1 →0 = 0\\n1 ∧1 = 1\\n1 ∨1 = 1\\n1 ⊕1 = 0\\n1 ↔1 = 1\\n1 →1 = 1\\n1.3\\nProof techniques\\nIn mathematics, a theorem is a statement that is true. A proof is a sequence\\nof mathematical statements that form an argument to show that a theorem is\\ntrue. The statements in the proof of a theorem include axioms (assumptions\\nabout the underlying mathematical structures), hypotheses of the theorem\\nto be proved, and previously proved theorems. The main question is “How\\ndo we go about proving theorems?” This question is similar to the question\\nof how to solve a given problem. Of course, the answer is that ﬁnding proofs,\\nor solving problems, is not easy; otherwise life would be dull! There is no\\nspeciﬁed way of coming up with a proof, but there are some generic strategies\\nthat could be of help. In this section, we review some of these strategies,\\nthat will be suﬃcient for this course. The best way to get a feeling of how\\nto come up with a proof is by solving a large number of problems. Here are\\n8\\nChapter 1.\\nIntroduction\\nsome useful tips. (You may take a look at the book How to Solve It, by G.\\nP´olya).\\n1. Read and completely understand the statement of the theorem to be\\nproved. Most often this is the hardest part.\\n2. Sometimes, theorems contain theorems inside them.\\nFor example,\\n“Property A if and only if property B”, requires showing two state-\\nments:\\n(a) If property A is true, then property B is true (A ⇒B).\\n(b) If property B is true, then property A is true (B ⇒A).\\nAnother example is the theorem “Set A equals set B.” To prove this,\\nwe need to prove that A ⊆B and B ⊆A. That is, we need to show\\nthat each element of set A is in set B, and that each element of set B\\nis in set A.\\n3. Try to work out a few simple cases of the theorem just to get a grip on\\nit (i.e., crack a few simple cases ﬁrst).\\n4. Try to write down the proof once you have it. This is to ensure the\\ncorrectness of your proof. Often, mistakes are found at the time of\\nwriting.\\n5. Finding proofs takes time, we do not come prewired to produce proofs.\\nBe patient, think, express and write clearly and try to be precise as\\nmuch as possible.\\nIn the next sections, we will go through some of the proof strategies.\\n1.3.1\\nDirect proofs\\nAs the name suggests, in a direct proof of a theorem, we just approach the\\ntheorem directly.\\nTheorem 1.3.1 If n is an odd positive integer, then n2 is odd as well.\\n1.3.\\nProof techniques\\n9\\nProof. An odd positive integer n can be written as n = 2k + 1, for some\\ninteger k ≥0. Then\\nn2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1.\\nSince 2(2k2 + 2k) is even, and “even plus one is odd”, we can conclude that\\nn2 is odd.\\nTheorem 1.3.2 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is an even integer, i.e.,\\nX\\nv∈V\\ndeg(v)\\nis even.\\nProof. If you do not see the meaning of this statement, then ﬁrst try it out\\nfor a few graphs. The reason why the statement holds is very simple: Each\\nedge contributes 2 to the summation (because an edge is incident on exactly\\ntwo distinct vertices).\\nActually, the proof above proves the following theorem.\\nTheorem 1.3.3 Let G = (V, E) be a graph. Then the sum of the degrees of\\nall vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2|E|.\\n1.3.2\\nConstructive proofs\\nThis technique not only shows the existence of a certain object, it actually\\ngives a method of creating it. Here is how a constructive proof looks like:\\nTheorem 1.3.4 There exists an object with property P.\\nProof. Here is the object: [. . .]\\nAnd here is the proof that the object satisﬁes property P: [. . .]\\nHere is an example of a constructive proof. A graph is called 3-regular, if\\neach vertex has degree three.\\n10\\nChapter 1.\\nIntroduction\\nTheorem 1.3.5 For every even integer n ≥4, there exists a 3-regular graph\\nwith n vertices.\\nProof. Deﬁne\\nV = {0, 1, 2, . . . , n −1},\\nand\\nE = {{i, i+1} : 0 ≤i ≤n−2}∪{{n−1, 0}}∪{{i, i+n/2} : 0 ≤i ≤n/2−1}.\\nThen the graph G = (V, E) is 3-regular.\\nConvince yourself that this graph is indeed 3-regular. It may help to draw\\nthe graph for, say, n = 8.\\n1.3.3\\nNonconstructive proofs\\nIn a nonconstructive proof, we show that a certain object exists, without\\nactually creating it. Here is an example of such a proof:\\nTheorem 1.3.6 There exist irrational numbers x and y such that xy is ra-\\ntional.\\nProof. There are two possible cases.\\nCase 1:\\n√\\n2\\n√\\n2 ∈Q.\\nIn this case, we take x = y =\\n√\\n2. In Theorem 1.3.9 below, we will prove\\nthat\\n√\\n2 is irrational.\\nCase 2:\\n√\\n2\\n√\\n2 ̸∈Q.\\nIn this case, we take x =\\n√\\n2\\n√\\n2 and y =\\n√\\n2. Since\\nxy =\\n\\x12√\\n2\\n√\\n2\\x13√\\n2\\n=\\n√\\n2\\n2 = 2,\\nthe claim in the theorem follows.\\nObserve that this proof indeed proves the theorem, but it does not give\\nan example of a pair of irrational numbers x and y such that xy is rational.\\n1.3.\\nProof techniques\\n11\\n1.3.4\\nProofs by contradiction\\nThis is how a proof by contradiction looks like:\\nTheorem 1.3.7 Statement S is true.\\nProof. Assume that statement S is false. Then, derive a contradiction (such\\nas 1 + 1 = 3).\\nIn other words, show that the statement “¬S ⇒false” is true. This is\\nsuﬃcient, because the contrapositive of the statement “¬S ⇒false” is the\\nstatement “true ⇒S”. The latter logical formula is equivalent to S, and\\nthat is what we wanted to show.\\nBelow, we give two examples of proofs by contradiction.\\nTheorem 1.3.8 Let n be a positive integer. If n2 is even, then n is even.\\nProof. We will prove the theorem by contradiction. So we assume that n2\\nis even, but n is odd. Since n is odd, we know from Theorem 1.3.1 that n2\\nis odd. This is a contradiction, because we assumed that n2 is even.\\nTheorem 1.3.9\\n√\\n2 is irrational, i.e.,\\n√\\n2 cannot be written as a fraction of\\ntwo integers m and n.\\nProof. We will prove the theorem by contradiction. So we assume that\\n√\\n2\\nis rational. Then\\n√\\n2 can be written as a fraction of two integers,\\n√\\n2 = m/n,\\nwhere m ≥1 and n ≥1. We may assume that m and n do not share any\\ncommon factors, i.e., the greatest common divisor of m and n is equal to\\none; if this is not the case, then we can get rid of the common factors. By\\nsquaring\\n√\\n2 = m/n, we get 2n2 = m2. This implies that m2 is even. Then,\\nby Theorem 1.3.8, m is even, which means that we can write m as m = 2k,\\nfor some positive integer k. It follows that 2n2 = m2 = 4k2, which implies\\nthat n2 = 2k2. Hence, n2 is even. Again by Theorem 1.3.8, it follows that n\\nis even.\\nWe have shown that m and n are both even. But we know that m and\\nn are not both even. Hence, we have a contradiction. Our assumption that\\n√\\n2 is rational is wrong. Thus, we can conclude that\\n√\\n2 is irrational.\\nThere is a nice discussion of this proof in the book My Brain is Open:\\nThe Mathematical Journeys of Paul Erd˝os by B. Schechter.\\n12\\nChapter 1.\\nIntroduction\\n1.3.5\\nThe pigeon hole principle\\nThis is a simple principle with surprising consequences.\\nPigeon Hole Principle: If n + 1 or more objects are placed into n\\nboxes, then there is at least one box containing two or more objects.\\nIn other words, if A and B are two sets such that |A| > |B|, then\\nthere is no one-to-one function from A to B.\\nTheorem 1.3.10 Let n be a positive integer. Every sequence of n2 + 1 dis-\\ntinct real numbers contains a subsequence of length n + 1 that is either in-\\ncreasing or decreasing.\\nProof. For example consider the sequence (20, 10, 9, 7, 11, 2, 21, 1, 20, 31) of\\n10 = 32 + 1 numbers. This sequence contains an increasing subsequence of\\nlength 4 = 3 + 1, namely (10, 11, 21, 31).\\nThe proof of this theorem is by contradiction, and uses the pigeon hole\\nprinciple.\\nLet (a1, a2, . . . , an2+1) be an arbitrary sequence of n2 + 1 distinct real\\nnumbers. For each i with 1 ≤i ≤n2 + 1, let inci denote the length of\\nthe longest increasing subsequence that starts at ai, and let deci denote the\\nlength of the longest decreasing subsequence that starts at ai.\\nUsing this notation, the claim in the theorem can be formulated as follows:\\nThere is an index i such that inci ≥n + 1 or deci ≥n + 1.\\nWe will prove the claim by contradiction. So we assume that inci ≤n\\nand deci ≤n for all i with 1 ≤i ≤n2 + 1.\\nConsider the set\\nB = {(b, c) : 1 ≤b ≤n, 1 ≤c ≤n},\\nand think of the elements of B as being boxes. For each i with 1 ≤i ≤n2+1,\\nthe pair (inci, deci) is an element of B. So we have n2+1 elements (inci, deci),\\nwhich are placed in the n2 boxes of B. By the pigeon hole principle, there\\nmust be a box that contains two (or more) elements. In other words, there\\nexist two integers i and j such that i < j and\\n(inci, deci) = (incj, decj).\\nRecall that the elements in the sequence are distinct. Hence, ai ̸= aj. We\\nconsider two cases.\\n1.3.\\nProof techniques\\n13\\nFirst assume that ai < aj. Then the length of the longest increasing\\nsubsequence starting at ai must be at least 1+incj, because we can append ai\\nto the longest increasing subsequence starting at aj. Therefore, inci ̸= incj,\\nwhich is a contradiction.\\nThe second case is when ai > aj. Then the length of the longest decreasing\\nsubsequence starting at ai must be at least 1+decj, because we can append ai\\nto the longest decreasing subsequence starting at aj. Therefore, deci ̸= decj,\\nwhich is again a contradiction.\\n1.3.6\\nProofs by induction\\nThis is a very powerful and important technique for proving theorems.\\nFor each positive integer n, let P(n) be a mathematical statement that\\ndepends on n. Assume we wish to prove that P(n) is true for all positive\\nintegers n. A proof by induction of such a statement is carried out as follows:\\nBasis: Prove that P(1) is true.\\nInduction step: Prove that for all n ≥1, the following holds: If P(n) is\\ntrue, then P(n + 1) is also true.\\nIn the induction step, we choose an arbitrary integer n ≥1 and assume\\nthat P(n) is true; this is called the induction hypothesis. Then we prove that\\nP(n + 1) is also true.\\nTheorem 1.3.11 For all positive integers n, we have\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\nProof. We start with the basis of the induction. If n = 1, then the left-hand\\nside is equal to 1, and so is the right-hand side. So the theorem is true for\\nn = 1.\\nFor the induction step, let n ≥1 and assume that the theorem is true for\\nn, i.e., assume that\\n1 + 2 + 3 + . . . + n = n(n + 1)\\n2\\n.\\n14\\nChapter 1.\\nIntroduction\\nWe have to prove that the theorem is true for n + 1, i.e., we have to prove\\nthat\\n1 + 2 + 3 + . . . + (n + 1) = (n + 1)(n + 2)\\n2\\n.\\nHere is the proof:\\n1 + 2 + 3 + . . . + (n + 1)\\n=\\n1 + 2 + 3 + . . . + n\\n|\\n{z\\n}\\n= n(n+1)\\n2\\n+(n + 1)\\n=\\nn(n + 1)\\n2\\n+ (n + 1)\\n=\\n(n + 1)(n + 2)\\n2\\n.\\nBy the way, here is an alternative proof of the theorem above: Let S =\\n1 + 2 + 3 + . . . + n. Then,\\nS\\n=\\n1\\n+\\n2\\n+\\n3\\n+\\n. . .\\n+\\n(n −2)\\n+\\n(n −1)\\n+\\nn\\nS\\n=\\nn\\n+\\n(n −1)\\n+\\n(n −2)\\n+\\n. . .\\n+\\n3\\n+\\n2\\n+\\n1\\n2S\\n=\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n. . .\\n+\\n(n + 1)\\n+\\n(n + 1)\\n+\\n(n + 1)\\nSince there are n terms on the right-hand side, we have 2S = n(n + 1). This\\nimplies that S = n(n + 1)/2.\\nTheorem 1.3.12 For every positive integer n, a −b is a factor of an −bn.\\nProof. A direct proof can be given by providing a factorization of an −bn:\\nan −bn = (a −b)(an−1 + an−2b + an−3b2 + . . . + abn−2 + bn−1).\\nWe now prove the theorem by induction. For the basis, let n = 1. The claim\\nin the theorem is “a −b is a factor of a −b”, which is obviously true.\\nLet n ≥1 and assume that a −b is a factor of an −bn. We have to prove\\nthat a −b is a factor of an+1 −bn+1. We have\\nan+1 −bn+1 = an+1 −anb + anb −bn+1 = an(a −b) + (an −bn)b.\\nThe ﬁrst term on the right-hand side is divisible by a −b. By the induction\\nhypothesis, the second term on the right-hand side is divisible by a −b as\\nwell. Therefore, the entire right-hand side is divisible by a −b. Since the\\nright-hand side is equal to an+1 −bn+1, it follows that a −b is a factor of\\nan+1 −bn+1.\\nWe now give an alternative proof of Theorem 1.3.3:\\n1.3.\\nProof techniques\\n15\\nTheorem 1.3.13 Let G = (V, E) be a graph with m edges. Then the sum\\nof the degrees of all vertices is equal to twice the number of edges, i.e.,\\nX\\nv∈V\\ndeg(v) = 2m.\\nProof. The proof is by induction on the number m of edges. For the basis of\\nthe induction, assume that m = 0. Then the graph G does not contain any\\nedges and, therefore, P\\nv∈V deg(v) = 0. Thus, the theorem is true if m = 0.\\nLet m ≥0 and assume that the theorem is true for every graph with m\\nedges. Let G be an arbitrary graph with m+1 edges. We have to prove that\\nP\\nv∈V deg(v) = 2(m + 1).\\nLet {a, b} be an arbitrary edge in G, and let G′ be the graph obtained\\nfrom G by removing the edge {a, b}. Since G′ has m edges, we know from\\nthe induction hypothesis that the sum of the degrees of all vertices in G′ is\\nequal to 2m. Using this, we obtain\\nX\\nv∈G\\ndeg(v) =\\nX\\nv∈G′\\ndeg(v) + 2 = 2m + 2 = 2(m + 1).\\n1.3.7\\nMore examples of proofs\\nRecall Theorem 1.3.5, which states that for every even integer n ≥4, there\\nexists a 3-regular graph with n vertices. The following theorem explains why\\nwe stated this theorem for even values of n.\\nTheorem 1.3.14 Let n ≥5 be an odd integer. There is no 3-regular graph\\nwith n vertices.\\nProof. The proof is by contradiction. So we assume that there exists a\\ngraph G = (V, E) with n vertices that is 3-regular. Let m be the number of\\nedges in G. Since deg(v) = 3 for every vertex, we have\\nX\\nv∈V\\ndeg(v) = 3n.\\nOn the other hand, by Theorem 1.3.3, we have\\nX\\nv∈V\\ndeg(v) = 2m.\\n16\\nChapter 1.\\nIntroduction\\nIt follows that 3n = 2m, which can be rewritten as m = 3n/2. Since m is an\\ninteger, and since gcd(2, 3) = 1, n/2 must be an integer. Hence, n is even,\\nwhich is a contradiction.\\nLet Kn be the complete graph on n vertices. This graph has a vertex set\\nof size n, and every pair of distinct vertices is joined by an edge.\\nIf G = (V, E) is a graph with n vertices, then the complement G of G is\\nthe graph with vertex set V that consists of those edges of Kn that are not\\npresent in G.\\nTheorem 1.3.15 Let n ≥2 and let G be a graph on n vertices. Then G is\\nconnected or G is connected.\\nProof. We prove the theorem by induction on the number n of vertices. For\\nthe basis, assume that n = 2. There are two possibilities for the graph G:\\n1. G contains one edge. In this case, G is connected.\\n2. G does not contain an edge. In this case, the complement G contains\\none edge and, therefore, G is connected.\\nSo for n = 2, the theorem is true.\\nLet n ≥2 and assume that the theorem is true for every graph with n\\nvertices. Let G be graph with n + 1 vertices. We have to prove that G is\\nconnected or G is connected. We consider three cases.\\nCase 1: There is a vertex v whose degree in G is equal to n.\\nSince G has n+1 vertices, v is connected by an edge to every other vertex\\nof G. Therefore, G is connected.\\nCase 2: There is a vertex v whose degree in G is equal to 0.\\nIn this case, the degree of v in the graph G is equal to n. Since G has n+1\\nvertices, v is connected by an edge to every other vertex of G. Therefore, G\\nis connected.\\nCase 3: For every vertex v, the degree of v in G is in {1, 2, . . . , n −1}.\\nLet v be an arbitrary vertex of G.\\nLet G′ be the graph obtained by\\ndeleting from G the vertex v, together with all edges that are incident on v.\\nSince G′ has n vertices, we know from the induction hypothesis that G′ is\\nconnected or G′ is connected.\\n1.3.\\nProof techniques\\n17\\nLet us ﬁrst assume that G′ is connected. Then the graph G is connected\\nas well, because there is at least one edge in G between v and some vertex\\nof G′.\\nIf G′ is not connected, then G′ must be connected. Since we are in Case 3,\\nwe know that the degree of v in G is in the set {1, 2, . . . , n −1}. It follows\\nthat the degree of v in the graph G is in this set as well. Hence, there is at\\nleast one edge in G between v and some vertex in G′. This implies that G is\\nconnected.\\nThe previous theorem can be rephrased as follows:\\nTheorem 1.3.16 Let n ≥2 and consider the complete graph Kn on n ver-\\ntices. Color each edge of this graph as either red or blue. Let R be the graph\\nconsisting of all the red edges, and let B be the graph consisting of all the\\nblue edges. Then R is connected or B is connected.\\nA graph is said to be planar, if it can be drawn (a better term is “embed-\\nded”) in the plane in such a way that no two edges intersect, except possibly\\nat their endpoints.\\nAn embedding of a planar graph consists of vertices,\\nedges, and faces. In the example below, there are 11 vertices, 18 edges, and\\n9 faces (including the unbounded face).\\nThe following theorem is known as Euler’s theorem for planar graphs.\\nApparently, this theorem was discovered by Euler around 1750. Legendre\\ngave the ﬁrst proof in 1794, see\\nhttp://www.ics.uci.edu/~eppstein/junkyard/euler/\\nTheorem 1.3.17 (Euler) Consider an embedding of a planar graph G. Let\\nv, e, and f be the number of vertices, edges, and faces (including the single\\n18\\nChapter 1.\\nIntroduction\\nunbounded face) of this embedding, respectively. Moreover, let c be the number\\nof connected components of G. Then\\nv −e + f = c + 1.\\nProof. The proof is by induction on the number of edges of G. To be more\\nprecise, we start with a graph having no edges, and prove that the theorem\\nholds for this case. Then, we add the edges one by one, and show that the\\nrelation v −e + f = c + 1 is maintained.\\nSo we ﬁrst assume that G has no edges, i.e., e = 0. Then the embedding\\nconsists of a collection of v points. In this case, we have f = 1 and c = v.\\nHence, the relation v −e + f = c + 1 holds.\\nLet e > 0 and assume that Euler’s formula holds for a subgraph of G\\nhaving e −1 edges. Let {u, v} be an edge of G that is not in the subgraph,\\nand add this edge to the subgraph. There are two cases depending on whether\\nthis new edge joins two connected components or joins two vertices in the\\nsame connected component.\\nCase 1: The new edge {u, v} joins two connected components.\\nIn this case, the number of vertices and the number of faces do not change,\\nthe number of connected components goes down by 1, and the number of\\nedges increases by 1. It follows that the relation in the theorem is still valid.\\nCase 2: The new edge {u, v} joins two vertices in the same connected com-\\nponent.\\nIn this case, the number of vertices and the number of connected com-\\nponents do not change, the number of edges increases by 1, and the number\\nof faces increases by 1 (because the new edge splits one face into two faces).\\nTherefore, the relation in the theorem is still valid.\\nEuler’s theorem is usually stated as follows:\\nTheorem 1.3.18 (Euler) Consider an embedding of a connected planar\\ngraph G. Let v, e, and f be the number of vertices, edges, and faces (in-\\ncluding the single unbounded face) of this embedding, respectively. Then\\nv −e + f = 2.\\nIf you like surprising proofs of various mathematical results, you should\\nread the book Proofs from THE BOOK by Aigner and Ziegler.\\nExercises\\n19\\nExercises\\n1.1 Use induction to prove that every integer n ≥2 can be written as a\\nproduct of prime numbers.\\n1.2 For every prime number p, prove that √p is irrational.\\n1.3 Let n be a positive integer that is not a perfect square. Prove that √n\\nis irrational.\\n1.4 Prove by induction that n4 −4n2 is divisible by 3, for all integers n ≥1.\\n1.5 Prove that\\nn\\nX\\ni=1\\n1\\ni2 < 2 −1/n,\\nfor every integer n ≥2.\\n1.6 Prove that 9 divides n3 + (n + 1)3 + (n + 2)3, for every integer n ≥0.\\n1.7 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers that are consecutive.\\n1.8 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are\\nalways two numbers such that one divides the other.\\n20\\nChapter 1.\\nIntroduction\\nChapter 2\\nFinite Automata and Regular\\nLanguages\\nIn this chapter, we introduce and analyze the class of languages that are\\nknown as regular languages. Informally, these languages can be “processed”\\nby computers having a very small amount of memory.\\n2.1\\nAn example: Controling a toll gate\\nBefore we give a formal deﬁnition of a ﬁnite automaton, we consider an\\nexample in which such an automaton shows up in a natural way. We consider\\nthe problem of designing a “computer” that controls a toll gate.\\nWhen a car arrives at the toll gate, the gate is closed. The gate opens as\\nsoon as the driver has payed 25 cents. We assume that we have only three\\ncoin denominations: 5, 10, and 25 cents. We also assume that no excess\\nchange is returned.\\nAfter having arrived at the toll gate, the driver inserts a sequence of coins\\ninto the machine. At any moment, the machine has to decide whether or not\\nto open the gate, i.e., whether or not the driver has paid 25 cents (or more).\\nIn order to decide this, the machine is in one of the following six states, at\\nany moment during the process:\\n• The machine is in state q0, if it has not collected any money yet.\\n• The machine is in state q1, if it has collected exactly 5 cents.\\n• The machine is in state q2, if it has collected exactly 10 cents.\\n22\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The machine is in state q3, if it has collected exactly 15 cents.\\n• The machine is in state q4, if it has collected exactly 20 cents.\\n• The machine is in state q5, if it has collected 25 cents or more.\\nInitially (when a car arrives at the toll gate), the machine is in state q0.\\nAssume, for example, that the driver presents the sequence (10,5,5,10) of\\ncoins.\\n• After receiving the ﬁrst 10 cents coin, the machine switches from state\\nq0 to state q2.\\n• After receiving the ﬁrst 5 cents coin, the machine switches from state\\nq2 to state q3.\\n• After receiving the second 5 cents coin, the machine switches from state\\nq3 to state q4.\\n• After receiving the second 10 cents coin, the machine switches from\\nstate q4 to state q5. At this moment, the gate opens. (Remember that\\nno change is given.)\\nThe ﬁgure below represents the behavior of the machine for all possible\\nsequences of coins. State q5 is represented by two circles, because it is a\\nspecial state: As soon as the machine reaches this state, the gate opens.\\nq0\\nq1\\nq2\\nq3\\nq4\\nq5\\n5\\n5\\n5\\n5\\n10\\n10\\n10\\n25\\n25\\n25\\n10, 25\\n5, 10, 25\\n5, 10\\n25\\nstart\\nObserve that the machine (or computer) only has to remember which\\nstate it is in at any given time. Thus, it needs only a very small amount\\nof memory: It has to be able to distinguish between any one of six possible\\ncases and, therefore, it only needs a memory of ⌈log 6⌉= 3 bits.\\n2.2.\\nDeterministic ﬁnite automata\\n23\\n2.2\\nDeterministic ﬁnite automata\\nLet us look at another example. Consider the following state diagram:\\nq1\\nq2\\nq3\\n0\\n0\\n1\\n1\\n0,1\\nWe say that q1 is the start state and q2 is an accept state. Consider the\\ninput string 1101. This string is processed in the following way:\\n• Initially, the machine is in the start state q1.\\n• After having read the ﬁrst 1, the machine switches from state q1 to\\nstate q2.\\n• After having read the second 1, the machine switches from state q2 to\\nstate q2. (So actually, it does not switch.)\\n• After having read the ﬁrst 0, the machine switches from state q2 to\\nstate q3.\\n• After having read the third 1, the machine switches from state q3 to\\nstate q2.\\nAfter the entire string 1101 has been processed, the machine is in state q2,\\nwhich is an accept state. We say that the string 1101 is accepted by the\\nmachine.\\nConsider now the input string 0101010. After having read this string\\nfrom left to right (starting in the start state q1), the machine is in state q3.\\nSince q3 is not an accept state, we say that the machine rejects the string\\n0101010.\\nWe hope you are able to see that this machine accepts every binary string\\nthat ends with a 1. In fact, the machine accepts more strings:\\n• Every binary string having the property that there are an even number\\nof 0s following the rightmost 1, is accepted by this machine.\\n24\\nChapter 2.\\nFinite Automata and Regular Languages\\n• Every other binary string is rejected by the machine. Observe that each\\nsuch string is either empty, consists of 0s only, or has an odd number\\nof 0s following the rightmost 1.\\nWe now come to the formal deﬁnition of a ﬁnite automaton:\\nDeﬁnition 2.2.1 A ﬁnite automaton is a 5-tuple M = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σ →Q is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\nYou can think of the transition function δ as being the “program” of the\\nﬁnite automaton M = (Q, Σ, δ, q, F). This function tells us what M can do\\nin “one step”:\\n• Let r be a state of Q and let a be a symbol of the alphabet Σ. If\\nthe ﬁnite automaton M is in state r and reads the symbol a, then it\\nswitches from state r to state δ(r, a). (In fact, δ(r, a) may be equal to\\nr.)\\nThe “computer” that we designed in the toll gate example in Section 2.1\\nis a ﬁnite automaton. For this example, we have Q = {q0, q1, q2, q3, q4, q5},\\nΣ = {5, 10, 25}, the start state is q0, F = {q5}, and δ is given by the following\\ntable:\\n5\\n10\\n25\\nq0\\nq1\\nq2\\nq5\\nq1\\nq2\\nq3\\nq5\\nq2\\nq3\\nq4\\nq5\\nq3\\nq4\\nq5\\nq5\\nq4\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nq5\\nThe example given in the beginning of this section is also a ﬁnite automa-\\nton. For this example, we have Q = {q1, q2, q3}, Σ = {0, 1}, the start state\\nis q1, F = {q2}, and δ is given by the following table:\\n2.2.\\nDeterministic ﬁnite automata\\n25\\n0\\n1\\nq1\\nq1\\nq2\\nq2\\nq3\\nq2\\nq3\\nq2\\nq2\\nLet us denote this ﬁnite automaton by M. The language of M, denoted\\nby L(M), is the set of all binary strings that are accepted by M. As we have\\nseen before, we have\\nL(M) = {w : w contains at least one 1 and ends with an even number of 0s}.\\nWe now give a formal deﬁnition of the language of a ﬁnite automaton:\\nDeﬁnition 2.2.2 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton and let w =\\nw1w2 . . . wn be a string over Σ. Deﬁne the sequence r0, r1, . . . , rn of states, in\\nthe following way:\\n• r0 = q,\\n• ri+1 = δ(ri, wi+1), for i = 0, 1, . . . , n −1.\\n1. If rn ∈F, then we say that M accepts w.\\n2. If rn ̸∈F, then we say that M rejects w.\\nIn this deﬁnition, w may be the empty string, which we denote by ϵ, and\\nwhose length is zero; thus in the deﬁnition above, n = 0. In this case, the\\nsequence r0, r1, . . . , rn of states has length one; it consists of just the state\\nr0 = q. The empty string is accepted by M if and only if the start state q\\nbelongs to F.\\nDeﬁnition 2.2.3 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton. The lan-\\nguage L(M) accepted by M is deﬁned to be the set of all strings that are\\naccepted by M:\\nL(M) = {w : w is a string over Σ and M accepts w }.\\nDeﬁnition 2.2.4 A language A is called regular, if there exists a ﬁnite au-\\ntomaton M such that A = L(M).\\n26\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe ﬁnish this section by presenting an equivalent way of deﬁning the\\nlanguage accepted by a ﬁnite automaton. Let M = (Q, Σ, δ, q, F) be a ﬁnite\\nautomaton. The transition function δ : Q × Σ →Q tells us that, when M\\nis in state r ∈Q and reads symbol a ∈Σ, it switches from state r to state\\nδ(r, a). Let Σ∗denote the set of all strings over the alphabet Σ. (Σ∗includes\\nthe empty string ϵ.) We extend the function δ to a function\\nδ : Q × Σ∗→Q,\\nthat is deﬁned as follows. For any state r ∈Q and for any string w over the\\nalphabet Σ,\\nδ(r, w) =\\n\\x1a r\\nif w = ϵ,\\nδ(δ(r, v), a)\\nif w = va, where v is a string and a ∈Σ.\\nWhat is the meaning of this function δ? Let r be a state of Q and let w be\\na string over the alphabet Σ. Then\\n• δ(r, w) is the state that M reaches, when it starts in state r, reads the\\nstring w from left to right, and uses δ to switch from state to state.\\nUsing this notation, we have\\nL(M) = {w : w is a string over Σ and δ(q, w) ∈F}.\\n2.2.1\\nA ﬁrst example of a ﬁnite automaton\\nLet\\nA = {w : w is a binary string containing an odd number of 1s}.\\nWe claim that this language A is regular. In order to prove this, we have to\\nconstruct a ﬁnite automaton M such that A = L(M).\\nHow to construct M? Here is a ﬁrst idea: The ﬁnite automaton reads the\\ninput string w from left to right and keeps track of the number of 1s it has\\nseen. After having read the entire string w, it checks whether this number\\nis odd (in which case w is accepted) or even (in which case w is rejected).\\nUsing this approach, the ﬁnite automaton needs a state for every integer\\ni ≥0, indicating that the number of 1s read so far is equal to i. Hence,\\nto design a ﬁnite automaton that follows this approach, we need an inﬁnite\\n2.2.\\nDeterministic ﬁnite automata\\n27\\nnumber of states. But, the deﬁnition of ﬁnite automaton requires the number\\nof states to be ﬁnite.\\nA better, and correct approach, is to keep track of whether the number\\nof 1s read so far is even or odd. This leads to the following ﬁnite automaton:\\n• The set of states is Q = {qe, qo}. If the ﬁnite automaton is in state qe,\\nthen it has read an even number of 1s; if it is in state qo, then it has\\nread an odd number of 1s.\\n• The alphabet is Σ = {0, 1}.\\n• The start state is qe, because at the start, the number of 1s read by the\\nautomaton is equal to 0, and 0 is even.\\n• The set F of accept states is F = {qo}.\\n• The transition function δ is given by the following table:\\n0\\n1\\nqe\\nqe\\nqo\\nqo\\nqo\\nqe\\nThis ﬁnite automaton M = (Q, Σ, δ, qe, F) can also be described by its state\\ndiagram, which is given in the ﬁgure below. The arrow that comes “out of\\nthe blue” and enters the state qe, indicates that qe is the start state. The\\nstate depicted with double circles indicates the accept state.\\nqe\\nqo\\n0\\n0\\n1\\n1\\nWe have constructed a ﬁnite automaton M that accepts the language A.\\nTherefore, A is a regular language.\\n28\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.2.2\\nA second example of a ﬁnite automaton\\nDeﬁne the language A as\\nA = {w : w is a binary string containing 101 as a substring}.\\nAgain, we claim that A is a regular language. In other words, we claim that\\nthere exists a ﬁnite automaton M that accepts A, i.e., A = L(M).\\nThe ﬁnite automaton M will do the following, when reading an input\\nstring from left to right:\\n• It skips over all 0s, and stays in the start state.\\n• At the ﬁrst 1, it switches to the state “maybe the next two symbols are\\n01”.\\n– If the next symbol is 1, then it stays in the state “maybe the next\\ntwo symbols are 01”.\\n– On the other hand, if the next symbol is 0, then it switches to the\\nstate “maybe the next symbol is 1”.\\n∗If the next symbol is indeed 1, then it switches to the accept\\nstate (but keeps on reading until the end of the string).\\n∗On the other hand, if the next symbol is 0, then it switches\\nto the start state, and skips 0s until it reads 1 again.\\nBy deﬁning the following four states, this process will become clear:\\n• q1: M is in this state if the last symbol read was 1, but the substring\\n101 has not been read.\\n• q10: M is in this state if the last two symbols read were 10, but the\\nsubstring 101 has not been read.\\n• q101: M is in this state if the substring 101 has been read in the input\\nstring.\\n• q: In all other cases, M is in this state.\\nHere is the formal description of the ﬁnite automaton that accepts the\\nlanguage A:\\n• Q = {q, q1, q10, q101},\\n2.2.\\nDeterministic ﬁnite automata\\n29\\n• Σ = {0, 1},\\n• the start state is q,\\n• the set F of accept states is equal to F = {q101}, and\\n• the transition function δ is given by the following table:\\n0\\n1\\nq\\nq\\nq1\\nq1\\nq10\\nq1\\nq10\\nq\\nq101\\nq101\\nq101\\nq101\\nThe ﬁgure below gives the state diagram of the ﬁnite automaton M =\\n(Q, Σ, δ, q, F).\\nq\\nq1\\nq10\\nq101\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nThis ﬁnite automaton accepts the language A consisting of all binary\\nstrings that contain the substring 101. As an exercise, how would you obtain\\na ﬁnite automaton that accepts the complement of A, i.e., the language\\nconsisting of all binary strings that do not contain the substring 101?\\n2.2.3\\nA third example of a ﬁnite automaton\\nThe ﬁnite automata we have seen so far have exactly one accept state. In\\nthis section, we will see an example of a ﬁnite automaton having more accept\\nstates.\\n30\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right},\\nwhere {0, 1}∗is the set of all binary strings, including the empty string ϵ. We\\nclaim that A is a regular language. To prove this, we have to construct a ﬁnite\\nautomaton M such that A = L(M). At ﬁrst sight, it seems diﬃcult (or even\\nimpossible?) to construct such a ﬁnite automaton: How does the automaton\\n“know” that it has reached the third symbol from the right? It is, however,\\npossible to construct such an automaton. The main idea is to remember the\\nlast three symbols that have been read. Thus, the ﬁnite automaton has eight\\nstates qijk, where i, j, and k range over the two elements of {0, 1}. If the\\nautomaton is in state qijk, then the following hold:\\n• If M has read at least three symbols, then the three most recently read\\nsymbols are ijk.\\n• If M has read only two symbols, then these two symbols are jk; more-\\nover, i = 0.\\n• If M has read only one symbol, then this symbol is k; moreover, i =\\nj = 0.\\n• If M has not read any symbol, then i = j = k = 0.\\nThe start state is q000 and the set of accept states is {q100, q110, q101, q111}.\\nThe transition function of M is given by the following state diagram.\\nq000\\nq100\\nq010\\nq110\\nq001\\nq101\\nq011\\nq111\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n2.3.\\nRegular operations\\n31\\n2.3\\nRegular operations\\nIn this section, we deﬁne three operations on languages. Later, we will answer\\nthe question whether the set of all regular languages is closed under these\\noperations. Let A and B be two languages over the same alphabet.\\n1. The union of A and B is deﬁned as\\nA ∪B = {w : w ∈A or w ∈B}.\\n2. The concatenation of A and B is deﬁned as\\nAB = {ww′ : w ∈A and w′ ∈B}.\\nIn words, AB is the set of all strings obtained by taking an arbitrary\\nstring w in A and an arbitrary string w′ in B, and gluing them together\\n(such that w is to the left of w′).\\n3. The star of A is deﬁned as\\nA∗= {u1u2 . . . uk : k ≥0 and ui ∈A for all i = 1, 2, . . . , k}.\\nIn words, A∗is obtained by taking any ﬁnite number of strings in A, and\\ngluing them together. Observe that k = 0 is allowed; this corresponds\\nto the empty string ϵ. Thus, ϵ ∈A∗.\\nTo give an example, let A = {0, 01} and B = {1, 10}. Then\\nA ∪B = {0, 01, 1, 10},\\nAB = {01, 010, 011, 0110},\\nand\\nA∗= {ϵ, 0, 01, 00, 001, 010, 0101, 000, 0001, 00101, . . .}.\\nAs another example, if Σ = {0, 1}, then Σ∗is the set of all binary strings\\n(including the empty string). Observe that a string always has a ﬁnite length.\\nBefore we proceed, we give an alternative (and equivalent) deﬁnition of\\nthe star of the language A: Deﬁne\\nA0 = {ϵ}\\n32\\nChapter 2.\\nFinite Automata and Regular Languages\\nand, for k ≥1,\\nAk = AAk−1,\\ni.e., Ak is the concatenation of the two languages A and Ak−1. Then we have\\nA∗=\\n∞\\n[\\nk=0\\nAk.\\nTheorem 2.3.1 The set of regular languages is closed under the union op-\\neration, i.e., if A and B are regular languages over the same alphabet Σ, then\\nA ∪B is also a regular language.\\nProof.\\nSince A and B are regular languages, there are ﬁnite automata\\nM1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2) that accept A and B,\\nrespectively. In order to prove that A ∪B is regular, we have to construct a\\nﬁnite automaton M that accepts A ∪B. In other words, M must have the\\nproperty that for every string w ∈Σ∗,\\nM accepts w ⇔M1 accepts w or M2 accepts w.\\nAs a ﬁrst idea, we may think that M could do the following:\\n• Starting in the start state q1 of M1, M “runs” M1 on w.\\n• If, after having read w, M1 is in a state of F1, then w ∈A, thus\\nw ∈A ∪B and, therefore, M accepts w.\\n• On the other hand, if, after having read w, M1 is in a state that is not\\nin F1, then w ̸∈A and M “runs” M2 on w, starting in the start state\\nq2 of M2. If, after having read w, M2 is in a state of F2, then we know\\nthat w ∈B, thus w ∈A ∪B and, therefore, M accepts w. Otherwise,\\nwe know that w ̸∈A ∪B, and M rejects w.\\nThis idea does not work, because the ﬁnite automaton M can read the input\\nstring w only once. The correct approach is to run M1 and M2 simulta-\\nneously. We deﬁne the set Q of states of M to be the Cartesian product\\nQ1 × Q2. If M is in state (r1, r2), this means that\\n• if M1 would have read the input string up to this point, then it would\\nbe in state r1, and\\n2.3.\\nRegular operations\\n33\\n• if M2 would have read the input string up to this point, then it would\\nbe in state r2.\\nThis leads to the ﬁnite automaton M = (Q, Σ, δ, q, F), where\\n• Q = Q1 × Q2 = {(r1, r2) : r1 ∈Q1 and r2 ∈Q2}.\\nObserve that\\n|Q| = |Q1| × |Q2|, which is ﬁnite.\\n• Σ is the alphabet of A and B (recall that we assume that A and B are\\nlanguages over the same alphabet).\\n• The start state q of M is equal to q = (q1, q2).\\n• The set F of accept states of M is given by\\nF = {(r1, r2) : r1 ∈F1 or r2 ∈F2} = (F1 × Q2) ∪(Q1 × F2).\\n• The transition function δ : Q × Σ →Q is given by\\nδ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)),\\nfor all r1 ∈Q1, r2 ∈Q2, and a ∈Σ.\\nTo ﬁnish the proof, we have to show that this ﬁnite automaton M indeed\\naccepts the language A∪B. Intuitively, this should be clear from the discus-\\nsion above. The easiest way to give a formal proof is by using the extended\\ntransition functions δ1 and δ2. (The extended transition function has been\\ndeﬁned after Deﬁnition 2.2.4.) Here we go: Recall that we have to prove that\\nM accepts w ⇔M1 accepts w or M2 accepts w,\\ni.e,\\nM accepts w ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\nIn terms of the extended transition function δ of the transition function δ of\\nM, this becomes\\nδ((q1, q2), w) ∈F ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.\\n(2.1)\\nBy applying the deﬁnition of the extended transition function, as given after\\nDeﬁnition 2.2.4, to δ, it can be seen that\\nδ((q1, q2), w) = (δ1(q1, w), δ2(q2, w)).\\n34\\nChapter 2.\\nFinite Automata and Regular Languages\\nThe latter equality implies that (2.1) is true and, therefore, M indeed accepts\\nthe language A ∪B.\\nWhat about the closure of the regular languages under the concatenation\\nand star operations? It turns out that the regular languages are closed under\\nthese operations. But how do we prove this?\\nLet A and B be two regular languages, and let M1 and M2 be ﬁnite\\nautomata that accept A and B, respectively. How do we construct a ﬁnite\\nautomaton M that accepts the concatenation AB? Given an input string\\nu, M has to decide whether or not u can be broken into two strings w and\\nw′ (i.e., write u as u = ww′), such that w ∈A and w′ ∈B. In words, M\\nhas to decide whether or not u can be broken into two substrings, such that\\nthe ﬁrst substring is accepted by M1 and the second substring is accepted by\\nM2. The diﬃculty is caused by the fact that M has to make this decision by\\nscanning the string u only once. If u ∈AB, then M has to decide, during\\nthis single scan, where to break u into two substrings. Similarly, if u ̸∈AB,\\nthen M has to decide, during this single scan, that u cannot be broken into\\ntwo substrings such that the ﬁrst substring is in A and the second substring\\nis in B.\\nIt seems to be even more diﬃcult to prove that A∗is a regular language,\\nif A itself is regular. In order to prove this, we need a ﬁnite automaton that,\\nwhen given an arbitrary input string u, decides whether or not u can be\\nbroken into substrings such that each substring is in A. The problem is that,\\nif u ∈A∗, the ﬁnite automaton has to determine into how many substrings,\\nand where, the string u has to be broken; it has to do this during one single\\nscan of the string u.\\nAs we mentioned already, if A and B are regular languages, then both\\nAB and A∗are also regular. In order to prove these claims, we will introduce\\na more general type of ﬁnite automaton.\\nThe ﬁnite automata that we have seen so far are deterministic.\\nThis\\nmeans the following:\\n• If the ﬁnite automaton M is in state r and if it reads the symbol a,\\nthen M switches from state r to the uniquely deﬁned state δ(r, a).\\nFrom now on, we will call such a ﬁnite automaton a deterministic ﬁnite\\nautomaton (DFA). In the next section, we will deﬁne the notion of a nonde-\\nterministic ﬁnite automaton (NFA). For such an automaton, there are zero\\nor more possible states to switch to. At ﬁrst sight, nondeterministic ﬁnite\\n2.4.\\nNondeterministic ﬁnite automata\\n35\\nautomata seem to be more powerful than their deterministic counterparts.\\nWe will prove, however, that DFAs have the same power as NFAs. As we will\\nsee, using this fact, it will be easy to prove that the class of regular languages\\nis closed under the concatenation and star operations.\\n2.4\\nNondeterministic ﬁnite automata\\nWe start by giving three examples of nondeterministic ﬁnite automata. These\\nexamples will show the diﬀerence between this type of automata and the\\ndeterministic versions that we have considered in the previous sections. After\\nthese examples, we will give a formal deﬁnition of a nondeterministic ﬁnite\\nautomaton.\\n2.4.1\\nA ﬁrst example\\nConsider the following state diagram:\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,ε\\n1\\n0,1\\nYou will notice three diﬀerences with the ﬁnite automata that we have\\nseen until now. First, if the automaton is in state q1 and reads the symbol 1,\\nthen it has two options: Either it stays in state q1, or it switches to state q2.\\nSecond, if the automaton is in state q2, then it can switch to state q3 without\\nreading a symbol; this is indicated by the edge having the empty string ϵ as\\nlabel. Third, if the automaton is in state q3 and reads the symbol 0, then it\\ncannot continue.\\nLet us see what this automaton can do when it gets the string 010110 as\\ninput. Initially, the automaton is in the start state q1.\\n• Since the ﬁrst symbol in the input string is 0, the automaton stays in\\nstate q1 after having read this symbol.\\n• The second symbol is 1, and the automaton can either stay in state q1\\nor switch to state q2.\\n36\\nChapter 2.\\nFinite Automata and Regular Languages\\n– If the automaton stays in state q1, then it is still in this state after\\nhaving read the third symbol.\\n– If the automaton switches to state q2, then it again has two op-\\ntions:\\n∗Either read the third symbol in the input string, which is 0,\\nand switch to state q3,\\n∗or switch to state q3, without reading the third symbol.\\nIf we continue in this way, then we see that, for the input string 010110,\\nthere are seven possible computations. All these computations are given in\\nthe ﬁgure below.\\nq1\\nq1\\n0\\n1\\nq1\\nq1\\n0\\n1\\n1\\nq1\\nq2\\n1\\n1\\nq1\\nq2\\nq1\\n0\\n0\\nε\\nq3\\nq3\\nhang\\nhang\\nε\\nq3\\nq4\\n1\\n0\\nq4\\n1\\nq2\\n0\\nε\\nq3\\nq3\\nhang\\n1\\nq4\\n1\\nq4\\nq4\\n0\\nConsider the lowest path in the ﬁgure above:\\n• When reading the ﬁrst symbol, the automaton stays in state q1.\\n• When reading the second symbol, the automaton switches to state q2.\\n• The automaton does not read the third symbol (equivalently, it “reads”\\nthe empty string ϵ), and switches to state q3. At this moment, the\\n2.4.\\nNondeterministic ﬁnite automata\\n37\\nautomaton cannot continue: The third symbol is 0, but there is no\\nedge leaving q3 that is labeled 0, and there is no edge leaving q3 that\\nis labeled ϵ. Therefore, the computation hangs at this point.\\nFrom the ﬁgure, you can see that, out of the seven possible computations,\\nexactly two end in the accept state q4 (after the entire input string 010110 has\\nbeen read). We say that the automaton accepts the string 010110, because\\nthere is at least one computation that ends in the accept state.\\nNow consider the input string 010. In this case, there are three possible\\ncomputations:\\n1. q1\\n0→q1\\n1→q1\\n0→q1\\n2. q1\\n0→q1\\n1→q2\\n0→q3\\n3. q1\\n0→q1\\n1→q2\\nϵ→q3 →hang\\nNone of these computations ends in the accept state (after the entire input\\nstring 010 has been read). Therefore, we say that the automaton rejects the\\ninput string 010.\\nThe state diagram given above is an example of a nondeterministic ﬁnite\\nautomaton (NFA). Informally, an NFA accepts a string, if there exists at least\\none path in the state diagram that (i) starts in the start state, (ii) does not\\nhang before the entire string has been read, and (iii) ends in an accept state.\\nA string for which (i), (ii), and (iii) does not hold is rejected by the NFA.\\nThe NFA given above accepts all binary strings that contain 101 or 11 as\\na substring. All other binary strings are rejected.\\n2.4.2\\nA second example\\nLet A be the language\\nA = {w ∈{0, 1}∗: w has a 1 in the third position from the right}.\\nThe following state diagram deﬁnes an NFA that accepts all strings that are\\nin A, and rejects all strings that are not in A.\\nq1\\nq2\\nq3\\nq4\\n0,1\\n1\\n0,1\\n0,1\\n38\\nChapter 2.\\nFinite Automata and Regular Languages\\nThis NFA does the following. If it is in the start state q1 and reads the\\nsymbol 1, then it either stays in state q1 or it “guesses” that this symbol\\nis the third symbol from the right in the input string. In the latter case,\\nthe NFA switches to state q2, and then it “veriﬁes” that there are indeed\\nexactly two remaining symbols in the input string. If there are more than\\ntwo remaining symbols, then the NFA hangs (in state q4) after having read\\nthe next two symbols.\\nObserve how this guessing mechanism is used: The automaton can only\\nread the input string once, from left to right. Hence, it does not know when\\nit reaches the third symbol from the right. When the NFA reads a 1, it can\\nguess that this is the third symbol from the right; after having made this\\nguess, it veriﬁes whether or not the guess was correct.\\nIn Section 2.2.3, we have seen a DFA for the same language A. Observe\\nthat the NFA has a much simpler structure than the DFA.\\n2.4.3\\nA third example\\nConsider the following state diagram, which deﬁnes an NFA whose alphabet\\nis {0}.\\nε\\nε\\n0\\n0\\n0\\n0\\n0\\nThis NFA accepts the language\\nA = {0k : k ≡0 mod 2 or k ≡0 mod 3},\\nwhere 0k is the string consisting of k many 0s. (If k = 0, then 0k = ϵ.)\\nObserve that A is the union of the two languages\\nA1 = {0k : k ≡0 mod 2}\\n2.4.\\nNondeterministic ﬁnite automata\\n39\\nand\\nA2 = {0k : k ≡0 mod 3}.\\nThe NFA basically consists of two DFAs: one of these accepts A1, whereas the\\nother accepts A2. Given an input string w, the NFA has to decide whether\\nor not w ∈A, which is equivalent to deciding whether or not w ∈A1 or\\nw ∈A2. The NFA makes this decision in the following way: At the start, it\\n“guesses” whether (i) it is going to check whether or not w ∈A1 (i.e., the\\nlength of w is even), or (ii) it is going to check whether or not w ∈A2 (i.e.,\\nthe length of w is a multiple of 3). After having made the guess, it veriﬁes\\nwhether or not the guess was correct. If w ∈A, then there exists a way of\\nmaking the correct guess and verifying that w is indeed an element of A (by\\nending in an accept state). If w ̸∈A, then no matter which guess is made,\\nthe NFA will never end in an accept state.\\n2.4.4\\nDeﬁnition of nondeterministic ﬁnite automaton\\nThe previous examples give you an idea what nondeterministic ﬁnite au-\\ntomata are and how they work. In this section, we give a formal deﬁnition\\nof these automata.\\nFor any alphabet Σ, we deﬁne Σϵ to be the set\\nΣϵ = Σ ∪{ϵ}.\\nRecall the notion of a power set: For any set Q, the power set of Q, denoted\\nby P(Q), is the set of all subsets of Q, i.e.,\\nP(Q) = {R : R ⊆Q}.\\nDeﬁnition 2.4.1 A nondeterministic ﬁnite automaton (NFA) is a 5-tuple\\nM = (Q, Σ, δ, q, F), where\\n1. Q is a ﬁnite set, whose elements are called states,\\n2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,\\n3. δ : Q × Σϵ →P(Q) is a function, called the transition function,\\n4. q is an element of Q; it is called the start state,\\n5. F is a subset of Q; the elements of F are called accept states.\\n40\\nChapter 2.\\nFinite Automata and Regular Languages\\nAs for DFAs, the transition function δ can be thought of as the “program”\\nof the ﬁnite automaton M = (Q, Σ, δ, q, F):\\n• Let r ∈Q, and let a ∈Σϵ. Then δ(r, a) is a (possibly empty) subset of\\nQ. If the NFA M is in state r, and if it reads a (where a may be the\\nempty string ϵ), then M can switch from state r to any state in δ(r, a).\\nIf δ(r, a) = ∅, then M cannot continue and the computation hangs.\\nThe example given in Section 2.4.1 is an NFA, where Q = {q1, q2, q3, q4},\\nΣ = {0, 1}, the start state is q1, the set of accept states is F = {q4}, and the\\ntransition function δ is given by the following table:\\n0\\n1\\nϵ\\nq1\\n{q1}\\n{q1, q2}\\n∅\\nq2\\n{q3}\\n∅\\n{q3}\\nq3\\n∅\\n{q4}\\n∅\\nq4\\n{q4}\\n{q4}\\n∅\\nDeﬁnition 2.4.2 Let M = (Q, Σ, δ, q, F) be an NFA, and let w ∈Σ∗. We\\nsay that M accepts w, if1\\n• w = ϵ and the start state q is an accept state, or\\n• there exists an integer m ≥1, such that w can be written as w =\\ny1y2 . . . ym, where yi ∈Σϵ for all i with 1 ≤i ≤m, and there exists a\\nsequence r0, r1, . . . , rm of states in Q, such that\\n– r0 = q,\\n– ri+1 ∈δ(ri, yi+1), for i = 0, 1, . . . , m −1, and\\n– rm ∈F.\\nOtherwise, we say that M rejects the string w.\\nThe NFA in the example in Section 2.4.1 accepts the string 01100. This\\ncan be seen by taking\\n• m = 6,\\n1Thanks to Antoine Vigneron for pointing out an error in a previous version of this\\ndeﬁnition.\\n2.5.\\nEquivalence of DFAs and NFAs\\n41\\n• w = 01ϵ100 = y1y2y3y4y5y6, and\\n• r0 = q1, r1 = q1, r2 = q2, r3 = q3, r4 = q4, r5 = q4, and r6 = q4.\\nDeﬁnition 2.4.3 Let M = (Q, Σ, δ, q, F) be an NFA. The language L(M)\\naccepted by M is deﬁned as\\nL(M) = {w ∈Σ∗: M accepts w }.\\n2.5\\nEquivalence of DFAs and NFAs\\nYou may have the impression that nondeterministic ﬁnite automata are more\\npowerful than deterministic ﬁnite automata. In this section, we will show\\nthat this is not the case.\\nThat is, we will prove that a language can be\\naccepted by a DFA if and only if it can be accepted by an NFA. In order\\nto prove this, we will show how to convert an arbitrary NFA to a DFA that\\naccepts the same language.\\nWhat about converting a DFA to an NFA? Well, there is (almost) nothing\\nto do, because a DFA is also an NFA. This is not quite true, because\\n• the transition function of a DFA maps a state and a symbol to a state,\\nwhereas\\n• the transition function of an NFA maps a state and a symbol to a set\\nof zero or more states.\\nThe formal conversion of a DFA to an NFA is done as follows: Let M =\\n(Q, Σ, δ, q, F) be a DFA. Recall that δ is a function δ : Q × Σ →Q. We\\ndeﬁne the function δ′ : Q × Σϵ →P(Q) as follows. For any r ∈Q and for\\nany a ∈Σϵ,\\nδ′(r, a) =\\n\\x1a {δ(r, a)}\\nif a ̸= ϵ,\\n∅\\nif a = ϵ.\\nThen N = (Q, Σ, δ′, q, F) is an NFA, whose behavior is exactly the same as\\nthat of the DFA M; the easiest way to see this is by observing that the state\\ndiagrams of M and N are equal. Therefore, we have L(M) = L(N).\\nIn the rest of this section, we will show how to convert an NFA to a DFA:\\nTheorem 2.5.1 Let N = (Q, Σ, δ, q, F) be a nondeterministic ﬁnite automa-\\nton. There exists a deterministic ﬁnite automaton M, such that L(M) =\\nL(N).\\n42\\nChapter 2.\\nFinite Automata and Regular Languages\\nProof.\\nRecall that the NFA N can (in general) perform more than one\\ncomputation on a given input string. The idea of the proof is to construct a\\nDFA M that runs all these diﬀerent computations simultaneously. (We have\\nseen this idea already in the proof of Theorem 2.3.1.) To be more precise,\\nthe DFA M will have the following property:\\n• the state that M is in after having read an initial part of the input\\nstring corresponds exactly to the set of all states that N can reach\\nafter having read the same part of the input string.\\nWe start by presenting the conversion for the case when N does not\\ncontain ϵ-transitions. In other words, the state diagram of N does not contain\\nany edge that has ϵ as a label. (Later, we will extend the conversion to the\\ngeneral case.) Let the DFA M be deﬁned as M = (Q′, Σ, δ′, q′, F ′), where\\n• the set Q′ of states is equal to Q′ = P(Q); observe that |Q′| = 2|Q|,\\n• the start state q′ is equal to q′ = {q}; so M has the “same” start state\\nas N,\\n• the set F ′ of accept states is equal to the set of all elements R of Q′\\nhaving the property that R contains at least one accept state of N, i.e.,\\nF ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• the transition function δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each\\nR ∈Q′ and for each a ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nδ(r, a).\\nLet us see what the transition function δ′ of M does. First observe that,\\nsince N is an NFA, δ(r, a) is a subset of Q. This implies that δ′(R, a) is the\\nunion of subsets of Q and, therefore, also a subset of Q. Hence, δ′(R, a) is\\nan element of Q′.\\nThe set δ(r, a) is equal to the set of all states of the NFA N that can be\\nreached from state r by reading the symbol a. We take the union of these\\nsets δ(r, a), where r ranges over all elements of R, to obtain the new set\\nδ′(R, a). This new set is the state that the DFA M reaches from state R, by\\nreading the symbol a.\\n2.5.\\nEquivalence of DFAs and NFAs\\n43\\nIn this way, we obtain the correspondence that was given in the beginning\\nof this proof.\\nAfter this warming-up, we can consider the general case. In other words,\\nfrom now on, we allow ϵ-transitions in the NFA N. The DFA M is deﬁned as\\nabove, except that the start state q′ and the transition function δ′ have to be\\nmodiﬁed. Recall that a computation of the NFA N consists of the following:\\n1. Start in the start state q and make zero or more ϵ-transitions.\\n2. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n3. Make zero or more ϵ-transitions.\\n4. Read one “real” symbol of Σ and move to a new state (or stay in the\\ncurrent state).\\n5. Make zero or more ϵ-transitions.\\n6. Etc.\\nThe DFA M will simulate this computation in the following way:\\n• Simulate 1. in one single step. As we will see below, this simulation is\\nimplicitly encoded in the deﬁnition of the start state q′ of M.\\n• Simulate 2. and 3. in one single step.\\n• Simulate 4. and 5. in one single step.\\n• Etc.\\nThus, in one step, the DFA M simulates the reading of one “real” symbol of\\nΣ, followed by making zero or more ϵ-transitions.\\nTo formalize this, we need the notion of ϵ-closure. For any state r of the\\nNFA N, the ϵ-closure of r, denoted by Cϵ(r), is deﬁned to be the set of all\\nstates of N that can be reached from r, by making zero or more ϵ-transitions.\\nFor any state R of the DFA M (hence, R ⊆Q), we deﬁne\\nCϵ(R) =\\n[\\nr∈R\\nCϵ(r).\\n44\\nChapter 2.\\nFinite Automata and Regular Languages\\nHow do we deﬁne the start state q′ of the DFA M? Before the NFA N\\nreads its ﬁrst “real” symbol of Σ, it makes zero or more ϵ-transitions. In\\nother words, at the moment when N reads the ﬁrst symbol of Σ, it can be\\nin any state of Cϵ(q). Therefore, we deﬁne q′ to be\\nq′ = Cϵ(q) = Cϵ({q}).\\nHow do we deﬁne the transition function δ′ of the DFA M? Assume that\\nM is in state R, and reads the symbol a. At this moment, the NFA N would\\nhave been in any state r of R. By reading the symbol a, N can switch to\\nany state in δ(r, a), and then make zero or more ϵ-transitions. Hence, the\\nNFA can switch to any state in the set Cϵ(δ(r, a)). Based on this, we deﬁne\\nδ′(R, a) to be\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nTo summarize, the NFA N = (Q, Σ, δ, q, F) is converted to the DFA\\nM = (Q′, Σ, δ′, q′, F ′), where\\n• Q′ = P(Q),\\n• q′ = Cϵ({q}),\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅},\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nThe results proved until now can be summarized in the following theorem.\\nTheorem 2.5.2 Let A be a language. Then A is regular if and only if there\\nexists a nondeterministic ﬁnite automaton that accepts A.\\n2.5.1\\nAn example\\nConsider the NFA N = (Q, Σ, δ, q, F), where Q = {1, 2, 3}, Σ = {a, b}, q = 1,\\nF = {2}, and δ is given by the following table:\\n2.5.\\nEquivalence of DFAs and NFAs\\n45\\na\\nb\\nϵ\\n1\\n{3}\\n∅\\n{2}\\n2\\n{1}\\n∅\\n∅\\n3\\n{2}\\n{2, 3}\\n∅\\nThe state diagram of N is as follows:\\n1\\n2\\n3\\na\\na\\nǫ\\nb\\na, b\\nWe will show how to convert this NFA N to a DFA M that accepts the\\nsame language. Following the proof of Theorem 2.5.1, the DFA M is speciﬁed\\nby M = (Q′, Σ, δ′, q′, F ′), where each of the components is deﬁned below.\\n• Q′ = P(Q). Hence,\\nQ′ = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}.\\n• q′ = Cϵ({q}). Hence, the start state q′ of M is the set of all states of\\nN that can be reached from N’s start state q = 1, by making zero or\\nmore ϵ-transitions. We obtain\\nq′ = Cϵ({q}) = Cϵ({1}) = {1, 2}.\\n• F ′ = {R ∈Q′ : R ∩F ̸= ∅}. Hence, the accept states of M are those\\nstates that contain the accept state 2 of N. We obtain\\nF ′ = {{2}, {1, 2}, {2, 3}, {1, 2, 3}}.\\n46\\nChapter 2.\\nFinite Automata and Regular Languages\\n• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each\\na ∈Σ,\\nδ′(R, a) =\\n[\\nr∈R\\nCϵ(δ(r, a)).\\nIn this example δ′ is given by\\nδ′(∅, a) = ∅\\nδ′(∅, b) = ∅\\nδ′({1}, a) = {3}\\nδ′({1}, b) = ∅\\nδ′({2}, a) = {1, 2}\\nδ′({2}, b) = ∅\\nδ′({3}, a) = {2}\\nδ′({3}, b) = {2, 3}\\nδ′({1, 2}, a) = {1, 2, 3}\\nδ′({1, 2}, b) = ∅\\nδ′({1, 3}, a) = {2, 3}\\nδ′({1, 3}, b) = {2, 3}\\nδ′({2, 3}, a) = {1, 2}\\nδ′({2, 3}, b) = {2, 3}\\nδ′({1, 2, 3}, a) = {1, 2, 3}\\nδ′({1, 2, 3}, b) = {2, 3}\\nThe state diagram of the DFA M is as follows:\\n2.5.\\nEquivalence of DFAs and NFAs\\n47\\n/0\\n{1}\\n{2}\\n{3}\\n{1,2}\\n{2,3}\\n{1,3}\\n{1,2,3}\\na,b\\nb\\na\\nb\\na\\na\\nb\\na,b\\na\\nb\\nb\\na\\nb\\na\\nWe make the following observations:\\n• The states {1} and {1, 3} do not have incoming edges. Therefore, these\\ntwo states cannot be reached from the start state {1, 2}.\\n• The state {3} has only one incoming edge; it comes from the state\\n{1}. Since {1} cannot be reached from the start state, {3} cannot be\\nreached from the start state.\\n• The state {2} has only one incoming edge; it comes from the state\\n{3}. Since {3} cannot be reached from the start state, {2} cannot be\\nreached from the start state.\\nHence, we can remove the four states {1}, {2}, {3}, and {1, 3}. The\\nresulting DFA accepts the same language as the DFA above.\\nThis leads\\nto the following state diagram, which depicts a DFA that accepts the same\\nlanguage as the NFA N:\\n48\\nChapter 2.\\nFinite Automata and Regular Languages\\n/0\\n{1,2}\\n{2,3}\\n{1,2,3}\\na,b\\na\\nb\\nb\\na\\nb\\na\\n2.6\\nClosure under the regular operations\\nIn Section 2.3, we have deﬁned the regular operations union, concatenation,\\nand star. We proved in Theorem 2.3.1 that the union of two regular lan-\\nguages is a regular language. We also explained why it is not clear that the\\nconcatenation of two regular languages is regular, and that the star of a reg-\\nular language is regular. In this section, we will see that the concept of NFA,\\ntogether with Theorem 2.5.2, can be used to give a simple proof of the fact\\nthat the regular languages are indeed closed under the regular operations.\\nWe start by giving an alternative proof of Theorem 2.3.1:\\nTheorem 2.6.1 The set of regular languages is closed under the union op-\\neration, i.e., if A1 and A2 are regular languages over the same alphabet Σ,\\nthen A1 ∪A2 is also a regular language.\\n2.6.\\nClosure under the regular operations\\n49\\nq1\\nM1\\nM2\\nq2\\nq0\\nq1\\nq2\\nε\\nε\\nM\\nFigure 2.1: The NFA M accepts L(M1) ∪L(M2).\\nProof.\\nSince A1 is regular, there is, by Theorem 2.5.2, an NFA M1 =\\n(Q1, Σ, δ1, q1, F1), such that A1 = L(M1). Similarly, there is an NFA M2 =\\n(Q2, Σ, δ2, q2, F2), such that A2 = L(M2). We may assume that Q1 ∩Q2 = ∅,\\nbecause otherwise, we can give new “names” to the states of Q1 and Q2.\\nFrom these two NFAs, we will construct an NFA M = (Q, Σ, δ, q0, F), such\\nthat L(M) = A1 ∪A2. The construction is illustrated in Figure 2.1. The\\nNFA M is deﬁned as follows:\\n1. Q = {q0} ∪Q1 ∪Q2, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = F1 ∪F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\n50\\nChapter 2.\\nFinite Automata and Regular Languages\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1,\\nδ2(r, a)\\nif r ∈Q2,\\n{q1, q2}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nTheorem 2.6.2 The set of regular languages is closed under the concatena-\\ntion operation, i.e., if A1 and A2 are regular languages over the same alphabet\\nΣ, then A1A2 is also a regular language.\\nProof.\\nLet M1 = (Q1, Σ, δ1, q1, F1) be an NFA, such that A1 = L(M1).\\nSimilarly, let M2 = (Q2, Σ, δ2, q2, F2) be an NFA, such that A2 = L(M2).\\nAs in the proof of Theorem 2.6.1, we may assume that Q1 ∩Q2 = ∅. We\\nwill construct an NFA M = (Q, Σ, δ, q0, F), such that L(M) = A1A2. The\\nconstruction is illustrated in Figure 2.2. The NFA M is deﬁned as follows:\\n1. Q = Q1 ∪Q2.\\n2. q0 = q1.\\n3. F = F2.\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q2}\\nif r ∈F1 and a = ϵ,\\nδ2(r, a)\\nif r ∈Q2.\\nTheorem 2.6.3 The set of regular languages is closed under the star oper-\\nation, i.e., if A is a regular language, then A∗is also a regular language.\\n2.6.\\nClosure under the regular operations\\n51\\nq1\\nM1\\nM2\\nq2\\nq2\\nε\\nε\\nε\\nq0\\nM\\nFigure 2.2: The NFA M accepts L(M1)L(M2).\\nq1\\nN\\nq1\\nq0\\nε\\nε\\nε\\nε\\nM\\nFigure 2.3: The NFA M accepts (L(N))∗.\\nProof. Let Σ be the alphabet of A and let N = (Q1, Σ, δ1, q1, F1) be an\\nNFA, such that A = L(N). We will construct an NFA M = (Q, Σ, δ, q0, F),\\nsuch that L(M) = A∗. The construction is illustrated in Figure 2.3. The\\nNFA M is deﬁned as follows:\\n52\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. Q = {q0} ∪Q1, where q0 is a new state.\\n2. q0 is the start state of M.\\n3. F = {q0} ∪F1. (Since ϵ ∈A∗, q0 has to be an accept state.)\\n4. δ : Q × Σϵ →P(Q) is deﬁned as follows: For any r ∈Q and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ,\\n{q1}\\nif r = q0 and a = ϵ,\\n∅\\nif r = q0 and a ̸= ϵ.\\nIn the ﬁnal theorem of this section, we mention (without proof) two more\\nclosure properties of the regular languages:\\nTheorem 2.6.4 The set of regular languages is closed under the complement\\nand intersection operations:\\n1. If A is a regular language over the alphabet Σ, then the complement\\nA = {w ∈Σ∗: w ̸∈A}\\nis also a regular language.\\n2. If A1 and A2 are regular languages over the same alphabet Σ, then the\\nintersection\\nA1 ∩A2 = {w ∈Σ∗: w ∈A1 and w ∈A2}\\nis also a regular language.\\n2.7\\nRegular expressions\\nIn this section, we present regular expressions, which are a means to describe\\nlanguages. As we will see, the class of languages that can be described by\\nregular expressions coincides with the class of regular languages.\\n2.7.\\nRegular expressions\\n53\\nBefore formally deﬁning the notion of a regular expression, we give some\\nexamples. Consider the expression\\n(0 ∪1)01∗.\\nThe language described by this expression is the set of all binary strings\\n1. that start with either 0 or 1 (this is indicated by (0 ∪1)),\\n2. for which the second symbol is 0 (this is indicated by 0), and\\n3. that end with zero or more 1s (this is indicated by 1∗).\\nThat is, the language described by this expression is\\n{00, 001, 0011, 00111, . . . , 10, 101, 1011, 10111, . . .}.\\nHere are some more examples (in all cases, the alphabet is {0, 1}):\\n• The language {w : w contains exactly two 0s} is described by the ex-\\npression\\n1∗01∗01∗.\\n• The language {w : w contains at least two 0s} is described by the ex-\\npression\\n(0 ∪1)∗0(0 ∪1)∗0(0 ∪1)∗.\\n• The language {w : 1011 is a substring of w} is described by the ex-\\npression\\n(0 ∪1)∗1011(0 ∪1)∗.\\n• The language {w : the length of w is even} is described by the expres-\\nsion\\n((0 ∪1)(0 ∪1))∗.\\n• The language {w : the length of w is odd} is described by the expres-\\nsion\\n(0 ∪1) ((0 ∪1)(0 ∪1))∗.\\n• The language {1011, 0} is described by the expression\\n1011 ∪0.\\n54\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The language {w :\\nthe ﬁrst and last symbols of w are equal} is de-\\nscribed by the expression\\n0(0 ∪1)∗0 ∪1(0 ∪1)∗1 ∪0 ∪1.\\nAfter these examples, we give a formal (and inductive) deﬁnition of regular\\nexpressions:\\nDeﬁnition 2.7.1 Let Σ be a non-empty alphabet.\\n1. ϵ is a regular expression.\\n2. ∅is a regular expression.\\n3. For each a ∈Σ, a is a regular expression.\\n4. If R1 and R2 are regular expressions, then R1 ∪R2 is a regular expres-\\nsion.\\n5. If R1 and R2 are regular expressions, then R1R2 is a regular expression.\\n6. If R is a regular expression, then R∗is a regular expression.\\nYou can regard 1., 2., and 3. as being the “building blocks” of regular\\nexpressions.\\nItems 4., 5., and 6. give rules that can be used to combine\\nregular expressions into new (and “larger”) regular expressions. To give an\\nexample, we claim that\\n(0 ∪1)∗101(0 ∪1)∗\\nis a regular expression (where the alphabet Σ is equal to {0, 1}). In order\\nto prove this, we have to show that this expression can be “built” using the\\n“rules” given in Deﬁnition 2.7.1. Here we go:\\n• By 3., 0 is a regular expression.\\n• By 3., 1 is a regular expression.\\n• Since 0 and 1 are regular expressions, by 4., 0∪1 is a regular expression.\\n• Since 0∪1 is a regular expression, by 6., (0∪1)∗is a regular expression.\\n• Since 1 and 0 are regular expressions, by 5., 10 is a regular expression.\\n2.7.\\nRegular expressions\\n55\\n• Since 10 and 1 are regular expressions, by 5., 101 is a regular expression.\\n• Since (0 ∪1)∗and 101 are regular expressions, by 5., (0 ∪1)∗101 is a\\nregular expression.\\n• Since (0 ∪1)∗101 and (0 ∪1)∗are regular expressions, by 5., (0 ∪\\n1)∗101(0 ∪1)∗is a regular expression.\\nNext we deﬁne the language that is described by a regular expression:\\nDeﬁnition 2.7.2 Let Σ be a non-empty alphabet.\\n1. The regular expression ϵ describes the language {ϵ}.\\n2. The regular expression ∅describes the language ∅.\\n3. For each a ∈Σ, the regular expression a describes the language {a}.\\n4. Let R1 and R2 be regular expressions and let L1 and L2 be the lan-\\nguages described by them, respectively. The regular expression R1∪R2\\ndescribes the language L1 ∪L2.\\n5. Let R1 and R2 be regular expressions and let L1 and L2 be the languages\\ndescribed by them, respectively. The regular expression R1R2 describes\\nthe language L1L2.\\n6. Let R be a regular expression and let L be the language described by\\nit. The regular expression R∗describes the language L∗.\\nWe consider some examples:\\n• The regular expression (0∪ϵ)(1∪ϵ) describes the language {01, 0, 1, ϵ}.\\n• The regular expression 0 ∪ϵ describes the language {0, ϵ}, whereas the\\nregular expression 1∗describes the language {ϵ, 1, 11, 111, . . .}. There-\\nfore, the regular expression (0 ∪ϵ)1∗describes the language\\n{0, 01, 011, 0111, . . . , ϵ, 1, 11, 111, . . .}.\\nObserve that this language is also described by the regular expression\\n01∗∪1∗.\\n56\\nChapter 2.\\nFinite Automata and Regular Languages\\n• The regular expression 1∗∅describes the empty language, i.e., the lan-\\nguage ∅. (You should convince yourself that this is correct.)\\n• The regular expression ∅∗describes the language {ϵ}.\\nDeﬁnition 2.7.3 Let R1 and R2 be regular expressions and let L1 and L2\\nbe the languages described by them, respectively. If L1 = L2 (i.e., R1 and\\nR2 describe the same language), then we will write R1 = R2.\\nHence, even though (0∪ϵ)1∗and 01∗∪1∗are diﬀerent regular expressions,\\nwe write\\n(0 ∪ϵ)1∗= 01∗∪1∗,\\nbecause they describe the same language.\\nIn Section 2.8.2, we will show that every regular language can be described\\nby a regular expression. The proof of this fact is purely algebraic and uses\\nthe following algebraic identities involving regular expressions.\\nTheorem 2.7.4 Let R1, R2, and R3 be regular expressions. The following\\nidentities hold:\\n1. R1∅= ∅R1 = ∅.\\n2. R1ϵ = ϵR1 = R1.\\n3. R1 ∪∅= ∅∪R1 = R1.\\n4. R1 ∪R1 = R1.\\n5. R1 ∪R2 = R2 ∪R1.\\n6. R1(R2 ∪R3) = R1R2 ∪R1R3.\\n7. (R1 ∪R2)R3 = R1R3 ∪R2R3.\\n8. R1(R2R3) = (R1R2)R3.\\n9. ∅∗= ϵ.\\n10. ϵ∗= ϵ.\\n11. (ϵ ∪R1)∗= R∗\\n1.\\n2.8.\\nEquivalence of regular expressions and regular languages 57\\n12. (ϵ ∪R1)(ϵ ∪R1)∗= R∗\\n1.\\n13. R∗\\n1(ϵ ∪R1) = (ϵ ∪R1)R∗\\n1 = R∗\\n1.\\n14. R∗\\n1R2 ∪R2 = R∗\\n1R2.\\n15. R1(R2R1)∗= (R1R2)∗R1.\\n16. (R1 ∪R2)∗= (R∗\\n1R2)∗R∗\\n1 = (R∗\\n2R1)∗R∗\\n2.\\nWe will not present the (boring) proofs of these identities, but urge you\\nto convince yourself informally that they make perfect sense. To give an\\nexample, we mentioned above that\\n(0 ∪ϵ)1∗= 01∗∪1∗.\\nWe can verify this identity in the following way:\\n(0 ∪ϵ)1∗\\n=\\n01∗∪ϵ1∗\\n(by identity 7)\\n=\\n01∗∪1∗\\n(by identity 2)\\n2.8\\nEquivalence of regular expressions and reg-\\nular languages\\nIn the beginning of Section 2.7, we mentioned the following result:\\nTheorem 2.8.1 Let L be a language. Then L is regular if and only if there\\nexists a regular expression that describes L.\\nThe proof of this theorem consists of two parts:\\n• In Section 2.8.1, we will prove that every regular expression describes\\na regular language.\\n• In Section 2.8.2, we will prove that every DFA M can be converted to\\na regular expression that describes the language L(M).\\nThese two results will prove Theorem 2.8.1.\\n58\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.8.1\\nEvery regular expression describes a regular lan-\\nguage\\nLet R be an arbitrary regular expression over the alphabet Σ. We will prove\\nthat the language described by R is a regular language. The proof is by\\ninduction on the structure of R (i.e., by induction on the way R is “built”\\nusing the “rules” given in Deﬁnition 2.7.1).\\nThe ﬁrst base case: Assume that R = ϵ.\\nThen R describes the lan-\\nguage {ϵ}. In order to prove that this language is regular, it suﬃces, by\\nTheorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q, F) that accepts this\\nlanguage. This NFA is obtained by deﬁning Q = {q}, q is the start state,\\nF = {q}, and δ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state\\ndiagram of M:\\nq\\nThe second base case: Assume that R = ∅. Then R describes the language\\n∅. In order to prove that this language is regular, it suﬃces, by Theorem 2.5.2,\\nto construct an NFA M = (Q, Σ, δ, q, F) that accepts this language. This\\nNFA is obtained by deﬁning Q = {q}, q is the start state, F = ∅, and\\nδ(q, a) = ∅for all a ∈Σϵ. The ﬁgure below gives the state diagram of M:\\nq\\nThe third base case: Let a ∈Σ and assume that R = a. Then R describes\\nthe language {a}. In order to prove that this language is regular, it suﬃces,\\nby Theorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q1, F) that accepts\\nthis language. This NFA is obtained by deﬁning Q = {q1, q2}, q1 is the start\\nstate, F = {q2}, and\\nδ(q1, a)\\n=\\n{q2},\\nδ(q1, b)\\n=\\n∅for all b ∈Σϵ \\\\ {a},\\nδ(q2, b)\\n=\\n∅for all b ∈Σϵ.\\nThe ﬁgure below gives the state diagram of M:\\n2.8.\\nEquivalence of regular expressions and regular languages 59\\nq1\\nq2\\na\\nThe ﬁrst case of the induction step: Assume that R = R1 ∪R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1 ∪L2, which, by Theorem 2.6.1, is regular.\\nThe second case of the induction step: Assume that R = R1R2, where\\nR1 and R2 are regular expressions. Let L1 and L2 be the languages described\\nby R1 and R2, respectively, and assume that L1 and L2 are regular. Then R\\ndescribes the language L1L2, which, by Theorem 2.6.2, is regular.\\nThe third case of the induction step: Assume that R = (R1)∗, where\\nR1 is a regular expression.\\nLet L1 be the language described by R1 and\\nassume that L1 is regular. Then R describes the language (L1)∗, which, by\\nTheorem 2.6.3, is regular.\\nThis concludes the proof of the claim that every regular expression de-\\nscribes a regular language.\\nTo give an example, consider the regular expression\\n(ab ∪a)∗,\\nwhere the alphabet is {a, b}. We will prove that this regular expression de-\\nscribes a regular language, by constructing an NFA that accepts the language\\ndescribed by this regular expression. Observe how the regular expression is\\n“built”:\\n• Take the regular expressions a and b, and combine them into the regular\\nexpression ab.\\n• Take the regular expressions ab and a, and combine them into the\\nregular expression ab ∪a.\\n• Take the regular expression ab ∪a, and transform it into the regular\\nexpression (ab ∪a)∗.\\nFirst, we construct an NFA M1 that accepts the language described by\\nthe regular expression a:\\n60\\nChapter 2.\\nFinite Automata and Regular Languages\\na\\nM1\\nNext, we construct an NFA M2 that accepts the language described by\\nthe regular expression b:\\nM2\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.2 to\\nM1 and M2. This gives an NFA M3 that accepts the language described by\\nthe regular expression ab:\\nM3\\na\\nε\\nb\\nNext, we apply the construction given in the proof of Theorem 2.6.1 to\\nM3 and M1. This gives an NFA M4 that accepts the language described by\\nthe regular expression ab ∪a:\\na\\nε\\nb\\na\\nε\\nε\\nM4\\nFinally, we apply the construction given in the proof of Theorem 2.6.3\\nto M4. This gives an NFA M5 that accepts the language described by the\\nregular expression (ab ∪a)∗:\\n2.8.\\nEquivalence of regular expressions and regular languages 61\\na\\nε\\nb\\na\\nε\\nε\\nε\\nε\\nε\\nM5\\n2.8.2\\nConverting a DFA to a regular expression\\nIn this section, we will prove that every DFA M can be converted to a regular\\nexpression that describes the language L(M). In order to prove this result,\\nwe need to solve recurrence relations involving languages.\\nSolving recurrence relations\\nLet Σ be an alphabet, let B and C be “known” languages in Σ∗such that\\nϵ ̸∈B, and let L be an “unknown” language such that\\nL = BL ∪C.\\nCan we “solve” this equation for L? That is, can we express L in terms of\\nB and C?\\nConsider an arbitrary string u in L. We are going to determine how u\\nlooks like. Since u ∈L and L = BL ∪C, we know that u is a string in\\nBL ∪C. Hence, there are two possibilities for u.\\n1. u is an element of C.\\n2. u is an element of BL. In this case, there are strings b ∈B and v ∈L\\nsuch that u = bv. Since ϵ ̸∈B, we have b ̸= ϵ and, therefore, |v| < |u|.\\n(Recall that |v| denotes the length, i.e., the number of symbols, of the\\nstring v.) Since v is a string in L, which is equal to BL ∪C, v is a\\nstring in BL ∪C. Hence, there are two possibilities for v.\\n62\\nChapter 2.\\nFinite Automata and Regular Languages\\n(a) v is an element of C. In this case,\\nu = bv, where b ∈B and v ∈C; thus, u ∈BC.\\n(b) v is an element of BL. In this case, there are strings b′ ∈B and\\nw ∈L such that v = b′w. Since ϵ ̸∈B, we have b′ ̸= ϵ and,\\ntherefore, |w| < |v|. Since w is a string in L, which is equal to\\nBL∪C, w is a string in BL∪C. Hence, there are two possibilities\\nfor w.\\ni. w is an element of C. In this case,\\nu = bb′w, where b, b′ ∈B and w ∈C; thus, u ∈BBC.\\nii. w is an element of BL. In this case, there are strings b′′ ∈B\\nand x ∈L such that w = b′′x. Since ϵ ̸∈B, we have b′′ ̸= ϵ\\nand, therefore, |x| < |w|. Since x is a string in L, which is\\nequal to BL ∪C, x is a string in BL ∪C. Hence, there are\\ntwo possibilities for x.\\nA. x is an element of C. In this case,\\nu = bb′b′′x, where b, b′, b′′ ∈B and x ∈C; thus, u ∈BBBC.\\nB. x is an element of BL. Etc., etc.\\nThis process hopefully convinces you that any string u in L can be written\\nas the concatenation of zero or more strings in B, followed by one string in\\nC. In fact, L consists of exactly those strings having this property:\\nLemma 2.8.2 Let Σ be an alphabet, and let B, C, and L be languages in\\nΣ∗such that ϵ ̸∈B and\\nL = BL ∪C.\\nThen\\nL = B∗C.\\nProof. First, we show that B∗C ⊆L. Let u be an arbitrary string in B∗C.\\nThen u is the concatenation of k strings of B, for some k ≥0, followed by\\none string of C. We proceed by induction on k.\\nThe base case is when k = 0. In this case, u is a string in C. Hence, u is\\na string in BL ∪C. Since BL ∪C = L, it follows that u is a string in L.\\n2.8.\\nEquivalence of regular expressions and regular languages 63\\nNow let k ≥1. Then we can write u = vwc, where v is a string in B,\\nw is the concatenation of k −1 strings of B, and c is a string of C. Deﬁne\\ny = wc. Observe that y is the concatenation of k −1 strings of B followed\\nby one string of C. Therefore, by induction, the string y is an element of L.\\nHence, u = vy, where v is a string in B and y is a string in L. This shows\\nthat u is a string in BL. Hence, u is a string in BL ∪C. Since BL ∪C = L,\\nit follows that u is a string in L. This completes the proof that B∗C ⊆L.\\nIt remains to show that L ⊆B∗C. Let u be an arbitrary string in L,\\nand let ℓbe its length (i.e., ℓis the number of symbols in u). We prove by\\ninduction on ℓthat u is a string in B∗C.\\nThe base case is when ℓ= 0. Then u = ϵ. Since u ∈L and L = BL ∪C,\\nu is a string in BL ∪C. Since ϵ ̸∈B, u cannot be a string in BL. Hence, u\\nmust be a string in C. Since C ⊆B∗C, it follows that u is a string in B∗C.\\nLet ℓ≥1. If u is a string in C, then u is a string in B∗C and we are done.\\nSo assume that u is not a string in C. Since u ∈L and L = BL ∪C, u is a\\nstring in BL. Hence, there are strings b ∈B and v ∈L such that u = bv.\\nSince ϵ ̸∈B, the length of b is at least one; hence, the length of v is less than\\nthe length of u. By induction, v is a string in B∗C. Hence, u = bv, where\\nb ∈B and v ∈B∗C. This shows that u ∈B(B∗C). Since B(B∗C) ⊆B∗C,\\nit follows that u ∈B∗C.\\nNote that Lemma 2.8.2 holds for any language B that does not contain\\nthe empty string ϵ. As an example, assume that B = ∅. Then the language\\nL satisﬁes the equation\\nL = BL ∪C = ∅L ∪C.\\nUsing Theorem 2.7.4, this equation becomes\\nL = ∅∪C = C.\\nWe now show that Lemma 2.8.2 also implies that L = C: Since ϵ ̸∈B,\\nLemma 2.8.2 implies that L = B∗C, which, using Theorem 2.7.4, becomes\\nL = B∗C = ∅∗C = ϵC = C.\\nThe conversion\\nWe will now use Lemma 2.8.2 to prove that every DFA can be converted to\\na regular expression.\\n64\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet M = (Q, Σ, δ, q, F) be an arbitrary deterministic ﬁnite automaton.\\nWe will show that there exists a regular expression that describes the lan-\\nguage L(M).\\nFor each state r ∈Q, we deﬁne\\nLr = {w ∈Σ∗:\\nthe path in the state diagram of M that starts\\nin state r and that corresponds to w ends in a\\nstate of F }.\\nIn words, Lr is the language accepted by M, if r were the start state.\\nWe will show that each such language Lr can be described by a regular\\nexpression. Since L(M) = Lq, this will prove that L(M) can be described by\\na regular expression.\\nThe basic idea is to set up equations for the languages Lr, which we then\\nsolve using Lemma 2.8.2. We claim that\\nLr =\\n[\\na∈Σ\\na · Lδ(r,a)\\nif r ̸∈F.\\n(2.2)\\nWhy is this true? Let w be a string in Lr. Then the path P in the state\\ndiagram of M that starts in state r and that corresponds to w ends in a\\nstate of F. Since r ̸∈F, this path contains at least one edge. Let r′ be the\\nstate that follows the ﬁrst state (i.e., r) of P. Then r′ = δ(r, b) for some\\nsymbol b ∈Σ. Hence, b is the ﬁrst symbol of w. Write w = bv, where v is\\nthe remaining part of w. Then the path P ′ = P \\\\ {r} in the state diagram\\nof M that starts in state r′ and that corresponds to v ends in a state of F.\\nTherefore, v ∈Lr′ = Lδ(r,b). Hence,\\nw ∈b · Lδ(r,b) ⊆\\n[\\na∈Σ\\na · Lδ(r,a).\\nConversely, let w be a string in S\\na∈Σ a · Lδ(r,a). Then there is a symbol b ∈Σ\\nand a string v ∈Lδ(r,b) such that w = bv. Let P ′ be the path in the state\\ndiagram of M that starts in state δ(r, b) and that corresponds to v. Since\\nv ∈Lδ(r,b), this path ends in a state of F. Let P be the path in the state\\ndiagram of M that starts in r, follows the edge to δ(r, b), and then follows P ′.\\nThis path P corresponds to w and ends in a state of F. Therefore, w ∈Lr.\\nThis proves the correctness of (2.2).\\n2.8.\\nEquivalence of regular expressions and regular languages 65\\nSimilarly, we can prove that\\nLr = ϵ ∪\\n [\\na∈Σ\\na · Lδ(r,a)\\n!\\nif r ∈F.\\n(2.3)\\nSo we now have a set of equations in the “unknowns” Lr, for r ∈Q. The\\nnumber of equations is equal to the size of Q. In other words, the number\\nof equations is equal to the number of unknowns. The regular expression for\\nL(M) = Lq is obtained by solving these equations using Lemma 2.8.2.\\nOf course, we have to convince ourselves that these equations have a so-\\nlution for any given DFA. Before we deal with this issue, we give an example.\\nAn example\\nConsider the deterministic ﬁnite automaton M = (Q, Σ, δ, q0, F), where Q =\\n{q0, q1, q2}, Σ = {a, b}, q0 is the start state, F = {q2}, and δ is given in the\\nstate diagram below. We show how to obtain the regular expression that\\ndescribes the language accepted by M.\\nq0\\nq1\\nq2\\na\\na\\na\\nb\\nb\\nb\\nFor this case, (2.2) and (2.3) give the following equations:\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nLq0\\n=\\na · Lq0 ∪b · Lq2\\nLq1\\n=\\na · Lq0 ∪b · Lq1\\nLq2\\n=\\nϵ ∪a · Lq1 ∪b · Lq0\\n66\\nChapter 2.\\nFinite Automata and Regular Languages\\nIn the third equation, Lq2 is expressed in terms of Lq0 and Lq1. Hence, if we\\nsubstitute the third equation into the ﬁrst one, and use Theorem 2.7.4, then\\nwe get\\nLq0\\n=\\na · Lq0 ∪b · (ϵ ∪a · Lq1 ∪b · Lq0)\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b.\\nWe obtain the following set of equations.\\n\\x1a Lq0\\n=\\n(a ∪bb) · Lq0 ∪ba · Lq1 ∪b\\nLq1\\n=\\nb · Lq1 ∪a · Lq0\\nLet L = Lq1, B = b, and C = a · Lq0. Then ϵ ̸∈B and the second equation\\nreads L = BL ∪C. Hence, by Lemma 2.8.2,\\nLq1 = L = B∗C = b∗a · Lq0.\\nIf we substitute Lq1 into the ﬁrst equation, then we get (again using Theo-\\nrem 2.7.4)\\nLq0\\n=\\n(a ∪bb) · Lq0 ∪ba · b∗a · Lq0 ∪b\\n=\\n(a ∪bb ∪bab∗a)Lq0 ∪b.\\nAgain applying Lemma 2.8.2, this time with L = Lq0, B = a∪bb∪bab∗a and\\nC = b, gives\\nLq0 = (a ∪bb ∪bab∗a)∗b.\\nThus, the regular expression that describes the language accepted by M is\\n(a ∪bb ∪bab∗a)∗b.\\nCompleting the correctness of the conversion\\nIt remains to prove that, for any DFA, the system of equations (2.2) and (2.3)\\ncan be solved. This will follow from the following (more general) lemma.\\n(You should verify that the equations (2.2) and (2.3) are in the form as\\nspeciﬁed in this lemma.)\\n2.8.\\nEquivalence of regular expressions and regular languages 67\\nLemma 2.8.3 Let n ≥1 be an integer and, for 1 ≤i ≤n and 1 ≤j ≤n,\\nlet Bij and Ci be regular expressions such that ϵ ̸∈Bij. Let L1, L2, . . . , Ln be\\nlanguages that satisfy\\nLi =\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci for 1 ≤i ≤n.\\nThen L1 can be expressed as a regular expression only involving the regular\\nexpressions Bij and Ci.\\nProof. The proof is by induction on n. The base case is when n = 1. In\\nthis case, we have\\nL1 = B11L1 ∪C1.\\nSince ϵ ̸∈B11, it follows from Lemma 2.8.2 that L1 = B∗\\n11C1. This proves\\nthe base case.\\nLet n ≥2 and assume the lemma is true for n −1. We have\\nLn\\n=\\n n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n=\\nBnnLn ∪\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn.\\nSince ϵ ̸∈Bnn, it follows from Lemma 2.8.2 that\\nLn\\n=\\nB∗\\nnn\\n  n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪Cn\\n!\\n=\\nB∗\\nnn\\n n−1\\n[\\nj=1\\nBnjLj\\n!\\n∪B∗\\nnnCn\\n=\\n n−1\\n[\\nj=1\\nB∗\\nnnBnjLj\\n!\\n∪B∗\\nnnCn\\nBy substituting this equation for Ln into the equations for Li, 1 ≤i ≤n −1,\\n68\\nChapter 2.\\nFinite Automata and Regular Languages\\nwe obtain\\nLi\\n=\\n n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\nBinLn ∪\\n n−1\\n[\\nj=1\\nBijLj\\n!\\n∪Ci\\n=\\n n−1\\n[\\nj=1\\n(BinB∗\\nnnBnj ∪Bij) Lj\\n!\\n∪BinB∗\\nnnCn ∪Ci.\\nThus, we have obtained n −1 equations in L1, L2, . . . , Ln−1.\\nSince ϵ ̸∈\\nBinB∗\\nnnBnj ∪Bij, it follows from the induction hypothesis that L1 can be\\nexpressed as a regular expression only involving the regular expressions Bij\\nand Ci.\\n2.9\\nThe pumping lemma and nonregular lan-\\nguages\\nIn the previous sections, we have seen that the class of regular languages is\\nclosed under various operations, and that these languages can be described by\\n(deterministic or nondeterministic) ﬁnite automata and regular expressions.\\nThese properties helped in developing techniques for showing that a language\\nis regular. In this section, we will present a tool that can be used to prove\\nthat certain languages are not regular. Observe that for a regular language,\\n1. the amount of memory that is needed to determine whether or not a\\ngiven string is in the language is ﬁnite and independent of the length\\nof the string, and\\n2. if the language consists of an inﬁnite number of strings, then this lan-\\nguage should contain inﬁnite subsets having a fairly repetitive struc-\\nture.\\nIntuitively, languages that do not follow 1. or 2. should be nonregular. For\\nexample, consider the language\\n{0n1n : n ≥0}.\\n2.9.\\nThe pumping lemma and nonregular languages\\n69\\nThis language should be nonregular, because it seems unlikely that a DFA can\\nremember how many 0s it has seen when it has reached the border between\\nthe 0s and the 1s. Similarly the language\\n{0n : n is a prime number}\\nshould be nonregular, because the prime numbers do not seem to have any\\nrepetitive structure that can be used by a DFA. To be more rigorous about\\nthis, we will establish a property that all regular languages must possess.\\nThis property is called the pumping lemma. If a language does not have this\\nproperty, then it must be nonregular.\\nThe pumping lemma states that any suﬃciently long string in a regular\\nlanguage can be pumped, i.e., there is a section in that string that can be\\nrepeated any number of times, so that the resulting strings are all in the\\nlanguage.\\nTheorem 2.9.1 (Pumping Lemma for Regular Languages) Let A be\\na regular language. Then there exists an integer p ≥1, called the pumping\\nlength, such that the following holds: Every string s in A, with |s| ≥p, can\\nbe written as s = xyz, such that\\n1. y ̸= ϵ (i.e., |y| ≥1),\\n2. |xy| ≤p, and\\n3. for all i ≥0, xyiz ∈A.\\nIn words, the pumping lemma states that by replacing the portion y in s\\nby zero or more copies of it, the resulting string is still in the language A.\\nProof. Let Σ be the alphabet of A. Since A is a regular language, there\\nexists a DFA M = (Q, Σ, δ, q, F) that accepts A. We deﬁne p to be the\\nnumber of states in Q.\\nLet s = s1s2 . . . sn be an arbitrary string in A such that n ≥p. Deﬁne\\nr1 = q, r2 = δ(r1, s1), r3 = δ(r2, s2), . . ., rn+1 = δ(rn, sn). Thus, when the\\nDFA M reads the string s from left to right, it visits the states r1, r2, . . . , rn+1.\\nSince s is a string in A, we know that rn+1 belongs to F.\\nConsider the ﬁrst p + 1 states r1, r2, . . . , rp+1 in this sequence. Since the\\nnumber of states of M is equal to p, the pigeonhole principle implies that\\nthere must be a state that occurs twice in this sequence. That is, there are\\nindices j and ℓsuch that 1 ≤j < ℓ≤p + 1 and rj = rℓ.\\n70\\nChapter 2.\\nFinite Automata and Regular Languages\\nq = r1\\nrn+1\\nr j = rℓ\\nread x\\nread y\\nread z\\nWe deﬁne x = s1s2 . . . sj−1, y = sj . . . sℓ−1, and z = sℓ. . . sn. Since j < ℓ,\\nwe have y ̸= ϵ, proving the ﬁrst claim in the theorem. Since ℓ≤p + 1, we\\nhave |xy| = ℓ−1 ≤p, proving the second claim in the theorem. To see that\\nthe third claim also holds, recall that the string s = xyz is accepted by M.\\nWhile reading x, M moves from the start state q to state rj. While reading\\ny, it moves from state rj to state rℓ= rj, i.e., after having read y, M is again\\nin state rj. While reading z, M moves from state rj to the accept state rn+1.\\nTherefore, the substring y can be repeated any number i ≥0 of times, and\\nthe corresponding string xyiz will still be accepted by M. It follows that\\nxyiz ∈A for all i ≥0.\\n2.9.1\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {0n1n : n ≥0}.\\nWe will prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. It is clear\\nthat s ∈A and |s| = 2p ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that, since |xy| ≤p, the string y contains only 0s. Moreover,\\nsince y ̸= ϵ, y contains at least one 0. But now we are in trouble: None of\\nthe strings xy0z = xz, xy2z = xyyz, xy3z = xyyyz, . . . , is contained in A.\\nHowever, by the pumping lemma, all these strings must be in A. Hence, we\\nhave a contradiction and we conclude that A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n71\\nSecond example\\nConsider the language\\nA = {w ∈{0, 1}∗: the number of 0s in w equals the number of 1s in w}.\\nAgain, we prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p1p. Then s ∈A\\nand |s| = 2p ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz contains more 0s than 1s,\\nwhich implies that this string is not contained in A. But, by the pumping\\nlemma, this string is contained in A. This is a contradiction and, therefore,\\nA is not a regular language.\\nThird example\\nConsider the language\\nA = {ww : w ∈{0, 1}∗}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p10p1. Then s ∈A\\nand |s| = 2p + 2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Therefore, the string xy2z = xyyz is not contained in A. But,\\nby the pumping lemma, this string is contained in A. This is a contradiction\\nand, therefore, A is not a regular language.\\nYou should convince yourself that by choosing s = 02p (which is a string\\nin A whose length is at least p), we do not obtain a contradiction. The reason\\nis that the string y may have an even length. Thus, 02p is the “wrong” string\\nfor showing that A is not regular. By choosing s = 0p10p1, we do obtain\\na contradiction; thus, this is the “correct” string for showing that A is not\\nregular.\\n72\\nChapter 2.\\nFinite Automata and Regular Languages\\nFourth example\\nConsider the language\\nA = {0m1n : m > n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 0p+11p. Then s ∈A\\nand |s| = 2p + 1 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nSince |xy| ≤p, the string y contains only 0s. Since y ̸= ϵ, y contains at\\nleast one 0. Consider the string xy0z = xz. The number of 1s in this string\\nis equal to p, whereas the number of 0s is at most equal to p. Therefore, the\\nstring xy0z is not contained in A. But, by the pumping lemma, this string\\nis contained in A. This is a contradiction and, therefore, A is not a regular\\nlanguage.\\nFifth example\\nConsider the language\\nA = {1n2 : n ≥0}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Consider the string s = 1p2. Then s ∈A\\nand |s| = p2 ≥p. By the pumping lemma, s can be written as s = xyz,\\nwhere y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nObserve that\\n|s| = |xyz| = p2\\nand\\n|xy2z| = |xyyz| = |xyz| + |y| = p2 + |y|.\\nSince |xy| ≤p, we have |y| ≤p. Since y ̸= ϵ, we have |y| ≥1. It follows that\\np2 < |xy2z| ≤p2 + p < (p + 1)2.\\nHence, the length of the string xy2z is strictly between two consecutive\\nsquares.\\nIt follows that this length is not a square and, therefore, xy2z\\nis not contained in A. But, by the pumping lemma, this string is contained\\nin A. This is a contradiction and, therefore, A is not a regular language.\\n2.9.\\nThe pumping lemma and nonregular languages\\n73\\nSixth example\\nConsider the language\\nA = {1n : n is a prime number}.\\nWe prove by contradiction that A is not a regular language.\\nAssume that A is a regular language. Let p ≥1 be the pumping length,\\nas given by the pumping lemma. Let n ≥p be a prime number, and consider\\nthe string s = 1n. Then s ∈A and |s| = n ≥p. By the pumping lemma, s\\ncan be written as s = xyz, where y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all i ≥0.\\nLet k be the integer such that y = 1k. Since y ̸= ϵ, we have k ≥1. For\\neach i ≥0, n + (i −1)k is a prime number, because xyiz = 1n+(i−1)k ∈A.\\nFor i = n + 1, however, we have\\nn + (i −1)k = n + nk = n(1 + k),\\nwhich is not a prime number, because n ≥2 and 1 + k ≥2.\\nThis is a\\ncontradiction and, therefore, A is not a regular language.\\nSeventh example\\nConsider the language\\nA = {w ∈{0, 1}∗:\\nthe number of occurrences of 01 in w is equal to\\nthe number of occurrences of 10 in w }.\\nSince this language has the same ﬂavor as the one in the second example,\\nwe may suspect that A is not a regular language. This is, however, not true:\\nAs we will show, A is a regular language.\\nThe key property is the following one: Let w be an arbitrary string in\\n{0, 1}∗. Then\\nthe absolute value of the number of occurrences of 01 in w minus\\nthe number of occurrences of 10 in w is at most one.\\nThis property holds, because between any two consecutive occurrences of\\n01, there must be exactly one occurrence of 10. Similarly, between any two\\nconsecutive occurrences of 10, there must be exactly one occurrence of 01.\\nWe will construct a DFA that accepts A. This DFA uses the following\\nﬁve states:\\n74\\nChapter 2.\\nFinite Automata and Regular Languages\\n• q: start state; no symbol has been read.\\n• q01: the last symbol read was 1; in the part of the string read so far, the\\nnumber of occurrences of 01 is one more than the number of occurrences\\nof 10.\\n• q10: the last symbol read was 0; in the part of the string read so far, the\\nnumber of occurrences of 10 is one more than the number of occurrences\\nof 01.\\n• q0\\nequal: the last symbol read was 0; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\n• q1\\nequal: the last symbol read was 1; in the part of the string read so far,\\nthe number of occurrences of 01 is equal to the number of occurrences\\nof 10.\\nThe set of accept states is equal to {q, q0\\nequal, q1\\nequal}. The state diagram of\\nthe DFA is given below.\\nq0\\nequal\\nq1\\nequal\\nq01\\nq10\\nq\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\nIn fact, the key property mentioned above implies that the language A\\nconsists of the empty string ϵ and all non-empty binary strings that start\\n2.9.\\nThe pumping lemma and nonregular languages\\n75\\nand end with the same symbol. As a result, A is the language described by\\nthe regular expression\\nϵ ∪0 ∪1 ∪0(0 ∪1)∗0 ∪1(0 ∪1)∗1.\\nThis gives an alternative proof for the fact that A is a regular language.\\nEighth example\\nConsider the language\\nL = {w ∈{0, 1}∗: w is the binary representation of a prime number}.\\nWe assume that for any positive integer, the leftmost bit in its binary repre-\\nsentation is 1. In other words, we assume that there are no 0’s added to the\\nleft of such a binary representation. Thus,\\nL = {10, 11, 101, 111, 1011, 1101, 10001, . . .}.\\nWe will prove that L is not a regular language.\\nAssume that L is a regular language. Let p ≥1 be the pumping length.\\nLet N > 2p be a prime number and let s ∈{0, 1}∗be the binary representa-\\ntion of N. Observe that |s| ≥p + 1. Also, the leftmost and rightmost bits of\\ns are 1.\\nSince s ∈L and |s| ≥p + 1 ≥p, the Pumping Lemma implies that we\\ncan write s = xyz, such that\\n1. |y| ≥1,\\n2. |xy| ≤p (and, thus, |z| ≥1), and\\n3. for all i ≥0, xyiz ∈L, i.e., xyiz is the binary representation of a prime\\nnumber.\\nDeﬁne A, B, and C to be the integers whose binary representations are\\nx, y, and z, respectively. Note that both y and z may have leading 0’s. In\\nfact, y may be a string consisting of 0’s only, in which case B = 0. However,\\nsince the rightmost bit of z is 1, we have C ≥1. Observe that\\nN = C + B · 2|z| + A · 2|z|+|y|.\\n(2.4)\\n76\\nChapter 2.\\nFinite Automata and Regular Languages\\nLet i = N, consider the bitstring xyiz = xyNz, and let M be the prime\\nnumber whose binary representation is given by this bitstring. Then,\\nM\\n=\\nC +\\nN−1\\nX\\nk=0\\nB · 2|z|+k|y| + A · 2|z|+N|y|\\n=\\nC + B · 2|z|\\nN−1\\nX\\nk=0\\n2k|y| + A · 2|z|+N|y|.\\nLet\\nT =\\nN−1\\nX\\nk=0\\n2k|y|.\\nThen\\n\\x002|y| −1\\n\\x01\\nT = 2N|y| −1.\\n(2.5)\\nBy Fermat’s Little Theorem, we have\\n2N ≡2\\n(mod N),\\nimplying that\\n2N|y| −1 =\\n\\x002N\\x01|y| −1 ≡2|y| −1\\n(mod N).\\nThus, (2.5) implies that\\n\\x002|y| −1\\n\\x01\\nT ≡2|y| −1\\n(mod N).\\n(2.6)\\nObserve that 2|y| ≤2p < N, because |y| ≤|xy| ≤p. Also, 2|y| ≥2, because\\ny ̸= ϵ. It follows that\\n1 ≤2|y| −1 < N,\\nimplying that\\n2|y| −1 ̸≡0\\n(mod N).\\nThis, together with (2.6), implies that\\nT ≡1\\n(mod N).\\nSince\\nM = C + B · 2|z| · T + A · 2|z|+N|y|,\\n2.10.\\nHigman’s Theorem\\n77\\nit follows that\\nM ≡C + B · 2|z| + A · 2|z|+|y|\\n(mod N).\\nThis, together with (2.4), implies that\\nM ≡0\\n(mod N),\\ni.e., N divides M. Since M > N, we conclude that M is not a prime number,\\nwhich is a contradiction. Thus, the language L is not regular.\\n2.10\\nHigman’s Theorem\\nLet Σ be a ﬁnite alphabet. For any two strings x and y in Σ∗, we say that x\\nis a subsequence of y, if x can be obtained by deleting zero or more symbols\\nfrom y. For example, 10110 is a subsequence of 0010010101010001. For any\\nlanguage L ⊆Σ∗, we deﬁne\\nSUBSEQ(L) := {x : there exists a y ∈L such that x is a subsequence of y}.\\nThat is, SUBSEQ(L) is the language consisting of the subsequences of all\\nstrings in L. In 1952, Higman proved the following result:\\nTheorem 2.10.1 (Higman) For any ﬁnite alphabet Σ and for any lan-\\nguage L ⊆Σ∗, the language SUBSEQ(L) is regular.\\n2.10.1\\nDickson’s Theorem\\nOur proof of Higman’s Theorem will use a theorem that was proved in 1913\\nby Dickson.\\nRecall that N denotes the set of positive integers. Let n ∈N. For any\\ntwo points p = (p1, p2, . . . , pn) and q = (q1, q2, . . . , qn) in Nn, we say that p is\\ndominated by q, if pi ≤qi for all i with 1 ≤i ≤n.\\nTheorem 2.10.2 (Dickson) Let S ⊆Nn, and let M be the set consisting of\\nall elements of S that are minimal in the relation “is dominated by”. Thus,\\nM = {q ∈S : there is no p in S \\\\ {q} such that p is dominated by q}.\\nThen, the set M is ﬁnite.\\n78\\nChapter 2.\\nFinite Automata and Regular Languages\\nWe will prove this theorem by induction on the dimension n. If n = 1,\\nthen either M = ∅(if S = ∅) or M consists of exactly one element (if S ̸= ∅).\\nTherefore, the theorem holds if n = 1. Let n ≥2 and assume the theorem\\nholds for all subsets of Nn−1. Let S be a subset of Nn and consider the set\\nM of minimal elements in S. If S = ∅, then M = ∅and, thus, M is ﬁnite.\\nAssume that S ̸= ∅. We ﬁx an arbitrary element q in M. If p ∈M \\\\ {q},\\nthen q is not dominated by p. Therefore, there exists an index i such that\\npi ≤qi −1. It follows that\\nM \\\\ {q} ⊆\\nn[\\ni=1\\n\\x00Ni−1 × [1, qi −1] × Nn−i\\x01\\n.\\nFor all i and k with 1 ≤i ≤n and 1 ≤k ≤qi −1, we deﬁne\\nSik = {p ∈S : pi = k}\\nand\\nMik = {p ∈M : pi = k}.\\nThen,\\nM \\\\ {q} =\\nn[\\ni=1\\nqi−1\\n[\\nk=1\\nMik.\\n(2.7)\\nLemma 2.10.3 Mik is a subset of the set of all elements of Sik that are\\nminimal in the relation “is dominated by”.\\nProof. Let p be an element of Mik, and assume that p is not minimal in\\nSik. Then there is an element r in Sik, such that r ̸= p and r is dominated\\nby p. Since p and r are both elements of S, it follows that p ̸∈M. This is a\\ncontradiction.\\nSince the set Sik is basically a subset of Nn−1, it follows from the induction\\nhypothesis that Sik contains ﬁnitely many minimal elements. This, combined\\nwith Lemma 2.10.3, implies that Mik is a ﬁnite set. Thus, by (2.7), M \\\\ {q}\\nis the union of ﬁnitely many ﬁnite sets. Therefore, the set M is ﬁnite.\\n2.10.2\\nProof of Higman’s Theorem\\nWe give the proof of Theorem 2.10.1 for the case when Σ = {0, 1}. If L = ∅\\nor SUBSEQ(L) = {0, 1}∗, then SUBSEQ(L) is obviously a regular language.\\n2.10.\\nHigman’s Theorem\\n79\\nHence, we may assume that L is non-empty and SUBSEQ(L) is a proper\\nsubset of {0, 1}∗.\\nWe ﬁx a string z of length at least two in the complement SUBSEQ(L) of\\nthe language SUBSEQ(L). Observe that this is possible, because SUBSEQ(L)\\nis an inﬁnite language. We insert 0s and 1s into z, such that, in the result-\\ning string z′, 0s and 1s alternate. For example, if z = 0011101011, then\\nz′ = 01010101010101. Let n = |z′| −1, where |z′| denotes the length of z′.\\nThen, n ≥|z| −1 ≥1.\\nA (0, 1)-alternation in a binary string x is any occurrence of 01 or 10 in x.\\nFor example, the string 1101001 contains four (0, 1)-alternations. We deﬁne\\nA = {x ∈{0, 1}∗: x has at most n many (0, 1)-alternations}.\\nLemma 2.10.4 SUBSEQ(L) ⊆A.\\nProof. Let x ∈SUBSEQ(L) and assume that x ̸∈A. Then, x has at least\\nn + 1 = |z′| many (0, 1)-alternations and, therefore, z′ is a subsequence of x.\\nIn particular, z is a subsequence of x. Since x ∈SUBSEQ(L), it follows that\\nz ∈SUBSEQ(L), which is a contradiction.\\nLemma 2.10.5 SUBSEQ(L) =\\n\\x10\\nA ∩SUBSEQ(L)\\n\\x11\\n∪A.\\nProof. Follows from Lemma 2.10.4.\\nLemma 2.10.6 The language A is regular.\\nProof.\\nThe complement A of A is the language consisting of all binary\\nstrings with at least n + 1 many (0, 1)-alternations. If, for example, n = 3,\\nthen A is described by the regular expression\\n(00∗11∗00∗11∗0(0 ∪1)∗) ∪(11∗00∗11∗00∗1(0 ∪1)∗) .\\nThis should convince you that the claim is true for any value of n.\\nFor any b ∈{0, 1} and for any k ≥0, we deﬁne Abk to be the language\\nconsisting of all binary strings that start with a b and have exactly k many\\n(0, 1)-alternations. Then, we have\\nA = {ϵ} ∪\\n 1[\\nb=0\\nn[\\nk=0\\nAbk\\n!\\n.\\n80\\nChapter 2.\\nFinite Automata and Regular Languages\\nThus, if we deﬁne\\nFbk = Abk ∩SUBSEQ(L),\\nand use the fact that ϵ ∈SUBSEQ(L) (which is true because L ̸= ∅), then\\nA ∩SUBSEQ(L) =\\n1[\\nb=0\\nn[\\nk=0\\nFbk.\\n(2.8)\\nFor any b ∈{0, 1} and for any k ≥0, consider the relation “is a subse-\\nquence of” on the language Fbk. We deﬁne Mbk to be the language consisting\\nof all strings in Fbk that are minimal in this relation. Thus,\\nMbk = {x ∈Fbk : there is no x′ in Fbk \\\\ {x} such that x′ is a subsequence of x}.\\nIt is clear that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Fbk : x is a subsequence of y}.\\nIf x ∈Mbk, y ∈Abk, and x is a subsequence of y, then y must be in\\nSUBSEQ(L) and, therefore, y must be in Fbk. To prove this, assume that\\ny ∈SUBSEQ(L).\\nThen, x ∈SUBSEQ(L), contradicting the fact that\\nx ∈Mbk ⊆Fbk ⊆SUBSEQ(L). It follows that\\nFbk =\\n[\\nx∈Mbk\\n{y ∈Abk : x is a subsequence of y}.\\n(2.9)\\nLemma 2.10.7 Let b ∈{0, 1} and 0 ≤k ≤n, and let x be an element of\\nMbk. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis regular.\\nProof. We will prove the claim by means of an example. Assume that b = 1,\\nk = 3, and x = 11110001000. Then, the language\\n{y ∈Abk : x is a subsequence of y}\\nis described by the regular expression\\n11111∗0000∗11∗0000∗.\\nThis should convince you that the claim is true in general.\\nExercises\\n81\\nLemma 2.10.8 For each b ∈{0, 1} and each 0 ≤k ≤n, the set Mbk is\\nﬁnite.\\nProof. Again, we will prove the claim by means of an example. Assume\\nthat b = 1 and k = 3. Any string in Fbk can be written as 1a0b1c0d, for some\\nintegers a, b, c, d ≥1. Consider the function ϕ : Fbk →N4 that is deﬁned by\\nϕ(1a0b1c0d) = (a, b, c, d). Then, ϕ is an injective function, and the following\\nis true, for any two strings x and x′ in Fbk:\\nx is a subsequence of x′ if and only if ϕ(x) is dominated by ϕ(x′).\\nIt follows that the elements of Mbk are in one-to-one correspondence with\\nthose elements of ϕ(Fbk) that are minimal in the relation “is dominated by”.\\nThe lemma thus follows from Dickson’s Theorem.\\nNow we can complete the proof of Higman’s Theorem:\\n• It follows from (2.9) and Lemmas 2.10.7 and 2.10.8, that Fbk is the\\nunion of ﬁnitely many regular languages. Therefore, by Theorem 2.3.1,\\nFbk is a regular language.\\n• It follows from (2.8) that A∩SUBSEQ(L) is the union of ﬁnitely many\\nregular languages. Therefore, again by Theorem 2.3.1, A∩SUBSEQ(L)\\nis a regular language.\\n• Since A ∩SUBSEQ(L) is regular and, by Lemma 2.10.6, A is regular,\\nit follows from Lemma 2.10.5 that SUBSEQ(L) is the union of two reg-\\nular languages. Therefore, by Theorem 2.3.1, SUBSEQ(L) is a regular\\nlanguage.\\n• Since SUBSEQ(L) is regular, it follows from Theorem 2.6.4 that the\\nlanguage SUBSEQ(L) is regular as well.\\nExercises\\n2.1 For each of the following languages, construct a DFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : the length of w is divisible by three}\\n82\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. {w : 110 is not a substring of w}\\n3. {w : w contains at least ﬁve 1s}\\n4. {w : w contains the substring 1011}\\n5. {w : w contains at least two 1s and at most two 0s}\\n6. {w : w contains an odd number of 1s or exactly two 0s}\\n7. {w : w begins with 1 and ends with 0}\\n8. {w : every odd position in w is 1}\\n9. {w : w has length at least 3 and its third symbol is 0}\\n10. {ϵ, 0}\\n2.2 For each of the following languages, construct an NFA, with the speciﬁed\\nnumber of states, that accepts the language. In all cases, the alphabet is\\n{0, 1}.\\n1. The language {w : w ends with 10} with three states.\\n2. The language {w : w contains the substring 1011} with ﬁve states.\\n3. The language {w : w contains an odd number of 1s or exactly two 0s}\\nwith six states.\\n2.3 For each of the following languages, construct an NFA that accepts the\\nlanguage. In all cases, the alphabet is {0, 1}.\\n1. {w : w contains the substring 11001}\\n2. {w : w has length at least 2 and does not end with 10}\\n3. {w : w begins with 1 or ends with 0}\\n2.4 Convert the following NFA to an equivalent DFA.\\nExercises\\n83\\n1\\n2\\na\\nb\\na, b\\n2.5 Convert the following NFA to an equivalent DFA.\\n1\\n3\\n2\\na\\na\\nb\\na\\nε,b\\n2.6 Convert the following NFA to an equivalent DFA.\\n0\\n1\\n2\\n3\\na, ǫ\\nb\\na\\nǫ\\nb\\n2.7 In the proof of Theorem 2.6.3, we introduced a new start state q0, which\\nis also an accept state. Explain why the following is not a valid proof of\\nTheorem 2.6.3:\\nLet N = (Q1, Σ, δ1, q1, F1) be an NFA, such that A = L(N). Deﬁne the\\nNFA M = (Q1, Σ, δ, q1, F), where\\n84\\nChapter 2.\\nFinite Automata and Regular Languages\\n1. F = {q1} ∪F1.\\n2. δ : Q1 × Σϵ →P(Q1) is deﬁned as follows: For any r ∈Q1 and for any\\na ∈Σϵ,\\nδ(r, a) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nδ1(r, a)\\nif r ∈Q1 and r ̸∈F1,\\nδ1(r, a)\\nif r ∈F1 and a ̸= ϵ,\\nδ1(r, a) ∪{q1}\\nif r ∈F1 and a = ϵ.\\nThen L(M) = A∗.\\n2.8 Prove Theorem 2.6.4.\\n2.9 Let A be a language over the alphabet Σ = {0, 1} and let A be the\\ncomplement of A. Thus, A is the language consisting of all binary strings\\nthat are not in A.\\nAssume that A is a regular language. Let M = (Q, Σ, δ, q, F) be a non-\\ndeterministic ﬁnite automaton (NFA) that accepts A.\\nConsider the NFA N = (Q, Σ, δ, q, F), where F = Q\\\\F is the complement\\nof F. Thus, N is obtained from M by turning all accept states into nonaccept\\nstates, and turning all nonaccept states into accept states.\\n1. Is it true that the language accepted by N is equal to A?\\n2. Assume now that M is a deterministic ﬁnite automaton (DFA) that\\naccepts A. Deﬁne N as above; thus, turn all accept states into nonac-\\ncept states, and turn all nonaccept states into accept states. Is it true\\nthat the language accepted by N is equal to A?\\n2.10 Recall the alternative deﬁnition for the star of a language A that we\\ngave just before Theorem 2.3.1.\\nIn Theorems 2.3.1 and 2.6.2, we have shown that the class of regular\\nlanguages is closed under the union and concatenation operations.\\nSince\\nA∗= S∞\\nk=0 Ak, why doesn’t this imply that the class of regular languages is\\nclosed under the star operation?\\n2.11 Let A and B be two regular languages over the same alphabet Σ. Prove\\nthat the diﬀerence of A and B, i.e., the language\\nA \\\\ B = {w : w ∈A and w ̸∈B}\\nis a regular language.\\nExercises\\n85\\n2.12 For each of the following regular expressions, give two strings that are\\nmembers and two strings that are not members of the language described by\\nthe expression. The alphabet is Σ = {a, b}.\\n1. a(ba)∗b.\\n2. (a ∪b)∗a(a ∪b)∗b(a ∪b)∗a(a ∪b)∗.\\n3. (a ∪ba ∪bb)(a ∪b)∗.\\n2.13 Give regular expressions describing the following languages.\\nIn all\\ncases, the alphabet is {0, 1}.\\n1. {w : w contains at least three 1s}.\\n2. {w : w contains at least two 1s and at most one 0},\\n3. {w : w contains an even number of 0s and exactly two 1s}.\\n4. {w : w contains exactly two 0s and at least two 1s}.\\n5. {w : w contains an even number of 0s and each 0 is followed by at least one 1}.\\n6. {w : every odd position in w is 1}.\\n2.14 Convert each of the following regular expressions to an NFA.\\n1. (0 ∪1)∗000(0 ∪1)∗\\n2. (((10)∗(00)) ∪10)∗\\n3. ((0 ∪1)(11)∗∪0)∗\\n2.15 Convert the following DFA to a regular expression.\\n86\\nChapter 2.\\nFinite Automata and Regular Languages\\n1\\n2\\n3\\na\\na\\nb\\nb\\na\\nb\\n2.16 Convert the following DFA to a regular expression.\\n1\\n2\\n3\\na, b\\na\\na\\nb\\nb\\n2.17 Convert the following DFA to a regular expression.\\na, b\\n2.18\\n1. Let A be a non-empty regular language. Prove that there exists\\nan NFA that accepts A and that has exactly one accept state.\\nExercises\\n87\\n2. For any string w = w1w2 . . . wn, we denote by wR the string obtained\\nby reading w backwards, i.e., wR = wnwn−1 . . . w2w1. For any language\\nA, we deﬁne AR to be the language obtained by reading all strings in\\nA backwards, i.e.,\\nAR = {wR : w ∈A}.\\nLet A be a non-empty regular language. Prove that the language AR\\nis also regular.\\n2.19 If n ≥1 is an integer and w = a1a2 . . . an is a string, then for any i\\nwith 0 ≤i < n, the string a1a2 . . . ai is called a proper preﬁx of w. (If i = 0,\\nthen a1a2 . . . ai = ϵ.)\\nFor any language L, we deﬁne MIN(L) to be the language\\nMIN(L) = {w ∈L : no proper preﬁx of w belongs to L}.\\nProve the following claim: If L is a regular language, then MIN(L) is regular\\nas well.\\n2.20 Use the pumping lemma to prove that the following languages are not\\nregular.\\n1. {anbmcn+m : n ≥0, m ≥0}.\\n2. {anbnc2n : n ≥0}.\\n3. {anbman : n ≥0, m ≥0}.\\n4. {a2n : n ≥0}. (Remark: a2n is the string consisting of 2n many a’s.)\\n5. {anbmck : n ≥0, m ≥0, k ≥0, n2 + m2 = k2}.\\n6. {uvu : u ∈{a, b}∗, u ̸= ϵ, v ∈{a, b}∗}.\\n2.21 Prove that the language\\n{ambn : m ≥0, n ≥0, m ̸= n}\\nis not regular. (Using the pumping lemma for this one is a bit tricky. You\\ncan avoid using the pumping lemma by combining results about the closure\\nunder regular operations.)\\n88\\nChapter 2.\\nFinite Automata and Regular Languages\\n2.22\\n1. Give an example of a regular language A and a non-regular lan-\\nguage B for which A ⊆B.\\n2. Give an example of a non-regular language A and a regular language\\nB for which A ⊆B.\\n2.23 Let A be a language consisting of ﬁnitely many strings.\\n1. Prove that A is a regular language.\\n2. Let n be the maximum length of any string in A. Prove that every\\ndeterministic ﬁnite automaton (DFA) that accepts A has at least n+1\\nstates. (Hint: How is the pumping length chosen in the proof of the\\npumping lemma?)\\n2.24 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L ̸= ∅if and only\\nif L contains a string of length less than p.\\n2.25 Let L be a regular language, let M be a DFA whose language is equal\\nto L, and let p be the number of states of M. Prove that L is an inﬁnite\\nlanguage if and only if L contains a string w with p ≤|w| ≤2p −1.\\n2.26 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. We deﬁne a binary relation RL on Σ∗× Σ∗, in the following way:\\nFor any two strings u and u′ in Σ∗,\\nuRLu′ if and only if (∀v ∈Σ∗: uv ∈L ⇔u′v ∈L) .\\nProve that RL is an equivalence relation.\\n2.27 Let Σ = {0, 1}, let\\nL = {w ∈Σ∗: |w| is odd},\\nand consider the relation RL deﬁned in Exercise 2.26.\\n1. Prove that for any two strings u and u′ in Σ∗,\\nuRLu′ ⇔|u| −|u′| is even.\\nExercises\\n89\\n2. Determine all equivalence classes of the relation RL.\\n2.28 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,\\nL ⊆Σ∗. Recall the equivalence relation RL that was deﬁned in Exercise 2.26.\\n1. Assume that L is a regular language, and let M = (Q, Σ, δ, q0, F) be\\na DFA that accepts L. Let u and u′ be strings in Σ∗. Let q be the\\nstate reached, when following the path in the state diagram of M, that\\nstarts in q0 and that is obtained by reading the string u. Similarly, let\\nq′ be the state reached, when following the path in the state diagram\\nof M, that starts in q0 and that is obtained by reading the string u′.\\nProve the following: If q = q′, then uRLu′.\\n2. Prove the following claim: If L is a regular language, then the equiva-\\nlence relation RL has a ﬁnite number of equivalence classes.\\n2.29 Let L be the language deﬁned by\\nL = {uuR : u ∈{0, 1}∗}.\\nIn words, a string is in L if and only if its length is even, and the second half\\nis the reverse of the ﬁrst half. Consider the equivalence relation RL that was\\ndeﬁned in Exercise 2.26.\\n1. Let m and n be two distinct positive integers and consider the two\\nstrings u = 0m1 and u′ = 0n1. Prove that ¬(uRLu′).\\n2. Prove that L is not a regular language, without using the pumping\\nlemma.\\n3. Use the pumping lemma to prove that L is not a regular language.\\n2.30 In this exercise, we will show that the converse of the pumping lemma\\ndoes, in general, not hold. Consider the language\\nA = {ambncn : m ≥1, n ≥0} ∪{bnck : n ≥0, k ≥0}.\\n1. Show that A satisﬁes the conclusion of the pumping lemma for p = 1.\\nThus, show that every string s in A whose length is at least p can be\\nwritten as s = xyz, such that y ̸= ϵ, |xy| ≤p, and xyiz ∈A for all\\ni ≥0.\\n90\\nChapter 2.\\nFinite Automata and Regular Languages\\n2. Consider the equivalence relation RA that was deﬁned in Exercise 2.26.\\nLet n and n′ be two distinct non-negative integers and consider the two\\nstrings u = abn and u′ = abn′. Prove that ¬(uRAu′).\\n3. Prove that A is not a regular language.\\nChapter 3\\nContext-Free Languages\\nIn this chapter, we introduce the class of context-free languages.\\nAs we\\nwill see, this class contains all regular languages, as well as some nonregular\\nlanguages such as {0n1n : n ≥0}.\\nThe class of context-free languages consists of languages that have some\\nsort of recursive structure. We will see two equivalent methods to obtain this\\nclass. We start with context-free grammars, which are used for deﬁning the\\nsyntax of programming languages and their compilation. Then we introduce\\nthe notion of (nondeterministic) pushdown automata, and show that these\\nautomata have the same power as context-free grammars.\\n3.1\\nContext-free grammars\\nWe start with an example. Consider the following ﬁve (substitution) rules:\\nS\\n→\\nAB\\nA\\n→\\na\\nA\\n→\\naA\\nB\\n→\\nb\\nB\\n→\\nbB\\nHere, S, A, and B are variables, S is the start variable, and a and b are\\nterminals. We use these rules to derive strings consisting of terminals (i.e.,\\nelements of {a, b}∗), in the following manner:\\n1. Initialize the current string to be the string consisting of the start\\nvariable S.\\n92\\nChapter 3.\\nContext-Free Languages\\n2. Take any variable in the current string and take any rule that has this\\nvariable on the left-hand side. Then, in the current string, replace this\\nvariable by the right-hand side of the rule.\\n3. Repeat 2. until the current string only contains terminals.\\nFor example, the string aaaabb can be derived in the following way:\\nS\\n⇒\\nAB\\n⇒\\naAB\\n⇒\\naAbB\\n⇒\\naaAbB\\n⇒\\naaaAbB\\n⇒\\naaaabB\\n⇒\\naaaabb\\nThis derivation can also be represented using a parse tree, as in the ﬁgure\\nbelow:\\nS\\nA\\nA\\nA\\nA\\na\\na\\na\\na\\nb\\nb\\nB\\nB\\nThe ﬁve rules in this example constitute a context-free grammar. The\\nlanguage of this grammar is the set of all strings that\\n3.1.\\nContext-free grammars\\n93\\n• can be derived from the start variable and\\n• only contain terminals.\\nFor this example, the language is\\n{ambn : m ≥1, n ≥1},\\nbecause every string of the form ambn, for some m ≥1 and n ≥1, can be\\nderived from the start variable, whereas no other string over the alphabet\\n{a, b} can be derived from the start variable.\\nDeﬁnition 3.1.1 A context-free grammar is a 4-tuple G = (V, Σ, R, S),\\nwhere\\n1. V is a ﬁnite set, whose elements are called variables,\\n2. Σ is a ﬁnite set, whose elements are called terminals,\\n3. V ∩Σ = ∅,\\n4. S is an element of V ; it is called the start variable,\\n5. R is a ﬁnite set, whose elements are called rules. Each rule has the\\nform A →w, where A ∈V and w ∈(V ∪Σ)∗.\\nIn our example, we have V = {S, A, B}, Σ = {a, b}, and\\nR = {S →AB, A →a, A →aA, B →b, B →bB}.\\nDeﬁnition 3.1.2 Let G = (V, Σ, R, S) be a context-free grammar. Let A be\\nan element in V and let u, v, and w be strings in (V ∪Σ)∗such that A →w\\nis a rule in R. We say that the string uwv can be derived in one step from\\nthe string uAv, and write this as\\nuAv ⇒uwv.\\nIn other words, by applying the rule A →w to the string uAv, we obtain\\nthe string uwv. In our example, we see that aaAbb ⇒aaaAbb.\\nDeﬁnition 3.1.3 Let G = (V, Σ, R, S) be a context-free grammar. Let u\\nand v be strings in (V ∪Σ)∗. We say that v can be derived from u, and write\\nthis as u\\n∗⇒v, if one of the following two conditions holds:\\n94\\nChapter 3.\\nContext-Free Languages\\n1. u = v or\\n2. there exist an integer k ≥2 and a sequence u1, u2, . . . , uk of strings in\\n(V ∪Σ)∗, such that\\n(a) u = u1,\\n(b) v = uk, and\\n(c) u1 ⇒u2 ⇒. . . ⇒uk.\\nIn other words, by starting with the string u and applying rules zero or\\nmore times, we obtain the string v. In our example, we see that aaAbB\\n∗⇒\\naaaabbbB.\\nDeﬁnition 3.1.4 Let G = (V, Σ, R, S) be a context-free grammar.\\nThe\\nlanguage of G is deﬁned to be the set of all strings in Σ∗that can be derived\\nfrom the start variable S:\\nL(G) = {w ∈Σ∗: S\\n∗⇒w}.\\nDeﬁnition 3.1.5 A language L is called context-free, if there exists a context-\\nfree grammar G such that L(G) = L.\\n3.2\\nExamples of context-free grammars\\n3.2.1\\nProperly nested parentheses\\nConsider the context-free grammar G = (V, Σ, R, S), where V = {S}, Σ =\\n{a, b}, and\\nR = {S →ϵ, S →aSb, S →SS}.\\nWe write the three rules in R as\\nS →ϵ|aSb|SS,\\nwhere you can think of “|” as being a short-hand for “or”.\\n3.2.\\nExamples of context-free grammars\\n95\\nBy applying the rules in R, starting with the start variable S, we obtain,\\nfor example,\\nS\\n⇒\\nSS\\n⇒\\naSbS\\n⇒\\naSbSS\\n⇒\\naSSbSS\\n⇒\\naaSbSbSS\\n⇒\\naabSbSS\\n⇒\\naabbSS\\n⇒\\naabbaSbS\\n⇒\\naabbabS\\n⇒\\naabbabaSb\\n⇒\\naabbabab\\nWhat is the language L(G) of this context-free grammar G? If we think\\nof a as being a left-parenthesis “(”, and of b as being a right-parenthesis “)”,\\nthen L(G) is the language consisting of all strings of properly nested paren-\\ntheses. Here is the explanation: Any string of properly nested parentheses is\\neither\\n• empty (which we derive from S by the rule S →ϵ),\\n• consists of a left-parenthesis, followed by an arbitrary string of properly\\nnested parentheses, followed by a right-parenthesis (these are derived\\nfrom S by ﬁrst applying the rule S →aSb), or\\n• consists of an arbitrary string of properly nested parentheses, followed\\nby an arbitrary string of properly nested parentheses (these are derived\\nfrom S by ﬁrst applying the rule S →SS).\\n3.2.2\\nA context-free grammar for a nonregular lan-\\nguage\\nConsider the language L1 = {0n1n : n ≥0}. We have seen in Section 2.9.1\\nthat L1 is not a regular language. We claim that L1 is a context-free language.\\n96\\nChapter 3.\\nContext-Free Languages\\nIn order to prove this claim, we have to construct a context-free grammar\\nG1 such that L(G1) = L1.\\nObserve that any string in L1 is either\\n• empty or\\n• consists of a 0, followed by an arbitrary string in L1, followed by a 1.\\nThis leads to the context-free grammar G1 = (V1, Σ, R1, S1), where V1 =\\n{S1}, Σ = {0, 1}, and R1 consists of the rules\\nS1 →ϵ|0S11.\\nHence, R1 = {S1 →ϵ, S1 →0S11}.\\nTo derive the string 0n1n from the start variable S1, we do the following:\\n• Starting with S1, apply the rule S1 →0S11 exactly n times. This gives\\nthe string 0nS11n.\\n• Apply the rule S1 →ϵ. This gives the string 0n1n.\\nIt is not diﬃcult to see that these are the only strings that can be derived\\nfrom the start variable S1. Thus, L(G1) = L1.\\nIn a symmetric way, we see that the context-free grammar G2 = (V2, Σ, R2, S2),\\nwhere V2 = {S2}, Σ = {0, 1}, and R2 consists of the rules\\nS2 →ϵ|1S20,\\nhas the property that L(G2) = L2, where L2 = {1n0n : n ≥0}. Thus, L2 is\\na context-free language.\\nDeﬁne L = L1 ∪L2, i.e.,\\nL = {0n1n : n ≥0} ∪{1n0n : n ≥0}.\\nThe context-free grammar G = (V, Σ, R, S), where V = {S, S1, S2}, Σ =\\n{0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2\\nS1\\n→\\nϵ|0S11\\nS2\\n→\\nϵ|1S20,\\nhas the property that L(G) = L. Hence, L is a context-free language.\\n3.2.\\nExamples of context-free grammars\\n97\\n3.2.3\\nA context-free grammar for the complement of\\na nonregular language\\nLet L be the (nonregular) language L = {0n1n : n ≥0}. We want to prove\\nthat the complement L of L is a context-free language. Hence, we want to\\nconstruct a context-free grammar G whose language is equal to L. Observe\\nthat a binary string w is in L if and only if\\n1. w = 0m1n, for some integers m and n with 0 ≤m < n, or\\n2. w = 0m1n, for some integers m and n with 0 ≤n < m, or\\n3. w contains 10 as a substring.\\nThus, we can write L as the union of the languages of all strings of type 1.,\\ntype 2., and type 3.\\nAny string of type 1. is either\\n• the string 1,\\n• consists of a string of type 1., followed by one 1, or\\n• consists of one 0, followed by an arbitrary string of type 1., followed by\\none 1.\\nThus, using the rules\\nS1 →1|S11|0S11,\\nwe can derive, from S1, all strings of type 1.\\nSimilarly, using the rules\\nS2 →0|0S2|0S21,\\nwe can derive, from S2, all strings of type 2.\\nAny string of type 3.\\n• consists of an arbitrary binary string, followed by the string 10, followed\\nby an arbitrary binary string.\\nUsing the rules\\nX →ϵ|0X|1X,\\n98\\nChapter 3.\\nContext-Free Languages\\nwe can derive, from X, all binary strings. Thus, by combining these with\\nthe rule\\nS3 →X10X,\\nwe can derive, from S3, all strings of type 3.\\nWe arrive at the context-free grammar G = (V, Σ, R, S), where V =\\n{S, S1, S2, S3, X}, Σ = {0, 1}, and R consists of the rules\\nS\\n→\\nS1|S2|S3\\nS1\\n→\\n1|S11|0S11\\nS2\\n→\\n0|0S2|0S21\\nS3\\n→\\nX10X\\nX\\n→\\nϵ|0X|1X\\nTo summarize, we have\\nS1\\n∗⇒0m1n, for all integers m and n with 0 ≤m < n,\\nS2\\n∗⇒0m1n, for all integers m and n with 0 ≤n < m,\\nX\\n∗⇒u, for each string u in {0, 1}∗,\\nand\\nS3\\n∗⇒w, for every binary string w that contains 10 as a substring.\\nFrom these observations, it follows that that L(G) = L.\\n3.2.4\\nA context-free grammar that veriﬁes addition\\nConsider the language\\nL = {anbmcn+m : n ≥0, m ≥0}.\\nUsing the pumping lemma for regular languages (Theorem 2.9.1), it can\\nbe shown that L is not a regular language. We will construct a context-\\nfree grammar G whose language is equal to L, thereby proving that L is a\\ncontext-free language.\\nFirst observe that ϵ ∈L. Therefore, we will take S →ϵ to be one of the\\nrules in the grammar.\\nLet us see how we can derive all strings in L from the start variable S:\\n3.3.\\nRegular languages are context-free\\n99\\n1. Every time we add an a, we also add a c. In this way, we obtain all\\nstrings of the form ancn, where n ≥0.\\n2. Given a string of the form ancn, we start adding bs. Every time we add\\na b, we also add a c. Observe that every b has to be added between\\nthe as and the cs. Therefore, we use a variable B as a “pointer” to\\nthe position in the current string where a b can be added: Instead of\\nderiving ancn from S, we derive the string anBcn. Then, from B, we\\nderive all strings of the form bmcm, where m ≥0.\\nWe obtain the context-free grammar G = (V, Σ, R, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R consists of the rules\\nS\\n→\\nϵ|A\\nA\\n→\\nϵ|aAc|B\\nB\\n→\\nϵ|bBc\\nThe facts that\\n• A\\n∗⇒anBcn, for every n ≥0,\\n• B\\n∗⇒bmcm, for every m ≥0,\\nimply that the following strings can be derived from the start variable S:\\n• S\\n∗⇒anBcn\\n∗⇒anbmcmcn = anbmcn+m, for all n ≥0 and m ≥0.\\nIn fact, no other strings in {a, b, c}∗can be derived from S. Therefore, we\\nhave L(G) = L. Since\\nS ⇒A ⇒B ⇒ϵ,\\nwe can simplify this grammar G, by eliminating the rules S →ϵ and A →ϵ.\\nThis gives the context-free grammar G′ = (V, Σ, R′, S), where V = {S, A, B},\\nΣ = {a, b, c}, and R′ consists of the rules\\nS\\n→\\nA\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\nFinally, observe that we do not need S; instead, we can use A as start\\nvariable. This gives our ﬁnal context-free grammar G′′ = (V, Σ, R′′, A), where\\nV = {A, B}, Σ = {a, b, c}, and R′′ consists of the rules\\nA\\n→\\naAc|B\\nB\\n→\\nϵ|bBc\\n100\\nChapter 3.\\nContext-Free Languages\\n3.3\\nRegular languages are context-free\\nWe mentioned already that the class of context-free languages includes the\\nclass of regular languages. In this section, we will prove this claim.\\nTheorem 3.3.1 Let Σ be an alphabet and let L ⊆Σ∗be a regular language.\\nThen L is a context-free language.\\nProof.\\nSince L is a regular language, there exists a deterministic ﬁnite\\nautomaton M = (Q, Σ, δ, q, F) that accepts L.\\nTo prove that L is context-free, we have to deﬁne a context-free grammar\\nG = (V, Σ, R, S), such that L = L(M) = L(G). Thus, G must have the\\nfollowing property: For every string w ∈Σ∗,\\nw ∈L(M) if and only if w ∈L(G),\\nwhich can be reformulated as\\nM accepts w if and only if S\\n∗⇒w.\\nWe will deﬁne the context-free grammar G in such a way that the following\\ncorrespondence holds for any string w = w1w2 . . . wn:\\n• Assume that M is in state A just after it has read the substring\\nw1w2 . . . wi.\\n• Then in the context-free grammar G, we have S\\n∗⇒w1w2 . . . wiA.\\nIn the next step, M reads the symbol wi+1 and switches from state A to,\\nsay, state B; thus, δ(A, wi+1) = B. In order to guarantee that the above\\ncorrespondence still holds, we have to add the rule A →wi+1B to G.\\nConsider the moment when M has read the entire string w. Let A be the\\nstate M is in at that moment. By the above correspondence, we have\\nS\\n∗⇒w1w2 . . . wnA = wA.\\nRecall that G must have the property that\\nM accepts w if and only if S\\n∗⇒w,\\nwhich is equivalent to\\nA ∈F if and only if S\\n∗⇒w.\\n3.3.\\nRegular languages are context-free\\n101\\nWe guarantee this property by adding to G the rule A →ϵ for every accept\\nstate A of M.\\nWe are now ready to give the formal deﬁnition of the context-free gram-\\nmar G = (V, Σ, R, S):\\n• V = Q, i.e., the variables of G are the states of M.\\n• S = q, i.e., the start variable of G is the start state of M.\\n• R consists of the rules\\nA →aB, where A ∈Q, a ∈Σ, B ∈Q, and δ(A, a) = B,\\nand\\nA →ϵ, where A ∈F.\\nIn words,\\n• every transition δ(A, a) = B of M (i.e., when M is in the state A and\\nreads the symbol a, it switches to the state B) corresponds to a rule\\nA →aB in the grammar G,\\n• every accept state A of M corresponds to a rule A →ϵ in the grammar\\nG.\\nWe claim that L(G) = L. In order to prove this, we have to show that\\nL(G) ⊆L and L ⊆L(G).\\nWe prove that L ⊆L(G). Let w = w1w2 . . . wn be an arbitrary string\\nin L. When the ﬁnite automaton M reads the string w, it visits the states\\nr0, r1, . . . , rn, where\\n• r0 = q, and\\n• ri+1 = δ(ri, wi+1) for i = 0, 1, . . . , n −1.\\nSince w ∈L = L(M), we know that rn ∈F.\\nIt follows from the way we deﬁned the grammar G that\\n• for each i = 0, 1, . . . , n −1, ri →wi+1ri+1 is a rule in R, and\\n• rn →ϵ is a rule in R.\\n102\\nChapter 3.\\nContext-Free Languages\\nTherefore, we have\\nS = q = r0 ⇒w1r1 ⇒w1w2r2 ⇒. . . ⇒w1w2 . . . wnrn ⇒w1w2 . . . wn = w.\\nThis proves that w ∈L(G).\\nThe proof of the claim that L(G) ⊆L is left as an exercise.\\nIn Sections 2.9.1 and 3.2.2, we have seen that the language {0n1n : n ≥\\n0} is not regular, but context-free. Therefore, the class of all context-free\\nlanguages properly contains the class of regular languages.\\n3.3.1\\nAn example\\nLet L be the language deﬁned as\\nL = {w ∈{0, 1}∗: 101 is a substring of w}.\\nIn Section 2.2.2, we have seen that L is a regular language. In that section,\\nwe constructed the following deterministic ﬁnite automaton M that accepts\\nL (we have renamed the states):\\n0\\n1\\n1\\n0\\n0\\n1\\n0,1\\nS\\nA\\nB\\nC\\nWe apply the construction given in the proof of Theorem 3.3.1 to convert\\nM to a context-free grammar G whose language is equal to L. According\\nto this construction, we have G = (V, Σ, R, S), where V = {S, A, B, C},\\nΣ = {0, 1}, the start variable S is the start state of M, and R consists of the\\nrules\\nS\\n→\\n0S|1A\\nA\\n→\\n0B|1A\\nB\\n→\\n0S|1C\\nC\\n→\\n0C|1C|ϵ\\n3.4.\\nChomsky normal form\\n103\\nConsider the string 010011011, which is an element of L. When the ﬁnite\\nautomaton M reads this string, it visits the states\\nS, S, A, B, S, A, A, B, C, C.\\nIn the grammar G, this corresponds to the derivation\\nS\\n⇒\\n0S\\n⇒\\n01A\\n⇒\\n010B\\n⇒\\n0100S\\n⇒\\n01001A\\n⇒\\n010011A\\n⇒\\n0100110B\\n⇒\\n01001101C\\n⇒\\n010011011C\\n⇒\\n010011011.\\nHence,\\nS\\n∗⇒010011011,\\nimplying that the string 010011011 is in the language L(G) of the context-free\\ngrammar G.\\nThe string 10011 is not in the language L. When the ﬁnite automaton\\nM reads this string, it visits the states\\nS, A, B, S, A, A,\\ni.e., after the string has been read, M is in the non-accept state A. In the\\ngrammar G, reading the string 10011 corresponds to the derivation\\nS\\n⇒\\n1A\\n⇒\\n10B\\n⇒\\n100S\\n⇒\\n1001A\\n⇒\\n10011A.\\nSince A is not an accept state in M, the grammar G does not contain the\\nrule A →ϵ. This implies that the string 10011 cannot be derived from the\\nstart variable S. Thus, 10011 is not in the language L(G) of G.\\n104\\nChapter 3.\\nContext-Free Languages\\n3.4\\nChomsky normal form\\nThe rules in a context-free grammar G = (V, Σ, R, S) are of the form\\nA →w,\\nwhere A is a variable and w is a string over the alphabet V ∪Σ. In this\\nsection, we show that every context-free grammar G can be converted to a\\ncontext-free grammar G′, such that L(G) = L(G′), and the rules of G′ are of\\na restricted form, as speciﬁed in the following deﬁnition:\\nDeﬁnition 3.4.1 A context-free grammar G = (V, Σ, R, S) is said to be in\\nChomsky normal form, if every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are elements of V , B ̸= S, and C ̸= S.\\n2. A →a, where A is an element of V and a is an element of Σ.\\n3. S →ϵ, where S is the start variable.\\nYou should convince yourself that, for such a grammar, R contains the\\nrule S →ϵ if and only if ϵ ∈L(G).\\nTheorem 3.4.2 Let Σ be an alphabet and let L ⊆Σ∗be a context-free lan-\\nguage. There exists a context-free grammar in Chomsky normal form, whose\\nlanguage is L.\\nProof. Since L is a context-free language, there exists a context-free gram-\\nmar G = (V, Σ, R, S), such that L(G) = L. We will transform G into a\\ngrammar that is in Chomsky normal form and whose language is equal to\\nL(G). The transformation consists of ﬁve steps.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe deﬁne G1 = (V1, Σ, R1, S1), where S1 is the start variable (which is a\\nnew variable), V1 = V ∪{S1}, and R1 = R ∪{S1 →S}. This grammar has\\nthe property that\\n• the start variable S1 does not occur on the right-hand side of any rule\\nin R1, and\\n• L(G1) = L(G).\\n3.4.\\nChomsky normal form\\n105\\nStep 2: An ϵ-rule is a rule that is of the form A →ϵ, where A is a variable\\nthat is not equal to the start variable. In the second step, we eliminate all\\nϵ-rules from G1.\\nWe consider all ϵ-rules, one after another. Let A →ϵ be one such rule,\\nwhere A ∈V1 and A ̸= S1. We modify G1 as follows:\\n1. Remove the rule A →ϵ from the current set R1.\\n2. For each rule in the current set R1 that is of the form\\n(a) B →A, add the rule B →ϵ to R1, unless this rule has already\\nbeen deleted from R1; observe that in this way, we replace the two-\\nstep derivation B ⇒A ⇒ϵ by the one-step derivation B ⇒ϵ;\\n(b) B →uAv (where u and v are strings that are not both empty),\\nadd the rule B →uv to R1; observe that in this way, we replace\\nthe two-step derivation B ⇒uAv ⇒uv by the one-step derivation\\nB ⇒uv;\\n(c) B →uAvAw (where u, v, and w are strings), add the rules B →\\nuvw, B →uAvw, and B →uvAw to R1; if u = v = w = ϵ and\\nthe rule B →ϵ has already been deleted from R1, then we do not\\nadd the rule B →ϵ;\\n(d) treat rules in which A occurs more than twice on the right-hand\\nside in a similar fashion.\\nWe repeat this process until all ϵ-rules have been eliminated.\\nLet R2\\nbe the set of rules, after all ϵ-rules have been eliminated. We deﬁne G2 =\\n(V2, Σ, R2, S2), where V2 = V1 and S2 = S1. This grammar has the property\\nthat\\n• the start variable S2 does not occur on the right-hand side of any rule\\nin R2,\\n• R2 does not contain any ϵ-rule (it may contain the rule S2 →ϵ), and\\n• L(G2) = L(G1) = L(G).\\nStep 3: A unit-rule is a rule that is of the form A →B, where A and B are\\nvariables. In the third step, we eliminate all unit-rules from G2.\\n106\\nChapter 3.\\nContext-Free Languages\\nWe consider all unit-rules, one after another. Let A →B be one such\\nrule, where A and B are elements of V2. We know that B ̸= S2. We modify\\nG2 as follows:\\n1. Remove the rule A →B from the current set R2.\\n2. For each rule in the current set R2 that is of the form B →u, where\\nu ∈(V2 ∪Σ)∗, add the rule A →u to the current set R2, unless this is\\na unit-rule that has already been eliminated.\\nObserve that in this way, we replace the two-step derivation A ⇒B ⇒\\nu by the one-step derivation A ⇒u.\\nWe repeat this process until all unit-rules have been eliminated.\\nLet\\nR3 be the set of rules, after all unit-rules have been eliminated. We deﬁne\\nG3 = (V3, Σ, R3, S3), where V3 = V2 and S3 = S2. This grammar has the\\nproperty that\\n• the start variable S3 does not occur on the right-hand side of any rule\\nin R3,\\n• R3 does not contain any ϵ-rule (it may contain the rule S3 →ϵ),\\n• R3 does not contain any unit-rule, and\\n• L(G3) = L(G2) = L(G1) = L(G).\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside.\\nFor each rule in the current set R3 that is of the form A →u1u2 . . . uk,\\nwhere k ≥3 and each ui is an element of V3 ∪Σ, we modify G3 as follows:\\n1. Remove the rule A →u1u2 . . . uk from the current set R3.\\n2. Add the following rules to the current set R3:\\nA\\n→\\nu1A1\\nA1\\n→\\nu2A2\\nA2\\n→\\nu3A3\\n...\\nAk−3\\n→\\nuk−2Ak−2\\nAk−2\\n→\\nuk−1uk\\n3.4.\\nChomsky normal form\\n107\\nwhere A1, A2, . . . , Ak−2 are new variables that are added to the current\\nset V3.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 . . . uk by the (k −1)-step derivation\\nA ⇒u1A1 ⇒u1u2A2 ⇒. . . ⇒u1u2 . . . uk−2Ak−2 ⇒u1u2 . . . uk.\\nLet R4 be the set of rules, and let V4 be the set of variables, after all rules\\nwith more than two symbols on the right-hand side have been eliminated. We\\ndeﬁne G4 = (V4, Σ, R4, S4), where S4 = S3. This grammar has the property\\nthat\\n• the start variable S4 does not occur on the right-hand side of any rule\\nin R4,\\n• R4 does not contain any ϵ-rule (it may contain the rule S4 →ϵ),\\n• R4 does not contain any unit-rule,\\n• R4 does not contain any rule with more than two symbols on the right-\\nhand side, and\\n• L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nStep 5: Eliminate all rules of the form A →u1u2, where u1 and u2 are not\\nboth variables.\\nFor each rule in the current set R4 that is of the form A →u1u2, where\\nu1 and u2 are elements of V4 ∪Σ, but u1 and u2 are not both contained in\\nV4, we modify G3 as follows:\\n1. If u1 ∈Σ and u2 ∈V4, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →U1u2 and U1 →u1, where U1 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒U1u2 ⇒u1u2.\\n2. If u1 ∈V4 and u2 ∈Σ, then replace the rule A →u1u2 in the current\\nset R4 by the two rules A →u1U2 and U2 →u2, where U2 is a new\\nvariable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the two-step derivation A ⇒u1U2 ⇒u1u2.\\n108\\nChapter 3.\\nContext-Free Languages\\n3. If u1 ∈Σ, u2 ∈Σ, and u1 ̸= u2, then replace the rule A →u1u2 in the\\ncurrent set R4 by the three rules A →U1U2, U1 →u1, and U2 →u2,\\nwhere U1 and U2 are new variables that are added to the current set\\nV4.\\nObserve that in this way, we replace the one-step derivation A ⇒u1u2\\nby the three-step derivation A ⇒U1U2 ⇒u1U2 ⇒u1u2.\\n4. If u1 ∈Σ, u2 ∈Σ, and u1 = u2, then replace the rule A →u1u2 = u1u1\\nin the current set R4 by the two rules A →U1U1 and U1 →u1, where\\nU1 is a new variable that is added to the current set V4.\\nObserve that in this way, we replace the one-step derivation A ⇒\\nu1u2 = u1u1 by the three-step derivation A ⇒U1U1 ⇒u1U1 ⇒u1u1.\\nLet R5 be the set of rules, and let V5 be the set of variables, after Step 5\\nhas been completed. We deﬁne G5 = (V5, Σ, R5, S5), where S5 = S4. This\\ngrammar has the property that\\n• the start variable S5 does not occur on the right-hand side of any rule\\nin R5,\\n• R5 does not contain any ϵ-rule (it may contain the rule S5 →ϵ),\\n• R5 does not contain any unit-rule,\\n• R5 does not contain any rule with more than two symbols on the right-\\nhand side,\\n• R5 does not contain any rule of the form A →u1u2, where u1 and u2\\nare not both variables of V5, and\\n• L(G5) = L(G4) = L(G3) = L(G2) = L(G1) = L(G).\\nSince the grammar G5 is in Chomsky normal form, the proof is complete.\\n3.4.\\nChomsky normal form\\n109\\n3.4.1\\nAn example\\nConsider the context-free grammar G = (V, Σ, R, A), where V = {A, B},\\nΣ = {0, 1}, A is the start variable, and R consists of the rules\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nWe apply the construction given in the proof of Theorem 3.4.2 to convert\\nthis grammar to a context-free grammar in Chomsky normal form whose\\nlanguage is the same as that of G. Throughout the construction, upper case\\nletters will denote variables.\\nStep 1: Eliminate the start variable from the right-hand side of the rules.\\nWe introduce a new start variable S, and add the rule S →A. This gives\\nthe following grammar:\\nS\\n→\\nA\\nA\\n→\\nBAB|B|ϵ\\nB\\n→\\n00|ϵ\\nStep 2: Eliminate all ϵ-rules.\\nWe take the ϵ-rule A →ϵ, and remove it. Then we consider all rules that\\ncontain A on the right-hand side. There are two such rules:\\n• S →A; we add the rule S →ϵ;\\n• A →BAB; we add the rule A →BB.\\nThis gives the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB\\nB\\n→\\n00|ϵ\\nWe take the ϵ-rule B →ϵ, and remove it. Then we consider all rules that\\ncontain B on the right-hand side. There are three such rules:\\n• A →BAB; we add the rules A →AB, A →BA, and A →A;\\n• A →B; we do not add the rule A →ϵ, because it has already been\\nremoved;\\n110\\nChapter 3.\\nContext-Free Languages\\n• A →BB; we add the rule A →B, but not the rule A →ϵ (because it\\nhas already been removed).\\nAt this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA|A\\nB\\n→\\n00\\nSince all ϵ-rules have been eliminated, this completes Step 2. (Observe that\\nthe rule S →ϵ is allowed, because S is the start variable.)\\nStep 3: Eliminate all unit-rules.\\nWe take the unit-rule A →A. We can remove this rule, without adding\\nany new rule. At this moment, we have the following grammar:\\nS\\n→\\nA|ϵ\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →A, remove it, and add the rules\\nS →BAB|B|BB|AB|BA.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BAB|B|BB|AB|BA\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule S →B, remove it, and add the rule S →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|B|BB|AB|BA\\nB\\n→\\n00\\nWe take the unit-rule A →B, remove it, and add the rule A →00. This\\ngives the following grammar:\\nS\\n→\\nϵ|BAB|BB|AB|BA|00\\nA\\n→\\nBAB|BB|AB|BA|00\\nB\\n→\\n00\\n3.5.\\nPushdown automata\\n111\\nSince all unit-rules have been eliminated, this concludes Step 3.\\nStep 4: Eliminate all rules having more than two symbols on the right-hand\\nside. There are two such rules:\\n• We take the rule S →BAB, remove it, and add the rules S →BA1\\nand A1 →AB.\\n• We take the rule A →BAB, remove it, and add the rules A →BA2\\nand A2 →AB.\\nThis gives the following grammar:\\nS\\n→\\nϵ|BB|AB|BA|00|BA1\\nA\\n→\\nBB|AB|BA|00|BA2\\nB\\n→\\n00\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nStep 4 is now completed.\\nStep 5: Eliminate all rules, whose right-hand side contains exactly two\\nsymbols, which are not both variables. There are three such rules:\\n• We replace the rule S →00 by the rules S →A3A3 and A3 →0.\\n• We replace the rule A →00 by the rules A →A4A4 and A4 →0.\\n• We replace the rule B →00 by the rules B →A5A5 and A5 →0.\\nThis gives the following grammar, which is in Chomsky normal form:\\nS\\n→\\nϵ|BB|AB|BA|BA1|A3A3\\nA\\n→\\nBB|AB|BA|BA2|A4A4\\nB\\n→\\nA5A5\\nA1\\n→\\nAB\\nA2\\n→\\nAB\\nA3\\n→\\n0\\nA4\\n→\\n0\\nA5\\n→\\n0\\n112\\nChapter 3.\\nContext-Free Languages\\n3.5\\nPushdown automata\\nIn this section, we introduce nondeterministic pushdown automata. As we\\nwill see, the class of languages that can be accepted by these automata is\\nexactly the class of context-free languages.\\nWe start with an informal description of a deterministic pushdown au-\\ntomaton. Such an automaton consists of the following, see also Figure 3.1.\\n1. There is a tape which is divided into cells. Each cell stores a symbol\\nbelonging to a ﬁnite set Σ, called the tape alphabet. There is a special\\nsymbol 2 that is not contained in Σ; this symbol is called the blank\\nsymbol. If a cell contains 2, then this means that the cell is actually\\nempty.\\n2. There is a tape head which can move along the tape, one cell to the\\nright per move. This tape head can also read the cell it currently scans.\\n3. There is a stack containing symbols from a ﬁnite set Γ, called the stack\\nalphabet. This set contains a special symbol $.\\n4. There is a stack head which can read the top symbol of the stack. This\\nhead can also pop the top symbol, and it can push symbols of Γ onto\\nthe stack.\\n5. There is a state control, which can be in any one of a ﬁnite number\\nof states. The set of states is denoted by Q. The set Q contains one\\nspecial state q, called the start state.\\nThe input for a pushdown automaton is a string in Σ∗. This input string\\nis stored on the tape of the pushdown automaton and, initially, the tape head\\nis on the leftmost symbol of the input string. Initially, the stack only contains\\nthe special symbol $, and the pushdown automaton is in the start state q.\\nIn one computation step, the pushdown automaton does the following:\\n1. Assume that the pushdown automaton is currently in state r. Let a be\\nthe symbol of Σ that is read by the tape head, and let A be the symbol\\nof Γ that is on top of the stack.\\n2. Depending on the current state r, the tape symbol a, and the stack\\nsymbol A,\\n3.5.\\nPushdown automata\\n113\\nstate control\\na a b a b b a b a b 2\\ntape\\n6\\n$\\nA\\nA\\nB\\nA\\nstack\\n-\\nFigure 3.1: A pushdown automaton.\\n(a) the pushdown automaton switches to a state r′ of Q (which may\\nbe equal to r),\\n(b) the tape head either moves one cell to the right or stays at the\\ncurrent cell, and\\n(c) the top symbol A is replaced by a string w that belongs to Γ∗. To\\nbe more precise,\\ni. if w = ϵ, then A is popped from the stack, whereas\\nii. if w = B1B2 . . . Bk, with k ≥1 and B1, B2, . . . , Bk ∈Γ, then\\nA is replaced by w, and Bk becomes the new top symbol of\\nthe stack.\\nLater, we will specify when the pushdown automaton accepts the input\\nstring.\\nWe now give a formal deﬁnition of a deterministic pushdown automaton.\\nDeﬁnition 3.5.1 A deterministic pushdown automaton is a 5-tuple M =\\n(Σ, Γ, Q, δ, q), where\\n114\\nChapter 3.\\nContext-Free Languages\\n1. Σ is a ﬁnite set, called the tape alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the stack alphabet; this alphabet contains the\\nspecial symbol $,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. δ is called the transition function, which is a function\\nδ : Q × (Σ ∪{2}) × Γ →Q × {N, R} × Γ∗.\\nThe transition function δ can be thought of as being the “program” of the\\npushdown automaton. This function tells us what the automaton can do in\\none “computation step”: Let r ∈Q, a ∈Σ ∪{2}, and A ∈Γ. Furthermore,\\nlet r′ ∈Q, σ ∈{R, N}, and w ∈Γ∗be such that\\nδ(r, a, A) = (r′, σ, w).\\n(3.1)\\nThis transition means that if\\n• the pushdown automaton is in state r,\\n• the tape head reads the symbol a, and\\n• the top symbol on the stack is A,\\nthen\\n• the pushdown automaton switches to state r′,\\n• the tape head moves according to σ: if σ = R, then it moves one cell\\nto the right; if σ = N, then it does not move, and\\n• the top symbol A on the stack is replaced by the string w.\\nWe will write the computation step (3.1) in the form of the instruction\\nraA →r′σw.\\nWe now specify the computation of the pushdown automaton M = (Σ, Γ, Q, δ, q).\\n3.6.\\nExamples of pushdown automata\\n115\\nStart conﬁguration: Initially, the pushdown automaton is in the start state\\nq, the tape head is on the leftmost symbol of the input string a1a2 . . . an, and\\nthe stack only contains the special symbol $.\\nComputation and termination: Starting in the start conﬁguration, the\\npushdown automaton performs a sequence of computation steps as described\\nabove. It terminates at the moment when the stack becomes empty. (Hence,\\nif the stack never gets empty, the pushdown automaton does not terminate.)\\nAcceptance: The pushdown automaton accepts the input string a1a2 . . . an ∈\\nΣ∗, if\\n1. the automaton terminates on this input, and\\n2. at the time of termination (i.e., at the moment when the stack gets\\nempty), the tape head is on the cell immediately to the right of the cell\\ncontaining the symbol an (this cell must contain the blank symbol 2).\\nIn all other cases, the pushdown automaton rejects the input string. Thus,\\nthe pushdown automaton rejects this string if\\n1. the automaton does not terminate on this input (i.e., the computation\\n“loops forever”) or\\n2. at the time of termination, the tape head is not on the cell immediately\\nto the right of the cell containing the symbol an.\\nWe denote by L(M) the language accepted by the pushdown automaton\\nM. Thus,\\nL(M) = {w ∈Σ∗: M accepts w}.\\nThe pushdown automaton described above is deterministic. For a non-\\ndeterministic pushdown automata, the current computation step may not\\nbe uniquely deﬁned, but the automaton can make a choice out of a ﬁnite\\nnumber of possibilities. In this case, the transition function δ is a function\\nδ : Q × (Σ ∪{2}) × Γ →Pf(Q × {N, R} × Γ∗),\\nwhere Pf(K) is the set of all ﬁnite subsets of the set K.\\nWe say that a nondeterministic pushdown automaton M accepts an input\\nstring, if there exists an accepting computation, in the sense as described for\\ndeterministic pushdown automata. We say that M rejects an input string, if\\nevery computation on this string is rejecting. As before, we denote by L(M)\\nthe set of all strings in Σ∗that are accepted by M.\\n116\\nChapter 3.\\nContext-Free Languages\\n3.6\\nExamples of pushdown automata\\n3.6.1\\nProperly nested parentheses\\nWe will show how to construct a deterministic pushdown automaton, that\\naccepts the set of all strings of properly nested parentheses. Observe that a\\nstring w in {(, )}∗is properly nested if and only if\\n• in every preﬁx of w, the number of “(” is greater than or equal to the\\nnumber of “)”, and\\n• in the complete string w, the number of “(” is equal to the number of\\n“)”.\\nWe will use the tape symbol a for “(”, and the tape symbol b for “)”.\\nThe idea is as follows. Recall that initially, the stack only contains the\\nspecial symbol $. The pushdown automaton reads the input string from left\\nto right. For every a it reads, it pushes the symbol S onto the stack, and\\nfor every b it reads, it pops the top symbol from the stack. In this way, the\\nnumber of symbols S on the stack will always be equal to the number of as\\nthat have been read minus the number of bs that have been read; additionally,\\nthe bottom of the stack will contain the special symbol $. The input string\\nis properly nested if and only if (i) this diﬀerence is always non-negative and\\n(ii) this diﬀerence is zero once the entire input string has been read. Hence,\\nthe input string is accepted if and only if during this process, (i) the stack\\nalways contains at least the special symbol $ and (ii) at the end, the stack\\nonly contains the special symbol $ (which will then be popped in the ﬁnal\\nstep).\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q), where Σ = {a, b}, Γ = {$, S}, Q = {q}, and the\\ntransition function δ is speciﬁed by the following instructions:\\n3.6.\\nExamples of pushdown automata\\n117\\nqa$ →qR$S\\nbecause of the a, S is pushed onto the stack\\nqaS →qRSS\\nbecause of the a, S is pushed onto the stack\\nqbS →qRϵ\\nbecause of the b, the top element is popped\\nfrom the stack\\nqb$ →qNϵ\\nthe number of bs read is larger than the number\\nof as read; the stack is made empty (hence,\\nthe computation terminates before the entire\\nstring has been read), and the input string is rejected\\nq2$ →qNϵ\\nthe entire input string has been read; the stack is\\nmade empty, and the input string is accepted\\nq2S →qNS\\nthe entire input string has been read, it contains\\nmore as than bs; no changes are made (thus, the\\nautomaton does not terminate), and the input string\\nis rejected\\n3.6.2\\nStrings of the form 0n1n\\nWe construct a deterministic pushdown automata that accepts the language\\n{0n1n : n ≥0}.\\nThe automaton uses two states q0 and q1, where q0 is the start state.\\nInitially, the automaton is in state q0.\\n• For each 0 that it reads, the automaton pushes one symbol S onto the\\nstack and stays in state q0.\\n• When the ﬁrst 1 is read, the automaton switches to state q1. From that\\nmoment,\\n– for each 1 that is read, the automaton pops the top symbol from\\nthe stack and stays in state q1;\\n– if a 0 is read, the automaton does not make any change and,\\ntherefore, does not terminate.\\nBased on this discussion, we obtain the deterministic pushdown automa-\\nton M = (Σ, Γ, Q, δ, q0), where Σ = {0, 1}, Γ = {$, S}, Q = {q0, q1}, q0 is\\nthe start state, and the transition function δ is speciﬁed by the following\\ninstructions:\\n118\\nChapter 3.\\nContext-Free Languages\\nq00$ →q0R$S\\npush S onto the stack\\nq00S →q0RSS\\npush S onto the stack\\nq01$ →q0N$\\nﬁrst symbol in the input is 1; loop forever\\nq01S →q1Rϵ\\nﬁrst 1 is encountered\\nq02$ →q0Nϵ\\ninput string is empty; accept\\nq02S →q0NS\\ninput only consists of 0s; loop forever\\nq10$ →q1N$\\n0 to the right of 1; loop forever\\nq10S →q1NS\\n0 to the right of 1; loop forever\\nq11$ →q1N$\\ntoo many 1s; loop forever\\nq11S →q1Rϵ\\npop top symbol from the stack\\nq12$ →q1Nϵ\\naccept\\nq12S →q1NS\\ntoo many 0s; loop forever\\n3.6.3\\nStrings with b in the middle\\nWe will construct a nondeterministic pushdown automaton that accepts the\\nset L of all strings in {a, b}∗having an odd length and whose middle symbol\\nis b, i.e.,\\nL = {vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|}.\\nThe idea is as follows. The automaton uses two states q and q′, where q\\nis the start state. These states have the following meaning:\\n• If the automaton is in state q, then it has not reached the middle symbol\\nb of the input string.\\n• If the automaton is in state q′, then it has read the middle symbol b.\\nObserve that since the automaton can only make one single pass over the\\ninput string, it has to “guess” (i.e., use nondeterminism) when it reaches the\\nmiddle of the string.\\n• If the automaton is in state q, then, when reading the current tape\\nsymbol,\\n– it either pushes one symbol S onto the stack and stays in state q\\n– or, in case the current tape symbol is b, it “guesses” that it has\\nreached the middle of the input string, by switching to state q′.\\n• If the automaton is in state q′, then, when reading the current tape\\nsymbol, it pops the top symbol S from the stack and stays in state q′.\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n119\\nIn this way, the number of symbols S on the stack will always be equal to the\\ndiﬀerence of (i) the number of symbols in the part to the left of the middle\\nsymbol b that have been read and (ii) the number of symbols in the part\\nto the right of the middle symbol b that have been read; additionally, the\\nbottom of the stack will contain the special symbol $.\\nThe input string is accepted if and only if, at the moment when the blank\\nsymbol 2 is read, the automaton is in state q′ and the top symbol on the\\nstack is $. In this case, the stack is made empty and, thus, the computation\\nterminates.\\nWe obtain the nondeterministic pushdown automaton M = (Σ, Γ, Q, δ, q),\\nwhere Σ = {a, b}, Γ = {$, S}, Q = {q, q′}, q is the start state, and the\\ntransition function δ is speciﬁed by the following instructions:\\nqa$ →qR$S\\npush S onto the stack\\nqaS →qRSS\\npush S onto the stack\\nqb$ →q′R$\\nreached the middle\\nqb$ →qR$S\\ndid not reach the middle; push S onto the stack\\nqbS →q′RS\\nreached the middle\\nqbS →qRSS\\ndid not reach the middle; push S onto the stack\\nq2$ →qN$\\ninput string is empty; loop forever\\nq2S →qNS\\nloop forever\\nq′a$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′aS →q′Rϵ\\npop top symbol from stack\\nq′b$ →q′Nϵ\\nstack is empty; terminate, but reject, because\\nthe entire input string has not been read\\nq′bS →q′Rϵ\\npop top symbol from stack\\nq′2$ →q′Nϵ\\naccept\\nq′2S →q′NS\\nloop forever\\nRemark 3.6.1 It can be shown that there is no deterministic pushdown\\nautomaton that accepts the language L. The reason is that a deterministic\\npushdown automaton cannot determine when it reaches the middle of the\\ninput string. Thus, unlike as for ﬁnite automata, nondeterministic pushdown\\nautomata are more powerful than their deterministic counterparts.\\n120\\nChapter 3.\\nContext-Free Languages\\n3.7\\nEquivalence of pushdown automata and\\ncontext-free grammars\\nThe main result of this section is that nondeterministic pushdown automata\\nand context-free grammars are equivalent in power:\\nTheorem 3.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then\\nA is context-free if and only if there exists a nondeterministic pushdown\\nautomaton that accepts A.\\nWe will only prove one direction of this theorem. That is, we will show\\nhow to convert an arbitrary context-free grammar to a nondeterministic push-\\ndown automaton.\\nLet G = (V, Σ, R, $) be a context-free grammar, where V is the set of\\nvariables, Σ is the set of terminals, R is the set of rules, and $ is the start\\nvariable. By Theorem 3.4.2, we may assume that G is in Chomsky normal\\nform. Hence, every rule in R has one of the following three forms:\\n1. A →BC, where A, B, and C are variables, B ̸= $, and C ̸= $.\\n2. A →a, where A is a variable and a is a terminal.\\n3. $ →ϵ.\\nWe will construct a nondeterministic pushdown automaton M that ac-\\ncepts the language L(G) of this grammar G. Observe that M must have the\\nfollowing property: For every string w = a1a2 . . . an ∈Σ∗,\\nw ∈L(G) if and only if M accepts w.\\nThis can be reformulated as follows:\\n$\\n∗⇒a1a2 . . . an\\nif and only if there exists a computation of M that starts in the initial\\nconﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n121\\nand ends in the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nwhere ∅indicates that the stack is empty.\\nAssume that $\\n∗⇒a1a2 . . . an. Then there exists a derivation (using the\\nrules of R) of the string a1a2 . . . an from the start variable $. We may assume\\nthat in each step in this derivation, a rule is applied to the leftmost variable\\nin the current string. Hence, because the grammar G is in Chomsky normal\\nform, at any moment during the derivation, the current string has the form\\na1a2 . . . ai−1AkAk−1 . . . A1,\\n(3.2)\\nfor some integers i and k with 1 ≤i ≤n + 1 and k ≥0, and variables\\nA1, A2, . . . , Ak. (In particular, at the start of the derivation, we have i = 1\\nand k = 1, and the current string is Ak = $. At the end of the derivation,\\nwe have i = n + 1 and k = 0, and the current string is a1a2 . . . an.)\\nWe will deﬁne the pushdown automaton M in such a way that the current\\nstring (3.2) corresponds to the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nBased on this discussion, we obtain the nondeterministic pushdown au-\\ntomaton M = (Σ, V, {q}, δ, q), where\\n• the tape alphabet is the set Σ of terminals of G,\\n• the stack alphabet is the set V of variables of G,\\n• the set of states consists of one state q, which is the start state, and\\n• the transition function δ is obtained from the rules in R, in the following\\nway:\\n122\\nChapter 3.\\nContext-Free Languages\\n– For each rule in R that is of the form A →BC, with A, B, C ∈V ,\\nthe pushdown automaton M has the instructions\\nqaA →qNCB, for all a ∈Σ.\\n– For each rule in R that is of the form A →a, with A ∈V and\\na ∈Σ, the pushdown automaton M has the instruction\\nqaA →qRϵ.\\n– If R contains the rule $ →ϵ, then the pushdown automaton M\\nhas the instruction\\nq2$ →qNϵ.\\nThis concludes the deﬁnition of M. It remains to prove that L(M) =\\nL(G), i.e., the language of the nondeterministic pushdown automaton M is\\nequal to the language of the context-free grammar G. Hence, we have to\\nshow that for every string w ∈Σ∗,\\nw ∈L(G) if and only if w ∈L(M),\\nwhich can be rewritten as\\n$\\n∗⇒w if and only if M accepts w.\\nClaim 3.7.2 Let a1a2 . . . an be a string in Σ∗, let A1, A2, . . . , Ak be variables\\nin V , and let i and k be integers with 1 ≤i ≤n + 1 and k ≥0. Then the\\nfollowing holds:\\n$\\n∗⇒a1a2 . . . ai−1AkAk−1 . . . A1\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\n3.7.\\nEquivalence of PDA’s and CFG’s\\n123\\na1 · · ·\\nai · · · an 2\\n6\\nA1\\n...\\nAk\\n-\\nProof. The claim can be proved by induction. Let\\nw = a1a2 . . . ai−1AkAk−1 . . . A1.\\nAssume that k ≥1 and assume that the claim is true for the string w. Then\\nwe have to show that the claim is still true after applying a rule in R to the\\nleftmost variable Ak in w. Since the grammar is in Chomsky normal form,\\nthe rule to be applied is either of the form Ak →BC or of the form Ak →ai.\\nIn both cases, the property mentioned in the claim is maintained.\\nWe now use Claim 3.7.2 to prove that L(M) = L(G). Let w = a1a2 . . . an\\nbe an arbitrary string in Σ∗. By applying Claim 3.7.2, with i = n + 1 and\\nk = 0, we see that w ∈L(G), i.e.,\\n$\\n∗⇒a1a2 . . . an,\\nif and only if there exists a computation of M from the initial conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n$\\n-\\nto the conﬁguration\\na1 · · ·\\nai · · · an 2\\n6\\n∅\\n-\\nBut this means that w ∈L(G) if and only if the automaton M accepts the\\nstring w.\\nThis concludes the proof of the fact that every context-free grammar can\\nbe converted to a nondeterministic pushdown automaton.\\nAs mentioned\\nalready, we will not give the conversion in the other direction. We ﬁnish this\\nsection with the following observation:\\n124\\nChapter 3.\\nContext-Free Languages\\nTheorem 3.7.3 Let Σ be an alphabet and let A ⊆Σ∗be a context-free lan-\\nguage. Then there exists a nondeterministic pushdown automaton that ac-\\ncepts A and has only one state.\\nProof. Since A is context-free, there exists a context-free grammar G0 such\\nthat L(G0) = A. By Theorem 3.4.2, there exists a context-free grammar G\\nthat is in Chomsky normal form and for which L(G) = L(G0). The construc-\\ntion given above converts G to a nondeterministic pushdown automaton M\\nthat has only one state and for which L(M) = L(G).\\n3.8\\nThe pumping lemma for context-free lan-\\nguages\\nIn Section 2.9, we proved the pumping lemma for regular languages and\\nused it to prove that certain languages are not regular. In this section, we\\ngeneralize the pumping lemma to context-free languages.\\nThe idea is to\\nconsider the parse tree (see Section 3.1) that describes the derivation of a\\nsuﬃciently long string in the context-free language L. Since the number of\\nvariables in the corresponding context-free grammar G is ﬁnite, there is at\\nleast one variable, say Aj, that occurs more than once on the longest root-\\nto-leaf path in the parse tree. The subtree which is sandwiched between two\\noccurrences of Aj on this path can be copied any number of times. This will\\nresult in a legal parse tree and, hence, in a “pumped” string that is in the\\nlanguage L.\\nTheorem 3.8.1 (Pumping Lemma for Context-Free Languages) Let\\nL be a context-free language. Then there exists an integer p ≥1, called the\\npumping length, such that the following holds: Every string s in L, with\\n|s| ≥p, can be written as s = uvxyz, such that\\n1. |vy| ≥1 (i.e., v and y are not both empty),\\n2. |vxy| ≤p, and\\n3. uvixyiz ∈L, for all i ≥0.\\n3.8.\\nThe pumping lemma for context-free languages\\n125\\n3.8.1\\nProof of the pumping lemma\\nThe proof of the pumping lemma will use the following result about parse\\ntrees:\\nLemma 3.8.2 Let G be a context-free grammar in Chomsky normal form,\\nlet s be a non-empty string in L(G), and let T be a parse tree for s. Let ℓbe\\nthe height of T, i.e., ℓis the number of edges on a longest root-to-leaf path\\nin T. Then\\n|s| ≤2ℓ−1.\\nProof. The claim can be proved by induction on ℓ. By looking at some\\nsmall values of ℓand using the fact that G is in Chomsky normal form, you\\nshould be able to verify the claim.\\nNow we can start with the proof of the pumping lemma. Let L be a\\ncontext-free language and let Σ be the alphabet of L. By Theorem 3.4.2, there\\nexists a context-free grammar in Chomsky normal form, G = (V, Σ, R, S),\\nsuch that L = L(G).\\nDeﬁne r to be the number of variables of G and deﬁne p = 2r. We will\\nprove that the value of p can be used as the pumping length. Consider an\\narbitrary string s in L such that |s| ≥p, and let T be a parse tree for s. Let\\nℓbe the height of T. Then, by Lemma 3.8.2, we have\\n|s| ≤2ℓ−1.\\nOn the other hand, we have\\n|s| ≥p = 2r.\\nBy combining these inequalities, we see that 2r ≤2ℓ−1, which can be rewrit-\\nten as\\nℓ≥r + 1.\\nConsider the nodes on a longest root-to-leaf path in T.\\nSince this path\\nconsists of ℓedges, it consists of ℓ+ 1 nodes. The ﬁrst ℓof these nodes store\\nvariables, which we denote by A0, A1, . . . , Aℓ−1 (where A0 = S), and the last\\nnode (which is a leaf) stores a terminal, which we denote by a.\\nSince ℓ−1 −r ≥0, the sequence\\nAℓ−1−r, Aℓ−r, . . . , Aℓ−1\\n126\\nChapter 3.\\nContext-Free Languages\\nof variables is well-deﬁned.\\nObserve that this sequence consists of r + 1\\nvariables. Since the number of variables in the grammar G is equal to r,\\nthe pigeonhole principle implies that there is a variable that occurs at least\\ntwice in this sequence. In other words, there are indices j and k, such that\\nℓ−1 −r ≤j < k ≤ℓ−1 and Aj = Ak. Refer to the ﬁgure below for an\\nillustration.\\nS\\nA j\\nAk\\nu\\nv\\nx\\ny\\nz\\ns\\nA0 = S\\nA1\\nAℓ−1−r\\nAℓ−r\\nAℓ−2\\nAℓ−1\\na\\nr +1\\nvariables\\nRecall that T is a parse tree for the string s. Therefore, the terminals\\nstored at the leaves of T, in the order from left to right, form s. As indicated\\nin the ﬁgure above, the nodes storing the variables Aj and Ak partition s\\ninto ﬁve substrings u, v, x, y, and z, such that s = uvxyz.\\nIt remains to prove that the three properties stated in the pumping lemma\\n3.8.\\nThe pumping lemma for context-free languages\\n127\\nhold. We start with the third property, i.e., we prove that\\nuvixyiz ∈L, for all i ≥0.\\nIn the grammar G, we have\\nS\\n∗⇒uAjz.\\n(3.3)\\nSince Aj\\n∗⇒vAky and Ak = Aj, we have\\nAj\\n∗⇒vAjy.\\n(3.4)\\nFinally, since Ak\\n∗⇒x and Ak = Aj, we have\\nAj\\n∗⇒x.\\n(3.5)\\nFrom (3.3) and (3.5), it follows that\\nS\\n∗⇒uAjz\\n∗⇒uxz,\\nwhich implies that the string uxz is in the language L. Similarly, it follows\\nfrom (3.3), (3.4), and (3.5) that\\nS\\n∗⇒uAjz\\n∗⇒uvAjyz\\n∗⇒uvvAjyyz\\n∗⇒uvvxyyz.\\nHence, the string uv2xy2z is in the language L. In general, for each i ≥0,\\nthe string uvixyiz is in the language L, because\\nS\\n∗⇒uAjz\\n∗⇒uviAjyiz\\n∗⇒uvixyiz.\\nThis proves that the third property in the pumping lemma holds.\\nNext we show that the second property holds. That is, we prove that\\n|vxy| ≤p.\\nConsider the subtree rooted at the node storing the variable\\nAj.\\nThe path from the node storing Aj to the leaf storing the terminal\\na is a longest path in this subtree. (Convince yourself that this is true.)\\nMoreover, this path consists of ℓ−j edges. Since Aj\\n∗⇒vxy, this subtree\\nis a parse tree for the string vxy (where Aj is used as the start variable).\\nTherefore, by Lemma 3.8.2, we can conclude that |vxy| ≤2ℓ−j−1. We know\\nthat ℓ−1 −r ≤j, which is equivalent to ℓ−j −1 ≤r. It follows that\\n|vxy| ≤2ℓ−j−1 ≤2r = p.\\n128\\nChapter 3.\\nContext-Free Languages\\nFinally, we show that the ﬁrst property in the pumping lemma holds.\\nThat is, we prove that |vy| ≥1. Recall that\\nAj\\n∗⇒vAky.\\nLet the ﬁrst rule used in this derivation be Aj →BC. (Since the variables\\nAj and Ak, even though they are equal, are stored at diﬀerent nodes of the\\nparse tree, and since the grammar G is in Chomsky normal form, this ﬁrst\\nrule exists.) Then\\nAj ⇒BC\\n∗⇒vAky.\\nObserve that the string BC has length two. Moreover, by applying rules of\\na grammar in Chomsky normal form, strings cannot become shorter. (Here,\\nwe use the fact that the start variable does not occur on the right-hand side\\nof any rule.) Therefore, we have |vAky| ≥2. But this implies that |vy| ≥1.\\nThis completes the proof of the pumping lemma.\\n3.8.2\\nApplications of the pumping lemma\\nFirst example\\nConsider the language\\nA = {anbncn : n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. Consider the string s = apbpcp.\\nObserve that s ∈A and |s| = 3p ≥p. Hence, by the pumping lemma, s can\\nbe written as s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all\\ni ≥0.\\nObserve that the pumping lemma does not tell us the location of the\\nsubstring vxy in the string s, it only gives us an upper bound on the length\\nof this substring. Therefore, we have to consider three cases, depending on\\nthe location of vxy in s.\\nCase 1: The substring vxy does not contain any c.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many as or more than p many bs. Since it contains\\nexactly p many cs, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\n3.8.\\nThe pumping lemma for context-free languages\\n129\\nCase 2: The substring vxy does not contain any a.\\nConsider the string uv2xy2z = uvvxyyz.\\nSince |vy| ≥1, this string\\ncontains more than p many bs or more than p many cs. Since it contains\\nexactly p many as, it follows that this string is not in the language A. This\\nis a contradiction because, by the pumping lemma, the string uv2xy2z is in\\nA.\\nCase 3: The substring vxy contains at least one a and at least one c.\\nSince s = apbpcp, this implies that |vxy| > p, which again contradicts the\\npumping lemma.\\nThus, in all of the three cases, we have obtained a contradiction. There-\\nfore, we have shown that the language A is not context-free.\\nSecond example\\nConsider the languages\\nA = {wwR : w ∈{a, b}∗},\\nwhere wR is the string obtained by writing w backwards, and\\nB = {ww : w ∈{a, b}∗}.\\nEven though these languages look similar, we will show that A is context-free\\nand B is not context-free.\\nConsider the following context-free grammar, in which S is the start vari-\\nable:\\nS →ϵ|aSa|bSb.\\nIt is easy to see that the language of this grammar is exactly the language A.\\nTherefore, A is context-free. Alternatively, we can show that A is context-\\nfree, by constructing a (nondeterministic) pushdown automaton that accepts\\nA. This automaton has two states q and q′, where q is the start state. If the\\nautomaton is in state q, then it did not yet ﬁnish reading the leftmost half of\\nthe input string; it pushes all symbols read onto the stack. If the automaton\\nis in state q′, then it is reading the rightmost half of the input string; for each\\nsymbol read, it checks whether it is equal to the symbol on top of the stack\\nand, if so, pops the top symbol from the stack. The pushdown automaton\\nuses nondeterminism to “guess” when to switch from state q to state q′ (i.e.,\\nwhen it has completed reading the leftmost half of the input string).\\n130\\nChapter 3.\\nContext-Free Languages\\nAt this point, you should convince yourself that the two approaches above,\\nwhich showed that A is context-free, do not work for B. The reason why\\nthey do not work is that the language B is not context-free, as we will prove\\nnow.\\nAssume that B is a context-free language. Let p ≥1 be the pumping\\nlength, as given by the pumping lemma. At this point, we must choose a\\nstring s in B, whose length is at least p, and that does not satisfy the three\\nproperties stated in the pumping lemma. Let us try the string s = apbapb.\\nThen s ∈B and |s| = 2p + 2 ≥p. Hence, by the pumping lemma, s can be\\nwritten as s = uvxyz, where (i) |vy| ≥1, (ii) |vxy| ≤p, and (iii) uvixyiz ∈B\\nfor all i ≥0. It may happen that p ≥3, u = ap−1, v = a, x = b, y = a,\\nand z = ap−1b. If this is the case, then properties (i), (ii), and (iii) hold,\\nand, thus, we do not get a contradiction. In other words, we have chosen\\nthe “wrong” string s. This string is “wrong”, because there is only one b\\nbetween the as. Because of this, v can be in the leftmost block of as, and\\ny can be in the rightmost block of as. Observe that if there were at least p\\nmany bs between the as, then this would not happen, because |vxy| ≤p.\\nBased on the discussion above, we choose s = apbpapbp. Observe that\\ns ∈B and |s| = 4p ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈B for all i ≥0. Based\\non the location of vxy in the string s, we distinguish three cases:\\nCase 1: The substring vxy overlaps both the leftmost half and the rightmost\\nhalf of s.\\nSince |vxy| ≤p, the substring vxy is contained in the “middle” part of s,\\ni.e., vxy is contained in the block bpap. Consider the string uv0xy0z = uxz.\\nSince |vy| ≥1, we know that at least one of v and y is non-empty.\\n• If v ̸= ϵ, then v contains at least one b from the leftmost block of bs in\\ns, whereas y does not contain any b from the rightmost block of bs in s.\\nTherefore, in the string uxz, the leftmost block of bs contains fewer bs\\nthan the rightmost block of bs. Hence, the string uxz is not contained\\nin B.\\n• If y ̸= ϵ, then y contains at least one a from the rightmost block of\\nas in s, whereas v does not contain any a from the leftmost block of\\nas in s. Therefore, in the string uxz, the leftmost block of as contains\\nmore as than the rightmost block of as. Hence, the string uxz is not\\ncontained in B.\\n3.8.\\nThe pumping lemma for context-free languages\\n131\\nIn both cases, we conclude that the string uxz is not an element of the\\nlanguage B. But, by the pumping lemma, this string is contained in B.\\nCase 2: The substring vxy is in the leftmost half of s.\\nIn this case, none of the strings uxz, uv2xy2z, uv3xy3z, uv4xy4z, etc.,\\nis contained in B.\\nBut, by the pumping lemma, each of these strings is\\ncontained in B.\\nCase 3: The substring vxy is in the rightmost half of s.\\nThis case is symmetric to Case 2: None of the strings uxz, uv2xy2z,\\nuv3xy3z, uv4xy4z, etc., is contained in B. But, by the pumping lemma, each\\nof these strings is contained in B.\\nTo summarize, in each of the three cases, we have obtained a contradic-\\ntion. Therefore, the language B is not context-free.\\nThird example\\nWe have seen in Section 3.2.4 that the language\\n{ambncm+n : m ≥0, n ≥0}\\nis context-free. Using the pumping lemma for regular languages, it is easy to\\nprove that this language is not regular. In other words, context-free gram-\\nmars can verify addition, whereas ﬁnite automata are not powerful enough\\nfor this. We now consider the problem of verifying multiplication: Let A be\\nthe language deﬁned as\\nA = {ambncmn : m ≥0, n ≥0}.\\nWe will prove by contradiction that A is not a context-free language.\\nAssume that A is context-free. Let p ≥1 be the pumping length, as\\ngiven by the pumping lemma. Consider the string s = apbpcp2. Then, s ∈A\\nand |s| = 2p + p2 ≥p. Hence, by the pumping lemma, s can be written as\\ns = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all i ≥0.\\nThere are three possible cases, depending on the locations of v and y in\\nthe string s.\\nCase 1: The substring v does not contain any a and does not contain any\\nb, and the substring y does not contain any a and does not contain any b.\\n132\\nChapter 3.\\nContext-Free Languages\\nConsider the string uv2xy2z. Since |vy| ≥1, this string consists of p\\nmany as, p many bs, but more than p2 many cs. Therefore, this string is not\\ncontained in A. But, by the pumping lemma, it is contained in A.\\nCase 2: The substring v does not contain any c and the substring y does\\nnot contain any c.\\nConsider again the string uv2xy2z. This string consists of p2 many cs.\\nSince |vy| ≥1, in this string,\\n• the number of as is at least p + 1 and the number of bs is at least p, or\\n• the number of as is at least p and the number of bs is at least p + 1.\\nTherefore, the number of as multiplied by the number of bs is at least p(p+1),\\nwhich is larger than p2. Therefore, uv2xy2z is not contained in A. But, by\\nthe pumping lemma, this string is contained in A.\\nCase 3: The substring v contains at least one b and the substring y contains\\nat least one c.\\nSince |vxy| ≤p, the substring vy does not contain any a. Thus, we can\\nwrite vy = bjck, where j ≥1 and k ≥1. Consider the string uxz. We can\\nwrite this string as uxz = apbp−jcp2−k. Since, by the pumping lemma, this\\nstring is contained in A, we have p(p−j) = p2−k, which implies that jp = k.\\nThus,\\n|vxy| ≥|vy| = j + k = j + jp ≥1 + p.\\nBut, by the pumping lemma, we have |vxy| ≤p.\\nObserve that, since |vxy| ≤p, the above three cases cover all possibilities\\nfor the locations of v and y in the string s. In each of the three cases, we\\nhave obtained a contradiction. Therefore, the language A is not context-free.\\nExercises\\n3.1 Construct context-free grammars that generate the following languages.\\nIn all cases, Σ = {0, 1}.\\n• {02n1n : n ≥0}\\n• {w : w contains at least three 1s}\\n• {w : the length of w is odd and its middle symbol is 0}\\nExercises\\n133\\n• {w : w is a palindrome}.\\nA palindrome is a string w having the property that w = wR, i.e.,\\nreading w from left to right gives the same result as reading w from\\nright to left.\\n• {w : w starts and ends with the same symbol}\\n• {w : w starts and ends with diﬀerent symbols}\\n3.2 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {0, 1}, S is the start variable, and R consists of the rules\\nS\\n→\\n0S|1A|ϵ\\nA\\n→\\n0B|1S\\nB\\n→\\n0A|1B\\nDeﬁne the following language L:\\nL = {w ∈{0, 1}∗:\\nw is the binary representation of a non-negative\\ninteger that is divisible by three } ∪{ϵ}.\\nProve that L = L(G). (Hint: The variables S, A, and B are used to\\nremember the remainder after division by three.)\\n3.3 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},\\nΣ = {a, b}, S is the start variable, and R consists of the rules\\nS\\n→\\naB|bA\\nA\\n→\\na|aS|BAA\\nB\\n→\\nb|bS|ABB\\n• Prove that ababba ∈L(G).\\n• Prove that L(G) is the set of all non-empty strings w over the alphabet\\n{a, b} such that the number of as in w is equal to the number of bs in\\nw.\\n3.4 Let A and B be context-free languages over the same alphabet Σ.\\n• Prove that the union A ∪B of A and B is also context-free.\\n• Prove that the concatenation AB of A and B is also context-free.\\n134\\nChapter 3.\\nContext-Free Languages\\n• Prove that the star A∗of A is also context-free.\\n3.5 Deﬁne the following two languages A and B:\\nA = {ambncn : m ≥0, n ≥0}\\nand\\nB = {ambmcn : m ≥0, n ≥0}.\\n• Prove that both A and B are context-free, by constructing two context-\\nfree grammars, one that generates A and one that generates B.\\n• We have seen in Section 3.8.2 that the language\\n{anbncn : n ≥0}\\nis not context-free. Explain why this implies that the intersection of\\ntwo context-free languages is not necessarily context-free.\\n• Use De Morgan’s Law to conclude that the complement of a context-\\nfree language is not necessarily context-free.\\n3.6 Let A be a context-free language and let B be a regular language.\\n• Prove that the intersection A ∩B of A and B is context-free.\\n• Prove that the set-diﬀerence\\nA \\\\ B = {w : w ∈A, w ̸∈B}\\nof A and B is context-free.\\n• Is the set-diﬀerence of two context-free languages necessarily context-\\nfree?\\n3.7 Let L be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that\\n• the number of as in w is equal to the number of bs in w,\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\nExercises\\n135\\nIn this exercise, you will prove that L is context-free.\\nLet A be the language consisting of all non-empty strings w over the\\nalphabet {a, b} such that the number of as in w is equal to the number of bs\\nin w. In Exercise 3.3, you have shown that A is context-free.\\nLet B be the language consisting of all strings w over the alphabet {a, b}\\nsuch that\\n• w does not contain the substring abba, and\\n• w does not contain the substring bbaa.\\n1. Give a regular expression that describes the complement of B.\\n2. Argue that B is a regular language.\\n3. Use Exercise 3.6 to argue that L is a context-free language.\\n3.8 Construct (deterministic or nondeterministic) pushdown automata that\\naccept the following languages.\\n1. {02n1n : n ≥0}.\\n2. {0n1m0n : n ≥1, m ≥1}.\\n3. {w ∈{0, 1}∗: w contains more 1s than 0s}.\\n4. {wwR : w ∈{0, 1}∗}.\\n(If w = w1 . . . wn, then wR = wn . . . w1.)\\n5. {w ∈{0, 1}∗: w is a palindrome}.\\n3.9 Let L be the language\\nL = {ambn : 0 ≤m ≤n ≤2m}.\\n1. Prove that L is context-free, by constructing a context-free grammar\\nwhose language is equal to L.\\n2. Prove that L is context-free, by constructing a nondeterministic push-\\ndown automaton that accepts L.\\n3.10 Prove that the following languages are not context-free.\\n136\\nChapter 3.\\nContext-Free Languages\\n• {an b a2n b a3n : n ≥0}.\\n• {anbnanbn : n ≥0}.\\n• {ambnck : m ≥0, n ≥0, k = max(m, n)}.\\n• {w#x : w is a substring of x, and w, x ∈{a, b}∗}.\\nFor example, the string aba#abbababbb is in the language, whereas the\\nstring aba#baabbaabb is not in the language. The alphabet is {a, b, #}.\\n•\\n{ w ∈{a, b, c}∗\\n:\\nw contains more b’s than a’s and\\nw contains more c’s than a’s }.\\n• {1n : n is a prime number}.\\n• {(abn)n : n ≥0}. (The parentheses are not part of the alphabet; thus,\\nthe alphabet is {a, b, }.)\\n3.11 Let L be a language consisting of ﬁnitely many strings. Show that L\\nis regular and, therefore, context-free. Let k be the maximum length of any\\nstring in L.\\n• Prove that every context-free grammar in Chomsky normal form that\\ngenerates L has more than log k variables. (The logarithm is in base\\n2.)\\n• Prove that there is a context-free grammar that generates L and that\\nhas only one variable.\\n3.12 Let L be a context-free language. Prove that there exists an integer\\np ≥1, such that the following is true: For every string s in L with |s| ≥p,\\nthere exists a string s′ in L such that |s| < |s′| ≤|s| + p.\\nChapter 4\\nTuring Machines and the\\nChurch-Turing Thesis\\nIn the previous chapters, we have seen several computational devices that\\ncan be used to accept or generate regular and context-free languages. Even\\nthough these two classes of languages are fairly large, we have seen in Sec-\\ntion 3.8.2 that these devices are not powerful enough to accept simple lan-\\nguages such as A = {ambncmn : m ≥0, n ≥0}. In this chapter, we introduce\\nthe Turing machine, which is a simple model of a real computer. Turing ma-\\nchines can be used to accept all context-free languages, but also languages\\nsuch as A. We will argue that every problem that can be solved on a real\\ncomputer can also be solved by a Turing machine (this statement is known\\nas the Church-Turing Thesis). In Chapter 5, we will consider the limitations\\nof Turing machines and, hence, of real computers.\\n4.1\\nDeﬁnition of a Turing machine\\nWe start with an informal description of a Turing machine. Such a machine\\nconsists of the following, see also Figure 4.1.\\n1. There are k tapes, for some ﬁxed k ≥1. Each tape is divided into\\ncells, and is inﬁnite both to the left and to the right. Each cell stores\\na symbol belonging to a ﬁnite set Γ, which is called the tape alphabet.\\nThe tape alphabet contains the blank symbol 2. If a cell contains 2,\\nthen this means that the cell is actually empty.\\n138\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nstate control\\n. . . 2 2 2 a a b a b b a b a b 2 2 2\\n. . .\\n?\\n. . . 2 2 2 b a a b 2 a b 2 2 2\\n. . .\\n?\\nFigure 4.1: A Turing machine with k = 2 tapes.\\n2. Each tape has a tape head which can move along the tape, one cell\\nper move. It can also read the cell it currently scans and replace the\\nsymbol in this cell by another symbol.\\n3. There is a state control, which can be in any one of a ﬁnite number of\\nstates. The ﬁnite set of states is denoted by Q. The set Q contains\\nthree special states: a start state, an accept state, and a reject state.\\nThe Turing machine performs a sequence of computation steps. In one\\nsuch step, it does the following:\\n1. Immediately before the computation step, the Turing machine is in a\\nstate r of Q, and each of the k tape heads is on a certain cell.\\n2. Depending on the current state r and the k symbols that are read by\\nthe tape heads,\\n(a) the Turing machine switches to a state r′ of Q (which may be\\nequal to r),\\n(b) each tape head writes a symbol of Γ in the cell it is currently\\nscanning (this symbol may be equal to the symbol currently stored\\nin the cell), and\\n4.1.\\nDeﬁnition of a Turing machine\\n139\\n(c) each tape head either moves one cell to the left, moves one cell to\\nthe right, or stays at the current cell.\\nWe now give a formal deﬁnition of a deterministic Turing machine.\\nDeﬁnition 4.1.1 A deterministic Turing machine is a 7-tuple\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject),\\nwhere\\n1. Σ is a ﬁnite set, called the input alphabet; the blank symbol 2 is not\\ncontained in Σ,\\n2. Γ is a ﬁnite set, called the tape alphabet; this alphabet contains the\\nblank symbol 2, and Σ ⊆Γ,\\n3. Q is a ﬁnite set, whose elements are called states,\\n4. q is an element of Q; it is called the start state,\\n5. qaccept is an element of Q; it is called the accept state,\\n6. qreject is an element of Q; it is called the reject state,\\n7. δ is called the transition function, which is a function\\nδ : Q × Γk →Q × Γk × {L, R, N}k.\\nThe transition function δ is basically the “program” of the Turing ma-\\nchine. This function tells us what the machine can do in “one computation\\nstep”: Let r ∈Q, and let a1, a2, . . . , ak ∈Γ.\\nFurthermore, let r′ ∈Q,\\na′\\n1, a′\\n2, . . . , a′\\nk ∈Γ, and σ1, σ2, . . . , σk ∈{L, R, N} be such that\\nδ(r, a1, a2, . . . , ak) = (r′, a′\\n1, a′\\n2, . . . , a′\\nk, σ1, σ2, . . . , σk).\\n(4.1)\\nThis transition means that if\\n• the Turing machine is in state r, and\\n• the head of the i-th tape reads the symbol ai, 1 ≤i ≤k,\\nthen\\n140\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• the Turing machine switches to state r′,\\n• the head of the i-th tape replaces the scanned symbol ai by the symbol\\na′\\ni, 1 ≤i ≤k, and\\n• the head of the i-th tape moves according to σi, 1 ≤i ≤k: if σi = L,\\nthen the tape head moves one cell to the left; if σi = R, then it moves\\none cell to the right; if σi = N, then the tape head does not move.\\nWe will write the computation step (4.1) in the form of the instruction\\nra1a2 . . . ak →r′a′\\n1a′\\n2 . . . a′\\nkσ1σ2 . . . σk.\\nWe now specify the computation of the Turing machine\\nM = (Σ, Γ, Q, δ, q, qaccept, qreject).\\nStart conﬁguration: The input is a string over the input alphabet Σ.\\nInitially, this input string is stored on the ﬁrst tape, and the head of this\\ntape is on the leftmost symbol of the input string. Initially, all other k −1\\ntapes are empty, i.e., only contain blank symbols, and the Turing machine is\\nin the start state q.\\nComputation and termination: Starting in the start conﬁguration, the\\nTuring machine performs a sequence of computation steps as described above.\\nThe computation terminates at the moment when the Turing machine en-\\nters the accept state qaccept or the reject state qreject. (Hence, if the Turing\\nmachine never enters the states qaccept and qreject, the computation does not\\nterminate.)\\nAcceptance: The Turing machine M accepts the input string w ∈Σ∗, if the\\ncomputation on this input terminates in the state qaccept. If the computation\\non this input terminates in the state qreject, then M rejects the input string\\nw.\\nWe denote by L(M) the language accepted by the Turing machine M.\\nThus, L(M) is the set of all strings in Σ∗that are accepted by M.\\nObserve that a string w ∈Σ∗does not belong to L(M) if and only if on\\ninput w,\\n• the computation of M terminates in the state qreject or\\n• the computation of M does not terminate.\\n4.2.\\nExamples of Turing machines\\n141\\n4.2\\nExamples of Turing machines\\n4.2.1\\nAccepting palindromes using one tape\\nWe will show how to construct a Turing machine with one tape, that decides\\nwhether or not any input string w ∈{a, b}∗is a palindrome. Recall that the\\nstring w is called a palindrome, if reading w from left to right gives the same\\nresult as reading w from right to left. Examples of palindromes are abba,\\nbaabbbbaab, and the empty string ϵ.\\nStart of the computation: The tape contains the input string w, the tape\\nhead is on the leftmost symbol of w, and the Turing machine is in the start\\nstate q0.\\nIdea: The tape head reads the leftmost symbol of w, deletes this symbol\\nand “remembers” it by means of a state.\\nThen the tape head moves to\\nthe rightmost symbol and tests whether it is equal to the (already deleted)\\nleftmost symbol.\\n• If they are equal, then the rightmost symbol is deleted, the tape head\\nmoves to the new leftmost symbol, and the whole process is repeated.\\n• If they are not equal, the Turing machine enters the reject state, and\\nthe computation terminates.\\nThe Turing machine enters the accept state as soon as the string currently\\nstored on the tape is empty.\\nWe will use the input alphabet Σ = {a, b} and the tape alphabet Γ =\\n{a, b, 2}. The set Q of states consists of the following eight states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost symbol was a; tape head is moving to the right\\nqb :\\nleftmost symbol was b; tape head is moving to the right\\nq′\\na :\\nreached rightmost symbol; test whether it is equal to a, and delete it\\nq′\\nb :\\nreached rightmost symbol; test whether it is equal to b, and delete it\\nqL :\\ntest was positive; tape head is moving to the left\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n142\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nqba →qbaR\\nq0b →qb2R\\nqab →qabR\\nqbb →qbbR\\nq02 →qaccept\\nqa2 →q′\\na2L\\nqb2 →q′\\nb2L\\nq′\\naa →qL2L\\nq′\\nba →qreject\\nqLa →qLaL\\nq′\\nab →qreject\\nq′\\nbb →qL2L\\nqLb →qLbL\\nq′\\na2 →qaccept\\nq′\\nb2 →qaccept\\nqL2 →q02R\\nYou should go through the computation of this Turing machine for some\\nsample inputs, for example abba, b, abb and the empty string (which is a\\npalindrome).\\n4.2.2\\nAccepting palindromes using two tapes\\nWe again consider the palindrome problem, but now we use a Turing machine\\nwith two tapes.\\nStart of the computation: The ﬁrst tape contains the input string w and\\nthe head of the ﬁrst tape is on the leftmost symbol of w. The second tape is\\nempty and its tape head is at an arbitrary position. The Turing machine is\\nin the start state q0.\\nIdea: First, the input string w is copied to the second tape. Then the head\\nof the ﬁrst tape moves back to the leftmost symbol of w, while the head of\\nthe second tape stays at the rightmost symbol of w. Finally, the actual test\\nstarts: The head of the ﬁrst tape moves to the right and, at the same time,\\nthe head of the second tape moves to the left. While moving, the Turing\\nmachine tests whether the two tape heads read the same symbol in each\\nstep.\\nThe input alphabet is Σ = {a, b} and the tape alphabet is Γ = {a, b, 2}.\\nThe set Q of states consists of the following ﬁve states:\\nq0 :\\nstart state; copy w to the second tape\\nq1 :\\nw has been copied; head of ﬁrst tape moves to the left\\nq2 :\\nhead of ﬁrst tape moves to the right; head of second tape moves\\nto the left; until now, all tests were positive\\nqaccept :\\naccept state\\nqreject :\\nreject state\\n4.2.\\nExamples of Turing machines\\n143\\nThe transition function δ is speciﬁed by the following instructions:\\nq0a2 →q0aaRR\\nq1aa →q1aaLN\\nq0b2 →q0bbRR\\nq1ab →q1abLN\\nq022 →q122LL\\nq1ba →q1baLN\\nq1bb →q1bbLN\\nq12a →q22aRN\\nq12b →q22bRN\\nq122 →qaccept\\nq2aa →q2aaRL\\nq2ab →qreject\\nq2ba →qreject\\nq2bb →q2bbRL\\nq222 →qaccept\\nAgain, you should run this Turing machine for some sample inputs.\\n4.2.3\\nAccepting anbncn using one tape\\nWe will construct1 a Turing machine with one tape that accepts the language\\n{anbncn : n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: In the previous examples, the tape alphabet Γ was equal to the union\\nof the input alphabet Σ and {2}. In this example, we will add one symbol\\nd to the tape alphabet. As we will see, this simpliﬁes the construction of\\nthe Turing machine. Thus, the input alphabet is Σ = {a, b, c} and the tape\\nalphabet is Γ = {a, b, c, d, 2}. Recall that the input string w belongs to Σ∗.\\nThe general approach is to split the computation into two stages.\\n1Thanks to Michael Fleming for pointing out an error in a previous version of this\\nconstruction.\\n144\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nStage 1: In this stage, we check if the string w is in the language described\\nby the regular expression a∗b∗c∗. If this is the case, then we walk back to\\nthe leftmost symbol. For this stage, we use the following states, besides the\\nstates qaccept and qreject:\\nqa :\\nstart state; we are reading the block of a’s\\nqb :\\nwe are reading the block of b’s\\nqc :\\nwe are reading the block of c’s\\nqL :\\nwalk to the leftmost symbol\\nStage 2: In this stage, we repeat the following: Walk along the string from\\nleft to right, replace the leftmost a by d, replace the leftmost b by d, replace\\nthe leftmost c by d, and walk back to the leftmost symbol.\\nFor this stage, we use the following states:\\nq′\\na :\\nstart state of Stage 2; search for the leftmost a\\nq′\\nb :\\nleftmost a has been replaced by d;\\nsearch for the leftmost b\\nq′\\nc :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nsearch for the leftmost c\\nq′\\nL :\\nleftmost a has been replaced by d;\\nleftmost b has been replaced by d;\\nleftmost c has been replaced by d;\\nwalk to the leftmost symbol\\nThe transition function δ is speciﬁed by the following instructions:\\nqaa →qaaR\\nqba →qreject\\nqab →qbbR\\nqbb →qbbR\\nqac →qccR\\nqbc →qccR\\nqad →cannot happen\\nqbd →cannot happen\\nqa2 →qL2L\\nqb2 →qL2L\\nqca →qreject\\nqLa →qLaL\\nqcb →qreject\\nqLb →qLbL\\nqcc →qccR\\nqLc →qLcL\\nqcd →cannot happen\\nqLd →cannot happen\\nqc2 →qL2L\\nqL2 →q′\\na2R\\n4.2.\\nExamples of Turing machines\\n145\\nq′\\naa →q′\\nbdR\\nq′\\nba →q′\\nbaR\\nq′\\nab →qreject\\nq′\\nbb →q′\\ncdR\\nq′\\nac →qreject\\nq′\\nbc →qreject\\nq′\\nad →q′\\nadR\\nq′\\nbd →q′\\nbdR\\nq′\\na2 →qaccept\\nq′\\nb2 →qreject\\nq′\\nca →qreject\\nq′\\nLa →q′\\nLaL\\nq′\\ncb →q′\\ncbR\\nq′\\nLb →q′\\nLbL\\nq′\\ncc →q′\\nLdL\\nq′\\nLc →q′\\nLcL\\nq′\\ncd →q′\\ncdR\\nq′\\nLd →q′\\nLdL\\nq′\\nc2 →qreject\\nq′\\nL2 →q′\\na2R\\nWe remark that Stage 1 is really necessary for this Turing machine: If we\\nomit this stage, and use only Stage 2, then the string aabcbc will be accepted.\\n4.2.4\\nAccepting anbncn using tape alphabet {a, b, c, 2}\\nWe consider again the language {anbncn : n ≥0}. In the previous section,\\nwe presented a Turing machine that uses an extra symbol d. The reader may\\nwonder if we can construct a Turing machine for this language that does not\\nuse any extra symbols. We will show below that this is indeed possible.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate q0.\\nIdea: Repeat the following Stages 1 and 2, until the string is empty.\\nStage 1. Walk along the string from left to right, delete the leftmost a,\\ndelete the leftmost b, and delete the rightmost c.\\nStage 2. Shift the substring of bs and cs one position to the left; then walk\\nback to the leftmost symbol.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, 2}.\\n146\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nFor Stage 1, we use the following states:\\nq0 :\\nstart state; tape head is on the leftmost symbol\\nqa :\\nleftmost a has been deleted; have not read b\\nqb :\\nleftmost b has been deleted; have not read c\\nqc :\\nleftmost c has been read; tape head moves to the right\\nq′\\nc :\\ntape head is on the rightmost c\\nq1 :\\nrightmost c has been deleted; tape head is on the rightmost\\nsymbol or 2\\nqaccept :\\naccept state\\nqreject :\\nreject state\\nThe transitions for Stage 1 are speciﬁed by the following instructions:\\nq0a →qa2R\\nqaa →qaaR\\nq0b →qreject\\nqab →qb2R\\nq0c →qreject\\nqac →qreject\\nq02 →qaccept\\nqa2 →qreject\\nqba →qreject\\nqca →qreject\\nqbb →qbbR\\nqcb →qreject\\nqbc →qccR\\nqcc →qccR\\nqb2 →qreject\\nqc2 →q′\\nc2L\\nq′\\ncc →q12L\\nFor Stage 2, we use the following states:\\nq1 :\\nas above; tape head is on the rightmost symbol or on 2\\nqc :\\ncopy c one cell to the left\\nqb :\\ncopy b one cell to the left\\nq2 :\\ndone with shifting; head moves to the left\\nAdditionally, we use a state q′\\n1 which has the following meaning: If the input\\nstring is of the form aibc, for some i ≥1, then after Stage 1, the tape contains\\nthe string ai−122, the tape head is on the 2 immediately to the right of the\\nas, and the Turing machine is in state q1. In this case, we move one cell to\\nthe left; if we then read 2, then i = 1, and we accept; otherwise, we read a,\\nand we reject.\\n4.2.\\nExamples of Turing machines\\n147\\nThe transitions for Stage 2 are speciﬁed by the following instructions:\\nq1a →cannot happen\\nq′\\n1a →qreject\\nq1b →qreject\\nq′\\n1b →cannot happen\\nq1c →qc2L\\nq′\\n1c →cannot happen\\nq12 →q′\\n12L\\nq′\\n12 →qaccept\\nqca →cannot happen\\nqba →cannot happen\\nqcb →qbcL\\nqbb →qbbL\\nqcc →qccL\\nqbc →cannot happen\\nqc2 →qreject\\nqb2 →q2bL\\nq2a →q2aL\\nq2b →cannot happen\\nq2c →cannot happen\\nq22 →q02R\\n4.2.5\\nAccepting ambncmn using one tape\\nWe will sketch how to construct a Turing machine with one tape that accepts\\nthe language\\n{ambncmn : m ≥0, n ≥0}.\\nRecall that we have proved in Section 3.8.2 that this language is not context-\\nfree.\\nThe input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, $, 2},\\nwhere the purpose of the symbol $ will become clear below.\\nStart of the computation: The tape contains the input string w and the\\ntape head is on the leftmost symbol of w. The Turing machine is in the start\\nstate.\\nIdea: Observe that a string ambnck is in the language if and only if for every\\na, the string contains n many cs. Based on this, the computation consists of\\nthe following stages:\\nStage 1. Walk along the input string w from left to right and check whether\\nw is an element of the language described by the regular expression a∗b∗c∗.\\nIf this is not the case, then reject the input string. Otherwise, go to Stage 2.\\nStage 2. Walk back to the leftmost symbol of w. Go to Stage 3.\\nStage 3. In this stage, the Turing machine does the following:\\n148\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n• Replace the leftmost a by the blank symbol 2.\\n• Walk to the leftmost b.\\n• Zigzag between the bs and cs; each time, replace the leftmost b by the\\nsymbol $, and replace the rightmost c by the blank symbol 2. If, for\\nsome b, there is no c left, the Turing machine rejects the input string.\\n• Continue zigzagging until there are no bs left. Then go to Stage 4.\\nObserve that in this third stage, the string ambnck is transformed to the\\nstring am−1$nck−n.\\nStage 4. In this stage, the Turing machine does the following:\\n• Replace each $ by b.\\n• Walk to the leftmost a.\\nHence, in this fourth stage, the string am−1$nck−n is transformed to the string\\nam−1bnck−n.\\nObserve that the input string ambnck is in the language if and only if the\\nstring am−1bnck−n is in the language. Therefore, the Turing machine repeats\\nStages 3 and 4, until there are no as left. At that moment, it checks whether\\nthere are any cs left; if so, it rejects the input string; otherwise, it accepts\\nthe input string.\\nWe hope that you believe that this description of the algorithm can be\\nturned into a formal description of a Turing machine.\\n4.3\\nMulti-tape Turing machines\\nIn Section 4.2, we have seen two Turing machines that accept palindromes;\\nthe ﬁrst Turing machine has one tape, whereas the second one has two tapes.\\nYou will have noticed that the two-tape Turing machine was easier to obtain\\nthan the one-tape Turing machine. This leads to the question whether multi-\\ntape Turing machines are more powerful than their one-tape counterparts.\\nThe answer is “no”:\\nTheorem 4.3.1 Let k ≥1 be an integer. Any k-tape Turing machine can\\nbe converted to an equivalent one-tape Turing machine.\\n4.3.\\nMulti-tape Turing machines\\n149\\nProof.2\\nWe will sketch the proof for the case when k = 2.\\nLet M =\\n(Σ, Γ, Q, δ, q, qaccept, qreject) be a two-tape Turing machine.\\nOur goal is to\\nconvert M to an equivalent one-tape Turing machine N. That is, N should\\nhave the property that for all strings w ∈Σ∗,\\n• M accepts w if and only if N accepts w,\\n• M rejects w if and only if N rejects w,\\n• M does not terminate on input w if and only if N does not terminate\\non input w.\\nThe tape alphabet of the one-tape Turing machine N is\\nΓ ∪{ ˙x : x ∈Γ} ∪{#}.\\nIn words, we take the tape alphabet Γ of M, and add, for each x ∈Γ, the\\nsymbol ˙x. Moreover, we add a special symbol #.\\nThe Turing machine N will be deﬁned in such a way that any conﬁgura-\\ntion of the two-tape Turing machine M, for example\\n. . . 2 1 0 0 1 2 . . .\\n6\\n. . . 2 a a b a 2 . . .\\n6\\ncorresponds to the following conﬁguration of the one-tape Turing machine\\nN:\\n. . .\\n2\\n#\\n1\\n0\\n˙0\\n1\\n#\\na\\n˙a\\nb\\na\\n#\\n2 . . .\\n6\\n2Thanks to Sergio Cabello for pointing out an error in a previous version of this proof.\\n150\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nThus, the contents of the two tapes of M are encoded on the single tape of\\nN. The dotted symbols are used to indicate the positions of the two tape\\nheads of M, whereas the three occurrences of the special symbol # are used\\nto mark the boundaries of the strings on the two tapes of M.\\nThe Turing machine N simulates one computation step of M, in the\\nfollowing way:\\n• Throughout the simulation of this step, N “remembers” the current\\nstate of M.\\n• At the start of the simulation, the tape head of N is on the leftmost\\nsymbol #.\\n• N walks along the string to the right until it ﬁnds the ﬁrst dotted\\nsymbol. (This symbol indicates the location of the head on the ﬁrst tape\\nof M.) N remembers this ﬁrst dotted symbol and continues walking\\nto the right until it ﬁnds the second dotted symbol.\\n(This symbol\\nindicates the location of the head on the second tape of M.) Again, N\\nremembers this second dotted symbol.\\n• At this moment, N is still at the second dotted symbol. N updates\\nthis part of the tape, by making the change that M would make on its\\nsecond tape. (This change is given by the transition function of M; it\\ndepends on the current state of M and the two symbols that M reads\\non its two tapes.)\\n• N walks to the left until it ﬁnds the ﬁrst dotted symbol.\\nThen, it\\nupdates this part of the tape, by making the change that M would\\nmake on its ﬁrst tape.\\n• In the previous two steps, in which the tape is updated, it may be\\nnecessary to shift a part of the tape.\\n• Finally, N remembers the new state of M and walks back to the left-\\nmost symbol #.\\nIt should be clear that the Turing machine N can be constructed by\\nintroducing appropriate states.\\n4.4.\\nThe Church-Turing Thesis\\n151\\n4.4\\nThe Church-Turing Thesis\\nWe all have some intuitive notion of what an algorithm is. This notion will\\nprobably be something like “an algorithm is a procedure consisting of com-\\nputation steps that can be speciﬁed in a ﬁnite amount of text”. For example,\\nany “computational process” that can be speciﬁed by a Java program, should\\nbe considered an algorithm. Similarly, a Turing machine speciﬁes a “com-\\nputational process” and, therefore, should be considered an algorithm. This\\nleads to the question of whether it is possible to give a mathematical deﬁni-\\ntion of an “algorithm”. We just saw that every Java program represents an\\nalgorithm and that every Turing machine also represents an algorithm. Are\\nthese two notions of an algorithm equivalent? The answer is “yes”. In fact,\\nthe following theorem states that many diﬀerent notions of “computational\\nprocess” are equivalent. (We hope that you have gained suﬃcient intuition,\\nso that none of the claims in this theorem comes as a surprise to you.)\\nTheorem 4.4.1 The following computation models are equivalent, i.e., any\\none of them can be converted to any other one:\\n1. One-tape Turing machines.\\n2. k-tape Turing machines, for any k ≥1.\\n3. Non-deterministic Turing machines.\\n4. Java programs.\\n5. C++ programs.\\n6. Lisp programs.\\nIn other words, if we deﬁne the notion of an algorithm using any of the\\nmodels in this theorem, then it does not matter which model we take: All\\nthese models give the same notion of an algorithm.\\nThe problem of deﬁning the notion of an algorithm goes back to David\\nHilbert. On August 8, 1900, at the Second International Congress of Math-\\nematicians in Paris, Hilbert presented a list of problems that he considered\\ncrucial for the further development of mathematics. Hilbert’s 10th problem\\nis the following:\\n152\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nDoes there exist a ﬁnite process that decides whether or not any\\ngiven polynomial with integer coeﬃcients has integral roots?\\nOf course, in our language, Hilbert asked whether or not there exists an\\nalgorithm that decides, when given an arbitrary polynomial equation (with\\ninteger coeﬃcients) such as\\n12x3y7z5 + 7x2y4z −x4 + y2z7 −z3 + 10 = 0,\\nwhether or not this equation has a solution in integers. In 1970, Matiyasevich\\nproved that such an algorithm does not exist. Of course, in order to prove\\nthis claim, we ﬁrst have to agree on what an algorithm is. In the beginning\\nof the twentieth century, mathematicians gave several deﬁnitions, such as\\nTuring machines (1936) and the λ-calculus (1936), and they proved that all\\nthese are equivalent. Later, after programming languages were invented, it\\nwas shown that these older notions of an algorithm are equivalent to notions\\nof an algorithm that are based on C programs, Java programs, Lisp programs,\\nPascal programs, etc.\\nIn other words, all attempts to give a rigorous deﬁnition of the notion of\\nan algorithm led to the same concept. Because of this, computer scientists\\nnowadays agree on what is called the Church-Turing Thesis:\\nChurch-Turing Thesis:\\nEvery computational process that is intuitively\\nconsidered to be an algorithm can be converted to a Turing machine.\\nIn other words, this basically states that we deﬁne an algorithm to be a\\nTuring machine. At this point, you should ask yourself, whether the Church-\\nTuring Thesis can be proved. Alternatively, what has to be done in order to\\ndisprove this thesis?\\nExercises\\n4.1 Construct a Turing machine with one tape, that accepts the language\\n{02n1n : n ≥0}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\nExercises\\n153\\n4.2 Construct a Turing machine with one tape, that accepts the language\\n{w : w contains twice as many 0s as 1s}.\\nAssume that, at the start of the computation, the tape head is on the leftmost\\nsymbol of the input string.\\n4.3 Let A be the language\\nA\\n=\\n{ w ∈{a, b, c}∗\\n:\\nw contains more bs than as and\\nw contains more cs than as }.\\nGive an informal description (in plain English) of a Turing machine with one\\ntape, that accepts the language A.\\n4.4 Construct a Turing machine with one tape that receives as input a non-\\nnegative integer x and returns as output the integer x + 1.\\nIntegers are\\nrepresented as binary strings.\\nStart of the computation: The tape contains the binary representation\\nof the input x. The tape head is on the leftmost symbol and the Turing\\nmachine is in the start state q0. For example, if x = 431, the tape looks as\\nfollows:\\n. . . 2 2 2 1 1 0 1 0 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x + 1. The tape head is on the leftmost symbol and the Turing\\nmachine is in the ﬁnal state q1. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 0 1 1 0 0 0 0 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state q1. As soon as state q1 is entered,\\nthe Turing machine terminates. At termination, the contents of the tape is\\nthe output of the Turing machine.\\n154\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\n4.5 Construct a Turing machine with two tapes that receives as input two\\nnon-negative integers x and y, and returns as output the integer x + y.\\nIntegers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost bit\\nof y. At the start, the Turing machine is in the start state q0.\\nEnd of the computation: The ﬁrst tape contains the binary representation\\nof x and its head is on the rightmost symbol of x. The second tape contains\\nthe binary representation of the integer x + y (thus, the integer y is “gone”).\\nThe head of the second tape is on the rightmost bit of x + y. The Turing\\nmachine is in the ﬁnal state q1.\\n4.6 Give an informal description (in plain English) of a Turing machine with\\none tape that receives as input two non-negative integers x and y, and returns\\nas output the integer x+y. Integers are represented as binary strings. If you\\nare an adventurous student, you may give a formal deﬁnition of your Turing\\nmachine.\\n4.7 Construct a Turing machine with one tape that receives as input an\\ninteger x ≥1 and returns as output the integer x−1. Integers are represented\\nin binary.\\nStart of the computation: The tape contains the binary representation of\\nthe input x. The tape head is on the rightmost symbol of x and the Turing\\nmachine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer x −1. The tape head is on the rightmost bit of x −1 and the\\nTuring machine is in the ﬁnal state q1.\\n4.8 Give an informal description (in plain English) of a Turing machine with\\nthree tapes that receives as input two non-negative integers x and y, and\\nreturns as output the integer xy. Integers are represented as binary strings.\\nStart of the computation: The ﬁrst tape contains the binary represen-\\ntation of x and its head is on the rightmost symbol of x. The second tape\\ncontains the binary representation of y and its head is on the rightmost sym-\\nbol of y. The third tape is empty and its head is at an arbitrary location.\\nThe Turing machine is in the start state q0.\\nExercises\\n155\\nEnd of the computation: The ﬁrst and second tapes are empty. The third\\ntape contains the binary representation of the product xy and its head is on\\nthe rightmost bit of xy. The Turing machine is in the ﬁnal state q1.\\nHint: Use the Turing machines of Exercises 4.5 and 4.7.\\n4.9 Construct a Turing machine with one tape that receives as input a string\\nof the form 1n for some integer n ≥0; thus, the input is a string of n many\\n1s. The output of the Turing machine is the string 1n21n. Thus, this Turing\\nmachine makes a copy of its input.\\nThe input alphabet is Σ = {1} and the tape alphabet is Γ = {1, 2}.\\nStart of the computation: The tape contains a string of the form 1n, for\\nsome integer n ≥0, the tape head is on the leftmost symbol, and the Turing\\nmachine is in the start state. For example, if n = 4, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 2 2\\n. . .\\n6\\nEnd of the computation: The tape contains the string 1n21n, the tape\\nhead is on the 2 in the middle of this string, and the Turing machine is in\\nthe ﬁnal state. For our example, the tape looks as follows:\\n. . . 2 2 2 1 1 1 1 2 1 1 1 1 2 2 2\\n. . .\\n6\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this state is entered, the\\nTuring machine terminates. At termination, the contents of the tape is the\\noutput of the Turing machine.\\n156\\nChapter 4.\\nTuring Machines and the Church-Turing Thesis\\nChapter 5\\nDecidable and Undecidable\\nLanguages\\nWe have seen in Chapter 4 that Turing machines form a model for “everything\\nthat is intuitively computable”. In this chapter, we consider the limitations\\nof Turing machines. That is, we ask ourselves the question whether or not\\n“everything” is computable. As we will see, the answer is “no”. In fact, we\\nwill even see that “most” problems are not solvable by Turing machines and,\\ntherefore, not solvable by computers.\\n5.1\\nDecidability\\nIn Chapter 4, we have deﬁned when a Turing machine accepts an input string\\nand when it rejects an input string. Based on this, we deﬁne the following\\nclass of languages.\\nDeﬁnition 5.1.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is decidable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the reject state.\\n158\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is decidable, if there exists an algorithm\\nthat (i) terminates on every input string w, and (ii) correctly tells us whether\\nw ∈A or w ̸∈A.\\nA language A that is not decidable is called undecidable.\\nFor such a\\nlanguage, there does not exist an algorithm that satisﬁes (i) and (ii) above.\\nIn Section 4.2, we have seen several examples of languages that are de-\\ncidable.\\nIn the following subsections, we will give some examples of decidable and\\nundecidable languages. These examples involve languages A whose elements\\nare pairs of the form (C, w), where C is some computation model (for ex-\\nample, a deterministic ﬁnite automaton) and w is a string over the alphabet\\nΣ. The pair (C, w) is in the language A if and only if the string w is in the\\nlanguage of the computation model C. For diﬀerent computation models C,\\nwe will ask the question whether A is decidable, i.e., whether an algorithm\\nexists that decides, for any input (C, w), whether or not this input belongs\\nto the language A. Since the input to any algorithm is a string over some\\nalphabet, we must encode the pair (C, w) as a string. In all cases that we\\nconsider, such a pair can be described using a ﬁnite amount of text. There-\\nfore, we assume, without loss of generality, that binary strings are used for\\nthese encodings. Throughout the rest of this chapter, we will denote the\\nbinary encoding of a pair (C, w) by\\n⟨C, w⟩.\\n5.1.1\\nThe language ADFA\\nWe deﬁne the following language:\\nADFA = {⟨M, w⟩:\\nM is a deterministic ﬁnite automaton that\\naccepts the string w}.\\nKeep in mind that ⟨M, w⟩denotes the binary string that forms an en-\\ncoding of the ﬁnite automaton M and the string w that is given as input to\\nM.\\nWe claim that the language ADFA is decidable. In order to prove this,\\nwe have to construct an algorithm with the following property, for any given\\ninput string u:\\n• If u is the encoding of a deterministic ﬁnite automaton M and a string\\nw (i.e., u is in the correct format ⟨M, w⟩), and if M accepts w, then\\n5.1.\\nDecidability\\n159\\nthe algorithm terminates in its accept state.\\n• In all other cases, the algorithm terminates in its reject state.\\nAn algorithm that exactly does this, is easy to obtain: On input u, the algo-\\nrithm ﬁrst checks whether or not u encodes a deterministic ﬁnite automaton\\nM and a string w. If this is not the case, then it terminates and rejects\\nthe input string u. Otherwise, the algorithm “constructs” M and w, and\\nthen simulates the computation of M on the input string w. If M accepts\\nw, then the algorithm terminates and accepts the input string u. If M does\\nnot accept w, then the algorithm terminates and rejects the input string u.\\nThus, we have proved the following result:\\nTheorem 5.1.2 The language ADFA is decidable.\\n5.1.2\\nThe language ANFA\\nWe deﬁne the following language:\\nANFA = {⟨M, w⟩:\\nM is a nondeterministic ﬁnite automaton that\\naccepts the string w}.\\nTo prove that this language is decidable, consider the algorithm that\\ndoes the following: On input u, the algorithm ﬁrst checks whether or not\\nu encodes a nondeterministic ﬁnite automaton M and a string w. If this is\\nnot the case, then it terminates and rejects the input string u. Otherwise,\\nthe algorithm constructs M and w. Since a computation of M (on input w)\\nis not unique, the algorithm ﬁrst converts M to an equivalent deterministic\\nﬁnite automaton N. Then, it proceeds as in Section 5.1.1.\\nObserve that the construction for converting a nondeterministic ﬁnite au-\\ntomaton to a deterministic ﬁnite automaton (see Section 2.5) is algorithmic,\\nin the sense that it can be described by an algorithm. Because of this, the\\nalgorithm described above is a valid algorithm; it accepts all strings u that\\nare in ANFA, and it rejects all strings u that are not in ANFA. Thus, we have\\nproved the following result:\\nTheorem 5.1.3 The language ANFA is decidable.\\n160\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.1.3\\nThe language ACFG\\nWe deﬁne the following language:\\nACFG = {⟨G, w⟩: G is a context-free grammar such that w ∈L(G)}.\\nWe claim that this language is decidable. In order to prove this claim, con-\\nsider a string u that encodes a context-free grammar G = (V, Σ, S, R) and a\\nstring w ∈Σ∗. Deciding whether or not w ∈L(G) is equivalent to deciding\\nwhether or not S\\n∗⇒w. A ﬁrst idea to decide this is by trying all possible\\nderivations that start with the start variable S and that use rules of R. The\\nproblem is that, in case w ̸∈L(G), it is not clear how many such derivations\\nhave to be checked before we can be sure that w is not in the language of\\nG: If w ∈L(G), then it may be that w can be derived from S, only by ﬁrst\\nderiving a very long string, say v, and then use rules to shorten it so as to\\nobtain the string w. Since there is no obvious upper bound on the length of\\nthe string v, we have to be careful.\\nThe trick is to do the following. First, convert the grammar G to an\\nequivalent grammar G′ in Chomsky normal form. (The construction given\\nin Section 3.4 can be described by an algorithm.) Let n be the length of the\\nstring w. Then, if w ∈L(G) = L(G′), any derivation of w in G′, from the\\nstart variable of G′, consists of exactly 2n−1 steps (where a “step” is deﬁned\\nas applying one rule of G′). Hence, we can decide whether or not w ∈L(G),\\nby trying all possible derivations, in G′, consisting of 2n −1 steps. If one of\\nthese (ﬁnite number of) derivations leads to the string w, then w ∈L(G).\\nOtherwise, w ̸∈L(G). Thus, we have proved the following result:\\nTheorem 5.1.4 The language ACFG is decidable.\\nIn fact, the arguments above imply the following result:\\nTheorem 5.1.5 Every context-free language is decidable.\\nProof. Let Σ be an alphabet and let A ⊆Σ∗be an arbitrary context-free\\nlanguage. There exists a context-free grammar in Chomsky normal form,\\nwhose language is equal to A. Given an arbitrary string w ∈Σ∗, we have\\nseen above how we can decide whether or not w can be derived from the\\nstart variable of this grammar.\\n5.1.\\nDecidability\\n161\\n5.1.4\\nThe language ATM\\nAfter having seen the languages ADFA, ANFA, and ACFG, it is natural to\\nconsider the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nWe will prove that this language is undecidable. Before we give the proof,\\nlet us mention what this means:\\nThere is no algorithm that, when given an arbitrary algorithm M\\nand an arbitrary input string w for M, decides in a ﬁnite amount\\nof time, whether or not M accepts w.\\nThe proof of the claim that ATM is undecidable is by contradiction. Thus,\\nwe assume that ATM is decidable. Then there exists a Turing machine H\\nthat has the following property. For every input string ⟨M, w⟩for H:\\n• If ⟨M, w⟩∈ATM (i.e., M accepts w), then H terminates in its accept\\nstate.\\n• If ⟨M, w⟩̸∈ATM (i.e., M rejects w or M does not terminate on input\\nw), then H terminates in its reject state.\\n• In particular, H terminates on any input ⟨M, w⟩.\\nWe construct a new Turing machine D, that does the following: On input\\n⟨M⟩, the Turing machine D uses H as a subroutine to determine what M\\ndoes when it is given its own description as input. Once D has determined\\nthis information, it does the opposite of what H does.\\nTuring machine D: On input ⟨M⟩, where M is a Turing machine,\\nthe new Turing machine D does the following:\\nStep 1: Run the Turing machine H on the input ⟨M, ⟨M⟩⟩.\\nStep 2:\\n• If H terminates in its accept state, then D terminates in its\\nreject state.\\n• If H terminates in its reject state, then D terminates in its\\naccept state.\\n162\\nChapter 5.\\nDecidable and Undecidable Languages\\nFirst observe that this new Turing machine D terminates on any input\\nstring ⟨M⟩, because H terminates on every input. Next observe that, for any\\ninput string ⟨M⟩for D:\\n• If ⟨M, ⟨M⟩⟩∈ATM (i.e., M accepts ⟨M⟩), then D terminates in its\\nreject state.\\n• If ⟨M, ⟨M⟩⟩̸∈ATM (i.e., M rejects ⟨M⟩or M does not terminate on\\ninput ⟨M⟩), then D terminates in its accept state.\\nThis means that for any string ⟨M⟩:\\n• If M accepts ⟨M⟩, then D rejects ⟨M⟩.\\n• If M rejects ⟨M⟩or M does not terminate on input ⟨M⟩, then D\\naccepts ⟨M⟩.\\nWe now consider what happens if we give the Turing machine D the string\\n⟨D⟩as input, i.e., we take M = D:\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩or D does not terminate on input ⟨D⟩, then D accepts\\n⟨D⟩.\\nSince D terminates on every input string, this means that\\n• If D accepts ⟨D⟩, then D rejects ⟨D⟩.\\n• If D rejects ⟨D⟩, then D accepts ⟨D⟩.\\nThis is clearly a contradiction. Therefore, the Turing machine H that decides\\nthe language ATM cannot exist and, thus, ATM is undecidable. We have\\nproved the following result:\\nTheorem 5.1.6 The language ATM is undecidable.\\n5.1.\\nDecidability\\n163\\n5.1.5\\nThe Halting Problem\\nWe deﬁne the following language:\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}.\\nTheorem 5.1.7 The language Halt is undecidable.\\nProof. The proof is by contradiction. Thus, we assume that the language\\nHalt is decidable. Then there exists a Java program H that takes as input a\\nstring of the form ⟨P, w⟩, where P is an arbitrary Java program and w is an\\narbitrary input for P. The program H has the following property:\\n• If ⟨P, w⟩∈Halt (i.e., program P terminates on input w), then H\\noutputs true.\\n• If ⟨P, w⟩̸∈Halt (i.e., program P does not terminate on input w), then\\nH outputs false.\\n• In particular, H terminates on any input ⟨P, w⟩.\\nWe will write the output of H as H(P, w). Moreover, we will denote by P(w)\\nthe computation obtained by running the program P on the input w. Hence,\\nH(P, w) =\\n\\x1a true\\nif P(w) terminates,\\nfalse\\nif P(w) does not terminate.\\nConsider the following algorithm Q, which takes as input the encoding\\n⟨P⟩of an arbitrary Java program P:\\nAlgorithm Q(⟨P⟩):\\nwhile H(P, ⟨P⟩) = true\\ndo have a beer\\nendwhile\\nSince H is a Java program, this new algorithm Q can also be written as\\na Java program. Observe that\\nQ(⟨P⟩) terminates if and only if H(P, ⟨P⟩) = false.\\n164\\nChapter 5.\\nDecidable and Undecidable Languages\\nThis means that for every Java program P,\\nQ(⟨P⟩) terminates if and only if P(⟨P⟩) does not terminate.\\n(5.1)\\nWhat happens if we run the Java program Q on the input string ⟨Q⟩?\\nIn other words, what happens if we run Q(⟨Q⟩)? Then, in (5.1), we have to\\nreplace all occurrences of P by Q. Hence,\\nQ(⟨Q⟩) terminates if and only if Q(⟨Q⟩) does not terminate.\\nThis is obviously a contradiction, and we can conclude that the Java program\\nH does not exist. Therefore, the language Halt is undecidable.\\nRemark 5.1.8 In this proof, we run the Java program Q on the input ⟨Q⟩.\\nThis means that the input to Q is a description of itself. In other words, we\\ngive Q itself as input. This is an example of what is called self-reference. An-\\nother example of self-reference can be found in Remark 5.1.8 of the textbook\\nIntroduction to Theory of Computation by A. Maheshwari and M. Smid.\\n5.2\\nCountable sets\\nThe proofs that we gave in Sections 5.1.4 and 5.1.5 seem to be bizarre. In\\nthis section, we will convince you that these proofs in fact use a technique\\nthat you have seen in the course COMP 1805: Cantor’s Diagonalization.\\nLet A and B be two sets and let f : A →B be a function. Recall that f\\nis called a bijection, if\\n• f is one-to-one (or injective), i.e., for any two distinct elements a and\\na′ in A, we have f(a) ̸= f(a′), and\\n• f is onto (or surjective), i.e., for each element b ∈B, there exists an\\nelement a ∈A, such that f(a) = b.\\nThe set of natural numbers is denoted by N. That is, N = {1, 2, 3, . . .}.\\nDeﬁnition 5.2.1 Let A and B be two sets. We say that A and B have the\\nsame size, if there exists a bijection f : A →B.\\nDeﬁnition 5.2.2 Let A be a set. We say that A is countable, if A is ﬁnite,\\nor A and N have the same size.\\n5.2.\\nCountable sets\\n165\\nIn other words, if A is an inﬁnite and countable set, then there exists a\\nbijection f : N →A, and we can write A as\\nA = {f(1), f(2), f(3), f(4), . . .}.\\nSince f is a bijection, every element of A occurs exactly once in the set on\\nthe right-hand side. This means that we can number the elements of A using\\nthe positive integers: Every element of A receives a unique number.\\nTheorem 5.2.3 The following sets are countable:\\n1. The set Z of integers:\\nZ = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.\\n2. The Cartesian product N × N:\\nN × N = {(m, n) : m ∈N, n ∈N}.\\n3. The set Q of rational numbers:\\nQ = {m/n : m ∈Z, n ∈Z, n ̸= 0}.\\nProof. To prove that the set Z is countable, we have to give each element of\\nZ a unique number in N. We obtain this numbering, by listing the elements\\nof Z in the following order:\\n0, 1, −1, 2, −2, 3, −3, 4, −4, . . .\\nIn this (inﬁnite) list, every element of Z occurs exactly once. The number of\\nan element of Z is given by its position in this list.\\nFormally, deﬁne the function f : N →Z by\\nf(n) =\\n\\x1a n/2\\nif n is even,\\n−(n −1)/2\\nif n is odd.\\nThis function f is a bijection and, therefore, the sets N and Z have the same\\nsize. Hence, the set Z is countable.\\nFor the proofs of the other two claims, we refer to the course COMP 1805.\\nWe now use Cantor’s Diagonalization principle to prove that the set of\\nreal numbers is not countable:\\n166\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.2.4 The set R of real numbers is not countable.\\nProof. Deﬁne\\nA = {x ∈R : 0 ≤x < 1}.\\nWe will prove that the set A is not countable. This will imply that the set\\nR is not countable, because A ⊆R.\\nThe proof that A is not countable is by contradiction. So we assume that\\nA is countable. Then there exists a bijection f : N →A. Thus, for each\\nn ∈N, f(n) is a real number between zero and one. We can write\\nA = {f(1), f(2), f(3), . . .},\\n(5.2)\\nwhere every element of A occurs exactly once in the set on the right-hand\\nside.\\nConsider the real number f(1). We can write this number in decimal\\nnotation as\\nf(1) = 0.d11d12d13 . . . ,\\nwhere each d1i is a digit in the set {0, 1, 2, . . . , 9}. In general, for every n ∈N,\\nwe can write the real number f(n) as\\nf(n) = 0.dn1dn2dn3 . . . ,\\nwhere, again, each dni is a digit in {0, 1, 2, . . . , 9}.\\nWe deﬁne the real number\\nx = 0.d1d2d3 . . . ,\\nwhere, for each integer n ≥1,\\ndn =\\n\\x1a 4\\nif dnn ̸= 4,\\n5\\nif dnn = 4.\\nObserve that x is a real number between zero and one, i.e., x ∈A. Therefore,\\nby (5.2), there is an element n ∈N, such that f(n) = x. We compare the\\nn-th digits of f(n) and x:\\n• The n-th digit of f(n) is equal to dnn.\\n• The n-th digit of x is equal to dn.\\n5.2.\\nCountable sets\\n167\\nSince f(n) and x are equal, their n-th digits must be equal, i.e., dnn = dn.\\nBut, by the deﬁnition of dn, we have dnn ̸= dn. This is a contradiction and,\\ntherefore, the set A is not countable.\\nNotice how we deﬁned the real number x: For each n ≥1, the n-th digit\\nof x is not equal to the n-th digit of f(n). Therefore, for each n ≥1, x ̸= f(n)\\nand, thus, x ̸∈A.\\nThe ﬁnal result of this section is the fact that for every set A, its power\\nset\\nP(A) = {B : B ⊆A}\\nis “strictly larger” than A. Deﬁne the function f : A →P(A) by\\nf(a) = {a},\\nfor any a in A. Since f is one-to-one, we can say that P(A) is “at least as\\nlarge as” A.\\nTheorem 5.2.5 Let A be an arbitrary set. Then A and P(A) do not have\\nthe same size.\\nProof. The proof is by contradiction. Thus, we assume that there exists a\\nbijection g : A →P(A). Deﬁne the set B as\\nB = {a ∈A : a ̸∈g(a)}.\\nSince B ∈P(A) and g is a bijection, there exists an element a in A such that\\ng(a) = B.\\nFirst assume that a ∈B. Since g(a) = B, we have a ∈g(a). But then,\\nfrom the deﬁnition of the set B, we have a ̸∈B, which is a contradiction.\\nNext assume that a ̸∈B.\\nSince g(a) = B, we have a ̸∈g(a).\\nBut\\nthen, from the deﬁnition of the set B, we have a ∈B, which is again a\\ncontradiction.\\nWe conclude that the bijection g does not exist. Therefore, A and P(A)\\ndo not have the same size.\\n168\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.2.1\\nThe Halting Problem revisited\\nNow that we know about countability, we give a diﬀerent way to look at the\\nproof in Section 5.1.5 of the fact that the language\\nHalt = {⟨P, w⟩:\\nP is a Java program that terminates on\\nthe input string w}\\nis undecidable.\\nYou should convince yourself that the proof given below\\nfollows the same reasoning as the one used in the proof of Theorem 5.2.4.\\nWe ﬁrst argue that the set of all Java programs is countable. Indeed,\\nevery Java program P can be described by a ﬁnite amount of text. In fact,\\nwe have been using ⟨P⟩to denote such a description by a binary string. For\\nany integer n ≥0, there are at most 2n (i.e., ﬁnitely many) Java programs\\nP whose description ⟨P⟩has length n. Therefore, to obtain a list of all Java\\nprograms, we do the following:\\n• List all Java programs P whose description ⟨P⟩has length 0. (Well,\\nthe empty string does not describe any Java program, so in this step,\\nnothing happens.)\\n• List all Java programs P whose description ⟨P⟩has length 1.\\n• List all Java programs P whose description ⟨P⟩has length 2.\\n• List all Java programs P whose description ⟨P⟩has length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every Java program occurs exactly once. Therefore, the\\nset of all Java programs is countable.\\nConsider an inﬁnite list\\nP1, P2, P3, . . .\\nin which every Java program occurs exactly once.\\nAssume that the language Halt is decidable. Then there exists a Java\\nprogram H that decides this language. We may assume that, on input ⟨P, w⟩,\\nH returns true if P terminates on input w, and false if P does not terminate\\non input w.\\nWe construct a new Java program D that does the following:\\n5.3.\\nRice’s Theorem\\n169\\nAlgorithm D:\\nOn input ⟨Pn⟩, where n is a positive integer, the\\nnew Java program D does the following:\\nStep 1: Run the Java program H on the input ⟨Pn, ⟨Pn⟩⟩.\\nStep 2:\\n• If H returns true, then D goes into an inﬁnite loop.\\n• If H returns false, then D returns true and terminates its com-\\nputation.\\nObserve that D can be written as a Java program. Therefore, there exists\\nan integer n ≥1 such that D = Pn. The next two observations follow from\\nthe pseudocode:\\n• If D terminates on input ⟨Pn⟩, then H returns false on input ⟨Pn, ⟨Pn⟩⟩,\\ni.e., Pn does not terminate on input ⟨Pn⟩.\\n• If D does not terminate on input ⟨Pn⟩, then H returns true on input\\n⟨Pn, ⟨Pn⟩⟩, i.e., Pn terminates on input ⟨Pn⟩.\\nThus,\\n• D terminates on input ⟨Pn⟩if and only if Pn does not terminate on\\ninput ⟨Pn⟩.\\nSince D = Pn, this becomes\\n• D terminates on input ⟨D⟩if and only if D does not terminate on input\\n⟨D⟩.\\nThus, we have obtained a contradiction.\\nRemark 5.2.6 We deﬁned the Java program D in such a way that, for each\\nn ≥1, the computation of D on input ⟨Pn⟩diﬀers from the computation of\\nPn on input ⟨Pn⟩. Hence, for each n ≥1, D ̸= Pn. However, since D is a\\nJava program, there must be an integer n ≥1 such that D = Pn.\\n170\\nChapter 5.\\nDecidable and Undecidable Languages\\n5.3\\nRice’s Theorem\\nWe have seen two examples of undecidable languages: ATM and Halt. In this\\nsection, we prove that many languages involving Turing machines (or Java\\nprograms) are undecidable.\\nDeﬁne T to be the set of binary encodings of all Turing machines, i.e.,\\nT = {⟨M⟩: M is a Turing machine with input alphabet {0,1}}.\\nTheorem 5.3.1 (Rice) Let P be a subset of T such that\\n1. P ̸= ∅, i.e., there exists a Turing machine M such that ⟨M⟩∈P,\\n2. P is a proper subset of T , i.e., there exists a Turing machine N such\\nthat ⟨N⟩̸∈P, and\\n3. for any two Turing machines M1 and M2 with L(M1) = L(M2),\\n(a) either both ⟨M1⟩and ⟨M2⟩are in P or\\n(b) none of ⟨M1⟩and ⟨M2⟩is in P.\\nThen the language P is undecidable.\\nYou can think of P as the set of all Turing machines that satisfy a certain\\nproperty. The ﬁrst two conditions state that at least one Turing machine\\nsatisﬁes this property and not all Turing machines satisfy this property. The\\nthird condition states that, for any Turing machine M, whether or not M\\nsatisﬁes this property only depends on the language L(M) of M.\\nHere are some examples of languages that satisfy the conditions in Rice’s\\nTheorem:\\nP1 = {⟨M⟩: M is a Turing machine and ϵ ∈L(M)},\\nP2 = {⟨M⟩: M is a Turing machine and L(M) = {1011, 001100}},\\nP3 = {⟨M⟩: M is a Turing machine and L(M) is a regular language}.\\nYou are encouraged to verify that Rice’s Theorem indeed implies that each\\nof P1, P2, and P3 is undecidable.\\n5.3.\\nRice’s Theorem\\n171\\n5.3.1\\nProof of Rice’s Theorem\\nThe strategy of the proof is as follows: Assuming that the language P is\\ndecidable, we show that the language\\nHalt = {⟨M, w⟩:\\nM is a Turing machine that terminates on\\nthe input string w}\\nis decidable. This will contradict Theorem 5.1.7.\\nThe assumption that P is decidable implies the existence of a Turing\\nmachine H that decides P. Observe that H takes as input a binary string\\n⟨M⟩encoding a Turing machine M. In order to show that Halt is decidable,\\nwe need a Turing machine that takes as input a binary string ⟨M, w⟩encoding\\na Turing machine M and a binary string w. In the rest of this section, we\\nwill explain how this Turing machine can be obtained.\\nLet M1 be a Turing machine that, for any input string, switches in its\\nﬁrst computation step from its start state to its reject state. In other words,\\nM1 is a Turing machine with L(M1) = ∅. We assume that\\n⟨M1⟩̸∈P.\\n(At the end of the proof, we will consider the case when ⟨M1⟩∈P.) We also\\nchoose a Turing machine M2 such that\\n⟨M2⟩∈P.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nif M terminates\\nthen run M2 on input x;\\nif M2 terminates in the accept state\\nthen terminate in the accept state\\nelse if M2 terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\n172\\nChapter 5.\\nDecidable and Undecidable Languages\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that for any string x,\\nx is accepted by TMw if and only if x is accepted by M2.\\nThus, L(TMw) = L(M2).\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩̸∈Halt.\\nThen it follows from the pseudocode that for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅. In particular, L(TMw) =\\nL(M1).\\nRecall that ⟨M1⟩̸∈P, whereas ⟨M2⟩∈P. Then the following follows from\\nthe third condition in Rice’s Theorem:\\n• If ⟨M, w⟩∈Halt, then ⟨TMw⟩∈P.\\n• If ⟨M, w⟩̸∈Halt, then ⟨TMw⟩̸∈P.\\nThus, we have obtained a connection between the languages P and Halt.\\nThis suggests that we proceed as follows.\\nAssume that the language P is decidable. Let H be a Turing machine\\nthat decides P. Then, for any Turing machine M,\\n• if ⟨M⟩∈P, then H accepts the string ⟨M⟩,\\n• if ⟨M⟩̸∈P, then H rejects the string ⟨M⟩, and\\n• H terminates on any input string.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\n5.4.\\nEnumerability\\n173\\nIt follows from the pseudocode that H′ terminates on any input. We\\nobserve the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that ⟨TMw⟩∈P.\\nSince H decides the language P, it follows that H accepts the string\\n⟨TMw⟩. Therefore, from the pseudocode, H′ accepts the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then we have seen before that ⟨TMw⟩̸∈\\nP. Since H decides the language P, it follows that H rejects (and\\nterminates on) the string ⟨TMw⟩. Therefore, from the pseudocode, H′\\nrejects (and terminates on) the string ⟨M, w⟩.\\nWe have shown that the Turing machine H′ decides the language Halt.\\nThis is a contradiction and, therefore, we conclude that the language P is\\nundecidable.\\nUntil now, we assumed that ⟨M1⟩̸∈P. If ⟨M1⟩∈P, then we repeat the\\nproof with P replaced by its complement P. This revised proof then shows\\nthat P is undecidable. Since for every language L,\\nL is decidable if and only if L is decidable,\\nwe again conclude that P is undecidable.\\n5.4\\nEnumerability\\nWe now come to the last class of languages in this chapter:\\nDeﬁnition 5.4.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We\\nsay that A is enumerable, if there exists a Turing machine M, such that for\\nevery string w ∈Σ∗, the following holds:\\n1. If w ∈A, then the computation of the Turing machine M, on the input\\nstring w, terminates in the accept state.\\n2. If w ̸∈A, then the computation of the Turing machine M, on the input\\nstring w, does not terminate in the accept state. That is, either the\\ncomputation terminates in the reject state or the computation does not\\nterminate.\\n174\\nChapter 5.\\nDecidable and Undecidable Languages\\nIn other words, the language A is enumerable, if there exists an algorithm\\nhaving the following property. If w ∈A, then the algorithm terminates on\\nthe input string w and tells us that w ∈A. On the other hand, if w ̸∈A,\\nthen either (i) the algorithm terminates on the input string w and tells us\\nthat w ̸∈A or (ii) the algorithm does not terminate on the input string w,\\nin which case it does not tell us that w ̸∈A.\\nIn Section 5.5, we will show where the term “enumerable” comes from.\\nThe following theorem follows immediately from Deﬁnitions 5.1.1 and 5.4.1.\\nTheorem 5.4.2 Every decidable language is enumerable.\\nIn the following subsections, we will give some examples of enumerable\\nlanguages.\\n5.4.1\\nHilbert’s problem\\nWe have seen Hilbert’s problem in Section 4.4: Is there an algorithm that\\ndecides, for any given polynomial p with integer coeﬃcients, whether or not\\np has integral roots? If we formulate this problem in terms of languages,\\nthen Hilbert asked whether or not the language\\nHilbert = {⟨p⟩:\\np is a polynomial with integer coeﬃcients\\nthat has an integral root}\\nis decidable. As usual, ⟨p⟩denotes the binary string that forms an encoding\\nof the polynomial p.\\nAs we mentioned in Section 4.4, it was proven by Matiyasevich in 1970\\nthat the language Hilbert is not decidable. We claim, that this language\\nis enumerable.\\nIn order to prove this claim, we have to construct an al-\\ngorithm Hilbert with the following property: For any input polynomial p\\nwith integer coeﬃcients,\\n• if p has an integral root, then algorithm Hilbert will ﬁnd one in a\\nﬁnite amount of time,\\n• if p does not have an integral root, then either algorithm Hilbert ter-\\nminates and tells us that p does not have an integral root, or algorithm\\nHilbert does not terminate.\\n5.4.\\nEnumerability\\n175\\nRecall that Z denotes the set of integers. Algorithm Hilbert does the\\nfollowing, on any input polynomial p with integer coeﬃcients.\\nLet n de-\\nnote the number of variables in p. Algorithm Hilbert tries all elements\\n(x1, x2, . . . , xn) ∈Zn, in a systematic way, and for each such element, it\\ncomputes p(x1, x2, . . . , xn). If this value is zero, then algorithm Hilbert\\nterminates and accepts the input.\\nWe observe the following:\\n• If ⟨p⟩∈Hilbert, then algorithm Hilbert terminates and accepts p,\\nprovided we are able to visit all elements (x1, x2, . . . , xn) ∈Zn in a\\n“systematic way”.\\n• If ⟨p⟩̸∈Hilbert, then p(x1, x2, . . . , xn) ̸= 0 for all (x1, x2, . . . , xn) ∈Zn\\nand, therefore, algorithm Hilbert does not terminate.\\nThese are exactly the requirements for the language Hilbert to be enumerable.\\nIt remains to explain how we visit all elements (x1, x2, . . . , xn) ∈Zn in a\\nsystematic way. For any integer d ≥0, let Hd denote the hypercube in Zn\\nwith sides of length 2d that is centered at the origin. That is, Hd consists\\nof the set of all points (x1, x2, . . . , xn) in Zn, such that −d ≤xi ≤d for all\\n1 ≤i ≤n and there exists at least one index j for which xj = d or xj = −d.\\nWe observe that Hd contains a ﬁnite number of elements. In fact, if d ≥1,\\nthen this number is equal to (2d + 1)n −(2d −1)n. The algorithm will visit\\nall elements (x1, x2, . . . , xn) ∈Zn, in the following order: First, it visits the\\norigin, which is the only element of H0. Then, it visits all elements of H1,\\nfollowed by all elements of H2, etc., etc.\\nTo summarize, we obtain the following algorithm, proving that the lan-\\nguage Hilbert is enumerable:\\n176\\nChapter 5.\\nDecidable and Undecidable Languages\\nAlgorithm Hilbert(⟨p⟩):\\nn := the number of variables in p;\\nd := 0;\\nwhile d ≥0\\ndo for each (x1, x2, . . . , xn) ∈Hd\\ndo R := p(x1, x2, . . . , xn);\\nif R = 0\\nthen terminate and accept\\nendif\\nendfor;\\nd := d + 1\\nendwhile\\nTheorem 5.4.3 The language Hilbert is enumerable.\\n5.4.2\\nThe language ATM\\nWe have shown in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.\\nis undecidable. In this section, we will prove that this language is enumerable.\\nThus, we have to construct an algorithm P having the following property,\\nfor any given input string u:\\n• If\\n– u encodes a Turing machine M and an input string w for M (i.e.,\\nu is in the correct format ⟨M, w⟩) and\\n– ⟨M, w⟩∈ATM (i.e., M accepts w),\\nthen algorithm P terminates in its accept state.\\n• In all other cases, either algorithm P terminates in its reject state, or\\nalgorithm P does not terminate.\\nOn input string u = ⟨M, w⟩, which is in the correct format, algorithm P does\\nthe following:\\n5.5.\\nWhere does the term “enumerable” come from?\\n177\\n1. It simulates the computation of M on input w.\\n2. If M terminates in its accept state, then P terminates in its accept\\nstate.\\n3. If M terminates in its reject state, then P terminates in its reject state.\\n4. If M does not terminate, then P does not terminate.\\nHence, if u = ⟨M, w⟩∈ATM, then M accepts w and, therefore, P accepts\\nu. On the other hand, if u = ⟨M, w⟩̸∈ATM, then M does not accept w. This\\nmeans that, on input w, M either terminates in its reject state or does not\\nterminate. But this implies that, on input u, P either terminates in its reject\\nstate or does not terminate. This proves that algorithm P has the properties\\nthat are needed in order to show that the language ATM is enumerable. We\\nhave proved the following result:\\nTheorem 5.4.4 The language ATM is enumerable.\\n5.5\\nWhere does the term “enumerable” come\\nfrom?\\nIn Deﬁnition 5.4.1, we have deﬁned what it means for a language to be\\nenumerable. In this section, we will see where this term comes from.\\nDeﬁnition 5.5.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. An\\nenumerator for A is a Turing machine E having the following properties:\\n1. Besides the standard features as in Section 4.1, E has a print tape and\\na print state. During its computation, E writes symbols of Σ on the\\nprint tape. Each time, E enters the print state, the current string on\\nthe print tape is sent to the printer and the print tape is made empty.\\n2. At the start of the computation, all tapes are empty and E is in the\\nstart state.\\n3. Every string w in A is sent to the printer at least once.\\n4. Every string w that is not in A is never sent to the printer.\\n178\\nChapter 5.\\nDecidable and Undecidable Languages\\nThus, an enumerator E for A really enumerates all strings in the language\\nA. There is no particular order in which the strings of A are sent to the\\nprinter. Moreover, a string in A may be sent to the printer multiple times.\\nIf the language A is inﬁnite, then the Turing machine E obviously does not\\nterminate; however, every string in A (and only strings in A) will be sent to\\nthe printer at some time during the computation.\\nTo give an example, let A = {0n : n ≥0}. The following Turing machine\\nis an enumerator for A.\\nTuring machine StringsOfZeros:\\nn := 0;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo write 0 on the print tape\\nendfor;\\nenter the print state;\\nn := n + 1\\nendwhile\\nIn the rest of this section, we will prove the following result.\\nTheorem 5.5.2 A language is enumerable if and only if it has an enumer-\\nator.\\nFor the ﬁrst part of the proof, assume that the language A has an enu-\\nmerator E. We construct the following Turing machine M, which takes an\\narbitrary string w as input:\\nTuring machine M(w):\\nrun E; every time E enters the print state:\\nlet v be the string on the print tape;\\nif w = v\\nthen terminate in the accept state\\nendif\\nThe Turing machine M has the following properties:\\n• If w ∈A, then w will be sent to the printer at some time during the\\n5.5.\\nWhere does the term “enumerable” come from?\\n179\\ncomputation of E. It follows from the pseudocode that, on input w,\\nM terminates in the accept state.\\n• If w ̸∈A, then E will never sent w to the printer. It follows from the\\npseudocode that, on input w, M does not terminate.\\nThus, M satisﬁes the conditions in Deﬁnition 5.4.1. We conclude that the\\nlanguage A is enumerable.\\nTo prove the converse, we now assume that A is enumerable. Let M be\\na Turing machine that satisﬁes the conditions in Deﬁnition 5.4.1.\\nWe ﬁx an inﬁnite list\\ns1, s2, s3, . . .\\nof all strings in Σ∗. For example, if Σ = {0, 1}, then we can take this list to\\nbe\\nϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .\\nWe construct the following Turing machine E, which takes the empty\\nstring as input:\\nTuring machine E:\\nn := 1;\\nwhile 1 + 1 = 2\\ndo for i := 1 to n\\ndo run M for n steps on the input string si;\\nif M accepts si within n steps\\nthen write si on the print tape;\\nenter the print state\\nendif\\nendfor;\\nn := n + 1\\nendwhile\\nWe claim that E is an enumerator for the language A. To prove this, it\\nis obvious that any string that is sent to the printer by E belongs to A.\\nIt remains to prove that every string in A will be sent to the printer by E.\\nLet w be a string in A. Then, on input w, the Turing machine M terminates\\nin the accept state. Let m be the number of steps made by M on input w.\\nLet i be the index such that w = si. Deﬁne n = max(m, i). Consider the\\n180\\nChapter 5.\\nDecidable and Undecidable Languages\\nn-th iteration of the while-loop and the i-th iteration of the for-loop. In this\\niteration, M accepts si = w in m ≤n steps and, therefore, w is sent to the\\nprinter.\\n5.6\\nMost languages are not enumerable\\nIn this section, we will prove that most languages are not enumerable. The\\nproof is based on the following two facts:\\n• The set consisting of all enumerable languages is countable; we will\\nprove this in Section 5.6.1.\\n• The set consisting of all languages is not countable; we will prove this\\nin Section 5.6.2.\\n5.6.1\\nThe set of enumerable languages is countable\\nWe deﬁne the set E as\\nE = {A : A ⊆{0, 1}∗is an enumerable language}.\\nIn words, E is the set whose elements are the enumerable languages. Every\\nelement of E is an enumerable language. Hence, every element of the set E\\nis itself a set consisting of strings.\\nLemma 5.6.1 The set E is countable.\\nProof. Let A ⊆{0, 1}∗be an enumerable language. There exists a Turing\\nmachine TA that satisﬁes the conditions in Deﬁnition 5.4.1.\\nThis Turing\\nmachine TA can be uniquely speciﬁed by a string in English. This string can\\nbe converted to a binary string sA. Hence, the binary string sA is a unique\\nencoding of the Turing machine TA.\\nConsider the set\\nS = {sA : A ⊆{0, 1}∗is an enumerable language}.\\nObserve that the function f : E →S, deﬁned by f(A) = sA for each A ∈E,\\nis a bijection. Therefore, the sets E and S have the same size. Hence, in\\norder to prove that the set E is countable, it is suﬃcient to prove that the\\nset S is countable.\\n5.6.\\nMost languages are not enumerable\\n181\\nWhy is the set S countable? For each integer n ≥0, there are exactly 2n\\nbinary strings of length n. Since there are binary strings that are not encod-\\nings of Turing machines, the set S contains at most 2n strings of length n.\\nIn particular, the number of strings in S having length n is ﬁnite. Therefore,\\nwe obtain an inﬁnite list of the elements of S in the following way:\\n• List all strings in S having length 0. (Well, the empty string is not in\\nS, so in this step, nothing happens.)\\n• List all strings in S having length 1.\\n• List all strings in S having length 2.\\n• List all strings in S having length 3.\\n• Etcetera, etcetera.\\nIn this inﬁnite list, every element of S occurs exactly once. Therefore, S is\\ncountable.\\n5.6.2\\nThe set of all languages is not countable\\nWe deﬁne the set L as\\nL = {A : A ⊆{0, 1}∗is a language}.\\nIn words, L is the set consisting of all languages. Every element of the set L\\nis a set consisting of strings.\\nLemma 5.6.2 The set L is not countable.\\nProof. We deﬁne the set B as\\nB = {w : w is an inﬁnite binary sequence}.\\nWe claim that this set is not countable. The proof of this claim is almost\\nidentical to the proof of Theorem 5.2.4. We assume that the set B is count-\\nable. Then there exists a bijection f : N →B. Thus, for each n ∈N, f(n) is\\nan inﬁnite binary sequence. We can write\\nB = {f(1), f(2), f(3), . . .},\\n(5.3)\\n182\\nChapter 5.\\nDecidable and Undecidable Languages\\nwhere every element of B occurs exactly once in the set on the right-hand\\nside.\\nWe deﬁne the inﬁnite binary sequence w = w1w2w3 . . ., where, for each\\ninteger n ≥1,\\nwn =\\n\\x1a 1\\nif the n-th bit of f(n) is 0,\\n0\\nif the n-th bit of f(n) is 1.\\nSince w ∈B, it follows from (5.3) that there is an element n ∈N, such that\\nf(n) = w. Hence, the n-th bits of f(n) and w are equal. But, by deﬁnition,\\nthese n-th bits are not equal. This is a contradiction and, therefore, the set\\nB is not countable.\\nIn the rest of the proof, we will show that the sets L and B have the same\\nsize. Since B is not countable, this will imply that L is not countable.\\nIn order to prove that L and B have the same size, we have to show that\\nthere exists a bijection\\ng : L →B.\\nWe ﬁrst observe that the set {0, 1}∗is countable, because for each integer\\nn ≥0, there are only ﬁnitely many (to be precise, exactly 2n) binary strings\\nof length n. In fact, we can write\\n{0, 1}∗= {ϵ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .}.\\nFor each integer n ≥1, we denote by sn the n-th string in this list. Hence,\\n{0, 1}∗= {s1, s2, s3, . . .}.\\n(5.4)\\nNow we are ready to deﬁne the bijection g : L →B: Let A ∈L, i.e.,\\nA ⊆{0, 1}∗is a language. We deﬁne the inﬁnite binary sequence g(A) as\\nfollows: For each integer n ≥1, the n-th bit of g(A) is equal to\\n\\x1a 1\\nif sn ∈A,\\n0\\nif sn ̸∈A.\\nIn words, the inﬁnite binary sequence g(A) contains a 1 exactly in those\\npositions n for which the string sn in (5.4) is in the language A.\\nTo give an example, assume that A is the language consisting of all binary\\nstrings that start with 0. The following table gives the corresponding inﬁnite\\nbinary sequence g(A) (this sequence is obtained by reading the rightmost\\ncolumn from top to bottom):\\n5.6.\\nMost languages are not enumerable\\n183\\n{0, 1}∗\\nA\\ng(A)\\nϵ\\nnot in A\\n0\\n0\\nin A\\n1\\n1\\nnot in A\\n0\\n00\\nin A\\n1\\n01\\nin A\\n1\\n10\\nnot in A\\n0\\n11\\nnot in A\\n0\\n000\\nin A\\n1\\n001\\nin A\\n1\\n010\\nin A\\n1\\n100\\nnot in A\\n0\\n011\\nin A\\n1\\n101\\nnot in A\\n0\\n110\\nnot in A\\n0\\n111\\nnot in A\\n0\\n...\\n...\\n...\\nThe function g deﬁned above has the following properties:\\n• If A and A′ are two diﬀerent languages in L, then g(A) ̸= g(A′).\\n• For every inﬁnite binary sequence w in B, there exists a language A in\\nL, such that g(A) = w.\\nThis means that the function g is a bijection from L to B.\\n5.6.3\\nThere are languages that are not enumerable\\nWe have proved that the set\\nE = {A : A ⊆{0, 1}∗is an enumerable language}\\nis countable, whereas the set\\nL = {A : A ⊆{0, 1}∗is a language}\\nis not countable. This means that there are “more” languages in L than\\nthere are in E, proving the following result:\\n184\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.6.3 There exist languages that are not enumerable.\\nThe proof given above shows the existence of languages that are not\\nenumerable. However, the proof does not give us a speciﬁc example of a\\nlanguage that is not enumerable. In the next sections, we will see examples\\nof such languages. Before we move on to these examples, we mention the\\ndiﬀerence between being countable and being enumerable:\\n• Any language A is countable, i.e., we can number the elements of A\\nand, thus, write\\nA = {s1, s2, s3, s4, . . .}.\\n• If the language A is enumerable, then, by Theorem 5.5.2, there is an\\nalgorithm that produces this numbering.\\n• If the language A is not enumerable, then, again by Theorem 5.5.2,\\nthere does not exist an algorithm that produces this numbering.\\n5.7\\nThe relationship between decidable and\\nenumerable languages\\nWe know from Theorem 5.4.2 that every decidable language is enumerable.\\nOn the other hand, we know from Theorems 5.1.6 and 5.4.4 that the converse\\nis not true. The following result should not come as a surprise:\\nTheorem 5.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then,\\nA is decidable if and only if both A and its complement A are enumerable.\\nProof. We ﬁrst assume that A is decidable. Then, by Theorem 5.4.2, A\\nis enumerable. Since A is decidable, it is not diﬃcult to see that A is also\\ndecidable. Then, again by Theorem 5.4.2, A is enumerable.\\nTo prove the converse, we assume that both A and A are enumerable.\\nSince A is enumerable, there exists a Turing machine M1, such that for any\\nstring w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M1, on the input string w, terminates\\nin the accept state of M1.\\n5.7.\\nDecidable versus enumerable languages\\n185\\n• If w ̸∈A, then the computation of M1, on the input string w, terminates\\nin the reject state of M1 or does not terminate.\\nSimilarly, since A is enumerable, there exists a Turing machine M2, such that\\nfor any string w ∈Σ∗, the following holds:\\n• If w ∈A, then the computation of M2, on the input string w, terminates\\nin the accept state of M2.\\n• If w ̸∈A, then the computation of M2, on the input string w, terminates\\nin the reject state of M2 or does not terminate.\\nWe construct a two-tape Turing machine M:\\nTwo-tape Turing machine M: For any input string w ∈Σ∗, M\\ndoes the following:\\n• M simulates the computation of M1, on input w, on the ﬁrst\\ntape, and, simultaneously, it simulates the computation of M2,\\non input w, on the second tape.\\n• If the simulation of M1 terminates in the accept state of M1,\\nthen M terminates in its accept state.\\n• If the simulation of M2 terminates in the accept state of M2,\\nthen M terminates in its reject state.\\nObserve the following:\\n• If w ∈A, then M1 terminates in its accept state and, therefore, M\\nterminates in its accept state.\\n• If w ̸∈A, then M2 terminates in its accept state and, therefore, M\\nterminates in its reject state.\\nWe conclude that the Turing machine M accepts all strings in A, and rejects\\nall strings that are not in A. This proves that the language A is decidable.\\nWe now use Theorem 5.7.1 to give examples of languages that are not\\nenumerable:\\n186\\nChapter 5.\\nDecidable and Undecidable Languages\\nTheorem 5.7.2 The language ATM is not enumerable.\\nProof. We know from Theorems 5.4.4 and 5.1.6 that the language ATM is\\nenumerable but not decidable. Combining these facts with Theorem 5.7.1\\nimplies that the language ATM is not enumerable.\\nThe following result can be proved in exactly the same way:\\nTheorem 5.7.3 The language Halt is not enumerable.\\n5.8\\nA language A such that both A and A are\\nnot enumerable\\nIn Theorem 5.7.2, we have seen that the complement of the language ATM\\nis not enumerable.\\nIn Theorem 5.4.4, however, we have shown that the\\nlanguage ATM itself is enumerable. In this section, we consider the language\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\nWe will show the following result:\\nTheorem 5.8.1 Both EQTM and its complement EQTM are not enumer-\\nable.\\n5.8.1\\nEQTM is not enumerable\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We construct\\na new Turing machine TMw that takes as input an arbitrary binary string x:\\nTuring machine TMw(x):\\nrun Turing machine M on input w;\\nterminate in the accept state\\nWe determine the language L(TMw) of this new Turing machine. In other\\nwords, we determine which strings x are accepted by TMw.\\n• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it\\nfollows from the pseudocode that every string x is accepted by TMw.\\nThus, L(TMw) = {0, 1}∗.\\n5.8.\\nBoth A and A not enumerable\\n187\\n• Assume that M does not terminate on input w, i.e., ⟨M, w⟩∈Halt.\\nThen it follows from the pseudocode that, for any string x, TMw does\\nnot terminate on input x. Thus, L(TMw) = ∅.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that rejects every input string;\\nconstruct the Turing machine TMw described above;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. By our choice of M1, we have L(M1) = ∅as well. Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt, i.e., ⟨M, w⟩∈Halt. Then we have seen\\nbefore that L(TMw) ̸= ∅= L(M1). Therefore, on input ⟨M1, TMw⟩, H\\neither terminates in the reject state or does not terminate. It follows\\n188\\nChapter 5.\\nDecidable and Undecidable Languages\\nfrom the pseudocode that, on input ⟨M, w⟩, H′ either terminates in the\\nreject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\n5.8.2\\nEQTM is not enumerable\\nThis proof is symmetric to the one in Section 5.8.1.\\nFor a ﬁxed Turing\\nmachine M and a ﬁxed binary string w, we will use the same Turing machine\\nTMw as in Section 5.8.1.\\nAssume that the language EQTM is enumerable. We will show that Halt\\nis enumerable as well, which will contradict Theorem 5.7.3.\\nSince EQTM is enumerable, there exists a Turing machine H having the\\nfollowing property, for any two Turing machines M1 and M2:\\n• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept\\nstate.\\n• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H either terminates in\\nthe reject state or does not terminate.\\nWe construct a new Turing machine H′ that takes as input an arbitrary\\nstring ⟨M, w⟩, where M is a Turing machine and w is a binary string:\\nTuring machine H′(⟨M, w⟩):\\nconstruct a Turing machine M1 that accepts every input string;\\nconstruct the Turing machine TMw of Section 5.8.1;\\nrun H on input ⟨M1, TMw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse if H terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nWe observe the following:\\nExercises\\n189\\n• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =\\n∅. Thus, by our choice of M1, we have L(TMw) ̸= L(M1). Therefore, H\\naccepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the\\npseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.\\n• Assume that ⟨M, w⟩̸∈Halt. Then L(TMw) = {0, 1}∗= L(M1) and, on\\ninput ⟨M1, TMw⟩, H either terminates in the reject state or does not\\nterminate. It follows from the pseudocode that, on input ⟨M, w⟩, H′\\neither terminates in the reject state or does not terminate.\\nThus, the Turing machine H′ has the properties needed to show that\\nthe language Halt is enumerable. This is a contradiction and, therefore, we\\nconclude that the language EQTM is not enumerable.\\nExercises\\n5.1 Prove that the language\\n{w ∈{0, 1}∗: w is the binary representation of 2n for some n ≥0}\\nis decidable. In other words, construct a Turing machine that gets as input\\nan arbitrary number x ∈N, represented in binary as a string w, and that\\ndecides whether or not x is a power of two.\\n5.2 Let F be the set of all functions f : N →N.\\nProve that F is not\\ncountable.\\n5.3 A function f : N →N is called computable, if there exists a Turing\\nmachine, that gets as input an arbitrary positive integer n, written in binary,\\nand gives as output the value of f(n), again written in binary. This Turing\\nmachine has a ﬁnal state. As soon as the Turing machine enters this ﬁnal\\nstate, the computation terminates, and the output is the binary string that\\nis written on its tape.\\nProve that there exist functions f : N →N that are not computable.\\n5.4 Let n be a ﬁxed positive integer, and let k be the number of bits in the\\nbinary representation of n. (Hence, k = 1 + ⌊log n⌋.) Construct a Turing\\nmachine with one tape, tape alphabet {0, 1, 2}, and exactly k + 1 states\\nq0, q1, . . . , qk, that does the following:\\n190\\nChapter 5.\\nDecidable and Undecidable Languages\\nStart of the computation: The tape is empty, i.e., every cell of the tape\\ncontains 2, and the Turing machine is in the start state q0.\\nEnd of the computation: The tape contains the binary representation of\\nthe integer n, the tape head is on the rightmost bit of the binary represen-\\ntation of n, and the Turing machine is in the ﬁnal state qk.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state qk. As soon as state qk is entered,\\nthe Turing machine terminates.\\n5.5 Give an informal description (in plain English) of a Turing machine\\nwith three tapes, that gets as input the binary representation of an arbitrary\\ninteger m ≥1, and returns as output the unary representation of m.\\nStart of the computation: The ﬁrst tape contains the binary representa-\\ntion of the input m. The other two tapes are empty (i.e., contain only 2s).\\nThe Turing machine is in the start state.\\nEnd of the computation: The third tape contains the unary representation\\nof m, i.e., a string consisting of m many ones. The Turing machine is in the\\nﬁnal state.\\nThe Turing machine in this exercise does not have an accept state or a\\nreject state; instead, it has a ﬁnal state. As soon as this ﬁnal state is entered,\\nthe Turing machine terminates.\\nHint: Use the second tape to maintain a string of ones, whose length is\\na power of two.\\n5.6 In this exercise, you are asked to prove that the busy beaver function\\nBB : N →N is not computable.\\nFor any integer n ≥1, we deﬁne TM n to be the set of all Turing machines\\nM, such that\\n• M has one tape,\\n• M has exactly n states,\\n• the tape alphabet of M is {0, 1, 2}, and\\n• M terminates, when given the empty string ϵ as input.\\nExercises\\n191\\nFor every Turing machine M ∈TM n, we deﬁne f(M) to be the number of\\nones on the tape, after the computation of M, on the empty input string,\\nhas terminated.\\nThe busy beaver function BB : N →N is deﬁned as\\nBB(n) := max{f(M) : M ∈TM n}, for every n ≥1.\\nIn words, BB(n) is the maximum number of ones that any Turing machine\\nwith n states can produce, when given the empty string as input, and as-\\nsuming the Turing machine terminates on this input.\\nProve that the function BB is not computable.\\nHint: Assume that BB is computable. Then there exists a Turing ma-\\nchine M that, for any given n ≥1, computes the value of BB(n). Fix a large\\ninteger n ≥1. Deﬁne (in plain English) a Turing machine that, when given\\nthe empty string as input, terminates and outputs a string consisting of more\\nthan BB(n) many ones. Use Exercises 5.4 and 5.5 to argue that there exists\\nsuch a Turing machine having O(log n) states. Then, if you assume that n\\nis large enough, the number of states is at most n.\\n5.7 Since the set\\nT = {M : M is a Turing machine}\\nis countable, there is an inﬁnite list\\nM1, M2, M3, M4, . . . ,\\nsuch that every Turing machine occurs exactly once in this list.\\nFor any positive integer n, let ⟨n⟩denote the binary representation of n;\\nobserve that ⟨n⟩is a binary string.\\nLet A be the language deﬁned as\\nA = {⟨n⟩:\\nthe Turing machine Mn terminates on the input string ⟨n⟩,\\nand it rejects this string}.\\nProve that the language A is undecidable.\\n5.8 Consider the three languages\\nEmpty = {⟨M⟩: M is a Turing machine for which L(M) = ∅},\\n192\\nChapter 5.\\nDecidable and Undecidable Languages\\nUselessState = {⟨M, q⟩:\\nM is a Turing machine, q is a state of M,\\nfor every input string w, the computation of M on\\ninput w never visits state q},\\nand\\nEQTM = {⟨M1, M2⟩:\\nM1 and M2 are Turing machines\\nand L(M1) = L(M2)}.\\n• Use Rice’s Theorem to show that Empty is undecidable.\\n• Use the ﬁrst part to show that UselessState is undecidable.\\n• Use the ﬁrst part to show that EQTM is undecidable.\\n5.9 Consider the language\\nREGTM = {⟨M⟩: M is a Turing machine whose language L(M) is regular}.\\nUse Rice’s Theorem to prove that REGTM is undecidable.\\n5.10 We have seen in Section 5.1.4 that the language\\nATM = {⟨M, w⟩: M is a Turing machine that accepts w}\\nis undecidable. Consider the language REGTM of Exercise 5.9. The questions\\nbelow will lead you through a proof of the claim that the language REGTM\\nis undecidable.\\nConsider a ﬁxed Turing machine M and a ﬁxed binary string w. We\\nconstruct a new Turing machine TMw that takes as input an arbitrary binary\\nstring x:\\nTuring machine TMw(x):\\nif x = 0n1n for some n ≥0\\nthen terminate in the accept state\\nelse run M on the input string w;\\nif M terminates in the accept state\\nthen terminate in the accept state\\nelse if M terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nendif\\nExercises\\n193\\nAnswer the following two questions:\\n• Assume that M accepts the string w. What is the language L(TMw) of\\nthe new Turing machine TMw?\\n• Assume that M does not accept the string w. What is the language\\nL(TMw) of the new Turing machine TMw?\\nThe goal is to prove that the language REGTM is undecidable. We will\\nprove this by contradiction. Thus, we assume that R is a Turing machine\\nthat decides REGTM. Recall what this means:\\n• If M is a Turing machine whose language is regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the accept state.\\n• If M is a Turing machine whose language is not regular, then R, when\\ngiven ⟨M⟩as input, will terminate in the reject state.\\nWe construct a new Turing machine R′ which takes as input an arbitrary\\nTuring machine M and an arbitrary binary string w:\\nTuring machine R′(⟨M, w⟩):\\nconstruct the Turing machine TMw described above;\\nrun R on the input ⟨TMw⟩;\\nif R terminates in the accept state\\nthen terminate in the accept state\\nelse if R terminates in the reject state\\nthen terminate in the reject state\\nendif\\nendif\\nProve that M accepts w if and only if R′ (when given ⟨M, w⟩as input),\\nterminates in the accept state.\\nNow ﬁnish the proof by arguing that the language REGTM is undecidable.\\n5.11 A Java program P is called a Hello-World-program, if the following is\\ntrue: When given the empty string ϵ as input, P outputs the string Hello\\nWorld and then terminates. (We do not care what P does when the input\\nstring is non-empty.)\\n194\\nChapter 5.\\nDecidable and Undecidable Languages\\nConsider the language\\nHW = {⟨P⟩: P is a Hello-World-program}.\\nThe questions below will lead you through a proof of the claim that the\\nlanguage HW is undecidable.\\nConsider a ﬁxed Java program P and a ﬁxed binary string w. We write\\na new Java program JPw which takes as input an arbitrary binary string x:\\nJava program JPw(x):\\nrun P on the input w;\\nprint Hello World\\n• Argue that P terminates on input w if and only if ⟨JPw⟩∈HW .\\nThe goal is to prove that the language HW is undecidable. We will prove this\\nby contradiction. Thus, we assume that H is a Java program that decides\\nHW . Recall what this means:\\n• If P is a Hello-World-program, then H, when given ⟨P⟩as input, will\\nterminate in the accept state.\\n• If P is not a Hello-World-program, then H, when given ⟨P⟩as input,\\nwill terminate in the reject state.\\nWe write a new Java program H′ which takes as input the binary encoding\\n⟨P, w⟩of an arbitrary Java program P and an arbitrary binary string w:\\nJava program H′(⟨P, w⟩):\\nconstruct the Java program JPw described above;\\nrun H on the input ⟨JPw⟩;\\nif H terminates in the accept state\\nthen terminate in the accept state\\nelse terminate in the reject state\\nendif\\nArgue that the following are true:\\nExercises\\n195\\n• For any input ⟨P, w⟩, H′ terminates.\\n• If P terminates on input w, then H′ (when given ⟨P, w⟩as input),\\nterminates in the accept state.\\n• If P does not terminate on input w, then H′ (when given ⟨P, w⟩as\\ninput), terminates in the reject state.\\nNow ﬁnish the proof by arguing that the language HW is undecidable.\\n5.12 Prove that the language Halt, see Section 5.1.5, is enumerable.\\n5.13 We deﬁne the following language:\\nL = {u\\n:\\nu = ⟨0, M, w⟩for some ⟨M, w⟩∈ATM,\\nor u = ⟨1, M, w⟩for some ⟨M, w⟩̸∈ATM } .\\nProve that neither L nor its complement L is enumerable.\\nHint: There are two ways to solve this exercise. In the ﬁrst solution, (i)\\nyou assume that L is enumerable, and then prove that ATM is decidable, and\\n(ii) you assume that L is enumerable, and then prove that ATM is decidable.\\nIn the second solution, (i) you assume that L is enumerable, and then prove\\nthat ATM is enumerable, and (ii) you assume that L is enumerable, and then\\nprove that ATM is enumerable.\\n196\\nChapter 5.\\nDecidable and Undecidable Languages\\nChapter 6\\nComplexity Theory\\nIn the previous chapters, we have considered the problem of what can be\\ncomputed by Turing machines (i.e., computers) and what cannot be com-\\nputed. We did not, however, take the eﬃciency of the computations into\\naccount. In this chapter, we introduce a classiﬁcation of decidable languages\\nA, based on the running time of the “best” algorithm that decides A. That\\nis, given a decidable language A, we are interested in the “fastest” algorithm\\nthat, for any given string w, decides whether or not w ∈A.\\n6.1\\nThe running time of algorithms\\nLet M be a Turing machine, and let w be an input string for M. We deﬁne\\nthe running time tM(w) of M on input w as\\ntM(w) := the number of computation steps made by M on input w.\\nAs usual, we denote by |w|, the number of symbols in the string w. We\\ndenote the set of non-negative integers by N0.\\nDeﬁnition 6.1.1 Let Σ be an alphabet, let T : N0 →N0 be a function, let\\nA ⊆Σ∗be a decidable language, and let F : Σ∗→Σ∗be a computable\\nfunction.\\n• We say that the Turing machine M decides the language A in time T,\\nif\\ntM(w) ≤T(|w|)\\nfor all strings w in Σ∗.\\n198\\nChapter 6.\\nComplexity Theory\\n• We say that the Turing machine M computes the function F in time\\nT, if\\ntM(w) ≤T(|w|)\\nfor all strings w ∈Σ∗.\\nIn other words, the “running time function” T is a function of the length\\nof the input, which we usually denote by n. For any n, the value of T(n) is\\nan upper bound on the running time of the Turing machine M, on any input\\nstring of length n.\\nTo give an example, consider the Turing machine of Section 4.2.1 that\\ndecides, using one tape, the language consisting of all palindromes. The tape\\nhead of this Turing machine moves from the left to the right, then back to\\nthe left, then to the right again, back to the left, etc. Each time it reaches\\nthe leftmost or rightmost symbol, it deletes this symbol. The running time\\nof this Turing machine, on any input string of length n, is\\nO(1 + 2 + 3 + . . . + n) = O(n2).\\nOn the other hand, the running time of the Turing machine of Section 4.2.2,\\nwhich also decides the palindromes, but using two tapes instead of just one,\\nis O(n).\\nIn Section 4.4, we mentioned that all computation models listed there are\\nequivalent, in the sense that if a language can be decided in one model, it\\ncan be decided in any of the other models. We just saw, however, that the\\nlanguage consisting of all palindromes allows a faster algorithm on a two-\\ntape Turing machine than on one-tape Turing machines. (Even though we\\ndid not prove this, it is true that Ω(n2) is a lower bound on the running\\ntime to decide palindromes on a one-tape Turing machine.) The following\\ntheorem can be proved.\\nTheorem 6.1.2 Let A be a language (resp. let F be a function) that can be\\ndecided (resp. computed) in time T by an algorithm of type M. Then there is\\nan algorithm of type N that decides A (resp. computes F) in time T ′, where\\nM\\nN\\nT ′\\nk-tape Turing machine\\none-tape Turing machine\\nO(T 2)\\none-tape Turing machine\\nJava program\\nO(T 2)\\nJava program\\nk-tape Turing machine\\nO(T 4)\\n6.2.\\nThe complexity class P\\n199\\n6.2\\nThe complexity class P\\nDeﬁnition 6.2.1 We say that algorithm M decides the language A (resp.\\ncomputes the function F) in polynomial time, if there exists an integer k ≥1,\\nsuch that the running time of M is O(nk), for any input string of length n.\\nIt follows from Theorem 6.1.2 that this notion of “polynomial time” does\\nnot depend on the model of computation:\\nTheorem 6.2.2 Consider the models of computation “Java program”, “k-\\ntape Turing machine”, and “one-tape Turing machine”. If a language can\\nbe decided (resp. a function can be computed) in polynomial time in one of\\nthese models, then it can be decided (resp. computed) in polynomial time in\\nall of these models.\\nBecause of this theorem, we can deﬁne the following two complexity\\nclasses:\\nP := {A : the language A is decidable in polynomial time},\\nand\\nFP := {F : the function F is computable in polynomial time}.\\n6.2.1\\nSome examples\\nPalindromes\\nLet Pal be the language\\nPal := {w ∈{a, b}∗: w is a palindrome}.\\nWe have seen that there exists a one-tape Turing machine that decides Pal\\nin O(n2) time. Therefore, Pal ∈P.\\nSome functions in FP\\nThe following functions are in the class FP:\\n• F1 : N0 →N0 deﬁned by F1(x) := x + 1,\\n• F2 : N2\\n0 →N0 deﬁned by F2(x, y) := x + y,\\n• F3 : N2\\n0 →N0 deﬁned by F3(x, y) := xy.\\n200\\nChapter 6.\\nComplexity Theory\\nr\\nb\\nb\\nb\\nr\\nr\\nb\\nG1\\nG2\\nFigure 6.1: The graph G1 is 2-colorable; r stands for red; b stands for blue.\\nThe graph G2 is not 2-colorable.\\nContext-free languages\\nWe have shown in Section 5.1.3 that every context-free language is decid-\\nable. The algorithm presented there, however, does not run in polynomial\\ntime. Using a technique called dynamic programming (which you will learn\\nin COMP 3804), the following result can be shown:\\nTheorem 6.2.3 Let Σ be an alphabet, and let A ⊆Σ∗be a context-free\\nlanguage. Then A ∈P.\\nObserve that, obviously, every language in P is decidable.\\nThe 2-coloring problem\\nLet G be a graph with vertex set V and edge set E.\\nWe say that G is\\n2-colorable, if it is possible to give each vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. only two colors are used to color all vertices.\\nSee Figure 6.1 for two examples. We deﬁne the following language:\\n2Color := {⟨G⟩: the graph G is 2-colorable},\\nwhere ⟨G⟩denotes the binary string that encodes the graph G.\\n6.2.\\nThe complexity class P\\n201\\nWe claim that 2Color ∈P. In order to show this, we have to construct an\\nalgorithm that decides in polynomial time, whether or not any given graph\\nis 2-colorable.\\nLet G be an arbitrary graph with vertex set V = {1, 2, . . . , m}. The edge\\nset of G is given by an adjacency matrix. This matrix, which we denote by\\nE, is a two-dimensional array with m rows and m columns. For all i and j\\nwith 1 ≤i ≤m and 1 ≤j ≤m, we have\\nE(i, j) =\\n\\x1a 1\\nif (i, j) is an edge of G,\\n0\\notherwise.\\nThe length of the input G, i.e., the number of bits needed to specify G, is\\nequal to m2 =: n. We will present an algorithm that decides, in O(n) time,\\nwhether or not the graph G is 2-colorable.\\nThe algorithm uses the colors red and blue. It gives the ﬁrst vertex the\\ncolor red. Then, the algorithm considers all vertices that are connected by\\nan edge to the ﬁrst vertex, and colors them blue. Now the algorithm is done\\nwith the ﬁrst vertex; it marks this ﬁrst vertex.\\nNext, the algorithm chooses a vertex i that already has a color, but that\\nhas not been marked. Then it considers all vertices j that are connected by\\nan edge to i. If j has the same color as i, then the input graph G is not\\n2-colorable. Otherwise, if vertex j does not have a color yet, the algorithm\\ngives j the color that is diﬀerent from i’s color. After having done this for\\nall neighbors j of i, the algorithm is done with vertex i, so it marks i.\\nIt may happen that there is no vertex i that already has a color but that\\nhas not been marked. (In other words, each vertex i that is not marked does\\nnot have a color yet.) In this case, the algorithm chooses an arbitrary vertex\\ni having this property, and colors it red. (This vertex i is the ﬁrst vertex in\\nits connected component that gets a color.)\\nThis procedure is repeated until all vertices of G have been marked.\\nWe now give a formal description of this algorithm. Vertex i has been\\nmarked, if\\n1. i has a color,\\n2. all vertices that are connected by an edge to i have a color, and\\n3. the algorithm has veriﬁed that each vertex that is connected by an edge\\nto i has a color diﬀerent from i’s color.\\n202\\nChapter 6.\\nComplexity Theory\\nThe algorithm uses two arrays f(1 . . . m) and a(1 . . . m), and a variable\\nM. The value of f(i) is equal to the color (red or blue) of vertex i; if i does\\nnot have a color yet, then f(i) = 0. The value of a(i) is equal to\\na(i) =\\n\\x1a 1\\nif vertex i has been marked,\\n0\\notherwise.\\nThe value of M is equal to the number of marked vertices. The algorithm\\nis presented in Figure 6.2. You are encouraged to convince yourself of the\\ncorrectness of this algorithm. That is, you should convince yourself that this\\nalgorithm returns YES if the graph G is 2-colorable, whereas it returns NO\\notherwise.\\nWhat is the running time of this algorithm? First we count the number\\nof iterations of the outer while-loop. In one iteration, either M increases by\\none, or a vertex i, for which a(i) = 0, gets the color red. In the latter case,\\nthe variable M is increased during the next iteration of the outer while-loop.\\nSince, during the entire outer while-loop, the value of M is increased from\\nzero to m, it follows that there are at most 2m iterations of the outer while-\\nloop. (In fact, the number of iterations is equal to m plus the number of\\nconnected components of G minus one.)\\nOne iteration of the outer while-loop takes O(m) time. Hence, the total\\nrunning time of the algorithm is O(m2), which is O(n). Therefore, we have\\nshown that 2Color ∈P.\\n6.3\\nThe complexity class NP\\nBefore we deﬁne the class NP, we consider some examples.\\nExample 6.3.1 Let G be a graph with vertex set V and edge set E, and\\nlet k ≥1 be an integer. We say that G is k-colorable, if it is possible to give\\neach vertex of V a color such that\\n1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and\\n2. at most k diﬀerent colors are used to color all vertices.\\nWe deﬁne the following language:\\nkColor := {⟨G⟩: the graph G is k-colorable}.\\n6.3.\\nThe complexity class NP\\n203\\nAlgorithm 2Color\\nfor i := 1 to m do f(i) := 0; a(i) := 0 endfor;\\nf(1) := red; M := 0;\\nwhile M ̸= m\\ndo (∗Find the minimum index i for which vertex i has not\\nbeen marked, but has a color already ∗)\\nbool := false; i := 1;\\nwhile bool = false and i ≤m\\ndo if a(i) = 0 and f(i) ̸= 0 then bool := true else i := i + 1 endif;\\nendwhile;\\n(∗If bool = true, then i is the smallest index such that\\na(i) = 0 and f(i) ̸= 0.\\nIf bool = false, then for all i, the following holds: if a(i) = 0, then\\nf(i) = 0; because M < m, there is at least one such i. ∗)\\nif bool = true\\nthen for j := 1 to m\\ndo if E(i, j) = 1\\nthen if f(i) = f(j)\\nthen return NO and terminate\\nelse if f(j) = 0\\nthen if f(i) = red\\nthen f(j) := blue\\nelse f(j) := red\\nendif\\nendif\\nendif\\nendif\\nendfor;\\na(i) := 1; M := M + 1;\\nelse i := 1;\\nwhile a(i) ̸= 0 do i := i + 1 endwhile;\\n(∗an unvisited connected component starts at vertex i ∗)\\nf(i) := red\\nendif\\nendwhile;\\nreturn YES\\nFigure 6.2:\\nAn algorithm that decides whether or not a graph G is 2-\\ncolorable.\\nWe have seen that for k = 2, this problem is in the class P. For k ≥3, it\\nis not known whether there exists an algorithm that decides, in polynomial\\ntime, whether or not any given graph is k-colorable. In other words, for\\n204\\nChapter 6.\\nComplexity Theory\\nk ≥3, it is not known whether or not kColor is in the class P.\\nExample 6.3.2 Let G be a graph with vertex set V = {1, 2, . . . , m} and\\nedge set E. A Hamilton cycle is a cycle in G that visits each vertex exactly\\nonce. Formally, it is a sequence v1, v2, . . . , vm of vertices such that\\n1. {v1, v2, . . . , vm} = V , and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nWe deﬁne the following language:\\nHC := {⟨G⟩: the graph G contains a Hamilton cycle}.\\nIt is not known whether or not HC is in the class P.\\nExample 6.3.3 The sum of subset language is deﬁned as follows:\\nSOS := {⟨a1, a2, . . . , am, b⟩:\\nm, a1, a2, . . . , am, b ∈N0 and\\n∃I ⊆{1, 2, . . . , m}, P\\ni∈I ai = b}.\\nAlso in this case, no polynomial-time algorithm is known that decides the\\nlanguage SOS. That is, it is not known whether or not SOS is in the class\\nP.\\nExample 6.3.4 An integer x ≥2 is a prime number, if there are no a, b ∈N\\nsuch that a ̸= x, b ̸= x, and x = ab. Hence, the language of all non-primes\\nthat are greater than or equal to two, is\\nNPrim := {⟨x⟩: x ≥2 and x is not a prime number}.\\nIt is not obvious at all, whether or not NPrim is in the class P. In fact, it\\nwas shown only in 2002 that NPrim is in the class P.\\nObservation 6.3.5 The four languages above have the following in com-\\nmon: If someone gives us a “solution” for any given input, then we can\\neasily, i.e., in polynomial time, verify whether or not this “solution” is a cor-\\nrect solution. Moreover, for any input to each of these four problems, there\\nexists a “solution” whose length is polynomial in the length of the input.\\n6.3.\\nThe complexity class NP\\n205\\nLet us again consider the language kColor. Let G be a graph with vertex\\nset V = {1, 2, . . . , m} and edge set E, and let k be a positive integer. We\\nwant to decide whether or not G is k-colorable. A “solution” is a coloring of\\nthe nodes using at most k diﬀerent colors. That is, a solution is a sequence\\nf1, f2, . . . , fm. (Interpret this as: vertex i receives color fi, 1 ≤i ≤m). This\\nsequence is a correct solution if and only if\\n1. fi ∈{1, 2, . . . , k}, for all i with 1 ≤i ≤m, and\\n2. for all i with 1 ≤i ≤m, and for all j with 1 ≤j ≤m, if (i, j) ∈E,\\nthen fi ̸= fj.\\nIf someone gives us this solution (i.e., the sequence f1, f2, . . . , fm), then\\nwe can verify in polynomial time whether or not these two conditions are\\nsatisﬁed. The length of this solution is O(m log k): for each i, we need about\\nlog k bits to represent fi. Hence, the length of the solution is polynomial in\\nthe length of the input, i.e., it is polynomial in the number of bits needed to\\nrepresent the graph G and the number k.\\nFor the Hamilton cycle problem, a solution consists of a sequence v1,\\nv2, . . . , vm of vertices. This sequence is a correct solution if and only if\\n1. {v1, v2, . . . , vm} = {1, 2, . . . , m} and\\n2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.\\nThese two conditions can be veriﬁed in polynomial time.\\nMoreover, the\\nlength of the solution is polynomial in the length of the input graph.\\nConsider the sum of subset problem. A solution is a sequence c1, c2, . . . , cm.\\nIt is a correct solution if and only if\\n1. ci ∈{0, 1}, for all i with 1 ≤i ≤m, and\\n2. Pm\\ni=1 ciai = b.\\nHence, the set I ⊆{1, 2, . . . , m} in the deﬁnition of SOS is the set of indices\\ni for which ci = 1. Again, these two conditions can be veriﬁed in polynomial\\ntime, and the length of the solution is polynomial in the length of the input.\\nFinally, let us consider the language NPrim. Let x ≥2 be an integer.\\nThe integers a and b form a “solution” for x if and only if\\n206\\nChapter 6.\\nComplexity Theory\\n1. 2 ≤a < x,\\n2. 2 ≤b < x, and\\n3. x = ab.\\nClearly, these three conditions can be veriﬁed in polynomial time. Moreover,\\nthe length of this solution, i.e., the total number of bits in the binary rep-\\nresentations of a and b, is polynomial in the number of bits in the binary\\nrepresentation of x.\\nLanguages having the property that the correctness of a proposed “solu-\\ntion” can be veriﬁed in polynomial time, form the class NP:\\nDeﬁnition 6.3.6 A language A belongs to the class NP, if there exist a\\npolynomial p and a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\nIn words, a language A is in the class NP, if for every string w, w ∈A if\\nand only if the following two conditions are satisﬁed:\\n1. There is a “solution” s, whose length |s| is polynomial in the length of\\nw (i.e., |s| ≤p(|w|), where p is a polynomial).\\n2. In polynomial time, we can verify whether or not s is a correct “solu-\\ntion” for w (i.e., ⟨w, s⟩∈B and B ∈P).\\nHence, the language B can be regarded to be the “veriﬁcation language”:\\nB = {⟨w, s⟩: s is a correct “solution” for w}.\\nWe have given already informal proofs of the fact that the languages\\nkColor, HC, SOS, and NPrim are all contained in the class NP. Below, we\\nformally prove that NPrim ∈NP. To prove this claim, we have to specify\\nthe polynomial p and the language B ∈P. First, we observe that\\nNPrim = {⟨x⟩:\\nthere exist a and b in N such that\\n2 ≤a < x, 2 ≤b < x and x = ab }.\\n(6.1)\\nWe deﬁne the polynomial p by p(n) := n + 2, and the language B as\\nB := {⟨x, a, b⟩: x ≥2, 2 ≤a < x, 2 ≤b < x and x = ab}.\\n6.3.\\nThe complexity class NP\\n207\\nIt is obvious that B ∈P: For any three positive integers x, a, and b, we\\ncan verify in polynomial time whether or not ⟨x, a, b⟩∈B. In order to do\\nthis, we only have to verify whether or not x ≥2, 2 ≤a < x, 2 ≤b < x,\\nand x = ab. If all these four conditions are satisﬁed, then ⟨x, a, b⟩∈B. If at\\nleast one of them is not satisﬁed, then ⟨x, a, b⟩̸∈B.\\nIt remains to show that for all x ∈N:\\n⟨x⟩∈NPrim ⇐⇒∃a, b : |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B.\\n(6.2)\\n(Remember that |⟨x⟩| denotes the number of bits in the binary representation\\nof x; |⟨a, b⟩| denotes the total number of bits of a and b, i.e., |⟨a, b⟩| =\\n|⟨a⟩| + |⟨b⟩|.)\\nLet x ∈NPrim. It follows from (6.1) that there exist a and b in N, such\\nthat 2 ≤a < x, 2 ≤b < x, and x = ab. Since x = ab ≥2 · 2 = 4 ≥2, it\\nfollows that ⟨x, a, b⟩∈B. Hence, it remains to show that\\n|⟨a, b⟩| ≤|⟨x⟩| + 2.\\nThe binary representation of x contains ⌊log x⌋+1 bits, i.e., |⟨x⟩| = ⌊log x⌋+1.\\nWe have\\n|⟨a, b⟩|\\n=\\n|⟨a⟩| + |⟨b⟩|\\n=\\n(⌊log a⌋+ 1) + (⌊log b⌋+ 1)\\n≤\\nlog a + log b + 2\\n=\\nlog ab + 2\\n=\\nlog x + 2\\n≤\\n⌊log x⌋+ 3\\n=\\n|⟨x⟩| + 2.\\nThis proves one direction of (6.2).\\nTo prove the other direction, we assume that there are positive integers\\na and b, such that |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B. Then it follows\\nimmediately from (6.1) and the deﬁnition of the language B, that x ∈NPrim.\\nHence, we have proved the other direction of (6.2). This completes the proof\\nof the claim that\\nNPrim ∈NP.\\n208\\nChapter 6.\\nComplexity Theory\\n6.3.1\\nP is contained in NP\\nIntuitively, it is clear that P ⊆NP, because a language is\\n• in P, if for every string w, it is possible to compute the “solution” s in\\npolynomial time,\\n• in NP, if for every string w and for any given “solution” s, it is possible\\nto verify in polynomial time whether or not s is a correct solution for\\nw (hence, we do not need to compute the solution s ourselves, we only\\nhave to verify it).\\nWe give a formal proof of this:\\nTheorem 6.3.7 P ⊆NP.\\nProof. Let A ∈P. We will prove that A ∈NP. Deﬁne the polynomial p\\nby p(n) := 0 for all n ∈N0, and deﬁne\\nB := {⟨w, ϵ⟩: w ∈A}.\\nSince A ∈P, the language B is also contained in P. It is easy to see that\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) = 0 and ⟨w, s⟩∈B.\\nThis completes the proof.\\n6.3.2\\nDeciding NP-languages in exponential time\\nLet us look again at the deﬁnition of the class NP. Let A be a language in\\nthis class. Then there exist a polynomial p and a language B ∈P, such that\\nfor all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.3)\\nHow do we decide whether or not any given string w belongs to the language\\nA? If we can ﬁnd a string s that satisﬁes the right-hand side in (6.3), then\\nwe know that w ∈A. On the other hand, if there is no such string s, then\\nw ̸∈A. How much time do we need to decide whether or not such a string s\\nexists?\\n6.3.\\nThe complexity class NP\\n209\\nAlgorithm NonPrime\\n(∗decides whether or not ⟨x⟩∈NPrim ∗)\\nif x = 0 or x = 1 or x = 2\\nthen return NO and terminate\\nelse a := 2;\\nwhile a < x\\ndo if x mod a = 0\\nthen return YES and terminate\\nelse a := a + 1\\nendif\\nendwhile;\\nreturn NO\\nendif\\nFigure 6.3: An algorithm that decides whether or not a number x is contained\\nin the language NPrim.\\nFor example, let A be the language\\nNPrim = {⟨x⟩: x ≥2 and x is not a prime number},\\nand let x ∈N. The algorithm in Figure 6.3 decides whether or not ⟨x⟩∈\\nNPrim.\\nIt is clear that this algorithm is correct. Let n be the length of the binary\\nrepresentation of x, i.e., n = ⌊log x⌋+ 1. If x > 2 and x is a prime number,\\nthen the while-loop makes x−2 iterations. Therefore, since n−1 = ⌊log x⌋≤\\nlog x, the running time of this algorithm is at least\\nx −2 ≥2n−1 −2,\\ni.e., it is at least exponential in the length of the input.\\nWe now prove that every language in NP can be decided in exponential\\ntime. Let A be an arbitrary language in NP. Let p be the polynomial, and\\nlet B ∈P be the language such that for all strings w,\\nw ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.\\n(6.4)\\nThe following algorithm decides, for any given string w, whether or not\\nw ∈A. It does so by looking at all possible strings s for which |s| ≤p(|w|):\\n210\\nChapter 6.\\nComplexity Theory\\nfor all s with |s| ≤p(|w|)\\ndo if ⟨w, s⟩∈B\\nthen return YES and terminate\\nendif\\nendfor;\\nreturn NO\\nThe correctness of the algorithm follows from (6.4). What is the running\\ntime? We assume that w and s are represented as binary strings. Let n be\\nthe length of the input, i.e., n = |w|.\\nHow many binary strings s are there whose length is at most p(|w|)? Any\\nsuch s can be described by a sequence of length p(|w|) = p(n), consisting of\\nthe symbols “0”, “1”, and the blank symbol. Hence, there are at most 3p(n)\\nmany binary strings s with |s| ≤p(n). Therefore, the for-loop makes at most\\n3p(n) iterations.\\nSince B ∈P, there is an algorithm and a polynomial q, such that this\\nalgorithm, when given any input string z, decides in q(|z|) time, whether or\\nnot z ∈B. This input z has the form ⟨w, s⟩, and we have\\n|z| = |w| + |s| ≤|w| + p(|w|) = n + p(n).\\nIt follows that the total running time of our algorithm that decides whether\\nor not w ∈A, is bounded from above by\\n3p(n) · q(n + p(n))\\n≤\\n22p(n) · q(n + p(n))\\n≤\\n22p(n) · 2q(n+p(n))\\n=\\n2p′(n),\\nwhere p′ is the polynomial that is deﬁned by p′(n) := 2p(n) + q(n + p(n)).\\nIf we deﬁne the class EXP as\\nEXP :=\\n{A :\\nthere exists a polynomial p, such that A can be\\ndecided in time 2p(n) } ,\\nthen we have proved the following theorem.\\nTheorem 6.3.8 NP ⊆EXP.\\n6.4.\\nNon-deterministic algorithms\\n211\\n6.3.3\\nSummary\\n• P ⊆NP. It is not known whether P is a proper subclass of NP, or\\nwhether P = NP. This is one of the most important open problems in\\ncomputer science. If you can solve this problem, then you will get one\\nmillion dollars; not from us, but from the Clay Mathematics Institute,\\nsee\\nhttp://www.claymath.org/prizeproblems/index.htm\\nMost people believe that P is a proper subclass of NP.\\n• NP ⊆EXP, i.e., each language in NP can be decided in exponential\\ntime. It is not known whether NP is a proper subclass of EXP, or\\nwhether NP = EXP.\\n• It follows from P ⊆NP and NP ⊆EXP, that P ⊆EXP. It can\\nbe shown that P is a proper subset of EXP, i.e., there exist languages\\nthat can be decided in exponential time, but that cannot be decided in\\npolynomial time.\\n• P is the class of those languages that can be decided eﬃciently, i.e., in\\npolynomial time. Sets that are not in P, are not eﬃciently decidable.\\n6.4\\nNon-deterministic algorithms\\nThe abbreviation NP stands for Non-deterministic Polynomial time. The al-\\ngorithms that we have considered so far are deterministic, which means that\\nat any time during the computation, the next computation step is uniquely\\ndetermined. In a non-deterministic algorithm, there are one or more possi-\\nbilities for being the next computation step, and the algorithm chooses one\\nof them.\\nTo give an example, we consider the language SOS, see Example 6.3.3.\\nLet m, a1, a2, . . . , am, and b be elements of N0. Then\\n⟨a1, a2, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, c2, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciai = b.\\nThe following non-deterministic algorithm decides the language SOS:\\n212\\nChapter 6.\\nComplexity Theory\\nAlgorithm SOS(m, a1, a2, . . . , am, b):\\ns := 0;\\nfor i := 1 to m\\ndo s := s | s := s + ai\\nendfor;\\nif s = b\\nthen return YES\\nelse return NO\\nendif\\nThe line\\ns := s | s := s + ai\\nmeans that either the instruction “s := s” or the instruction “s := s + ai” is\\nexecuted.\\nLet us assume that ⟨a1, a2, . . . , am, b⟩∈SOS. Then there are c1, c2, . . . , cm ∈\\n{0, 1} such that Pm\\ni=1 ciai = b. Assume our algorithm does the following, for\\neach i with 1 ≤i ≤m: In the i-th iteration,\\n• if ci = 0, then it executes the instruction “s := s”,\\n• if ci = 1, then it executes the instruction “s := s + ai”.\\nThen after the for-loop, we have s = b, and the algorithm returns YES;\\nhence, the algorithm has correctly found out that ⟨a1, a2, . . . , am, b⟩∈SOS.\\nIn other words, in this case, there exists at least one accepting computation.\\nOn the other hand, if ⟨a1, a2, . . . , am, b⟩̸∈SOS, then the algorithm always\\nreturns NO, no matter which of the two instructions is executed in each\\niteration of the for-loop. In this case, there is no accepting computation.\\nDeﬁnition 6.4.1 Let M be a non-deterministic algorithm. We say that M\\naccepts a string w, if there exists at least one computation that, on input w,\\nreturns YES.\\nDeﬁnition 6.4.2 We say that a non-deterministic algorithm M decides a\\nlanguage A in time T, if for every string w, the following holds: w ∈A if\\nand only if there exists at least one computation that, on input w, returns\\nYES and that takes at most T(|w|) time.\\n6.5.\\nNP-complete languages\\n213\\nThe non-deterministic algorithm that we have seen above decides the\\nlanguage SOS in linear time: Let ⟨a1, a2, . . . , am, b⟩∈SOS, and let n be the\\nlength of this input. Then\\nn = |⟨a1⟩| + |⟨a2⟩| + . . . + |⟨am⟩| + |⟨b⟩| ≥m.\\nFor this input, there is a computation that returns YES and that takes\\nO(m) = O(n) time.\\nAs in Section 6.2, we deﬁne the notion of “polynomial time” for non-\\ndeterministic algorithms. The following theorem relates this notion to the\\nclass NP that we deﬁned in Deﬁnition 6.3.6.\\nTheorem 6.4.3 A language A is in the class NP if and only if there exists\\na non-deterministic Turing machine (or Java program) that decides A in\\npolynomial time.\\n6.5\\nNP-complete languages\\nLanguages in the class P are considered easy, i.e., they can be decided in\\npolynomial time. People believe (but cannot prove) that P is a proper sub-\\nclass of NP. If this is true, then there are languages in NP that are hard,\\ni.e., cannot be decided in polynomial time.\\nIntuition tells us that if P ̸= NP, then the hardest languages in NP are\\nnot contained in P. These languages are called NP-complete. In this section,\\nwe will give a formal deﬁnition of this concept.\\nIf we want to talk about the “hardest” languages in NP, then we have to\\nbe able to compare two languages according to their “diﬃculty”. The idea is\\nas follows: We say that a language B is “at least as hard” as a language A,\\nif the following holds: If B can be decided in polynomial time, then A can\\nalso be decided in polynomial time.\\nDeﬁnition 6.5.1 Let A ⊆{0, 1}∗and B ⊆{0, 1}∗be languages. We say\\nthat A ≤P B, if there exists a function\\nf : {0, 1}∗→{0, 1}∗\\nsuch that\\n1. f ∈FP and\\n214\\nChapter 6.\\nComplexity Theory\\n2. for all strings w in {0, 1}∗,\\nw ∈A ⇐⇒f(w) ∈B.\\nIf A ≤P B, then we also say that “B is at least as hard as A”, or “A is\\npolynomial-time reducible to B”.\\nWe ﬁrst show that this formal deﬁnition is in accordance with the intuitive\\ndeﬁnition given above.\\nTheorem 6.5.2 Let A and B be languages such that B ∈P and A ≤P B.\\nThen A ∈P.\\nProof. Let f : {0, 1}∗→{0, 1}∗be the function in FP for which\\nw ∈A ⇐⇒f(w) ∈B.\\n(6.5)\\nThe following algorithm decides whether or not any given binary string w is\\nin A:\\nu := f(w);\\nif u ∈B\\nthen return YES\\nelse return NO\\nendif\\nThe correctness of this algorithm follows immediately from (6.5). So it\\nremains to show that the running time is polynomial in the length of the\\ninput string w.\\nSince f ∈FP, there exists a polynomial p such that the function f can\\nbe computed in time p. Similarly, since B ∈P, there exists a polynomial q,\\nsuch that the language B can be decided in time q.\\nLet n be the length of the input string w, i.e., n = |w|. Then the length\\nof the string u is less than or equal to p(|w|) = p(n). (Why?) Therefore, the\\nrunning time of our algorithm is bounded from above by\\np(|w|) + q(|u|) ≤p(n) + q(p(n)).\\nSince the function p′, deﬁned by p′(n) := p(n)+q(p(n)), is a polynomial, this\\nproves that A ∈P.\\nThe following theorem states that the relation ≤P is reﬂexive and tran-\\nsitive. We leave the proof as an exercise.\\n6.5.\\nNP-complete languages\\n215\\nTheorem 6.5.3 Let A, B, and C be languages. Then\\n1. A ≤P A, and\\n2. if A ≤P B and B ≤P C, then A ≤P C.\\nWe next show that the languages in P are the easiest languages in NP:\\nTheorem 6.5.4 Let A be a language in P, and let B be an arbitrary lan-\\nguage such that B ̸= ∅and B ̸= {0, 1}∗. Then A ≤P B.\\nProof. We choose two strings u and v in {0, 1}∗, such that u ∈B and v ̸∈B.\\n(Observe that this is possible.) Deﬁne the function f : {0, 1}∗→{0, 1}∗by\\nf(w) :=\\n\\x1a u\\nif w ∈A,\\nv\\nif w ̸∈A.\\nThen it is clear that for any binary string w,\\nw ∈A ⇐⇒f(w) ∈B.\\nSince A ∈P, the function f can be computed in polynomial time, i.e.,\\nf ∈FP.\\n6.5.1\\nTwo examples of reductions\\nSum of subsets and knapsacks\\nWe start with a simple reduction. Consider the two languages\\nSOS := {⟨a1, . . . , am, b⟩:\\nm, a1, . . . , am, b ∈N0 and there exist\\nc1, . . . , cm ∈{0, 1}, such that Pm\\ni=1 ciai = b}\\nand\\nKS\\n:=\\n{⟨w1, . . . , wm, k1, . . . , km, W, K⟩:\\nm, w1, . . . , wm, k1, . . . , km, W, K ∈N0\\nand there exist c1, . . . , cm ∈{0, 1},\\nsuch that Pm\\ni=1 ciwi ≤W and Pm\\ni=1 ciki ≥K}.\\nThe notation KS stands for knapsack: We have m pieces of food. The\\ni-th piece has weight wi and contains ki calories. We want to decide whether\\nor not we can ﬁll our knapsack with a subset of the pieces of food such that\\nthe total weight is at most W, and the total amount of calories is at least K.\\n216\\nChapter 6.\\nComplexity Theory\\nTheorem 6.5.5 SOS ≤P KS.\\nProof. Let us ﬁrst see what we have to show. According to Deﬁnition 6.5.1,\\nwe need a function f ∈FP, that maps input strings for SOS to input strings\\nfor KS, in such a way that\\n⟨a1, . . . , am, b⟩∈SOS ⇐⇒f(⟨a1, . . . , am, b⟩) ∈KS.\\nIn order for f(⟨a1, . . . , am, b⟩) to be an input string for KS, this function\\nvalue has to be of the form\\nf(⟨a1, . . . , am, b⟩) = ⟨w1, . . . , wm, k1, . . . , km, W, K⟩.\\nWe deﬁne\\nf(⟨a1, . . . , am, b⟩) := ⟨a1, . . . , am, a1, . . . , am, b, b⟩.\\nIt is clear that f ∈FP. We have\\n⟨a1, . . . , am, b⟩∈SOS\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai = b\\n⇐⇒\\nthere exist c1, . . . , cm ∈{0, 1} such that Pm\\ni=1 ciai ≤b and Pm\\ni=1 ciai ≥b\\n⇐⇒\\n⟨a1, . . . , am, a1, . . . , am, b, b⟩∈KS\\n⇐⇒\\nf(⟨a1, . . . , am, b⟩) ∈KS.\\nCliques and Boolean formulas\\nWe will deﬁne two languages A = 3SAT and B = Clique that have, at\\nﬁrst sight, nothing to do with each other. Then we show that, nevertheless,\\nA ≤P B.\\nLet G be a graph with vertex set V and edge set E. A subset V ′ of V is\\ncalled a clique, if each pair of distinct vertices in V ′ is connected by an edge\\nin E. We deﬁne the following language:\\nClique := {⟨G, k⟩: k ∈N and G has a clique with k vertices}.\\nWe encourage you to prove the following claim:\\n6.5.\\nNP-complete languages\\n217\\nTheorem 6.5.6 Clique ∈NP.\\nNext we consider Boolean formulas ϕ, with variables x1, x2, . . . , xm, hav-\\ning the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.6)\\nwhere each Ci, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nEach ℓi\\na is either a variable or the negation of a variable. An example of such\\na formula is\\nϕ = (x1 ∨¬x1 ∨¬x2) ∧(x3 ∨x2 ∨x4) ∧(¬x1 ∨¬x3 ∨¬x4).\\nA formula ϕ of the form (6.6) is said to be satisﬁable, if there exists a truth-\\nvalue in {0, 1} for each of the variables x1, x2, . . . , xm, such that the entire\\nformula ϕ is true. Our example formula is satisﬁable: If we take x1 = 0 and\\nx2 = 1, and give x3 and x4 an arbitrary value, then\\nϕ = (0 ∨1 ∨0) ∧(x3 ∨1 ∨x4) ∧(1 ∨¬x3 ∨¬x4) = 1.\\nWe deﬁne the following language:\\n3SAT := {⟨ϕ⟩: ϕ is of the form (6.6) and is satisﬁable}.\\nAgain, we encourage you to prove the following claim:\\nTheorem 6.5.7 3SAT ∈NP.\\nObserve that the elements of Clique (which are pairs consisting of a graph\\nand a positive integer) are completely diﬀerent from the elements of 3SAT\\n(which are Boolean formulas). We will show that 3SAT ≤P Clique. Recall\\nthat this means the following: If the language Clique can be decided in\\npolynomial time, then the language 3SAT can also be decided in polynomial\\ntime. In other words, any polynomial-time algorithm that decides Clique can\\nbe converted to a polynomial-time algorithm that decides 3SAT.\\nTheorem 6.5.8 3SAT ≤P Clique.\\n218\\nChapter 6.\\nComplexity Theory\\nProof. We have to show that there exists a function f ∈FP, that maps\\ninput strings for 3SAT to input strings for Clique, such that for each Boolean\\nformula ϕ that is of the form (6.6),\\n⟨ϕ⟩∈3SAT ⇐⇒f(⟨ϕ⟩) ∈Clique.\\nThe function f maps the binary string encoding an arbitrary Boolean formula\\nϕ to a binary string encoding a pair (G, k), where G is a graph and k is a\\npositive integer. We have to deﬁne this function f in such a way that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\nLet\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an arbitrary Boolean formula in the variables x1, x2, . . . , xm, where each\\nCi, 1 ≤i ≤k, is of the form\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3.\\nRemember that each ℓi\\na is either a variable or the negation of a variable.\\nThe formula ϕ is mapped to the pair (G, k), where the vertex set V and\\nthe edge set E of the graph G are deﬁned as follows:\\n• V = {v1\\n1, v1\\n2, v1\\n3, . . . , vk\\n1, vk\\n2, vk\\n3}. The idea is that each vertex vi\\na corre-\\nsponds to one term ℓi\\na.\\n• The pair (vi\\na, vj\\nb) of vertices form an edge in E if and only if\\n– i ̸= j and\\n– ℓi\\na is not the negation of ℓj\\nb.\\nTo give an example, let ϕ be the Boolean formula\\nϕ = (x1 ∨¬x2 ∨¬x3) ∧(¬x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3),\\n(6.7)\\ni.e., k = 3, C1 = x1 ∨¬x2 ∨¬x3, C2 = ¬x1 ∨x2 ∨x3, and C3 = x1 ∨x2 ∨x3.\\nThe graph G that corresponds to this formula is given in Figure 6.4.\\nIt is not diﬃcult to see that the function f can be computed in polynomial\\ntime. So it remains to prove that\\nϕ is satisﬁable ⇐⇒G has a clique with k vertices.\\n(6.8)\\n6.5.\\nNP-complete languages\\n219\\n¬x2\\n¬x3\\nx1\\n¬x1\\nx2\\nx3\\nx1\\nx2\\nx3\\nFigure 6.4: The formula ϕ in (6.7) is mapped to this graph. The vertices on\\nthe top represent C1; the vertices on the left represent C2; the vertices on\\nthe right represent C3.\\nTo prove this, we ﬁrst assume that the formula\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nis satisﬁable. Then there exists a truth-value in {0, 1} for each of the variables\\nx1, x2, . . . , xm, such that the entire formula ϕ is true. Hence, for each i with\\n1 ≤i ≤k, there is at least one term ℓi\\na in\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n3\\nthat is true (i.e., has value 1).\\nLet V ′ be the set of vertices obtained by choosing for each i, 1 ≤i ≤k,\\nexactly one vertex vi\\na such that ℓi\\na has value 1.\\nIt is clear that V ′ contains exactly k vertices. We claim that this set is\\na clique in G. To prove this claim, let vi\\na and vj\\nb be two distinct vertices in\\nV ′. It follows from the deﬁnition of V ′ that i ̸= j and ℓi\\na = ℓj\\nb = 1. Hence,\\nℓi\\na is not the negation of ℓj\\nb. But this means that the vertices vi\\na and vj\\nb are\\nconnected by an edge in G.\\nThis proves one direction of (6.8). To prove the other direction, we assume\\nthat the graph G contains a clique V ′ with k vertices.\\n220\\nChapter 6.\\nComplexity Theory\\nThe vertices of G consist of k groups, where each group contains exactly\\nthree vertices. Since vertices within the same group are not connected by\\nedges, the clique V ′ contains exactly one vertex from each group. Hence, for\\neach i with 1 ≤i ≤k, there is exactly one a, such that vi\\na ∈V ′. Consider\\nthe corresponding term ℓi\\na. We know that this term is either a variable or\\nthe negation of a variable, i.e., ℓi\\na is either of the form xj or of the form ¬xj.\\nIf ℓi\\na = xj, then we give xj the truth-value 1. Otherwise, we have ℓi\\na = ¬xj,\\nin which case we give xj the truth-value 0. Since V ′ is a clique, each variable\\ngets at most one truth-value. If a variable has no truth-value yet, then we\\ngive it an arbitrary truth-value.\\nIf we substitute these truth-values into ϕ, then the entire formula has\\nvalue 1. Hence, ϕ is satisﬁable.\\nIn order to get a better understanding of this proof, you should verify the\\nproof for the formula ϕ in (6.7) and the graph G in Figure 6.4.\\n6.5.2\\nDeﬁnition of NP-completeness\\nReductions, as deﬁned in Deﬁnition 6.5.1, allow us to compare two language\\naccording to their diﬃculty. A language B in NP is called NP-complete,\\nif B belongs to the most diﬃcult languages in NP; in other words, B is at\\nleast as hard as any other language in NP.\\nDeﬁnition 6.5.9 Let B ⊆{0, 1}∗be a language. We say that B is NP-\\ncomplete, if\\n1. B ∈NP and\\n2. A ≤P B, for every language A in NP.\\nTheorem 6.5.10 Let B be an NP-complete language. Then\\nB ∈P ⇐⇒P = NP.\\nProof. Intuitively, this theorem should be true: If the language B is in P,\\nthen B is an easy language. On the other hand, since B is NP-complete,\\nit belongs to the most diﬃcult languages in NP. Hence, the most diﬃcult\\nlanguage in NP is easy. But then all languages in NP must be easy, i.e.,\\nP = NP.\\n6.5.\\nNP-complete languages\\n221\\nWe give a formal proof. Let us ﬁrst assume that B ∈P. We already\\nknow that P ⊆NP. Hence, it remains to show that NP ⊆P. Let A be an\\narbitrary language in NP. Since B is NP-complete, we have A ≤P B. Then,\\nby Theorem 6.5.2, we have A ∈P.\\nTo prove the converse, assume that P = NP. Since B ∈NP, it follows\\nimmediately that B ∈P.\\nTheorem 6.5.11 Let B and C be languages, such that C ∈NP and B ≤P\\nC. If B is NP-complete, then C is also NP-complete.\\nProof. First, we give an intuitive explanation of the claim: By assumption,\\nB belongs to the most diﬃcult languages in NP, and C is at least as hard as\\nB. Since C ∈NP, it follows that C belongs to the most diﬃcult languages\\nin NP. Hence, C is NP-complete.\\nTo give a formal proof, we have to show that A ≤P C, for all languages A\\nin NP. Let A be an arbitrary language in NP. Since B is NP-complete, we\\nhave A ≤P B. Since B ≤P C, it follows from Theorem 6.5.3, that A ≤P C.\\nTherefore, C is NP-complete.\\nTheorem 6.5.11 can be used to prove the NP-completeness of languages:\\nLet C be a language, and assume that we want to prove that C is NP-\\ncomplete. We can do this in the following way:\\n1. We ﬁrst prove that C ∈NP.\\n2. Then we ﬁnd a language B that looks “similar” to C, and for which\\nwe already know that it is NP-complete.\\n3. Finally, we prove that B ≤P C.\\n4. Then, Theorem 6.5.11 tells us that C is NP-complete.\\nOf course, this leads to the question “How do we know that the language\\nB is NP-complete?” In order to apply Theorem 6.5.11, we need a “ﬁrst” NP-\\ncomplete language; the NP-completeness of this language must be proven\\nusing Deﬁnition 6.5.9.\\nObserve that it is not clear at all that there exist NP-complete languages!\\nFor example, consider the language 3SAT. If we want to use Deﬁnition 6.5.9\\nto show that this language is NP-complete, then we have to show that\\n222\\nChapter 6.\\nComplexity Theory\\n• 3SAT ∈NP. We know from Theorem 6.5.7 that this is true.\\n• A ≤P 3SAT, for every language A ∈NP. Hence, we have to show this\\nfor languages A such as kColor, HC, SOS, NPrim, KS, Clique, and\\nfor inﬁnitely many other languages.\\nIn 1971, Cook has exactly done this: He showed that the language 3SAT\\nis NP-complete. Since his proof is rather technical, we will prove the NP-\\ncompleteness of another language.\\n6.5.3\\nAn NP-complete domino game\\nWe are given a ﬁnite collection of tile types. For each such type, there are\\narbitrarily many tiles of this type. A tile is a square that is partitioned into\\nfour triangles. Each of these triangles contains a symbol that belongs to a\\nﬁnite alphabet Σ. Hence, a tile looks as follows:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\na\\nb\\nc\\nd\\nWe are also given a square frame, consisting of cells. Each cell has the same\\nsize as a tile, and contains a symbol of Σ.\\nThe problem is to decide whether or not this domino game has a solution.\\nThat is, can we completely ﬁll the frame with tiles such that\\n• for any two neighboring tiles s and s′, the two triangles of s and s′ that\\ntouch each other contain the same symbol, and\\n• each triangle that touches the frame contains the same symbol as the\\ncell of the frame that is touched by this triangle.\\nThere is one ﬁnal restriction: The orientation of the tiles is ﬁxed, they cannot\\nbe rotated.\\nLet us give a formal deﬁnition of this problem. We assume that the sym-\\nbols belong to the ﬁnite alphabet Σ = {0, 1}m, i.e., each symbol is encoded\\nas a bit-string of length m. Then, a tile type can be encoded as a tuple of\\nfour bit-strings, i.e., as an element of Σ4. A frame consisting of t rows and t\\ncolumns can be encoded as a string in Σ4t.\\n6.5.\\nNP-complete languages\\n223\\nWe denote the language of all solvable domino games by Domino:\\nDomino\\n:=\\n{⟨m, k, t, R, T1, . . . , Tk⟩:\\nm ≥1, k ≥1, t ≥1, R ∈Σ4t, Ti ∈Σ4, 1 ≤i ≤k,\\nframe R can be ﬁlled using tiles of types\\nT1, . . . , Tk.}\\nWe will prove the following theorem.\\nTheorem 6.5.12 The language Domino is NP-complete.\\nProof. It is clear that Domino ∈NP: A solution consists of a t × t matrix,\\nin which the (i, j)-entry indicates the type of the tile that occupies position\\n(i, j) in the frame. The number of bits needed to specify such a solution is\\npolynomial in the length of the input. Moreover, we can verify in polynomial\\ntime whether or not any given “solution” is correct.\\nIt remains to show that\\nA ≤P Domino, for every language A in NP.\\nLet A be an arbitrary language in NP. Then there exist a polynomial p and\\na non-deterministic Turing machine M, that decides the language A in time\\np. We may assume that this Turing machine has only one tape.\\nOn input w = a1a2 . . . an, the Turing machine M starts in the start state\\nz0, with its tape head on the cell containing the symbol a1. We may assume\\nthat during the entire computation, the tape head never moves to the left of\\nthis initial cell. Hence, the entire computation “takes place” in and to the\\nright of the initial cell. We know that\\nw ∈A\\n⇐⇒\\non input w, there exists an accepting computation\\nthat makes at most p(n) computation steps.\\nAt the end of such an accepting computation, the tape only contains the\\nsymbol 1, which we may assume to be in the initial cell, and M is in the ﬁnal\\nstate z1. In this case, we may assume that the accepting computation makes\\nexactly p(n) computation steps. (If this is not the case, then we extend the\\ncomputation using the instruction z11 →z11N.)\\nWe need one more technical detail: We may assume that za →z′bR and\\nza′ →z′′b′L are not both instructions of M. Hence, the state of the Turing\\nmachine uniquely determines the direction in which the tape head moves.\\n224\\nChapter 6.\\nComplexity Theory\\nWe have to deﬁne a domino game, that depends on the input string w\\nand the Turing machine M, such that\\nw ∈A ⇐⇒this domino game is solvable.\\nThe idea is to encode an accepting computation of the Turing machine M as\\na solution of the domino game. In order to do this, we use a frame in which\\neach row corresponds to one computation step. This frame consists of p(n)\\nrows. Since an accepting computation makes exactly p(n) computation steps,\\nand since the tape head never moves to the left of the initial cell, this tape\\nhead can visit only p(n) cells. Therefore, our frame will have p(n) columns.\\nThe domino game will use the following tile types:\\n1. For each symbol a in the alphabet of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\n#\\na\\nIntuition: Before and after the computation step, the tape head is not\\non this cell.\\n2. For each instruction za →z′bR of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\nz′\\nb\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the right.\\n3. For each instruction za →z′bL of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz′\\n(z, a)\\n#\\nb\\n6.5.\\nNP-complete languages\\n225\\nIntuition: Before the computation step, the tape head is on this cell;\\nthe tape head makes one step to the left.\\n4. For each instruction za →z′bN of the Turing machine M:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\n(z, a)\\n#\\n(z′, b)\\nIntuition: Before and after the computation step, the tape head is on\\nthis cell.\\n5. For each state z and for each symbol a in the alphabet of the Turing\\nmachine M, there are two tile types:\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\nz\\na\\n#\\n(z, a)\\n\\x00\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@@\\n#\\na\\nz\\n(z, a)\\nIntuition: The leftmost tile indicates that the tape head enters this cell\\nfrom the left; the righmost tile indicates that the tape head enters this\\ncell from the right.\\nThis speciﬁes all tile types. The p(n) × p(n) frame is given in Figure 6.5.\\nThe top row corresponds to the start of the computation, whereas the bottom\\nrow corresponds to the end of the computation. The left and right columns\\ncorrespond to the part of the tape in which the tape head can move.\\nThe encodings of these tile types and the frame can be computed in\\npolynomial time.\\nIt can be shown that, for any input string w, any accepting computation\\nof length p(n) of the Turing machine M can be encoded as a solution of\\nthis domino game. Conversely, any solution of this domino game can be\\n“translated” to an accepting computation of length p(n) of M, on input\\nstring w. Hence, the following holds.\\nw ∈A\\n⇐⇒\\nthere exists an accepting computation that makes\\np(n) computation steps\\n⇐⇒\\nthe domino game is solvable.\\n226\\nChapter 6.\\nComplexity Theory\\n(z0, a1)\\na2\\n. . .\\nan\\n✷\\n. . .\\n✷\\n#\\n#\\n#\\n#\\n#\\n...\\n#\\n...\\n✷\\n✷\\n✷\\n✷\\n✷\\n. . .\\n(z1, 1)\\np(n)\\np(n)\\nFigure 6.5: The p(n) × p(n) frame for the domino game.\\nTherefore, we have A ≤P Domino. Hence, the language Domino is NP-\\ncomplete.\\nAn example of a domino game\\nWe have deﬁned the domino game corresponding to a Turing machine that\\nsolves a decision problem. Of course, we can also do this for Turing machines\\nthat compute functions. In this section, we will exactly do this for a Turing\\nmachine that computes the successor function x →x + 1.\\nWe will design a Turing machine with one tape, that gets as input the\\nbinary representation of a natural number x, and that computes the binary\\nrepresentation of x + 1.\\nStart of the computation: The tape contains a 0 followed by the binary\\nrepresentation of the integer x ∈N0. The tape head is on the leftmost bit\\n(which is 0), and the Turing machine is in the start state z0. Here is an\\nexample, where x = 431:\\n6.5.\\nNP-complete languages\\n227\\n0 1 1 0 1 0 1 1 1 1 2\\n6\\nEnd of the computation: The tape contains the binary representation of\\nthe number x + 1. The tape head is on the rightmost 1, and the Turing\\nmachine is in the ﬁnal state z1. For our example, the tape looks as follows:\\n0 1 1 0 1 1 0 0 0 0 2\\n6\\nOur Turing machine will use the following states:\\nz0 :\\nstart state; tape head moves to the right\\nz1 :\\nﬁnal state\\nz2 :\\ntape head moves to the left; on its way to the left, it has not read 0\\nThe Turing machine has the following instructions:\\nz00 →z00R\\nz21 →z20L\\nz01 →z01R\\nz20 →z11N\\nz02 →z22L\\nIn Figure 6.6, you can see the sequence of states and tape contents of this\\nTuring machine on input x = 11.\\nWe now construct the domino game that corresponds to the computation\\nof this Turing machine on input x = 11. Following the general construction\\nin Section 6.5.3, we obtain the following tile types:\\n1. The three symbols of the alphabet yield three tile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n2\\n2\\n2. The ﬁve instructions of the Turing machine yield ﬁve tile types:\\n228\\nChapter 6.\\nComplexity Theory\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n0\\n(z0, 1)\\n0\\n1\\n1\\n2\\n0\\n1\\n(z0, 0)\\n1\\n1\\n2\\n0\\n1\\n0\\n(z0, 1)\\n1\\n2\\n0\\n1\\n0\\n1\\n(z0, 1)\\n2\\n0\\n1\\n0\\n1\\n1\\n(z0, 2)\\n0\\n1\\n0\\n1\\n(z2, 1)\\n2\\n0\\n1\\n0\\n(z2, 1)\\n0\\n2\\n0\\n1\\n(z2, 0)\\n0\\n0\\n2\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\nFigure 6.6: The computation of the Turing machine on input x = 11. The\\npair (state,symbol) indicates the position of the tape head.\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n3. The states z0 and z2, and the three symbols of the alphabet yield twelve\\ntile types:\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz0\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\n#\\nz2\\n(z2, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@@\\nz2\\n#\\n(z2, 2)\\n2\\nThe computation of the Turing machine on input x = 11 consists of nine\\ncomputation steps. During this computation, the tape head visits exactly\\nsix cells. Therefore, the frame for the domino game has nine rows and six\\ncolumns.\\nThis frame is given in Figure 6.7.\\nIn Figure 6.8, you ﬁnd the\\nsolution of the domino game.\\nObserve that this solution is nothing but\\nan equivalent way of writing the computation of Figure 6.6.\\nHence, the\\ncomputation of the Turing machine corresponds to a solution of the domino\\ngame; in fact, the converse also holds.\\n6.5.\\nNP-complete languages\\n229\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\nFigure 6.7: The frame for the domino game for input x = 11.\\n230\\nChapter 6.\\nComplexity Theory\\n0\\n1\\n(z1, 1)\\n0\\n0\\n2\\n(z0, 0)\\n1\\n0\\n1\\n1\\n2\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n#\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n(z1, 1)\\n(z2, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n0\\n(z2, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz2\\n(z2, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz2\\n#\\n2\\n(z0, 2)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 2)\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n1\\n(z0, 1)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 0)\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\nz0\\n0\\n(z0, 0)\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\nz0\\n#\\n(z0, 1)\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n0\\n0\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n1\\n1\\n\\x00\\x00\\x00\\x00\\x00@\\n@\\n@\\n@\\n@\\n#\\n#\\n2\\n2\\nFigure 6.8: The solution for the domino game for input x = 11.\\n6.5.\\nNP-complete languages\\n231\\n6.5.4\\nExamples of NP-complete languages\\nIn Section 6.5.3, we have shown that Domino is NP-complete. Using this\\nresult, we will apply Theorem 6.5.11 to prove the NP-completeness of some\\nother languages.\\nSatisﬁability\\nWe consider Boolean formulas ϕ, in the variables x1, x2, . . . , xm, having the\\nform\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.9)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nEach ℓi\\nj is either a variable or the negation of a variable. Such a formula ϕ\\nis said to be satisﬁable, if there exists a truth-value in {0, 1} for each of the\\nvariables x1, x2, . . . , xm, such that the entire formula ϕ is true. We deﬁne the\\nfollowing language:\\nSAT := {⟨ϕ⟩: ϕ is of the form (6.9) and is satisﬁable}.\\nWe will prove that SAT is NP-complete.\\nIt is clear that SAT ∈NP. If we can show that\\nDomino ≤P SAT,\\nthen it follows from Theorem 6.5.11 that SAT is NP-complete. (In Theo-\\nrem 6.5.11, take B := Domino and C := SAT.)\\nHence, we need a function f ∈FP, that maps input strings for Domino\\nto input strings for SAT, in such a way that for every domino game D, the\\nfollowing holds:\\ndomino game D is solvable ⇐⇒the formula encoded by the\\nstring f(⟨D⟩) is satisﬁable.\\n(6.10)\\nLet us consider an arbitrary domino game D. Let k be the number of\\ntile types, and let the frame have t rows and t columns. We denote the tile\\ntypes by T1, T2, . . . , Tk.\\n232\\nChapter 6.\\nComplexity Theory\\nWe map this domino game D to a Boolean formula ϕ, such that (6.10)\\nholds. The formula ϕ will have variables\\nxijℓ, 1 ≤i ≤t, 1 ≤j ≤t, 1 ≤ℓ≤k.\\nThese variables can be interpretated as follows:\\nxijℓ= 1 ⇐⇒there is a tile of type Tℓat position (i, j) of the frame.\\nWe deﬁne:\\n• For all i and j with 1 ≤i ≤t and 1 ≤j ≤t:\\nC1\\nij := xij1 ∨xij2 ∨. . . ∨xijk.\\nThis formula expresses the condition that there is at least one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j ≤t, and 1 ≤ℓ< ℓ′ ≤k:\\nC2\\nijℓℓ′ := ¬xijℓ∨¬xijℓ′.\\nThis formula expresses the condition that there is at most one tile at\\nposition (i, j).\\n• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j < t, 1 ≤ℓ≤k and 1 ≤ℓ′ ≤k,\\nsuch that i < t and the right symbol on a tile of type Tℓis not equal\\nto the left symbol on a tile of type Tℓ′:\\nC3\\nijℓℓ′ := ¬xijℓ∨¬xi,j+1,ℓ′.\\nThis formula expresses the condition that neighboring tiles in the same\\nrow “ﬁt” together. There are symmetric formulas for neighboring tiles\\nin the same column.\\n• For all j and ℓwith 1 ≤j ≤t and 1 ≤ℓ≤k, such that the top symbol\\non a tile of type Tℓis not equal to the symbol at position j of the upper\\nboundary of the frame:\\nC4\\njℓ:= ¬x1jℓ.\\nThis formula expresses the condition that tiles that touch the upper\\nboundary of the frame “ﬁt” there. There are symmetric formulas for\\nthe lower, left, and right boundaries of the frame.\\n6.5.\\nNP-complete languages\\n233\\nThe formula ϕ is the conjunction of all these formulas C1\\nij, C2\\nijℓℓ′, C3\\nijℓℓ′, and\\nC4\\njℓ. The complete formula ϕ consists of\\nO(t2k + t2k2 + t2k2 + tk) = O(t2k2)\\nterms, i.e., its length is polynomial in the length of the domino game. This\\nimplies that ϕ can be constructed in polynomial time. Hence, the function\\nf that maps the domino game D to the Boolean formula ϕ, is in the class\\nFP. It is not diﬃcult to see that (6.10) holds for this function f. Therefore,\\nwe have proved the following result.\\nTheorem 6.5.13 The language SAT is NP-complete.\\nIn Section 6.5.1, we have deﬁned the language 3SAT.\\nTheorem 6.5.14 The language 3SAT is NP-complete.\\nProof. It is clear that 3SAT ∈NP. If we can show that\\nSAT ≤P 3SAT,\\nthen the claim follows from Theorem 6.5.11. Let\\nϕ = C1 ∧C2 ∧. . . ∧Ck\\nbe an input for SAT, in the variables x1, x2, . . . , xm. We map ϕ, in polynomial\\ntime, to an input ϕ′ for 3SAT, such that\\nϕ is satisﬁable ⇐⇒ϕ′ is satisﬁable.\\n(6.11)\\nFor each i with 1 ≤i ≤k, we do the following. Consider\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\n• If ki = 1, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n1 ∨ℓi\\n1.\\n• If ki = 2, then we deﬁne\\nC′\\ni := ℓi\\n1 ∨ℓi\\n2 ∨ℓi\\n2.\\n234\\nChapter 6.\\nComplexity Theory\\n• If ki = 3, then we deﬁne\\nC′\\ni := Ci.\\n• If ki ≥4, then we deﬁne\\nC′\\ni\\n:=\\n(ℓi\\n1 ∨ℓi\\n2 ∨zi\\n1) ∧(¬zi\\n1 ∨ℓi\\n3 ∨zi\\n2) ∧(¬zi\\n2 ∨ℓi\\n4 ∨zi\\n3) ∧. . .\\n∧(¬zi\\nki−3 ∨ℓi\\nki−1 ∨ℓi\\nki),\\nwhere zi\\n1, . . . , zi\\nki−3 are new variables.\\nLet\\nϕ′ := C′\\n1 ∧C′\\n2 ∧. . . ∧C′\\nk.\\nThen ϕ′ is an input for 3SAT, and (6.11) holds.\\nTheorems 6.5.6, 6.5.8, 6.5.11, and 6.5.14 imply:\\nTheorem 6.5.15 The language Clique is NP-complete.\\nThe traveling salesperson problem\\nWe are given two positive integers k and m, a set of m cities, and an integer\\nm × m matrix M, where\\nM(i, j) = the cost of driving from city i to city j,\\nfor all i, j ∈{1, 2, . . . , m}. We want to decide whether or not there is a tour\\nthrough all cities whose total cost is less than or equal to k. This problem is\\nNP-complete.\\nBin packing\\nWe are given three positive integers m, k, and ℓ, a set of m objects having\\nvolumes a1, a2, . . . , am, and k bins. Each bin has volume ℓ. We want to\\ndecide whether or not all objects ﬁt within these bins. This problem is NP-\\ncomplete.\\nHere is another interpretation of this problem: We are given m jobs that\\nneed time a1, a2, . . . , am to complete. We are also given k processors, and an\\ninteger ℓ. We want to decide whether or not it is possible to divide the jobs\\nover the k processors, such that no processor needs more than ℓtime.\\nExercises\\n235\\nTime tables\\nWe are given a set of courses, class rooms, and professors.\\nWe want to\\ndecide whether or not there exists a time table such that all courses are\\nbeing taught, no two courses are taught at the same time in the same class\\nroom, no professor teaches two courses at the same time, and conditions such\\nas “Prof. L. Azy does not teach before 1pm” are satisﬁed. This problem is\\nNP-complete.\\nMotion planning\\nWe are given two positive integers k and ℓ, a set of k polyhedra, and two\\npoints s and t in Q3. We want to decide whether or not there exists a path\\nbetween s and t, that does not intersect any of the polyhedra, and whose\\nlength is less than or equal to ℓ. This problem is NP-complete.\\nMap labeling\\nWe are given a map with m cities, where each city is represented by a point.\\nFor each city, we are given a rectangle that is large enough to contain the\\nname of the city. We want to decide whether or not these rectangles can be\\nplaced on the map, such that\\n• no two rectangles overlap,\\n• For each i with 1 ≤i ≤m, the point that represents city i is a corner\\nof its rectangle.\\nThis problem is NP-complete.\\nThis list of NP-complete problems can be extended almost arbitrarily:\\nFor thousands of problems, it is known that they are NP-complete. For all\\nof these, it is not known, whether or not they can be solved eﬃciently (i.e.,\\nin polynomial time). Collections of NP-complete problems can be found in\\nthe book\\n• M.R. Garey and D.S. Johnson. Computers and Intractability: A Guide\\nto the Theory of NP-Completeness. W.H. Freeman, New York, 1979,\\nand on the web page\\nhttp://www.nada.kth.se/~viggo/wwwcompendium/\\n236\\nChapter 6.\\nComplexity Theory\\nExercises\\n6.1 Prove that the function F : N →N, deﬁned by F(x) := 2x, is not in FP.\\n6.2 Prove Theorem 6.5.3.\\n6.3 Prove that the language Clique is in the class NP.\\n6.4 Prove that the language 3SAT is in the class NP.\\n6.5 We deﬁne the following languages:\\n• Sum of subset:\\nSOS := {⟨a1, a2, . . . , am, b⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai = b}.\\n• Set partition:\\nSP := {⟨a1, a2, . . . , am⟩: ∃I ⊆{1, 2, . . . , m},\\nX\\ni∈I\\nai =\\nX\\ni̸∈I\\nai}.\\n• Bin packing: BP is the set of all strings ⟨s1, s2, . . . , sm, B⟩for which\\n1. 0 < si < 1, for all i,\\n2. B ∈N,\\n3. the numbers s1, s2, . . . , sm ﬁt into B bins, where each bin has size\\none, i.e., there exists a partition of {1, 2, . . . , m} into subsets Ik,\\n1 ≤k ≤B, such that P\\ni∈Ik si ≤1 for all k, 1 ≤k ≤B.\\nFor example, ⟨1/6, 1/2, 1/5, 1/9, 3/5, 1/5, 1/2, 11/18, 3⟩∈BP, because\\nthe eight fractions ﬁt into three bins:\\n1/6 + 1/9 + 11/18 ≤1, 1/2 + 1/2 = 1, and 1/5 + 3/5 + 1/5 = 1.\\n1. Prove that SOS ≤P SP.\\n2. Prove that the language SOS is NP-complete. You may use the fact\\nthat the language SP is NP-complete.\\nExercises\\n237\\n3. Prove that the language BP is NP-complete. Again, you may use the\\nfact that the language SP is NP-complete.\\n6.6 Prove that 3Color ≤P 3SAT.\\nHint: For each vertex i, and for each of the three colors k, introduce a\\nBoolean variable xik.\\n6.7 The (0, 1)-integer programming language IP is deﬁned as follows:\\nIP := {⟨A, c⟩:\\nA is an integer m × n matrix for some m, n ∈N,\\nc is an integer vector of length m, and\\n∃x ∈{0, 1}n such that Ax ≤c (componentwise) }.\\nProve that the language IP is NP-complete. You may use the fact that\\nthe language SOS is NP-complete.\\n6.8 Let ϕ be a Boolean formula in the variables x1, x2, . . . , xm.\\nWe say that ϕ is in disjunctive normal form (DNF) if it is of the form\\nϕ = C1 ∨C2 ∨. . . ∨Ck,\\n(6.12)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∧ℓi\\n2 ∧. . . ∧ℓi\\nki.\\nEach ℓi\\nj is a literal, which is either a variable or the negation of a variable.\\nWe say that ϕ is in conjunctive normal form (CNF) if it is of the form\\nϕ = C1 ∧C2 ∧. . . ∧Ck,\\n(6.13)\\nwhere each Ci, 1 ≤i ≤k, is of the following form:\\nCi = ℓi\\n1 ∨ℓi\\n2 ∨. . . ∨ℓi\\nki.\\nAgain, each ℓi\\nj is a literal.\\nWe deﬁne the following two languages:\\nDNFSAT := {⟨ϕ⟩: ϕ is in DNF-form and is satisﬁable},\\nand\\nCNFSAT := {⟨ϕ⟩: ϕ is in CNF-form and is satisﬁable}.\\n238\\nChapter 6.\\nComplexity Theory\\n1. Prove that the language DNFSAT is in P.\\n2. What is wrong with the following argument: Since we can rewrite\\nany Boolean formula in DNF-form, we have CNFSAT ≤P DNFSAT.\\nHence, since CNFSAT is NP-complete and since DNFSAT ∈P, we\\nhave P = NP.\\n3. Prove directly that for every language A in P, A ≤P CNFSAT. “Di-\\nrectly” means that you should not use the fact that CNFSAT is NP-\\ncomplete.\\n6.9 1 Prove that the polynomial upper bound on the length of the string y\\nin the deﬁnition of NP is necessary, in the sense that if it is left out, then\\nany enumerable language would satisfy the condition.\\nMore precisely, we say that the language A belongs to the class E, if there\\nexists a language B ∈P, such that for every string w,\\nw ∈A ⇐⇒∃y : ⟨w, y⟩∈B.\\nProve that E is equal to the class of all enumerable languages.\\n1Thanks to Antoine Vigneron for poining out an error in a previous version of this\\nexercise.\\nChapter 7\\nSummary\\nWe have seen several diﬀerent models for “processing” languages, i.e., pro-\\ncessing sets of strings over some ﬁnite alphabet. For each of these models,\\nwe have asked the question which types of languages can be processed, and\\nwhich types of languages cannot be processed. In this ﬁnal chapter, we give\\na brief summary of these results.\\nRegular languages:\\nThis class of languages was considered in Chapter 2.\\nThe following statements are equivalent:\\n1. The language A is regular, i.e., there exists a deterministic ﬁnite au-\\ntomaton that accepts A.\\n2. There exists a nondeterministic ﬁnite automaton that accepts A.\\n3. There exists a regular expression that describes A.\\nThis claim was proved by the following conversions:\\n1. Every nondeterministic ﬁnite automaton can be converted to an equiv-\\nalent deterministic ﬁnite automaton.\\n2. Every deterministic ﬁnite automaton can be converted to an equivalent\\nregular expression.\\n3. Every regular expression can be converted to an equivalent nondeter-\\nministic ﬁnite automaton.\\n240\\nChapter 7.\\nSummary\\nWe have seen that the class of regular languages is closed under the regular\\noperations: If A and B are regular languages, then\\n1. A ∪B is regular,\\n2. AB is regular,\\n3. A∗is regular,\\n4. A is regular, and\\n5. A ∩B is regular.\\nFinally, the pumping lemma for regular languages gives a property that\\nevery regular language possesses. We have used this to prove that languages\\nsuch as {anbn : n ≥0} are not regular.\\nContext-free languages:\\nThis class of languages was considered in Chap-\\nter 3. We have seen that every regular language is context-free. Moreover,\\nthere exist languages, for example {anbn : n ≥0}, that are context-free, but\\nnot regular. The following statements are equivalent:\\n1. The language A is context-free, i.e., there exists a context-free grammar\\nwhose language is A.\\n2. There exists a context-free grammar in Chomsky normal form whose\\nlanguage is A.\\n3. There exists a nondeterministic pushdown automaton that accepts A.\\nThis claim was proved by the following conversions:\\n1. Every context-free grammar can be converted to an equivalent context-\\nfree grammar in Chomsky normal form.\\n2. Every context-free grammar in Chomsky normal form can be converted\\nto an equivalent nondeterministic pushdown automaton.\\n3. Every nondeterministic pushdown automaton can be converted to an\\nequivalent context-free grammar. (This conversion was not covered in\\nthis book.)\\nChapter 7.\\nSummary\\n241\\nNondeterministic pushdown automata are more powerful than determin-\\nistic pushdown automata: There exists a nondeterministic pushdown au-\\ntomaton that accepts the language\\n{vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|},\\nbut there is no deterministic pushdown automaton that accepts this language.\\n(We did not prove this in this book.)\\nWe have seen that the class of context-free languages is closed under\\nthe union, concatenation, and star operations: If A and B are context-free\\nlanguages, then\\n1. A ∪B is context-free,\\n2. AB is context-free, and\\n3. A∗is context-free.\\nHowever,\\n1. the intersection of two context-free languages is not necessarily context-\\nfree, and\\n2. the complement of a context-free language is not necessarily context-\\nfree.\\nFinally, the pumping lemma for context-free languages gives a property\\nthat every context-free language possesses. We have used this to prove that\\nlanguages such as {anbncn : n ≥0} are not context-free.\\nThe Church-Turing Thesis:\\nIn Chapter 4, we considered “reasonable”\\ncomputational devices that model real computers. Examples of such devices\\nare Turing machines (with one or more tapes) and Java programs. It turns\\nout that all known “reasonable” devices are equivalent, i.e., can be converted\\nto each other. This led to the Church-Turing Thesis:\\n• Every computational process that is intuitively considered to be an\\nalgorithm can be converted to a Turing machine.\\n242\\nChapter 7.\\nSummary\\nDecidable and enumerable languages:\\nThese classes of languages were\\nconsidered in Chapter 5. They are deﬁned based on “reasonable” computa-\\ntional devices, such as Turing machines and Java programs. We have seen\\nthat\\n1. every context-free language is decidable, and\\n2. every decidable language is enumerable.\\nMoreover,\\n1. there exist languages, for example {anbncn : n ≥0}, that are decidable,\\nbut not context-free,\\n2. there exist languages, for example the Halting Problem, that are enu-\\nmerable, but not decidable,\\n3. there exist languages, for example the complement of the Halting Prob-\\nlem, that are not enumerable.\\nIn fact,\\n1. the class of all languages is not countable, whereas\\n2. the class of all enumerable languages is countable.\\nThe following statements are equivalent:\\n1. The language A is decidable.\\n2. Both A and its complement A are enumerable.\\nComplexity classes:\\nThese classes of languages were considered in Chap-\\nter 6.\\n1. The class P consists of all languages that can be decided in polynomial\\ntime by a deterministic Turing machine.\\n2. The class NP consists of all languages that can be decided in poly-\\nnomial time by a nondeterministic Turing machine. Equivalently, a\\nlanguage A is in the class NP, if for every string w ∈A, there exists a\\n“solution” s, such that (i) the length of s is polynomial in the length\\nof w, and (ii) the correctness of s can be veriﬁed in polynomial time.\\nChapter 7.\\nSummary\\n243\\nThe following properties hold:\\n1. Every context-free language is in P. (We did not prove this).\\n2. Every language in P is also in NP.\\n3. It is not known if there exist languages that are in NP, but not in P.\\n4. Every language in NP is decidable.\\nWe have introduced reductions to deﬁne the notion of a language B to be\\n“at least as hard” as a language A. A language B is called NP-complete, if\\n1. B belongs to the class NP, and\\n2. B is at least as hard as every language in the class NP.\\nWe have seen that NP-complete exist.\\nThe ﬁgure below summarizes the relationships among the various classes\\nof languages.\\n244\\nChapter 7.\\nSummary\\nregular\\ncontext-free\\nP\\nNP\\ndecidable\\nenumerable\\nall languages\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "# 1. MODELLERİ YÜKLEME\n",
        "# Metin için SentenceTransformer\n",
        "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Görsel için CLIP\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Görselleri işlemek için dönüştürücü\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    return inputs['pixel_values']\n",
        "\n",
        "# 2. EMBEDDING FONKSİYONLARI\n",
        "# Metin embedding'i\n",
        "def get_text_embedding(text):\n",
        "    return text_model.encode(text)\n",
        "\n",
        "# Görsel embedding'i\n",
        "def get_image_embedding(image_path):\n",
        "    image_tensor = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(image_tensor)\n",
        "    return image_features.squeeze().numpy()\n",
        "\n",
        "# Manually reduce the image embedding size to match the text embedding size\n",
        "def resize_image_embedding(image_embedding, target_dim=384):\n",
        "    return image_embedding[:target_dim]\n",
        "\n",
        "# Kombine embedding\n",
        "def combine_embeddings(text_embedding, image_embedding, alpha=0.5, beta=0.5):\n",
        "    image_embedding = resize_image_embedding(image_embedding)\n",
        "    return alpha * text_embedding + beta * image_embedding\n",
        "\n",
        "# 3. FAISS VE METADATA\n",
        "embedding_dim = 384  # Updated dimension after resizing\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "metadata_store = {}\n",
        "\n",
        "# 4. VERİ EKLEME\n",
        "# Görsel ve metin eklemek için fonksiyon\n",
        "def add_to_database(id, text, image_path):\n",
        "    text_embedding = get_text_embedding(text)\n",
        "    image_embedding = get_image_embedding(image_path)\n",
        "    combined_embedding = combine_embeddings(text_embedding, image_embedding)\n",
        "\n",
        "    # Embedding'i FAISS'e ekle\n",
        "    index.add(np.array([combined_embedding], dtype='float32'))\n",
        "\n",
        "    # Metadata'yı sakla\n",
        "    metadata_store[id] = {\n",
        "        \"text\": text,\n",
        "        \"image_path\": image_path\n",
        "    }\n",
        "\n",
        "# Örnek veri ekleme\n",
        "add_to_database(\"dfa-example\", \"This is a DFA diagram.\", \"/content/page_30_full_page_shape_1.png\")\n",
        "add_to_database(\"nfa-example\", \"This is an NFA diagram.\", \"/content/page_31_full_page_shape_1.png\")\n",
        "\n",
        "# 5. SORGU YAPMA VE CEVAP ÜRETME\n",
        "# Sorgu fonksiyonu\n",
        "def query_database(user_query, top_k=1):\n",
        "    query_embedding = get_text_embedding(user_query)\n",
        "    distances, indices = index.search(np.array([query_embedding], dtype='float32'), k=top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        if idx != -1:\n",
        "            id = list(metadata_store.keys())[idx]\n",
        "            metadata = metadata_store[id]\n",
        "            results.append(metadata)\n",
        "    return results\n",
        "\n",
        "# Sorguya göre cevap oluşturma\n",
        "def generate_response(user_query):\n",
        "    results = query_database(user_query)\n",
        "    if not results:\n",
        "        return \"Sorry, I couldn't find relevant information.\"\n",
        "\n",
        "    # İlk sonuç için cevap üretimi\n",
        "    metadata = results[0]\n",
        "    text = metadata['text']\n",
        "    image_path = metadata['image_path']\n",
        "    response = f\"Relevant text: {text}\\nRelated image path: {image_path}\"\n",
        "    return response\n",
        "\n",
        "# Kullanıcı sorgusu\n",
        "user_query = \"What is a DFA?\"\n",
        "response = generate_response(user_query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy3gtEnenXP-",
        "outputId": "aae89d57-5838-4101-c50f-a5e42e38886d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant text: This is a DFA diagram.\n",
            "Related image path: /content/page_30_full_page_shape_1.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor, pipeline\n",
        "\n",
        "# 1. MODELLERİ YÜKLEME\n",
        "# Metin için SentenceTransformer\n",
        "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Görsel için CLIP\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Özetleme için BART modelini yükleme\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Görselleri işlemek için dönüştürücü\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    return inputs['pixel_values']\n",
        "\n",
        "# 2. EMBEDDING FONKSİYONLARI\n",
        "# Metin embedding'i\n",
        "def get_text_embedding(text):\n",
        "    return text_model.encode(text)\n",
        "\n",
        "# Görsel embedding'i\n",
        "def get_image_embedding(image_path):\n",
        "    image_tensor = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(image_tensor)\n",
        "    return image_features.squeeze().numpy()\n",
        "\n",
        "# Manually reduce the image embedding size to match the text embedding size\n",
        "def resize_image_embedding(image_embedding, target_dim=384):\n",
        "    return image_embedding[:target_dim]\n",
        "\n",
        "# Kombine embedding\n",
        "def combine_embeddings(text_embedding, image_embedding, alpha=0.5, beta=0.5):\n",
        "    image_embedding = resize_image_embedding(image_embedding)\n",
        "    return alpha * text_embedding + beta * image_embedding\n",
        "\n",
        "# 3. FAISS VE METADATA\n",
        "embedding_dim = 384  # Updated dimension after resizing\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "metadata_store = {}\n",
        "\n",
        "# 4. METİN ÖZETLEME\n",
        "def summarize_text(text):\n",
        "    # Metni özetleme\n",
        "    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# 5. VERİ EKLEME\n",
        "# Görsel ve metin eklemek için fonksiyon\n",
        "def add_to_database(id, file_path, image_path):\n",
        "    # Uzun metni özetle\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    summarized_text = summarize_text(text)\n",
        "    print(f\"ozet: {summarize_text}\")\n",
        "    # Metin embedding'ini al\n",
        "    text_embedding = get_text_embedding(summarized_text)\n",
        "\n",
        "    # Görsel embedding'ini al\n",
        "    image_embedding = get_image_embedding(image_path)\n",
        "\n",
        "    # Kombine embedding'i oluştur\n",
        "    combined_embedding = combine_embeddings(text_embedding, image_embedding)\n",
        "\n",
        "    # Embedding'i FAISS'e ekle\n",
        "    index.add(np.array([combined_embedding], dtype='float32'))\n",
        "\n",
        "    # Metadata'yı sakla\n",
        "    metadata_store[id] = {\n",
        "        \"text\": summarized_text,\n",
        "        \"image_path\": image_path\n",
        "    }\n",
        "\n",
        "# Örnek veri ekleme\n",
        "# Add database entries for the numbers with the updated text and image file paths\n",
        "add_to_database(\"example-35\", \"/content/text/35_full_page.txt\", \"/content/img/page_35_full_page_shape_1.png\")\n",
        "add_to_database(\"example-37\", \"/content/text/37_full_page.txt\", \"/content/img/page_37_full_page_shape_1.png\")\n",
        "add_to_database(\"example-38\", \"/content/text/38_full_page.txt\", \"/content/img/page_38_full_page_shape_1.png\")\n",
        "add_to_database(\"example-43\", \"/content/text/43_full_page.txt\", \"/content/img/page_43_full_page_shape_1.png\")\n",
        "add_to_database(\"example-45\", \"/content/text/45_full_page.txt\", \"/content/img/page_45_full_page_shape_1.png\")\n",
        "add_to_database(\"example-46\", \"/content/text/46_full_page.txt\", \"/content/img/page_46_full_page_shape_1.png\")\n",
        "add_to_database(\"example-53\", \"/content/text/53_full_page.txt\", \"/content/img/page_53_full_page_shape_1.png\")\n",
        "add_to_database(\"example-55\", \"/content/text/55_full_page.txt\", \"/content/img/page_55_full_page_shape_1.png\")\n",
        "add_to_database(\"example-57\", \"/content/text/57_full_page.txt\", \"/content/img/page_57_full_page_shape_1.png\")\n",
        "add_to_database(\"example-59\", \"/content/text/59_full_page.txt\", \"/content/img/page_59_full_page_shape_1.png\")\n",
        "add_to_database(\"example-66\", \"/content/text/66_full_page.txt\", \"/content/img/page_66_full_page_shape_1.png\")\n",
        "add_to_database(\"example-67\", \"/content/text/67_full_page.txt\", \"/content/img/page_67_full_page_shape_1.png\")\n",
        "add_to_database(\"example-68\", \"/content/text/68_full_page.txt\", \"/content/img/page_68_full_page_shape_1.png\")\n",
        "add_to_database(\"example-69\", \"/content/text/69_full_page.txt\", \"/content/img/page_69_full_page_shape_1.png\")\n",
        "'''\n",
        "add_to_database(\"example-78\", \"/content/text/78_full_page.txt\", \"/content/img/page_78_full_page_shape_1.png\")\n",
        "add_to_database(\"example-82\", \"/content/text/82_full_page.txt\", \"/content/img/page_82_full_page_shape_1.png\")\n",
        "add_to_database(\"example-91\", \"/content/text/91_full_page.txt\", \"/content/img/page_91_full_page_shape_1.png\")\n",
        "add_to_database(\"example-94\", \"/content/text/94_full_page.txt\", \"/content/img/page_94_full_page_shape_1.png\")\n",
        "add_to_database(\"example-110\", \"/content/text/110_full_page.txt\", \"/content/img/page_110_full_page_shape_1.png\")\n",
        "'''\n",
        "# 6. SORGU YAPMA VE CEVAP ÜRETME\n",
        "# Sorgu fonksiyonu\n",
        "def query_database(user_query, top_k=1):\n",
        "    query_embedding = get_text_embedding(user_query)\n",
        "    distances, indices = index.search(np.array([query_embedding], dtype='float32'), k=top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        if idx != -1:\n",
        "            id = list(metadata_store.keys())[idx]\n",
        "            metadata = metadata_store[id]\n",
        "            results.append(metadata)\n",
        "    return results\n",
        "\n",
        "# Sorguya göre cevap oluşturma\n",
        "def generate_response(user_query):\n",
        "    results = query_database(user_query)\n",
        "    if not results:\n",
        "        return \"Sorry, I couldn't find relevant information.\"\n",
        "\n",
        "    # İlk sonuç için cevap üretimi\n",
        "    metadata = results[0]\n",
        "    text = metadata['text']\n",
        "    image_path = metadata['image_path']\n",
        "    response = f\"Relevant text: {text}\\nRelated image path: {image_path}\"\n",
        "    return response\n",
        "\n",
        "# Kullanıcı sorgusu\n",
        "user_query = \"What is a DFA?\"\n",
        "response = generate_response(user_query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "oE7v6RGU7Na3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a15f0f53-d827-481f-ca2a-91e66298fc32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "ozet: <function summarize_text at 0x7be4b6638af0>\n",
            "Relevant text: The state diagram of N is as follows. We will show how to convert this NFA N to a DFA M that accepts the same language. Following the proof of Theorem 2.5.1, M is speciﬁed by M = (Q′, Σ, δ′, q′, F ′)\n",
            "Related image path: /content/img/page_53_full_page_shape_1.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "# 1. T5 Modeli ve Tokenizer'ı Yükleme\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# 2. CLIP Modeli ve Processor'ı Yükleme (Görsel Özellik Çıkartmak için)\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# 3. Görseli İşleme (CLIP ile)\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    return inputs['pixel_values']\n",
        "\n",
        "def get_image_features(image_path):\n",
        "    image_tensor = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(image_tensor)\n",
        "    return image_features.squeeze().numpy()\n",
        "\n",
        "# 4. T5 Modeline Metinle Açıklama Üretme\n",
        "def generate_text_description(image_path, text_input):\n",
        "    image_features = get_image_features(image_path)\n",
        "\n",
        "    # Görsel ve metni birleştirerek açıklama oluşturmak\n",
        "    input_text = f\"Describe the following image based on this information: {text_input}\"\n",
        "\n",
        "    # T5 modeline metin tokenizasyonu\n",
        "    input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # T5 ile açıklama üretme\n",
        "    output = t5_model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Sonucu çözme ve yazdırma\n",
        "    description = t5_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return description\n",
        "\n",
        "# 5. Örnek Kullanım\n",
        "image_path = \"/content/download.png\"  # Görsel dosyasının yolu\n",
        "text_input = \"This is an image of a DFA (Deterministic Finite Automaton).\"  # Metin açıklaması\n",
        "\n",
        "description = generate_text_description(image_path, response)\n",
        "print(description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcbzAM-eGCeC",
        "outputId": "a6d04e31-7998-4690-94e2-fe28db798fc8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor, pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# 1. MODELLERİ YÜKLEME\n",
        "# Metin için SentenceTransformer\n",
        "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Görsel için CLIP\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Özetleme için BART modelini yükleme\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# T5 Modeli ve Tokenizer'ı Yükleme\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# 2. GÖRSEL İŞLEME\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    return inputs['pixel_values']\n",
        "\n",
        "# 3. EMBEDDING FONKSİYONLARI\n",
        "# Metin embedding'i\n",
        "def get_text_embedding(text):\n",
        "    return text_model.encode(text)\n",
        "\n",
        "# Görsel embedding'i\n",
        "def get_image_embedding(image_path):\n",
        "    image_tensor = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(image_tensor)\n",
        "    return image_features.squeeze().numpy()\n",
        "\n",
        "# Manually reduce the image embedding size to match the text embedding size\n",
        "def resize_image_embedding(image_embedding, target_dim=384):\n",
        "    return image_embedding[:target_dim]\n",
        "\n",
        "# Kombine embedding\n",
        "def combine_embeddings(text_embedding, image_embedding, alpha=0.5, beta=0.5):\n",
        "    image_embedding = resize_image_embedding(image_embedding)\n",
        "    return alpha * text_embedding + beta * image_embedding\n",
        "\n",
        "# 4. FAISS VE METADATA\n",
        "embedding_dim = 384  # Updated dimension after resizing\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "metadata_store = {}\n",
        "\n",
        "# 5. METİN ÖZETLEME\n",
        "def summarize_text(text):\n",
        "    # Metni özetleme\n",
        "    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "# 6. VERİ EKLEME\n",
        "def add_to_database(id, file_path, image_path):\n",
        "    # Uzun metni özetle\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    summarized_text = summarize_text(text)\n",
        "\n",
        "    # Metin embedding'ini al\n",
        "    text_embedding = get_text_embedding(summarized_text)\n",
        "\n",
        "    # Görsel embedding'ini al\n",
        "    image_embedding = get_image_embedding(image_path)\n",
        "\n",
        "    # Kombine embedding'i oluştur\n",
        "    combined_embedding = combine_embeddings(text_embedding, image_embedding)\n",
        "\n",
        "    # Embedding'i FAISS'e ekle\n",
        "    index.add(np.array([combined_embedding], dtype='float32'))\n",
        "\n",
        "    # Metadata'yı sakla\n",
        "    metadata_store[id] = {\n",
        "        \"text\": summarized_text,\n",
        "        \"image_path\": image_path\n",
        "    }\n",
        "\n",
        "# 7. SORGU YAPMA VE CEVAP ÜRETME\n",
        "def query_database(user_query, top_k=1):\n",
        "    query_embedding = get_text_embedding(user_query)\n",
        "    distances, indices = index.search(np.array([query_embedding], dtype='float32'), k=top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        if idx != -1:\n",
        "            id = list(metadata_store.keys())[idx]\n",
        "            metadata = metadata_store[id]\n",
        "            results.append(metadata)\n",
        "    return results\n",
        "\n",
        "# Kullanıcı sorgusuna dayalı açıklama üretme\n",
        "def generate_text_description(image_path, text_input):\n",
        "    image_features = get_image_embedding(image_path)\n",
        "    input_text = f\"Describe the following image based on this information: {text_input}\"\n",
        "\n",
        "    # T5 modeline metin tokenizasyonu\n",
        "    input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # T5 ile açıklama üretme\n",
        "    output = t5_model.generate(input_ids, max_length=150, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Sonucu çözme ve yazdırma\n",
        "    description = t5_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return description\n",
        "\n",
        "# Kullanıcı sorgusu ve veri ekleme örneği\n",
        "user_query = \"What is a DFA?\"\n",
        "response = generate_response(user_query)\n",
        "print(response)\n",
        "\n",
        "# Örnek Veri Ekleyip Sorgulama\n",
        "add_to_database(\"example-35\", \"/content/text/35_full_page.txt\", \"/content/img/page_35_full_page_shape_1.png\")\n",
        "add_to_database(\"example-35\", \"/content/text/35_full_page.txt\", \"/content/img/page_35_full_page_shape_1.png\")\n",
        "add_to_database(\"example-37\", \"/content/text/37_full_page.txt\", \"/content/img/page_37_full_page_shape_1.png\")\n",
        "add_to_database(\"example-38\", \"/content/text/38_full_page.txt\", \"/content/img/page_38_full_page_shape_1.png\")\n",
        "add_to_database(\"example-43\", \"/content/text/43_full_page.txt\", \"/content/img/page_43_full_page_shape_1.png\")\n",
        "add_to_database(\"example-45\", \"/content/text/45_full_page.txt\", \"/content/img/page_45_full_page_shape_1.png\")\n",
        "add_to_database(\"example-46\", \"/content/text/46_full_page.txt\", \"/content/img/page_46_full_page_shape_1.png\")\n",
        "add_to_database(\"example-53\", \"/content/text/53_full_page.txt\", \"/content/img/page_53_full_page_shape_1.png\")\n",
        "add_to_database(\"example-55\", \"/content/text/55_full_page.txt\", \"/content/img/page_55_full_page_shape_1.png\")\n",
        "add_to_database(\"example-57\", \"/content/text/57_full_page.txt\", \"/content/img/page_57_full_page_shape_1.png\")\n",
        "description = generate_text_description(\"/content/img/page_35_full_page_shape_1.png\", response)\n",
        "print(description)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGjh_Zs8Hxgs",
        "outputId": "9bc1aa1b-54cd-451a-a45c-ddb2369f2e95"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I couldn't find relevant information.\n",
            "the following image based on this information: Sorry, I couldn't find relevant information.\n"
          ]
        }
      ]
    }
  ]
}